Introduction and motivation 1.1 A bit of methodological history

By their very nature, corpus-linguistic studies have always been based on frequencies of occurrence of linguistic elements as well as frequencies of co-occurrence of linguistic elements with either other linguistic elements or the co(n)textual characteristics of these linguistic expressions. Essentially, this means that all corpus-linguistic analyses, wherever they are located on a purely hypothetical scale from purely qualitative to purely quantitative work, have been at least implicitly based on statements such as:

• n x = 0 -that is, some x does not occur in a (part of a) corpus;

• n x > 0 -that is, some x occurs in a (part of a) corpus either as the frequently used category of a hapax (i.e., n x = 1) or more than once (i.e., n x > 1); • n x > n y or n x = n or n x < n y -that is, some x occurs more/less/ equally often than/as some y in a (part of a) corpus; • any of the above pertaining to particular co(n)texts such as the presence of some other linguistic expression z, some discoursecontextual feature z, . . .

Especially over the last ten years or so, corpus linguists have begun to take this (in some sense obvious) fact into consideration and have followed the general development in linguistics towards more and more sophisticated quantitative methods. One might have expected corpus linguistics to spearhead this development because, following the above logic, corpus linguistics is inherently quantitative. However, it is other disciplines that have been more explicitly driving this change in the methodological landscape. One of these 'other disciplines' is psycholinguistics, in which, for several decades, very many statistical analyses were analyses of variance (ANOVAs) of experiments involving fully factorial designs with repeated measures -that is to say, designs in which:

• the number of predictors was usually small (typically, two or three); • the predictors were all categorical by nature or they were continuous/numeric by nature but factorised/discretised by the researcher (e.g., when frequencies were dichotomised into low versus high); and, • the predictors were completely crossed so that each subject in the experiment saw each combination of levels of the predictors multiple times.

So, one might consider as prototypical an experiment with two predictors A (with levels A 1 and A 2 ) and B (with levels B 1 and B 2 ), which give rise to four kinds of experimental stimuli (A 1 B 1 , A 1 B 2 , A 2 B 1 and A 2 B 2 ), and every subject sees each of these combinations repeatedly and equally many times. This design would then be analysed with repeated-measures ANOVAs, one on averages for the different subjects, one on averages for the different experimental stimuli, so that the fact that all the data points for each subject/item are related to one another is accommodated.

Psycholinguistics is currently undergoing a major change in statistical methodology, one in which the above ANOVA paradigm (a not uncontroversial method in any case; see

Third and most importantly, they provide 'the usual' type of results for all predictors of interest (in the context of (G)LMM, these are referred to as 'fixed effects'), but they can also address the fact that data points are related because they were provided by the same subject or for the same item. This means that they can increase the precision of the regression results considerably by offering sophisticated ways to partial out, or accommodate, for instance, speaker-or item-specific effects (or other effects extraneous to the specific research question; see

Given these advantages, these kinds of models are now rapidly becoming mainstream in the psycholinguistic literature. Returning to corpus linguistics, it is instructive to compare its methodological challenges and trajectories to those of psycholinguistics. Two observations are particularly pertinent in this connection. On the one hand, corpus-linguistic observational data are typically much messier and unbalanced than psycholinguistic experimental data because many confounding and moderator variables that psycholinguists can control for (by randomising, blocking, etc.) plague corpus-linguistic analyses. On the other hand, while that means that corpuslinguistic statistics would stand to benefit immensely from more advanced statistics in general, and (G)LMM in particular, they are in fact still further behind on a cline of statistical sophistication. Indeed, many practitioners are only beginning to use monofactorial statistics, and fewer have yet made the move towards regression modelling -'linear modelling' for numeric dependent variables and 'generalised linear modelling' for ordinal and categorical (binary or polytomous) dependent variables -that would correspond to the previous ANOVA state-of-the-art in psycholinguistics, and only very few people are (already) using (G)LMM. This comparison can be shown as in Figure

This paper and its objectives

The general goal of this paper is to help to increase the number of corpus linguists who recognise the problems of the approaches on the left and, thus, decide to move towards the approaches on the right. This kind of recognition in psycholinguistics was hurried along considerably through several publications that not only summarised the advantages of (G)LMM -most notably

However, in spite of the indubitable quality of the above publications and the mark they have left on psycholinguistics, there are two reasons why I think corpus linguistics could still potentially benefit considerably from this paper. First, most of these papers are psycholinguistic in nature and involve experimental data of the kind prevalent in psycholinguistic analyses, which could, understandably, make it more difficult to a core corpus linguist to translate their messages into 'his or her language'.

Second, this also means that these papers -as well as, it seems, most other publications on (G)LMM -only focus on statistical designs in which the random effects are crossed (along the lines discussed above). However, a second domain in which (G)LMM is extremely useful is one that characterises the vast majority of corpus-linguistic studies and that is routinely ignored (and that pertains to most of my own earlier work, too): random effects can be not just crossed but also 'nested' across multiple levels (hence 'multi-level analysis'). For example, in most corpora, speakers/writers are nested into files, which are nested into registers, which are nested into modes. That is, corpora do not usually feature data from each speaker in each register in each mode -rather, a particular speaker was recorded, which was transcribed into one file and one file only, which represents one register and one register only, which represents one mode and one mode only. For instance, the sentence in Example 1 was produced by one speaker (labelled A) in one file (S1B-045) representing one sub-register ('spoken public dialog') representing one register ('spoken dialog') representing one mode ('dialog') in the British component of the International Corpus of English (ICE-GB):

(1) And all this happened really before you started picking up a camera and becoming a movie maker All of these characteristics from each of these different levels are hierarchically nested into each other -it is not that, later in this corpus, the same file name is also used for a file with private dialogue, unscripted monologue, printed creative writing, etc. This has an extremely important consequence: both in psycholinguistics and in corpus linguistics, the vast majority of datasets violate the assumptions of the simplest test that an analyst might normally think of first -namely, that the data points are independent of one another. In psycholinguistics, data points are dependent on one another because subjects provide multiple responses and items are tested more than once (and these effects are crossed). However, in corpus linguistics, this is even worse: as in psycholinguistics, speakers/writers often provide more than one data point, and we have more than one instance of, say, a constructional choice per verb, but we also have the hierarchically nested/multi-level structure that the corpus comes in: perhaps the speaker (or as a convenient heuristic, the file) is not even the right level of resolution for the current phenomenon -perhaps most of the variability must be explained by looking at registers? And perhaps the frequent distinction between modes -speaking versus writing -is actually not relevant for phenomenon X (for which we need to look at sub-registers)? In other words, at present, many corpus-linguistic studies make do with a simple chi-squared test or even a more advanced regression, when in fact the data analysed violate the assumption of the independence of data points that these tests routinely come with by ignoring:

(i) speaker-/writer-/file-specific effects and lexically specific effects; and, (ii) the multiple levels of structure that corpus data typically come in.

As a consequence, the results that the majority of corpus-linguistic studies report are likely to be very anti-conservative (i.e., too likely to return a significant result) and imprecise (because the results are tainted to an unknown degree by idiosyncrasies from which one can, and should not, generalise) and, just to acknowledge that quite openly, this also applies potentially to several earlier studies of mine. What is needed is an approach that combines the logic of mixed-effects modelling (to deal with the variation resulting from (i) and the logic of multi-level modelling (to deal with the variation resulting from (ii). Thus, the hopefully not too high-flying goal of this paper is to become for corpus linguistics what the abovequoted papers have become for psycholinguistics: a first go-to resource that explains to corpus linguists what (generalised) linear mixed-effects/multilevel modelling ((G)LMM/MLM) has to offer and that provides them with a concrete example and some instructions on how to perform such analyses. While, given the complexity of these modelling approaches, the coverage can not, of course, be exhaustive, I hope that the attractiveness of the methods to be exemplified below will help make them more common in the discipline and, thus, benefit us all in terms of the greater precision and reliability of the results.

Towards the above objectives, Section 2 provides a brief but necessary introduction to (G)LMM on the basis of very small constructed datasets just to highlight the general logic of the method. More linguistically then, Section 3 discusses a linguistic phenomenon of particle placement, the constituent order alternation exemplified in Example 2, with CONSTRUCTION being a binary response with two levels:

(2) a. John picked up the book.

CONSTRUCTION: V-Part-DO b. John picked the book up.

CONSTRUCTION: V-DO-Part

Particle placement has been very thoroughly researched in terms of its fixed effects so it is well-known which determinants have which, and how strong an, effect on the choice of construction in both mono-and multifactorial analyses (see

(G)LMM: a very brief introduction by example

In this section, I will introduce the logic and application of (G)LMM on the basis of several small datasets. Three things are important to bear in mind. First, as will become obvious, the data are simulated and have been designed to have particular immediately obvious characteristics. Second, I am using a linear regression model for this initial explanation. Both of these choices have been made for didactic/expository reasons. I am using simulated data because these data have characteristics that, when plotted, illustrate clearly the patterns that (G)LMEM are particularly good at detecting, and this is something that authentic data would make much more difficult. Second, I am not using a logistic/multinomial model, (i.e., a model with a categorical response), for this example even though such models are more typical of corpus-linguistic applications. This is because these latter kinds of models involve additional conceptual steps (e.g., the use of link functions) that make the exposition less transparent than desirable. Third, the examples in this section are clearly not representative of the real-life applications and challenges of (G)LMM because, as will be seen presently, my examples involve only three speakers (to facilitate plotting, etc.) whereas real-life applications will be more complex. However, the example in Section 3 is a fully fledged one that mirrors what corpus linguists would actually study much more accurately and I recommend

Introduction: the traditional approach

Imagine a dataset in which you try to model a numeric response y 1 as a function of a numeric predictor x 1 . You have thirty data points -ten from each of three speakers (labelled just 1, 2 and 3). Given the current state of the art in corpus linguistics, it is reasonable to expect this to be analysed with a linear regression and summarised as shown in Figure

Varying intercepts

In this case, even the simplest (G)LMM can help because, rather than being restricted to one intercept for all three speakers (0.8696 in the above linear model), it can let every speaker get his or her own intercept and then adjusts the slope to make best use of the different intercepts (1|SPEAKER1 means 'fit a separate intercept (1) for each of the three different speakers included in the variable/vector SPEAKER1'):

The difference in the results is striking in both the numeric output and the plot. Again, there is a positive correlation, but now for every oneunit increase of x 1 , y 1 increases by more than 5.5 times as much as before (0.95377 instead of 0.1682), the effect of x 1 is highly significant (t = 56.25 and, not shown, 2 = 126.9, df = 1, p < 10 -10 ) and, now that the different speakers are not forced to share the same intercept anymore, the model has a very high degree of explanatory power (R 2 conditional = 0.998). Thus, by accounting for the fact that the data points of each speaker are not independent and modelling them as speaker-specific results -giving them separate intercepts -a non-significant result of a model that anyway violated its assumptions suddenly turned into a highly significant result with nearly perfect predictive accuracy.

Varying slopes

Imagine now a dataset with the same structure (two variables, y 2 and x 2 ), three speakers and thirty data points. As Figure

The (G)LMM with varying intercepts in the following block of code does better (R 2 marginal = 0.376, R 2 conditional = 0.83) but is still not a really good model of the data (see the right-hand panel).

Varying intercepts and slopes

Imagine, finally, yet another dataset with, again, the same structure (two variables y 3 and x 3 , three speakers and thirty data points). As before, a linear regression indicates a positive correlation (for every one-unit increase of x 3 , y 3 increases by 0.3533) but the correlation between x 3 and y 3 is not significant (multiple R 2 ≈0.099, p≈0.091) and, as before, it does not take much training to see in Figure

To save space, I do not provide graphs and results for (G)LMM analyses with varying intercepts and varying slopes and only wish to observe that while both these models already do much better than the 'regular' linear regression, they are, again, not convincing when one plots the resulting regression lines. Rather, what is needed here is a (G)LMM that has separate intercepts and separate slopes for each speaker (as determined by model comparison tests (not shown, p varying intercepts < 10 -11 and p varying slopes < 10 -15 ); in the code quoted below (1+X3|SPEAKER3) means 'fit separate intercepts (1) and (+) slopes of x 3 for each speaker (X3|SPEAKER3)'. This model results in a virtually perfect fit (R 2 marginal = 0.872, R 2 conditional = 0.998), with an effect of x 3 on y 3 that is significant (t = 3.401 and, not shown, 2 = 56.203, df = 1, p < 10 -13 ) but is again very different -namely, nearly six times as strong as the linear regression would make us believe (2.0491 rather than 0.3533). Figure

To conclude, while this section could only scratch the surface and dealt, merely, with numeric dependent variables (y 1 , y 2 and y 3 ) rather than the categorical dependent variables that are more frequent in corpus linguistics, I hope it has become clear how much more precision and reliability (G)LMM has to offer, not to mention the fact that the regular fixed-effects regression would really not even have been permitted in each case given the dependence of the data points. Again, they can protect both against statistical Type I and Type II errors and the better regression coefficients that result allow for better explanation of the phenomena under investigation. In the following section, I discuss the example of particle placement, which will involve a binary categorical dependent variable and add the multi-level perspective that corpus data routinely require. In spite of the didactic nature of this paper, given considerations of space, I cannot discuss all aspects of regression modelling in detail here; instead, I will provide some guidance on the general logic of the overall process and the multi-level perspective as well as help with regard to the interpretation of the results; the discussion here is modelled after Gries's (2013) characterisation of regression modelling.

A ME/MLM analysis of particle placement

The data

The data studied here are those from Gries (2006: Section 4): 1,192 instances of CONSTRUCTION: V-DO-Part and 1,129 instances of CONSTRUCTION: V-Part-DO from the ICE-GB. Since the main point of this paper is didactic, each of these instances was annotated for only two fixed effects:

-TYPE: the type of head of direct object: 'lexical' or 'non-lexical'; -LOGLENGTH: the logged length of the DO in words.

Thus, Example 1 ('. . . before you started picking up a camera. . . '), would be annotated as in

A 'regular' fixed-effects only logistic regression

If this dataset were to be analysed statistically, the analysis most likely to be found would be a binary logistic regression without random effects (BLR) that models, or tries to predict, the probability of the dependent variable CONSTRUCTION: V-Part-DO (the second level of CONSTRUCTION alphabetically) on the basis of three predictors: the numeric independent variable LOGLENGTH, the categorical independent variable TYPE, and the interaction of the two, which allows the regression to 'consider' the possibility that the effect of LOGLENGTH might not be the same for TYPE: LEXICAL and TYPE: NON-LEXICAL, and this would be represented in R by a regression equation CONSTRUCTION ∼ ('as a function of') LOGLENGTH*TYPE; these would be its results:

In a nutshell, the three predictors, the two fixed-effect independent variables and their interaction, are significantly correlated with the choice of construction (p < 0.0001); the correlation is strong (Nagelkerke R 2 = 0.609), and the classification accuracy is high (C = 0.888 and 79.02 percent of all constructional choices are predicted correctly), as can be seen from the code available from my website. If researchers are already so advanced as to use multi-factorial modelling, then this is probably the most widely used kind of analysis of such data. However, it is at least incomplete, if not inappropriate, because it pretends that the 2,321 data points are all independent of one another, which we know they are not: they exhibit inter-relations because they were produced by fewer than 2,321 speakers, because of the lexical items in the verb-particle constructions, and because of the levels of corpus sampling. Thus, let us now turn to the better kind of analysis.

A (G)LMM/MLM analysis

While (G)LMM/MLM is currently a hot topic in linguistics, there are still many methodological questions that are hotly debated, answered differently in every new article one reads, or answered in some references and glossed over in others (see

(i) begin with a model that contains the most comprehensive fixedeffects structure that can be fit given the variables to be explored and find the optimal random-effects structure (varying intercepts for one or more predictors and/or varying slopes for one or more predictors); and, (ii) once the optimal random-effects structure has been found, find the optimal fixed-effects structure.

In both these steps, 'optimal' means according to some criterion such as significance testing/p-values or information criteria. With p-values this would mean that the final model, m, contains (i) only random effects that make m significantly better than if these were not in m and (ii) only fixed effects predictors -again, independent variables and their interactions -that make m significantly better than if these were not in m or that are required for higher-level interactions.

Step I: finding the optimal random-effects structure

As a first step, we define the three new variables that most explicitly reflect the nested structure of the data: LEVEL1, LEVEL2 and LEVEL3. Then we fit a first model that, as above, contains all fixed effects -LOGLENGTH, TYPE, and their interaction LOGLENGTH:TYPE -as well as varying intercepts for each level of corpus sampling -(1|LEVEL1), (1|LEVEL2), (1|LEVEL3) -and for each verb (1|VERB) and each particle (1|PARTICLE). Given the logic outlined in Sections 2.2 and 2.4, that means that the baseline probability of CONSTRUCTION: V-Part-DO can be different for each of these random effects; in other words, the model allows every corpus part (at each level of corpus organisation), every verb and every particle to have a different baseline 'preference' for CONSTRUCTION: V-Part-DO. 8 8 As indicated, there is currently a debate concerning what the maximal random-effects structure is with which one should begin a model selection process. One currently influential paper is

The summary of the random effects looks like this: the larger the values for the variances/standard deviations, the more variability is located 'within' that random effect; this suggests that, in this case, the (crossed) random effects of lexical items contribute more than the (nested) random effects of the corpus organisation.

To trim down the random-effects structure, let us first test whether the random effect with the largest amount of variance can be omitted: if that one did not make a significant contribution, this would suggest that the other random effects would not either. To that end, we first fit a new, smaller model that is just like model.1.sep but does not contain (1|VERB).

After that, we compare that new model without the random effect (1|VERB), model.2.sep, to the old model with that random effect, model.1.sep, with the anova function: 9 9 Just like most other aspects of (G)LMEM, this use of anova to determine whether random effects can/should be included in a model (following

The difference between the models is significant, which means (1|VERB) needs to stay in our model or, put differently, we need to stick with model.1.sep and cannot use the simpler alternative of model.2.sep because distinguishing different baseline constructional preferences for verbs is significantly necessary (and also supported by the smaller AIC-value of the first model; cf.

The random effect with the next smaller variance is LEVEL3, i.e., SUBREGISTER. A model comparison analogous to the one above shows that this, too, needs to stay in the model: distinguishing different constructional preferences for sub-registers is significantly necessary: Now what about LEVEL2, (i.e., REGISTER)? Here, for the first time, we see we need to abandon our first model, model.1.sep, because model.5.sep, the model in which we do not care about registers, is not significantly worse than the model in which we do (and probably because the sub-registers already account for most of the variability registers would account for, though this need not always be the case). Thus, model.5.sep becomes our new reference model, which is again also supported by the fact that the AIC-value of model.5.sep, is smaller than that of model.1.sep).

Can we then also assume that the difference between speaking and writing (LEVEL1 or MODE) plays no significant role? We generate a sixth we have now identified the optimal random-effects structure, which turns out to be much more complex than corpus-linguistic studies usually assume. Whatever the results of the fixed-effects part of the analysis in the next section will be, we have already established that it is very much necessary to incorporate verb-and particle-specific effects into the statistical analysis, and on top of that we have also already learned that the statistical analysis benefits significantly from distinguishing the thirteen different sub-registers of the ICE-GB whereas distinguishing the registers or speaking from writing does not do anything. Let us now move on to finding the optimal fixed-effects structure.

Step II: finding the optimal fixed-effects structure

We proceed according to the same logic as before, just with fixed-effects this time: we create a new model that is the same as an old reference model, but delete -this time fixed -effects from it, where we follow the usual rule that effects can only be deleted if they are neither significant themselves nor participate in a significant higher-order interaction (see

The interaction LOGLENGTH:TYPE cannot be deleted with a significant loss in explanatory power, which means no more model simplification can be attempted; we already have the right fixed-effects structure.

The differences between the BLR and the (G)LMM/MLM approach

As a first general step, let us see where the two kinds of models differ in their classifications, which is represented in Figure

However, this is a rather coarse resolution and it is more interesting to see, for all the effects included in the model, where the (G)LMM/MLM approach fares better than the BLR. One of many ways to explore this is to compute for each verb, particle and sub-register, how much more often the (G)LMM/MLM approach makes the correct classification compared to the BLR approach (in percent). For verbs and particles, these results are shown in the left-and right-hand panel respectively of Figure

First, the graph clearly shows yet again how important it is to use lexically specific effects in our corpus-linguistic analyses: there are many verbs and particles whose classification accuracy is improved by 20 percent, or even much more, when the more appropriate (G)LMM/MLM approach is used. Also, it is worth anticipating a likely objection: note that the degree to which the (G)LMM/MLM approach is better is not a straightforward function of the frequency of the verbs/particles. Thus, the argument 'Once we have more data everything will be classified better even with BLRs' is flawed: even for some medium-or even higher-frequency verbs and particles (such as get/have and together/back), substantial improvements in classification accuracy are obtained. What about the sub-registers? A similar plot can be drawn and is represented in Figure

Again, it clearly defies what might seem to be straightforward expectations: increased classification accuracy is neither straightforwardly correlated with sample size nor with corpus linguists' most cherished contrast: speaking versus writing. However, it is clear that the sub-registers differ strongly from one another (which is why the random effect of SUBREGISTER had to be included) and that they differ strongly in terms of how much they benefit from their idiosyncrasies being taken into account. Funnily enough, it is the sub-register 'creative' that benefits nearly the least from its particular characteristics being taken into account, but one would probably need a real (Biberian) multi-dimensional analysis to try to make sense of the sub-register effects here.

The final way to explore the data further, which is to be exemplified here, would be to look at how the significant interaction of the fixed effects, LOGLENGTH:TYPE, plays out in the different sub-registers (averaging over all verbs and particles). This is represented in Figure

Conclusion

Mixed-effects modelling, a method of doing regression analyses in which idiosyncrasies of sampled units -speakers, words, etc. -can be accounted for elegantly, is a tool that is becoming more and more frequently used, especially in psycholinguistic analyses of (mostly) experimental but also observational data. Given the fact that corpus-linguistic data are much more unbalanced and messier than experimental data, it is time that corpus linguists avail themselves of that same family of methods. Not only would this end the way in which corpus linguists nearly always violate basic assumptions of our statistical tests -because we would finally take into consideration that our data points are not, usually, independent -but it would also allow us to, again finally, take more seriously the exploration of corpus data on the multiple levels of sampling that corpora come in. The multi-level aspects of mixed-effects modelling in the previous literature on this method in psycholinguistics or sociolinguistics has hardly ever been discussed at all