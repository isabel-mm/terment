import os
import spacy
import re
import argparse # Para poder aplicar el script a los argumentos necesarios
from collections import defaultdict

def extract_sentences(text_folder, glossary_path):
    nlp = spacy.load("en_core_web_trf")

    # Cargar tÃ©rminos del glosario. Se eliminan, por si acaso, lÃ­neas vacÃ­as
    with open(glossary_path, "r", encoding="utf-8") as file:
        glossary_terms = [term.strip() for term in file.readlines() if term.strip()]

    # Ordenar tÃ©rminos por longitud (nÃºmero de caracteres) para priorizar la anotaciÃ³n de tÃ©rminos polilÃ©xicos (para evitar solapamientos, se anota antes 'multimodal corpus' que 'corpus')
    glossary_terms.sort(key=len, reverse=True)

    # Diccionario para almacenar oraciones por tÃ©rmino
    sentences_by_term = defaultdict(list)

    # Procesar cada archivo en la carpeta
    for file_name in os.listdir(text_folder):
        file_path = os.path.join(text_folder, file_name)

        # Leer el contenido del archivo
        with open(file_path, "r", encoding="utf-8") as file:
            text = file.read()

        # Procesar con spaCy para tokenizar y segmentar en oraciones
        doc = nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents]  # Extraer oraciones y limpiar espacios

        # Filtrar solo oraciones bien formadas (empiezan con mayÃºscula y terminan en punto), i.e., se excluyen epÃ­grafes, tablas, etc.
        valid_sentences = [sent for sent in sentences if re.match(r"^[A-Z].*\.$", sent)]

        # Filtrar SOLO oraciones que contienen tÃ©rminos del glosario
        for sentence in valid_sentences:
            for term in glossary_terms:
                if re.search(rf"\b{re.escape(term)}\b", sentence):  # Coincidencia exacta de palabra completa
                    sentences_by_term[term].append(sentence)

    # Verificar la cantidad de oraciones recolectadas y el nÃºmero de tÃ©rminos que hay en el glosario
    print(f"\nðŸ”¹ Se han extraÃ­do {sum(len(s) for s in sentences_by_term.values())} oraciones en total.")
    print(f"ðŸ”¹ NÃºmero de tÃ©rminos en el glosario: {len(glossary_terms)}")

    # Asegurar que cada tÃ©rmino aparece al menos 4 veces (es la freq. mÃ­nima de apariciÃ³n del Ãºltimo tÃ©rmino)
    selected_sentences = set()
    for term, sentences in sentences_by_term.items():
        if len(sentences) >= 4:
            selected_sentences.update(sentences[:4])  # Tomar 4 oraciones por tÃ©rmino
        else:
            selected_sentences.update(sentences)  # Tomar todas si hay menos de 4

    # Guardar las oraciones en un archivo de salida
    output_path = os.path.join(text_folder, "oraciones_filtradas.txt")
    with open(output_path, "w", encoding="utf-8") as file:
        for sent in selected_sentences:
            file.write(sent + "\n")

    # Mostrar algunas de las oraciones seleccionadas
    print("\nEjemplo de oraciones seleccionadas:")
    for i, sent in enumerate(list(selected_sentences)[:5]):
        print(f"{i+1}. {sent}")

    print(f"\nðŸ“‚ Oraciones guardadas en: {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extraer oraciones que contienen tÃ©rminos del glosario")
    parser.add_argument("--text-folder", type=str, required=True, help="Carpeta que contiene los textos")
    parser.add_argument("--glossary", type=str, required=True, help="Archivo de glosario con los tÃ©rminos")

    args = parser.parse_args()
    extract_sentences(args.text_folder, args.glossary)
