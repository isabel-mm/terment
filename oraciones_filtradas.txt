What distinguishes then our 1 The type of content of social media platforms is not restricted to only one.
The exploration can be done in different levels: all corpus or by user type (news agencies or individuals).
Because all statistical tests make assumptions about types of data (they are quite picky), it is necessary to decide which type the data at hand most closely correspond to, in order to choose the most appropriate test.
In many of the well-known corpora of English, the ambition has been to cover a general and very common type of discourse (such as 'conversation in a variety of English') or a very large population (such as 'second-language learners of English').
That said, it requires more data to be able to make valid observations about a large group of people and a general type of discourse than a small group of people and a specific type of discourse.
Anybody who wants the claims made based on a corpus to be accepted by the research community needs to show in some way that the corpus material is appropriate for the type of research done.
The corpus builders have thus augmented the existing data by adding various types of information-about the speakers (sex, age, social class, region of origin, educational background), about the text (type of narrative; whether a stand-alone story or part of a 'narrative chain') and about the utterance (the roles of the participants vis√†-vis the narration; type of quotative verb used to signal who said what in a narrative; to what degree the discourse is represented as being verbatim or more or less indirect).
This type of information is highly useful, as it not only makes it possible to study and compare different types of narrative, but it also shows how the corpus is balanced (or not) with respect to type of narrative.
As a result, it is at least technically possible for this corpus to have conflicting constituent trees and sentence span borders for sentence type annotation.
While this approach is very versatile, it is also labor intensive: for each new type of information, a new script must be written which traverses the corpus in search of some information.
These analyses are particularly useful for illustrating the uses to which AMs can be put in that, though they rely on the same test, each makes use of a rather different type of association and for different purposes.
Other associated verbs of this type are render, get, and set.
A corpus that has used this type of data to the best advantage is the Corpus of Early English Correspondence, compiled by Terttu Nevalainen, Helena Raumolin-Brunberg and a team of collaborators (Raumolin-Brunberg and Nevalainen 2007).
Providing this type of social background information of course poses new challenges, requiring a thorough knowledge of the social structure of past societies.
Parameters include, for instance, the letter writer's gender, year of birth, occupation, rank, their father's rank, their education, religion, place of residence, any history of migration, their careers and social mobility, the general type of contents of their letters, and how well their letters can be authentified.
As existing spoken corpora vary greatly in the type and number of annotations as well as their data format, possibilities for corpus search diverge significantly.
It is known that the recording context has a strong influence on the type of constructions used, both in child-directed speech and by the children themselves.
The main goal of this type of recording is to link the characteristics of the speech of the child to her immediate linguistic and extra-linguistic environment.
This is very valuable information to estimate the quantity and type of input a child is exposed to.
This both makes it easier to type in the speaker of an utterance during transcription and largely anonymizes the data (given that the metadata are stored in a different file).
This type of glosses is also common in general linguistic publications and specifies both the shape and function of all morphemes.
Quantity here refers both to (i) the size of individual corpora (e.g., number of participants, length and interval of recordings) and (ii) the number (and type) of languages we have data for.
So far, this type of data is only available for a small number of participants in two languages.
Corpora usually have their own idiosyncratic study designs, including the number and age of children involved, the sampling regime, and the type of contexts in which recordings are taken, let alone the different conventions used for annotations.
When building general reference corpora from the web we need to choose seed words that are likely to appear across a wide range of topics, but there has been some debate about exactly what constitute good seeds for a corpus of this type.
As mentioned above, the code can also be run directly in the console (the way to do this is to type the code after the prompt (the last angled bracket) and press [enter]); however, it is easier to save and edit code that is written in Panel 1.
To find out what the current working directory is, type getwd() in the code editor and run the code.
The function str() can be used to see what type of data has been loaded: str(cl.order).
Each provides a different type of information about a distribution of values.
This function is a good function to start with as it chooses what type of graph to use based on the kind(s) of variables to be plotted.
The plot() function will here display the relative frequency of each clause type order (mc-sc or sc-mc) per conjunction (when, before, after or because); that is, this function can be used to plot the variables ORDER and CONJ, using the code below.
To get a list of the names of all the available colors, type colors() in the code editor and run the code.
A histogram is a type of bar graph that groups values into a series of intervals (or bins).
It displays the number of times each preposition type is found in a certain context.
The table consists of 257 lines (one line per preposition type) and 19 columns (one column per variable).
Type I and type II errors are part and parcel of the procedure.
It overestimates statistical significance (type I error) in cases where there is no real difference and the variation is purely due to chance; the type I error rates (false hits) ranged from 48 to 99% depending on the linguistic feature investigated.
The Mann-Whitney U test, which takes individual variation into account performed considerably better with type I error rate (as expected) around 5%.
Below, we will use it to analyze a corpus based measure, namely the type-token ratio (TTR) of different texts that have the same overall length.
The second independent variable is the type of newspaper, which is a categorical variable, since it falls into three distinct categories, namely The Times, the Daily Telegraph, and the Guardian.
The output below reports on a model that contains the type of newspaper as a second independent variable, which allows the regression to adjust the estimates for each newspaper type.
This statistical noise explains why the models' coefficients do not correspond exactly to the number of milliseconds that we specified for each type of example.
The first independent variable concerns the type of conversation from which the examples are drawn.
In any type of GL(M)M, the additive linear term consists of a number of sub-terms which are simply added up.
Finally, two fixed effects candidates are annotated (the type of the head of the direct object and the logarithmised length of the direct object in words).
They have been fruitfully used in models of linguistic variation, where the task is to find out which linguistic and extralinguistic factors determine the use of near-synonyms (e.g. let, allow or permit), alternating syntactic constructions (e.g. the double-object vs. to-dative) or sociolinguistic variants (e.g. the type of /r/ used by speakers of a particular dialect).
Nine covariates are tested, which represent such social factors as the sex, age and education level of the speakers, and such linguistic factors as polarity, type of determination and proximity of the copula to its referent.
The most important predictors are the speaker's age, polarity, type of determination and proximity.
Often, the next kind of information the reader needs to learn about is some descriptive statistics of the data of the type discussed in Chap. 17.
The synthesist's domain-specific knowledge here is indispensable concerning what type(s) of questions have been sufficiently addressed in the primary literature, for example, or what questions are theoretically and/or practically relevant to address.
Broader domains also expose the study to 'apples and oranges'type critiques wherein the heterogeneity of the sample is such that aggregation of findings across studies is deemed inappropriate.
This will include paper titles, the type of publication (journal article, book chapter, doctoral dissertation), author names, year of publication, and so forth.
Now we use a computer to collect language data of any size, type, and variety; classify them, process them, analyze them, and utilize them in various works.
For example, for a question-answering system, some of the typical features (or functions) involve the recognition of a linguistic expression as a question, analysis of the type of the question, understanding the syntactic structure of a question, capturing meaning embedded into it, recognizing the sentiment hidden in the question, searching out an appropriate document that may carry an answer, extraction of the answer from a document, evaluation of the answer before responding, and finally generating the answer either in text or speech mode.
This implies that numerous factors control the content, type, and use of a corpus as well as numerous ways that one can employ to classify corpora.
Here the question of representation is not linked with the language as a whole but with the language used by a particular type of people who are primarily non-native speakers.
The chapter concludes with a description of the many different areas of linguistics (e.g. lexicography and sociolinguistics) that have benefited from the use of linguistic corpora, followed by a linguistic analysis illustrating that corpus-based methodology as well as the theory of construction grammar can provide evidence that appositives in English are a type of construction.
While creative (or imaginative) writing was the most common type of writing in the BNC, in the ICE it is not as prominent.
Legal written English was excluded from the ICE on the grounds that it is a highly specialized type of English firmly grounded in a tradition dating back hundreds of years, and thus does not truly represent English as it is written in the 1990s (the years during which texts for the ICE are being collected).
This bias is a simple reflection of the fact that those creating the BNC and ICE Corpora felt that spontaneous dialogues are a very important type of spoken English and should therefore be amply represented.
Judgment, or purposive, or expert choice sampling, a second type of sampling, was used to create the Brown Corpus.
One could collect only articles written by a single author, but this again might lead to a misrepresentation of the type of writing typically found in a genre.
It is equally important to consider the quality and type of microphone to be used to make recordings.
Wireless microphones are appropriate in recording situations of this type too, and avoid the problem of the speaker being constrained by the length of the cord attaching the microphone and recorder.
As anyone who has ever transcribed speech knows, the flow of speech is much faster than the ability of the transcriber to type.
For instance, he considered such factors as whether the formality of the particular type of writing influenced the choice of either retaining or deleting that; whether that deletion was less common in formal writing than in informal writing; whether the particular verb influenced use or non-use of that; and whether including or not including that resulted in a difference in meaning.
To begin investigating how pseudo-titles and corresponding appositives were used in the regional varieties of ICE being studied, a cross tabulation of the variables "country" and "type" was generated.
In other words, three variables are interacting: "type" (pseudo-title or appositive), "form" (simple noun phrase, genitive noun phrase, noun phrase with post-modification), and "length" (1-4 words or 5 words or more).
There are ways to get transcriptions of spontaneous speech, but mainly from conversations broadcast on radio or television, a highly restricted type of language.
For example, good practice for building a corpus is to accurately document the type of language it contains.
Corpus studies do not make it possible to draw this type of conclusion.
A typical example of this type of data is the corpus that contains newspaper archives or parliamentary debates.
Another distinction that can be made regarding the types of existing corpora relates to the type of processing carried out on the linguistic data of the corpus.
That being said, good practices in corpus linguistics and experimental linguistics require obtaining information about participants beforehand, which can eventually eliminate this type of bias.
On the other hand, this type of research is corpus-based, because it starts from a hypothesis (e.g. "passive sentences tend to be used more frequently with state verbs"), and seeks to verify it in the corpus, which, in that way, only works as an analysis tool.
A qualitative study on this same question would identify some examples of spelling mistakes and analyze in detail the contexts in which they occur, for example the grammatical category of the words concerned, whether they are rare or frequent words, occurring in a long or a short sentence, what type of phonemes is poorly transcribed, etc.
Finally, this corpus should contain a synchronic type of data, corresponding to the current pronunciation, rather than to its diachronic evolution.
In the literature, this type of structure is associated with the presentation of new events in discourse.
For this type of case, the use of a syntactically annotated corpus becomes compulsory.
The role of this type of formulation largely depends on the context and cannot be automatically inferred from the linguistic form employed.
In order to study the distribution and prevalence of this type of structure in the French spoken in Parisian suburbs, all the occurrences of indirect questions were collected in a small oral corpus of approximately 350,000 words.
The level of experience also has an influence on the type of message produced.
The authors therefore coded every occurrence according to the type of process described: previous or past.
For a number of research questions, this type of information is crucial, for example, when it comes to determining the meaning of a derived word.
This problem arises somewhat differently in the case of judicial inquiries, in particular because of the type of linguistic material involved.
Even more strikingly, the structure of the type then I appeared 10 times more frequently in the COBUILD corpus that the I then construction, repeatedly employed in the declaration.
For instance, it is possible to study the type of lexical errors made during various developmental stages or the diversification in the repertoire of speech acts which are available to the child.
This type of study should also compare acquisition processes in spoken and written data.
We will discuss the different possible terms of comparison, depending on the type of research question being considered.
It is necessary to neutralize the differences in the type of data used in order to bring out the differences between languages.
On the contrary, the disadvantages of one type can often be counterbalanced, at least partly, by the advantages of the other, and vice versa.
The first type of explanation concerns the influence of the source language, which inevitably leaves traces in translations.
This can be measured thanks to the type/token ratio (see Chapter 8).
The major drawback of these interfaces is that they do not authorize any type of search.
If this type of search is not enabled by the interface, all the verbal forms must be looked up one by one with their exact forms.
This type of information is nonetheless crucial for the quantitative comparison of linguistic productions between the two genders.
However, when this type of information remains unavailable, these gaps lead to serious limitations in data analysis (see Chapter 8).
In addition to the big generic corpora, for certain types of research, using corpora belonging to a specific type of genre may prove to be a wise choice.
Representative of another type of language register, TED conferences have recently made it possible to create a large parallel corpus.
Finally, if applicable, the command should specify the type of speaker whose words have to be analyzed.
The command also helps you to calculate the type/token ratio (see Chapter 8), which represents a measure of lexical diversity.
Conversely, maison is associated with appartements and √©tages, which specifies the type of housing in question, as well as bureaux which shows a different use from that of maison.
At this age, her type/token ratio was 0.37.
At 3 years and 5 months old, the most frequent word was √ßa with 54 occurrences, and her type/token ratio was 0.21.
At this age, his type/token ratio was 0.38.
The type/token ratio was 0.24.
The type/token ratio cannot therefore be considered as a reliable measure of linguistic development.
Finally, for handwritten data, there is no solution other than to manually type it on the computer.
This type of tool has made the collection of web-based corpora extremely easy.
For example, this type of information includes the date of a newspaper article, the place where a conversation was recorded or the characteristics of the speakers taking part in the dialogue.
In particular, these questions will determine the type of interactions that should be recorded, the constraints regarding the context as well as the information contained in the transcript.
Audio recordings make it almost impossible to grasp this type of information, which later have to be added to the corpus so that the interactions can be understood and analyzed by the experts who will listen to these recordings.
For studies on how interpersonal relationships influence interactions, it will be necessary to have as much information as possible about the degree of relatedness between participants, whereas for studies on the use of discourse markers such as bon or ben, this type of information is not so relevant.
While it is true that laws differ from country to country, it is not possible to distribute the texts of an author during his/her lifetime without demanding some type of compensation.
For example, adjectives such as gentil or gentille (in French) are the masculine and feminine variants of the same word (or type) from a morphological point of view.
Conversely, many other annotations made on corpus studies are devoted to a single type of element to be studied.
We have offered examples of this type of research in Chapter 2.
For example, an element like word can be embedded into a sentence type of element.
However, in order to be interpreted, a document marked up in XML coding must follow certain syntax, in the form of a document type definition (DTD), which specifies the list of tags with their potential attributes, as well as the possible order of appearance for tags.
It was by means of this type of comparison that the annotations offered for the Google Books corpus in section 7.2 were considered as suitable tagging and parsing systems.
As we have seen, annotation always requires a certain type of interpretation and this may vary slightly from one annotator to another.
Then, we will present two types of descriptive statistics that, respectively, make it possible to measure lexical diversity (the type/token ratio), and to calculate lexical dispersion in a corpus.
For example, in the spoken corpus of French from French-speaking Switzerland OFROM (see Chapter 5), the type/token ratio is 0.02, whereas in the French section of the children's literature corpus ByCoGliJe, it is equal to 0.04.
The problem with the type/token ratio is that it is very sensitive to the size of a corpus (for this reason, the type/token value 9/12 = 0.25 obtained for sentence (1) is in no way representative).
For example, the type/token ratio in the Sciences Humaines written corpus that contains around 260,000 word occurrences is 0.06, but it drops to 0.007 for the 2012 installment of the Le Monde corpus, which has over 26 million word occurrences.
We must therefore avoid using this type of measurement on corpora of different sizes.
The primary goal of research in corpus linguistics is to be able to draw conclusions in relation to a linguistic phenomenon, not only at the level of the observed data sample, but also to generalize it to all data of the same type.
As such, the only type of information we could report is the frequency with which every variable condition appeared in the data.
For example, this type of test can be used in studies seeking to find out whether advanced learners produce significantly more discourse connectives than intermediate learners, or whether speakers produce more fallacious arguments when speaking at political party meetings or at electoral campaigns.
We also showed how to measure the lexical richness of a corpus using the type/token ratio, and presented the limitations of such a measurement.
The type/token ratio can only be used for comparing texts of similar length.
The type/token ratio can also be used for comparing texts and their translations, only in cases of similar length.
The reports or articles presenting this type of results generally follow a very precise structure.
In the first type, we solely extracted the contextualized embeddings of the target words, and used them as the only features for training traditional off-the-shelf classification algorithms.
In the second type of approach, we use the original pre-trained LLM (i.e. MacBERTh or GysBERT), and finetune its parameters in order to perform the classification task at hand.
To distinguish this type of collection from other collections of texts, we will refer to it as a linguistic corpus, and the term corpus will always refer to a linguistic corpus in this book unless specified otherwise.
Both words occur frequently in the PP [through NP], sometimes preceded by a verb of seeing, which is not surprising given that they refer to a type of window.
In corpus linguistics (and the social sciences more generally), hypotheses of this type are the exception rather than the norm -we are more likely to deal with statements about tendencies (think Most swans are white or Most examples of windscreen are British English), where the search for counterexamples is not a viable research strategy.
On closer inspection, however, it becomes apparent that we may be dealing with a different type of exception here: the word pavement has additional senses to the one cited in (5a) above, one of which does exist in American English.
We can turn this claim into a hypothesis involving two variables (Variety and Suffix Variant), but not one of the type "All x are y".
However, before, say, "pressing down" can be used as an operational definition, at least three questions need to be asked: first, what type of object is to be used for pressing (what material it is made of and what shape it has); second, how much pressure is to be applied; and third, how the "difficulty" of pressing down is to be determined.
Unfortunately, the latter type of operational definition is more common in linguistics (and the social sciences in general), but there are procedures to deal with the problem of subjectiveness at least to some extent.
If we limit ourselves just to metaphorical expressions of this type, i.e. expressions that explicitly mention both semantic fields involved in the metaphorical expression, it becomes possible to retrieve metaphors of anger semi-automatically.
Relative quantitative differences are expressed and dealt with in different ways depending on the type of data they involve.
For example, the measure of Referential Distance discussed in Chapter 3, Section 3.2 yields cardinal data ranging from 0 to whatever maximum distance we decide on and it would be possible, and reasonable, to calculate the mean referential distance of a particular type of referring expression.
For example, 56.18 percent of all possessive constructions in our sample are s-possessives and 43.82 percent are of -possessives; if there were a random relationship between type of construction and givenness of the modifier, we should find the same proportions for the 183 constructions with old modifiers, i.e. 183 √ó 0.5618 = 102.81 s-possessives and 183 √ó 0.4382 = 80.19 of -possessives.
If there were a random relationship between type of construction and givenness of the modifier, we should find the same proportions for both types of possessive construction: there should be 200 √ó 0.514 = 102.8 s-possessives with old modifiers and 97.2 with new modifiers, as well as 156 √ó 0.514 = 80.18 of -possessives with old modifiers and 156 √ó 0.486 = 75.82 of -possessives with new modifiers.
As mentioned in the preceding chapter, nominal data (or data that are best treated like nominal data) are the type of data most frequently encountered in corpus linguistics.
In CFA, an intersection of variables whose observed frequency is significantly higher than expected is referred to as a type and one whose observed frequency is significantly lower is referred to as an antitype (but if we do not like this terminology, we do not have to use it and can keep talking about "more or less frequent than expected", as we do with bivariate ùúí 2 tests).
There is an alternative, however: speakers sometimes use adverbs that explicitly refer to the type of beginning.
When looking at occurrences of linguistic items in this way, they are referred to as types; the type frequency of the s-possessive in the BNC is 268 450 (again, ignoring upper and lower case).
The type frequency of the, of course, is 1.
Let us look at one more example of the type/token distinction before we move on.
When studying words, we will normally be interested in how often they are used under a particular condition, so it is their token frequency that is relevant to us; but we could imagine designs where we are mainly interested in whether a word occurs at all, in which case all that is relevant is whether its type frequency is one or zero.
There are five word tokens that contain this suffix (directly, briefly, wisely, truly, and wisely), so its token frequency is five; however, there are only four types, since wisely occurs twice, so its type frequency in this passage is four.
In contrast, the type frequency of an affix is a fairly direct reflection of the importance of the affix for the lexicon of a language: obviously an affix that occurs in many different words is more important than one that occurs only in a few words.
Note that in order to compare type frequencies, we have to correct for the size of the sample: all else being equal, a larger sample will contain more types than a smaller one simply because it offers more opportunities for different types to occur (a point we will return to in more detail in the next subsection).
The TTR is the percentage of types in a sample are different from each other; or, put differently, it is the mean probability that we will encounter a new type if we go through the sample item by item.
Put differently, if we go through the occurrences of -icle in the BNC item by item, the probability that the next item instantiating this suffix will be a type we have not seen before is 0.15 percent, so we will encounter a new type on average once 9 Morphology every 670 words.
For mini-, the type-token ratio is much higher: it occurs in 382 different words, so its TTR is 382 /1702 = 0.2244.
Put differently, if we go through the occurrences of mini-in the BNC word by word, the probability that the next instance is a new type would be 22.4 percent, so we will encounter a new type about every four to five hits.
Likewise, observing the type frequency (i.e. the TTR) of an affix under different conditions provides information about the relationship between these conditions and the affix itself, albeit one that is mediated by the lexicon: it tells us how important the suffix in question is for the subparts of the lexicon that are relevant under those conditions.
While type frequency is a useful way of measuring the importance of affixes in general or under specific conditions, it has one drawback: it does not tell us whether the affix plays a productive role in a language at the time from which we take our samples (i.e. whether speakers at that time made use of it when coining new words).
We will refer to this measure as the hapax-token ratio (or HTR) by analogy with the term type-token ratio.
Take the TTR: if we interpret it as the probability of encountering a new type as we move through our samples, we are treating it like a nominal variable Type, with the values new and seen before.
As we move through the text word by word, initially all words are new types and hapaxes, so the type-and hapax-counts rise at the same rate as the token counts.
Six words later, there is another occurrence of a, so type and hapax counts remain, respectively, at 12 and 11 as the token count rises to 14, and so on.
The more types have already occurred, the more types there are to be reused (put simply, speakers will encounter fewer and fewer communicative situations that require a new type), which makes it less and less probable that new types (including new hapaxes) will occur.
Clearly, these are not "established" in any meaningful sense, so let us add the requirement that a type must occur in the BNC at least twice to count as established.
Since -ist is roughly equal to -olog-in terms of type frequency, let us choose this suffix for comparison.
Instead, we need to look at the type-token ratio and the hapax-token ratio.
In order to compare the two genres in terms of the type-token and hapaxtoken ratios, they need to have the same size.
The type of English used in Britain is quite different from the type of English used in the United States.
The type of English used in Britain or the United States also varies from region to region or among people from different socio-economic classes.
The type of English used 150 years ago in the United States is quite different from the type of English used in the United States today.
Spoken language does not generally have the same type of planning and opportunities for revision that we find in many types of written language.
In terms of how the information is conveyed, we see differences in the type-token ratio.
On the one hand, these kinds of studies are relatively easy to carry out without the necessary background knowledge for a more sophisticated type of study.
If a word is repeated, it counts as a new token but not as a new type.
To get those concordance lines for your sample, go to the main page of COCA, click on "chart" and type in "sa*_v" in the search box.
For example, if you type in the word baby, you will see that it occurs over 60,000 times in the COCA corpus.
Go to COCA, hit "Browse" and type in say in the word box.
These main POS categories identify the word as you type it into the search box.
The second type of voice in English is called the passive voice.
For example, in a sentence such as The boy is seen by the ghost, even though the boy is still in the subject position of the sentence, it is the ghost who is doing the seeing in this type of sentence.
We can call the first of these passive types, the long passive (a passive sentence that includes the "by phrase"), and the second type, the short passive (a passive sentence without the "by phrase").
For each type, determine whether there is a preference for the "by phrase".
Specific information such as the length of each text (by number of words), the topic, or the type of each text (if you included different types within a specific genre) can also be included in individual text files.
This type of information is not a part of the text analysis but it can provide important interpretive information used in the functional analysis of your results.
This type of information is included in the "headers" of the text but will not be read by the concordance software.
Also, the variable scale and type determines the types of statistical analyses that can be done.
That is, the use of first person pronouns will depend on the class size (lecture versus seminar-type).
Then, we characterize each observation for its features, which in this case would be syntactic position and type of article.
So it will be string, and it will be a nominal type of data (all strings are nominal).
Knowing all of this, you isolate this one pronoun type because you are interested in the use of first person pronouns (I, me, we, us).
Imagine that you would like to find out about the relationship between article type (a, an, the, zero article) and their position (subject or object).
H 0 -There is no relationship between type of article use and clause position.
In other words, if there were no relationship between the article type and the position, we would get an even distribution of the frequencies.
It means that we cannot reject the null hypothesis stating that there is no relationship between the article type and position.
In other words, any of the articles could pretty much randomly occur in any position, as there is no relationship between the position and the type of article used.
Instead of looking at the potential relationship between article type and syntactic position, the focus of the investigation would be to see whether one type of article, when in a certain position, occurs more often in one register over another, and versus in another position.
That is, if we did a difference type of test (e.g., like ANOVA), there would not be a significant difference between teacher and student presentations in terms of the use of "I mean".
That is, in a difference type of test (e.g., One-Way ANOVA), this would show a significant difference between teacher and student presentations in terms of "ok" use.
When writing a paper of this type, you may want to think about writing a small paper for each of these five sections and then putting these sections together for the final product.
The purpose of the brief description is to point you to a way forward if you become interested in this type of research.
In the next few paragraphs I will focus in turn on spoken, written, and web-based language sampling and examine compilation issues specific to each type.
One is concerned with the fact that words can theoretically have identical type and token frequencies, but may still be very differently distributed.
As for the latter problem, lexical gravity G (see Daudaravic Àáius and Marcinkevic Àáiene Àô2004) is an interesting attempt to include type frequencies of collocations in association measures.
One of the earliest corpus-driven studies of this type was Salem's (1987) analysis of repeated lexical phrases in a corpus of French government resolutions.
For example, a frame occurring 200 times with 25 distinct fillers would have a type-token ratio of .
Quite commonly one and the same verb takes different kinds of complement with different relative frequencies, such that one type is preferred and other ones are more marginal.
Its wide temporal coverage and large scope of lexical types make the OED an ideal basis for studies that investigate diachronic type frequency changes in phenomena such as the way-construction.
Each type in the database is annotated in terms of a time stamp and five variables that pertain to the form as well as the meaning of the ment-types in the database.
Conversely, the type nonattachment illustrates the prefixation of a bipartite ment-type, resulting in a right-branching structure.
Importantly, the appearance of a rightbranching type does not testify to the productivity of the suffix -ment, but rather to the productivity of the respective prefix.
A clear case of a transitive ment-type would be punishment; a clear case of an intransitive type is settlement.
While the configuration of abhorment does occur more often than expected in the third corpus period (o = 75, e = 52.4), this difference is not large enough to be statistically significant, so that the configuration of abhorment is not a type.
This type can be seen as the prototype of the early borrowings with which the word-formation process originated.
Both types represent borrowed forms that encode means, type 2 (o = 10, e = 0.8) with transitive verbal stems, type 3 with nominal stems (o = 3, e < 0.1).
Type 5 (o = 174, e = 121.9) represents the most common pattern overall: in this type the suffix combines with a complex verbal stem that encodes a transitive action.
The popularity of this type continues into the fourth period (o = 150, e = 93.3).
Type 9 (o = 25, e = 3.4) is identical to type 8.
Type 10 (o = 19, e = 1.7), a second right-branching type, is structurally identical, but encodes a result rather than an action, in formations such as malnourishment.
We are not deontologically justified in making statements about the relevance of a phenomenon observed to occur in one discourse type unless, where it is possible, we compare how the phenomenon behaves elsewhere.
The overall topic is a study of the discourse type of White House press briefings during the opening period of the Arab Uprisings.
In these studies, the vocatives in the datasets were categorized according to vocative type (for example, endearments, kin titles, etc.) and function (relational, summons, etc.) and, although not discussed here, position (initial, medial, or final).
In the 1980s, the Brown-type compilation model started spreading to other parts of the English-speaking world (India, Australia, and New Zealand).
The nonstandard indirect word order occurs both in wh-type questions (e.g. I wonder when are they coming) as well as in yes/no-type embedded questions (e.g. I wonder are they coming) in both ELFA and MICASE.
The top three interrogative words beginning a wh-embedded inversion are the same (in the same rank order) for both corpora: what (ELFA: 66% of all WH-embedded inversions, MICASE: 59%), how (ELFA: 15%, MICASE: 22%), and why (ELFA: 7%, MICASE: 10%), and for both speaker groups it is the cliticized what's that is especially closely associated with embedded inversions in the WH-type (what + BE is the most common wh-word + predicate combination in these embedded inversions, and in ELFA 22.6% of these are cliticized, in MICASE 29%).
Further exclusion criteria are needed for the purposes of a meta-analysis of this type; in particular, only experimental or quasi-experimental studies with a pre/post-test or a treatment/control group design, or both, can provide appropriate comparative data.
It is precisely this type of quantitative reporting that is likely to be consistent over many studies, thus lending itself to comparison and synthesis.
We use the generic term "book" on purpose as the titles we have selected are not homogeneous in type, with some being closer to reference grammars, some others to pedagogical grammars, while the last type deals with grammar integrated with other language skills (reading, writing, etc.).
We will then provide more details on exactly which type of information is presented in the corpus-informed books, but also how is it presented.
The most frequent singular noun consisting of two roots on the list is chairman, and other nouns of this type include football, newspaper, background, and bedroom.
Among the highest-ranking nouns of this type is lifespan, followed by words such as database, policeman and classroom.
The database contains potentially relevant information about text categories under two headings: Document type contains information about "the format, genre, or other characteristics of the document", and Publication section allows users to limit searches to a specific section of the publication.
Like its name suggests, the type-token ratio is the ratio of the number of different words in a text (types) to the number of all words in the text (tokens).
Our analyses rely on a particular type of linguistic data -a large, custombuilt corpus of tweets -as well as a recent computational approach to modeling lexical semantic phenomena -neural word embeddings.
In particular, a writing class is ideally suited to such study as the teacher could set out rules for the type of files that students submit and dictate the format that file names should take.
This would allow the teacher to tailor future lessons to the needs of the students as the corpus would help highlight any common or frequent errors and, hopefully, aid in discovering in why this type of error was made.
However, it would be quite difficult to create the type of student specific corpus mentioned above.
This is useful, for example, when a classification has been made on a scale of very negative, negative, neutral, positive, and very positive type values, but one may want to combine the results to return the corpus grouped into positive, neutral, and negative.
When analysing corpora, we also need to think about the type and format of the data that needs to be obtained from the corpus in order to answer the research question.
Let us now think about how the different notions of a 'word' (type, lemma and lexeme) influence the kind of analysis we can carry out in corpus linguistics.
However, while type is a very useful category, it may obscure some meaningful differences, e.g. uses of the form clean as an adjective (a clean shirt) versus as a verb (to clean something).
Finally, since word counts (mostly token and type counts) are a part of almost every statistical equation that is discussed in this book, it is important that you have a good grasp of these definitions.
The relative frequency can therefore be considered as the mean frequency ('mean' is a statistical term for a type of average; see Section 1.3)that is, the mean of the frequencies of the word in hypothetical samples of x tokens from the corpus, where x is the basis for normalization (in this case one million).
The reason for this is that relative frequency is used not only to compare frequencies of a particular type in two (or more) corpora, but it is also used to present evidence about the frequency of words and phrases in a form that is easier for a reader to grasp than the absolute frequency would be.
For example, imagine a very small corpus that contains 11,000 tokens, including the type homeostasis which appears only once in the corpus.
The simplest lexical diversity statistic is the type/token ratio (see Section 2.2 for the definition of types and tokens).
For the two texts from the 'Think about' task the type/token ratio is 0.8 (28/35) and 0.93 (28/30) respectively.
However, we have to remember that the type/token ratio is very sensitive to the length of the text; it decreases as the text becomes longer and more words get used again (recycled).
Simple type/token ratio (TTR) was used to compare the texts because they were of the same length (2,000 tokens).
Our options are type, lemma or lexeme.
There are different concepts of a 'word' -token, type, lemma and lexeme.
As the term suggests, it is a ratio of two probabilities from the crosstab table comparing the probability of a particular linguistic outcome (e.g. the definite article) occurring in one context type relative to the same outcome occurring in the other context type.
There was a significant association between the context type and article type (œá2 (1) = 85.25, p < .001).
Second, as in any type of quantitative analysis, the variables should be measured and coded accurately and consistently.
Because when studying lexico-grammatical variation, we usually presuppose a certain uniformity among texts of the same type, we don't have to worry about this problem too much apart from making sure that in our sample, linguistic features come from a range of texts.
It needs to be noted that in this section we have explored a type of logistic regression called binomial logistic regression, that is a logistic regression with an outcome variable with two categories like the and a/an.
These were the context type and length of the noun phrase.
We can also type out the p-values (although with large corpora with many files these are always very low) or specify the CIs.
For example, we use a different type of language when talking informally to friends than when we are asked to write a research report.
Again, variables from Group B share a communicative function, which can be labelled as 'academic-type description' with passives and many modified nouns and nominal forms ending in -tion, -ment, -ness and -ity.
Post-hoc tests are pair-wise comparisons of individual group means (similar to the t-test discussed above) with a correction for multiple testing; with multiple testing the probability of a falsely positive result (a so-called type I error) increases.
For instance, with four groups (as in our example), we can run six pair-wise comparisons, which will increase the type I error to 26.5%.
In this section, we take a look at a technique which is somewhat similar to factor analysis discussed in Chapter 5 (Section 5.4), but that has been primarily developed for the analysis of cross-tabulation tables with categorical data, the type of data which was discussed in Chapter 4 (Section 4.3).
A type of speaker (e.g. male or female, young or old) or a type of context (e.g. syntactic position) which favours the use of a particular variant of the sociolinguistic variable.
With mixed-effects models, it is important to report the type of mixed-effects model used as well as the details of the model.
The second part of the riddle was clear and matched the type of language in the sample.
Since the notion of word is a little ambiguous here, it is useful to introduce a common distinction between (word) type and (word) token.
In this parlance, a frequency list lists the types in one column and their token frequencies in the other; often you will find the expression type frequency referring to the number of different types attested in a corpus (or in a 'slot' such as a syntactically defined slot in a grammatical construction).
These observations are a subset of Zipf's laws, the most famous of which states that the frequency of any type is approximately proportional to its rank in a frequency list, and such a distribution is often referred to as a Zipfian distribution.
For instance, you could type this to load the package dplyr: library(dplyr) ¬∂.
Second, if you know the name of a function or construct and just need some information about it, type a question mark directly followed by the name of the function at the R prompt to access the help file for this function (e.g., ?help ¬∂, ?sqrt ¬∂, ?"[" ¬∂).
It is important to note that -unlike arrays in Perl -vectors can only store elements of one data type.
The double quotes around the 1 and 2 indicate that these are now understood as character strings, which also means you cannot use them for calculations anymore (unless you change their data type back using as.numeric).
As you saw above, the function table takes as an argument one or more vectors or factors and provides the token frequency of each type or each combination of types.
In LibreOffice Calc, you choose the Menu "File: Save As . . . " and choose "Text CSV (.csv)" from the "Save as type"/Formats menu.
Once you have understood and maybe fixed the instructions in the loop and want to execute it all in one go, then just type the real beginning of the loop (for‚Ä¢(i‚Ä¢in‚Ä¢1:3)‚Ä¢{ ¬∂) or copy and paste from the script file.
Sometimes, names such as qwe or asd will be used because they are superfast and virtually error-free to type on an American QWERTY keyboard, but in the long run, i.e., for your real research projects, names such as sentences.without.tags or cleaned.corpus.file are infinitely more revealing than the kind of names (e.g., qwe or aa) I have mostly used so far.
Finally, we will use sort and table to create our frequency list of the three-grams, quantile to explore their distribution, and plot with type="h" again to visualize the results.
Finally, we will use sort and table to create our frequency list of the three-grams, quantile to explore their distribution, and plot with type="h" again to visualize the results.
Let us begin by discussing how we compute type-token ratios and vocabulary-growth curves using a small vector tokens as a 'corpus', something that I always recommend to get started on a new project: Create a data set realistic enough in its make-up but small enough to be seen on one screen, and start developing your code with that.
It is a line plot that shows positions of all tokens of a vector on the x-axis (as in Section 5.1.2) and all type frequencies on the y-axis; in other words, each point's y-coordinate divided by each point's x-coordinate provides the point's type token ratio.
The x-coordinates of each point are obvious: It's just the numbers from 1 to 10, one for every element of tokens, which means that, given what we said above, the y-coordinates are then the type-token ratios of each token slot multiplied by the number of tokens of each slot.
For instance, note how the rightmost value is 7, i.e., the type-token ratio of the whole vector (0.7) times the length of the vector (10).
So far so good, but now the two lines that do the magic of creating the vector type.freqs we want: Remember that type.freqs only contains 0s so far; the first line now inserts a 1 into each slot that belongs to a new type in tokens, as you can see in the output.
After that we store the results we obtain by sapplying functions to the list to count lengths and compute type-token ratios (with an anonymous/inline function); we compute a mean type-token ratio using all types and tokens, and a mean of all lengths of utterances with mean, but to avoid the effect that outliers might have we use the argument trim=0.05 to discard both the smallest and the largest 5 percent of the data.
We use lines to add the type-token ratio plot and add smoothers with lines(lowess(...)).
For the comparison of the first and last recording, we just generate regular scatterplots of type-token ratios against mlus, but we jitter the points to avoid overplotting and add the usual grid.
Biber is not the only linguist to observe this (Halliday does this in a more theoretically-informed way, and Matthiessen, as noted, has added statistical rigour to that model), but Biber has carried out more extensive investigations of this type than anyone else.
We are able then to identify the type of research being reported in each, basically on stylistic grounds.
A token is a single instance of any word at a particular place in the corpus; a type is an individual word-form, which can occur many times in the corpus: thus one type is usually instantiated by multiple tokens.
So for instance, if we see on a frequency list for an English corpus that the is the most frequent word, occurring 60,000 times, then we could say that the single type the is represented in this corpus by 60,000 tokens. Frequency lists can also be compiled for any annotation that is present in the corpus, and we can talk about types and tokens of annotation tags in the same way.
Any given type will be more frequent in a bigger corpus; for that reason, corpus frequencies are often given in relative terms, as frequencies per thousand words or per million words.
In sum, there are compelling arguments to include type frequencies from theoretical considerations as well as from neighboring disciplines such as psycholinguistics or computational linguistics, and there are promising first results within corpus linguistics proper, but more exploration is definitely required.
In particular, all of the above approaches only deal with the minimal amount of information one should include-the more comprehensive information regarding token and type frequency distributions and entropies still awaits first exploration.
While much of that work has, of course, been successful because, for example, high token frequencies in b and c are positively correlated with high type frequencies, and high token frequencies in a are negatively correlated with clumpy distributions, it is unclear how potentially skewed the results are for cases where those correlations do not hold.
Even though the basics of computerizing spoken and written data are the same, depending on the type of data (speech or written), the methods used for computerizing differ.
Corpora are not defined in terms of specific properties of texts, so texts can be of any type and may exhibit a large range of diverse properties (cf. Chapter 3 on corpus composition).
The three occurrences of that's are counted as three tokens, but as one type.
Type frequencies are then lower than token frequencies, and the type/token ratio can reveal interesting insights into properties of texts, essentially telling us how lexically variable a text or corpus is.
However, by type frequency, bimorphemic and 3-morpheme word types are most frequent.
The median token frequency is 1 morpheme, the median type frequency is 2 morphemes.
One sub-type is a reference corpus: a general corpus taken to be a standard reference for corpus-based research on a given language.
Dynamic corpora grow over time as more and more texts are included (monitor corpora are a type of dynamic corpus).
They tend to look at predictors such as salience, accessibility, referential distance, and animacy to explain the choice of one type of reference over another.
In corpus linguistics, we worry about both type frequency and token frequency (cf. 2.2.3) that can tell us different things about our corpora.
The second type of treebank annotations encodes dependency relations.
A TAG comes from a closed and cross-linguistically fixed list of category choices and indicates the type of expression being used for the relevant instantiation of a particular functional category.
It is also on this level that clause boundaries are inserted that mark the beginning of clauses, represented by '##' or '#' , depending on the type of clause.
List 5 IVs and 5 DVs for each variable type.
This type of regression has subtypes such as logistic or multinomial and this distinction has to do with the distribution of the data and the type of DV, whether it is numeric or 2-way categorical or 2+-way categorical among other possibilities.
Needless to say, corpus linguistic studies of this sort cannot explain why languages opt for one or the other type of subject and object encoding, and the explanation for this lies in deep and intricate histories of language evolution with many stages of development, each a story worth telling in itself.
This is a context that can be assumed to trigger either the use of some form of pronoun or zero across languages, regardless of the overall content of the texts involved, and hence it will make sense to compare rates of pronoun versus zero within this 'envelop of variation' (Torres Cacoullos & Travis 2019 on comparisons along this type of context) (cf. Schnell & Schiborr to appear for a case study based on Multi-CAST).
We'll talk more about this type of issue in Section 4.4.1, where we'll explore ways of dealing with such features of the text in order to retain relatively 'clean' data.
This, however, isn't the only type of consideration necessary, but we also need to bear in mind ethical issues involved in the collection -such as asking people for permission to record them or to publish their recordings, etc.and which type of format that data should be stored in so as to be most useful to us, and potentially also other researchers.
Please bear in mind, though, that for the former type of exercise, simply following the steps blindly without trying to understand why you're doing them will not allow you to learn properly.
We can refer to the former type as snapshot corpora, whereas the common term for the latter is monitor corpora.
One additional highly important attribute of the corpus is the fact that it contains a very large amount of meta-information, i.e. information about who produced which type of document(s) and in which contexts.
However, over the years, researchers have realised that even this type of size and stratification may not be sufficient for performing certain tasks -such as research in lexicography or collocations (see Chapter 10) -efficiently, and hence started devising ways of creating a variety of different types of corpora, representative of both spoken and written language, and of varying sizes, ranging from very small specialised corpora to some that comprise many millions of words.
Another type of meta-information is represented by a table of contents (in the front matter) or an index (in the back matter) of a scholarly book, where the meta-information serves as a kind of navigational aid in accessing individual parts of the book, and where the information is clearly linked to the content -or organisation thereof -itself.
For the moment, simply try to identify the 'head' and 'body' sections of the page and see whether you can possibly also understand which type of meta-information the different parts of the header may relate to.
Among the paragraphs, we also encounter one special type, that of the heading, which acts as a kind of guide to the particular chapter or (sub-)section the individual paragraphs occur in.
The problem we have with encodings is one of a similar nature, only that in this case it doesn't have anything to do with proprietary formats, but rather the way that individual characters are physically represented inside the computer, which is as a special type of number.
Due to this fact, if you simply try to open these files by (double-)clicking on them, you'll usually get some form of message saying that your operating system doesn't recognise this particular file type, and asking you to identify a program to open the file with.
Header elements may for instance be the page title (contained in the <title>‚Ä¶</title> tag, where the forward slash indicates the end of the tag in the closing part), which then appears in the title bar of your browser, or possibly meta-information (<meta>‚Ä¶</meta>), such as the text (content) type or encoding/character set (charset), as well as style (sheet) information/references (<style>‚Ä¶</style> or <link rel="stylesheet" href="./corpus.css" type="text/css" />, in our case) responsible for some of the page formatting.
This obviously requires careful consideration and also to develop at least some initial hypotheses about what you'll encounter in which type(s) of data, which techniques to apply, etc.
These texts were/are essentially created by volunteers who scan or type in the materials, thus converting them into an easily readable electronic format, without any special formatting or layout.
The quality of this type of export may vary from browser to browser, though.
Under 'Save as type:', select 'Web Page, HTML only' or 'Webpage, HTML only" for the type.
The first thing you obviously need to consider is what type of spoken data you may want to analyse.
As mentioned earlier, this type of meta-information definitely doesn't represent any textual content that we want to keep and analyse, so let's practise identifying and removing this.
There are numerous compression formats, but the most common one is .zip, for which most operating systems not only provide direct support, but also generally have options to create and manage this type of archive from within whichever graphical file manager they offer.
In the search box, type in the word, round as indicated in the following graphic.
This type of basic initial coding needn't be very complex, but can consist of a few easily identifiable markers that you insert somewhere in a text.
Of course, adding this type of information can be quite time-consuming, so, in later sections, we'll explore ways of adding similar types of information automatically, as well as looking into ways in which we can exploit such annotated data even more extensively in order to search for and identify more complex linguistic patterns.
Just seeing the results may also not yet allow you to see the usefulness of creating/using this type of class, but this will soon become clearer when we introduce quantification.
To do this, simply type the initial phrase into an empty document and then search for (rarely) (have) and replace this by \2 \1, making sure that the option for regex replacements is switched on.
Examples of type a) allow us to work with different dialectal variants, or sometimes also to cover spelling variants in historical texts, where the spelling may not yet have been standardised, so that even one and the same text may contain alternate forms that represent exactly the same word meaning.
As a sub-type of this query option, there are also two highly useful menu options for querying in only spoken or written texts.
This type of display may already be highly useful for identifying most of the potential for grammatical and semantic polysemy, but the maximum number may not allow you to extract enough samples in cases where a search term has a high degree of polysemy on both levels.
Later on, depending on how much information you might need about a text, you can also output as much meta-information as required (or is actually available), in case you need to determine and/or distinguish which particular type of texts the results come from.
As we've already seen above, there are quite a few cases where we may have difficulty in assigning multiple words to one single type, but it isn't only for multi-word units that we encounter such problems.
Even for single units that are clearly delimited by spaces or punctuation, we may end up having problems assigning them to one and the same type because of such issues as polysemy discussed above, but also alternative spellings (colour vs. color) due to dialectal or historical variants, typos (teh instead of the), or capitalisation/non-capitalisation.
A similar thing can be done by expanding abbreviations to their full form, but with both lemmatisation and expansion, one always has to bear in mind that the individual forms may potentially also cause a change in meaning of the whole textual unit in which they occur, or may have been used deliberately as a (stylistic) feature of a particular type of genre.
The most useful output format in many areas of corpus linguistics for such a list is generally to have a list that is first sorted according to the frequency in descending order, that is, with the most common words occurring first, and then 'alphabetically' if the same number of tokens occurs for more than one type.
If we're working in the area of lexicography, though, and are trying to create a comprehensive dictionary of a particular type of (sub-)language, we may well want to start with an alphabetically sorted list first, and then investigate the individual types according to their specific features that would allow us to classify and describe them optimally.
Just as with auxiliaries and pronouns, we also ought to be very careful when eliminating prepositions and conjunctions from our frequency lists because they may equally tell us something about the domain or genre of a particular text. Imagine, for example, a text from the domain of finance about developments on the stock market, where certain values rise above/to or fall below/down to certain thresholds, etc., where the verbs on their own may not give us enough grounds to distinguish the type of domain, just like the verbs that form part of phrasal/prepositional verb combinations are often semantically relatively empty.
Unfortunately, when using the latter option, you have to type and add each word individually, so it's often best to prepare a list beforehand and then add to this manually if you find that it doesn't filter the list enough yet.
Paste the data from the general subcorpus into the cells below rank_g, 'type', and freq_g, and the other data into the corresponding rows rank_n, type_n, and freq_n, for now leaving the cells in between the sets empty.
For the general set, you should sort according to type, and for the newspaper data, according to type_n.
Once you've pasted a total, click in the box immediately above cell A1 of the spreadsheet and type n_general and n_newspapers, respectively, followed by pressing the Enter key.
Transfer the type that only exists in one list into that row, making sure that the rank is also moved to the appropriate place for the list it occurs in, and setting the rank and frequency in the list where the type's missing to 0.
Next, place the cursor in the first empty cell in the rel_g column, 3 across from the word about, type =, then click in the cell below freq_g that contains the frequency for this word, type /, and click on the number that contains the total for the tokens in the general corpus (Excel), or type n_general (Calc), then press the enter key.
To verify the results for any particular type in either subcorpus, you can use the hyperlinks in the columns labelled 'TOKENS 1' and 'TOKENS 2', respectively to have KWIC concordances displayed in the frame in the bottom half.
The initially perhaps odd-seeming type sil is further proof of this, as it represents an abbreviation for 'silence', that is, a pause of undefined length, while utt represents markup for an utterance.
Another reason for why the number of tokens has risen here is that the hyphen in fact appears to be ill-defined; as in most instances in this particular type of data, it isn't actually a true hyphen as it would appear in hyphenated words or at the end of a line that has been hyphenated, which would not occur in this type of data, anyway.
Both of these have their individual sections for restrictions further down the page, to allow you to narrow down your choices, based on domain information for the context-governed type, and age, social class, and sex of respondents -not the speakers, whose characteristics can be selected separately -for the demographically sampled data.
Therefore, the list produced by BNCweb actually also contains one redundant column, depending on whichever list wasn't selected, and where all the type frequencies are set to 0.
To transfer and align the data is probably the most difficult and time-consuming part of the exercise because it's easy enough to make mistakes when creating new rows and moving the data around, as well as adding the noughts to the relevant cells where a type doesn't exist in one of the corpora.
Provided that you type the formulae into the formula bar correctly and select the right cells, calculating all the relative frequencies accurately and efficiently should also not present any problems, although, if you haven't used spreadsheets extensively, filling cells by dragging may take some getting used to.
The occurrence of v. demonstrates that parts of the general subcorpus contain references to legal matters, where the type is one of the two possible abbreviations for versus (the other being vs.).
The most striking difference in type frequency for types occurring in both subcorpora is that for upon, which occurs more than 10 times as frequently in the general corpus (ratio 10.604).
One particularly interesting type in the table in this context is wilt, which, on the surface, would appear to be an archaic form of WILL, so we'd expect to find more occurrences of it in the humanities texts.
Upon closer inspection, however, it turns out that this type in most instances in both corpora in fact represents a typo, that is, the misspelt form of will, or in the science writing a mis-categorisation of the nominalised form of the verb wilt in two cases.
We'll discuss the other 'type' in more detail in Section 10.8.
In the second, we can then construct one or more appropriate search patterns that'll allow us to create concordances that illustrate which word classes co-occur with which type of construction.
In another type, such combinations generally create a complex meaning, as in, for example, up until or down below, where the resulting meaning is a mix of the semantic properties of both elements.
Uncheck the box for 'N-Grams', and type in fair as your search term.
Select the 'Collocates' tab, type in fair as your search term and set the 'Min. Collocate Frequency' option to 2, in order to avoid one-off constructions, also known as singletons or hapax legomena.
Thus, in order to investigate the collocates of fair, you simply type fair in the box next to 'WORD(S)', and then click on 'COL-LOCATES'.
Before we can take any detailed look at the technical aspects of such an advanced enrichment process and the best type(s) of format for this, though, we still need to deal with a few terminological distinctions, and try to understand the historical developments and motivation underlying the different formats.
This first official type of markup language, however, as we'll see further below, was relatively complex and also had a number of serious drawbacks.
The third type usually either specifies formatting instructions, such as line breaks, etc., contains links to external resources, such as a style sheet that specifies the layout and formatting options, or can be used to include other information that doesn't require a containing element, such as, for example, a comment on a specific piece of data.
Go through the above sample paragraph and make a list of how many elements there are and which type they belong to.
SGML also has two other major disadvantages compared to HTML or XML, the first being that it absolutely requires a DTD (Document Type Definition) specifying its structure, in order to allow any type of serious processing, and -last but not least -that it's not supported by any 'standard' (browser) software.
As this is a kind of formatting that we'll equally want to apply to all such units, it would be cumbersome to have to type this out repeatedly, so we can take advantage of a feature of CSS that allows us to list the different elements a definition applies to by separating them from each other by a comma, just like in an ordinary written list.
For yes and no answers, we can employ a similar type of traffic signal analogy and encode them like one-way street signs.
In terms of attributes, you should be able to find 27, where 'n' represents numerical identifiers for all the 14 textual elements, 'pos' the 12 PoS categories for the words, and 'type' which type of punctuation is present at the end of the syntactic unit.
However, as XML forbids the use of overlapping tags, and overlap passages span across different speaker turns, using <overlap>‚Ä¶</overlap> elements would break the document's well-formedness, so it's best to use the empty tags <overlap type="start" /> and <overlap type="end" />, possibly in combination with a number ('n') attribute.
The type of value assigned to any given variable depends on its meaning.
One type selects a subset of the more important data variables and eliminates the remainder from the analysis, using some definition of importance.
The other type abstracts a new and usually much smaller set of variables on the basis of the existing ones.
One type selects a subset of the more important data variables and eliminates the remainder from the analysis, using some definition of importance.
The other type abstracts a new and usually much smaller set of variables on the basis of the existing ones.
For example, data for a study of student performance at university might include variables like personality type, degree of motivation, score on intelligence tests, scholastic record, family background, class, ethnicity, age, and health.
For some of these there is self-evident redundancy: between personality type and motivation, say, or between scholastic record and family background, where support for learning at home is reflected in performance in school.
In the foregoing example, the researcher-defined variables personality type, motivation, scholastic record, and score on intelligence tests might be replaced by a 'general intelligence' variable based on similarity of variability among these variables in the data, and family background, class, ethnicity, age, and health with a 'social profile' one, thereby reducing data dimensionality from nine to two.
TheSOM algorithm implements a particular type, the Voronoi tesselation , in which a reference vector is the centroid, and the area of k associated vectors surrounding the centroid is a neighbourhood; the neighbourhood of a given reference vector v r in a Voronoi tesselation is defined as the set of vectors closer to v r than to any other reference vector (ibid.: 59f.).
The primary clustering of the DECTE speakers therefore has a clear sociolinguistic interpretation based on educational level and employment type.
The indication is that the primary differentiating factor among the Gateshead speakers is gender, though the existence of cluster B suggests that educational level and type of employment are also factors.
In this paper, we present the development of a webbased corpus from Twitter posts, named ILiAD: An Interactive Corpus for Linguistic Annotated Data.
By doing this, we can have a better view of the multilayered nature of the corpus.
This can give users the opportunity to explore the corpus from different angles and linguistic perspectives.
From the collected data, we applied six filters to make sure that the corpus reflects comparable linguistic data for all account users.
Keeping quote tweets in the data would add repeated tweets to the corpus and also would add patterns and word counts that do not correspond to a specified account.
The motivation was to prepare it for the linguistic analysis within the corpus.
An important observation is that removing stop words is a compromise for the corpus, since certain word combinations are affected, especially those which appear together with the words in the list.
Future versions of this work aim to efficiently implement analysis considering the role of stop words in the corpus.
A second group of NLP techniques implemented is the identification of entities in the corpus, and that includes mentions of people, physical locations, and established organisations.
The motivation is to be able to contextualise the information in the corpus within the overall world of social media.
The exploration can be done in different levels: all corpus or by user type (news agencies or individuals).
An example for the organizations mentioned in the corpus is shown in the figure below.
The app presents a wide range of visualizations and analyses from the Twitter corpus.
With the inclusion of Twitter metrics, this tool gives all exploration opportunities to understand the whole corpus.
R and shiny R have proven to be an efficient combination to develop and deploy the corpus.
In this paper, we have presented the development of a linguistic corpus based on the Twitter posts.
The tool also gives users interactive and reactive power throughout all the data, which not only offers a corpus to analyse, but a corpus to interact with and query in a more organic way, compared to more traditional approaches of presenting corpora.
In the current version, we have selected a relatively small number of users in the corpus, as compared to other larger projects with similar goals.
In corpus linguistics, we are almost always dealing with nominal data.
In the context of corpus linguistics, the most obvious role for the Fisher exact test is to measure dependencies between collocations, or in the case of Stefanowitsch and Gries, dependencies between words and constructions.
Note that it is not given that the results of a Fisher exact test can be extended beyond the corpus, due to the mathematical assumptions it is based on.
This handout is primarily directed towards corpus linguistics, but as mentioned in section 5 above, we sometimes deal with ordinal data in linguistics, typically in the context of an experimental or sociolinguistic study.
In the context of corpus linguistics, their use is somewhat questionable, and the reader should be aware that it is regrettably not unusual to find these tests employed in ways which do not fit well with their assumptions.
The interpretation of the Phi/Cram√©r V should differ in a corpus based syntax study, an experimental situation, or the evaluation of a sociolinguistic survey.
The tasks of corpus linguists are manifold and complex.
This handbook aims to address all these areas with contributions by many of their leading experts, to be a comprehensive practical resource for junior and more v vi Introduction senior corpus linguists, and to represent the whole research cycle from corpus creation, method, and analyses to reporting results for publication.
Parts IV-VI aim to offer a user-friendly introduction to the variety of statistical techniques that have been used, or have started to be used, more extensively in corpus linguistics.
It is important to note that the chapters on mixed effects regression modeling and generalized additive mixed models in particular are primarily meant for readers to get a grasp of what these techniques are (more and more corpus linguistic papers rely on such methods and it is important for corpus linguists to understand the current literature).
Part VI aims to pull everything together by providing guidelines for how to write a corpus linguistic paper and how to meta-analyze corpus linguistic research.
This chapter covers the basics of compiling linguistic material in the form of a corpus.
One of the ways of selecting material for a corpus is by stratified sampling, where the hierarchical structure (or 'strata') of the population is determined in advance.
For example, a researcher who is interested in spoken workplace discourse could document demographic information about speakers' job titles and ages and whether interactions involve peers or managers/subordinates, and then include in the corpus a predetermined proportion of texts from each category.
There is a range of possible considerations to take in deciding about sampling procedures for a corpus, one of which concerns to what extent to organize the overall design around text production or text reception.
A concept that is intrinsically related to representativeness is balance, which has to do with the proportions of the different samples included in the corpus.
Indeed, this needs addressing before it is possible to determine fully the design of a corpus.
If spoken material is to be included in the corpus, it needs to be transcribed, that is, rendered in written form to be searchable by computer.
As a consequence, it has become popular among corpus builders to include material from online sources (see Chap. 15), which represent a great variety of genres, ranging from research articles to blogs.
This is no small task, but it tends to be underestimated by beginner corpus compilers.
Before any material can be included in the corpus, however, each transcript needs to be checked against the recorded episode to ensure that the transcription is not only correct, but also sufficiently detailed for the specific research purposes.
With less data-a smaller sample-the claims one is able to make based on one's corpus findings will be more modest.
But most importantly, as discussed above, the issue of representativeness needs to be addressed before a corpus, regardless of size, can be considered appropriate for a given study.
To a certain extent, restrictions on copyright may be alleviated through concepts such as 'fair use', as texts in a corpus are typically used for research or teaching purposes only, with no bearing on the market.
A second step may be to manipulate the actual linguistic data (that is, what the A. √Ñdel people represented in the corpus said or wrote) by changing also names and places mentioned which could in some way give away the source. In the case of image data, this would involve masking participants' identity in various ways.
Even if the corpus compilers are deeply familiar with the material, it is still the case that memory is both short and fallible, so if they want to use the corpus in a few years' time, important details of the specific context of the data may well have been forgotten.
In addition, if the corpus is made available to others, they need to know what is in it in order to make an informed decision about whether the design of the corpus is appropriate for answering their specific research questions.
Anybody who wants the claims made based on a corpus to be accepted by the research community needs to show in some way that the corpus material is appropriate for the type of research done.
With incomplete description of the corpus, people will be left wondering whether the material was in fact valid for the study.
Some large corpus projects intended to attract large numbers of users, such as the British National Corpus (BNC) and the Michigan Corpus of Academic Spoken English (MICASE), provide relatively detailed reports online.
Another reason for documenting what is in the corpus is to enable researchers to draw on various variables in a systematic way when analyzing data from the corpus.
In a study of that-complementation in English, for each hit in the corpus, the researchers considered external variables such as the L1 of the speaker who had produced the hit and whether the hit came from a written or spoken mode.
There is a great deal to be said about how best to format corpus material, but this section will merely offer a few hints on technicalities.
Those who are reasonably computer literate and whose corpus needs are relatively simple are likely to be able to do all the formatting themselves.
However, those who wish to compile a corpus involving complex types of information or do advanced types of (semi-)automatic corpus searches would be helped by collaborating with a computational linguist or computer programmer (see Chap. 9).
A plain text format (such as .txt) is often used for corpus files.
MS Word formats are avoided, as these add various types of information to the file and do not work with corpus tools such as concordance programs.
Markup allows the corpus builder to include important information about each file in the corpus.
There are many considerations for formatting corpus material in ways that follow current standards and best practice.
A corpus can be designed to be the key material for many different research projects for a long time to come, or it can be created with a single project in mind, with no concrete plan to make it available to others.
If, say for various reasons related to copyright, it is not possible to make the complete set of corpus files available to others, the corpus could still be made searchable online and concordance lines from the corpus be shown.
Another consideration in sharing corpus resources involves how to make these accessible to others and how to preserve digital data.
The easiest option is to find an archive for the corpus, such as The Oxford Text Archive or CLARIN.
In order for the comparison to be valid, however, the two sets ((sub-)corpus A and (sub-)corpus B) need to be maximally comparable with regard to all or most factors, except for the one being contrasted.
The authors hope that the use of the corpus "will advance the linguistic theory of narrative as a primary mode of everyday spoken interaction" (315).
The corpus comprises selected extracts of narratives, 153 in all, for a total of around 150,000 words, taken from the demographically sampled 'casual conversations' section of the BNC, which is balanced by sex, age group, region and social class, and which totals approximately 4.5 million words.
This example is somewhat unusual in that the authors do not collect the data themselves, but instead use a selection of data from an existing corpus.
In the article, they describe (i) the extraction techniques, (ii) selection criteria and (iii) sampling methods used in constructing the corpus.
The corpus builders have thus augmented the existing data by adding various types of information-about the speakers (sex, age, social class, region of origin, educational background), about the text (type of narrative; whether a stand-alone story or part of a 'narrative chain') and about the utterance (the roles of the participants vis√†-vis the narration; type of quotative verb used to signal who said what in a narrative; to what degree the discourse is represented as being verbatim or more or less indirect).
This type of information is highly useful, as it not only makes it possible to study and compare different types of narrative, but it also shows how the corpus is balanced (or not) with respect to type of narrative.
At the time of creation, the NC was the first corpus of conversational narratives to be annotated, so there was no established practice to follow regarding what analytical categories to annotate.
It represents information of a kind that many corpus creators have expressed an interest in, but which few corpus projects have included (see Chap. 16 for more information).
In order to promote and make better use of corpus enrichment, there is a need for collaborative work between linguists with a deep knowledge of the needs to different areas such as Second Language Acquisition or Historical Linguistics and experts in Computational Linguistics or Natural Language Processing.
The reality of contemporary corpus linguistics is that the corpora we rely on, in most cases, are simply too large for manually adding annotation, and the automated annotation of electronic texts has become the primary focus in the development of annotation methods.
While the raw text of an unannotated corpus has its own unique value, (wisely) annotated corpora offer great advantages over the raw text when it comes to the investigation of linguistic phenomena.
A corpus that has been annotated with the needs of linguists in mind can greatly facilitate the exploration of such phenomena by reducing the time and effort involved.
Even if an automatically annotated corpus is unlikely to meet all the expectations of a researcher in terms of its categories of annotation, it can still be an invaluable resource.
These tags, of which there are many, are assigned as part of an "idiom tagging" step after the initial assignment of POS tags and involve matching the sequences of POS-tagged words in the corpus against templates of multiword sequences.
In lemmatization, each orthographic word encountered in a corpus is assigned a lemma, or 'base form', which provides a level of abstraction from any inflection that might appear in the original orthographic word.
In corpus linguistic studies, lemmas can be particularly useful in making complex searches more tractable, especially when a single word appears in many forms.
The choice of lemmatization software often depends on the kinds of language found in the corpus materials.
Lemmatization can be an extremely useful tool in the corpus builder's toolkit, especially when searches of the annotated corpus may need to be run over many inflected forms.
When annotating a corpus, we may also be interested in adding information about the relationships that exist between elements in our texts above the level of the individual word that may help shed light on larger patterns in our corpus.
Another kind of higher-level annotation is syntactic parsing, which aims to provide information about the grammatical structure of sentences in a corpus.
While syntactic structures can be assigned to sentences manually (and may be necessary when developing a corpus for a language for which few syntacticallyannotated corpus resources have already been developed; see Sect. 2.3), it is more common for syntactic annotations to be added automatically by a syntactic parser, a program that provides information about different kinds of syntactic relationships that exist between words in a given text (parses).
These probabilistic parsers have become increasingly common in corpus and computational linguistics, and generally work quite well for annotating arbitrary corpus texts in many languages.
By tracing these relationships, it is possible to extract features of corpus sentences-subjects of active and passive verbs, objects of prepositions, etc.-that would be difficult to retrieve from searches of unannotated text alone.
At the outset, this involves making decisions as to what kinds of annotations should be added, what conventions should be followed for representing that information consistently, and what tools will be used to apply those conventions to corpus source materials.
As mentioned in Sect. 2.2.1, this is a common task in corpus development, and one on which other forms of linguistic annotation (e.g., lemmatization, syntactic annotation) often rely.
In the most straightforward case, it may be possible to use existing annotation software to automatically apply a given set of POS tagging conventions to corpus source materials.
While this discussion has assumed that appropriate tagsets and tagging models exist for the language or variety represented in your corpus, in many cases, existing sets of POS categories may not be suitable for the data, or may not exist at all.
In situations like these, it is possible to train some kinds of annotation software to apply new tagging conventions to your corpus sources.
This involves manually annotating samples of corpus text as examples of how the new annotation categories should be assigned.
Typically, these measures take into account frequencies in the whole corpus.
While annotation facilitates linguistic research and enables more immediate access to certain kinds of patterns in a corpus, one should acknowledge the potential for valuable linguistic research to be carried out even on unannotated corpora (cf. Chap. 8).
The architecture chosen for a certain corpus refers to the conceptual division of different types of objects contained in a corpus, such as texts, annotations and metadata.
This chapter presents some of the key 50 A. Zeldes characteristics distinguishing different corpus architectures.
The focus is on abstract data models and the ways in which they are realized in concrete formats for corpus representation, as well as consequences for the usability of the resulting corpora.
The fundamentals of corpus architecture begin with conceptualizing corpora as collections of documents, possibly arranged in subcorpora, and often carrying metadata.
To do justice to examples such as (5), a corpus architecture must be capable of mapping word forms and annotations to any number of tokens, in the sense of minimal units.
Often these nodes and edges will be annotated with labels, which usually have a category name and a value (e.g. POS=NOUN); in some complex architectures, annotations can potentially include more complex data types, such as hierarchical feature structures, in which annotations can contain not only simple values, but also further nested annotations (see ISO 24612 for a standardized representation for such structures in corpus markup).
However, as the corpus architecture grows more complex or 'multilayered', the pressure to separate annotations into different files and/or more complex formats grows.
The corpus, which is expanded every year and currently contains over 129,000 tokens, is collected from eight open access sources: Wikinews news reports, biographies, fiction, reddit forum discussions, Wikimedia interviews, wikiHow how-to guides and Wikivoyage travel guides.
The complexity of the corpus architecture results from its annotations: as the data is collected, student annotators iteratively apply a large number of annotation schemes to their data using different formats and tools, including document structure in TEI XML, POS tagging, syntactic parsing, entity and coreference annotations and discourse parses in Rhetorical Structure Theory.
As a result, it is at least technically possible for this corpus to have conflicting constituent trees and sentence span borders for sentence type annotation.
If these layers are generated by separate automatic or manual annotation tools, then such conflicts are in fact likely to occur over the course of the corpus.
One consequence of using a TH layer for the architecture of the corpus is that the data may now in effect have two conflicting tokenizations: on the 'learner' layer, the first '?' and the second 'Da' stand at adjacent token positions; on the TH1 layer, they do not.
In practice, a large part of the choice of corpus architecture is often dictated by the annotation tools that researchers wish to use, and the properties of their representation formats.
An important set of tools influencing the choice of corpus architecture is NLP pipelines and APIs, which allow users to construct automatically tagged and parsed representations with complex data models (and these can be manually corrected if needed).
Finally, corpus architecture considerations also interact with the choice of search and visualization facilities that one intends to use.
While this approach is very versatile, it is also labor intensive: for each new type of information, a new script must be written which traverses the corpus in search of some information.
This can be especially tricky, as there is often some variation in how these items appear across a corpus (or even within a text); for example, although the hyphenated form, well-being, is clearly the most common option found in the Corpus of Contemporary American English (COCA) (>8000 occurrences), the forms well being and wellbeing each occur several hundred times.
This point can be illustrated by considering two different content words, each occurring 42 times (per 100,000 words) in the corpus of Donald Trump's campaign speeches: military (n.), and Virginia (n.).
While frequency appears to suggest some sort of parity in salience, these occurrences had very different distributions throughout the corpus.
Results of this analysis demonstrate that the list performs considerably better coverage-wise in academic writing than in other written genres, in both in the academic portion of the corpus from which the list was constructed (13.8%), and a different academic sub-corpus from the BNC (13.7%) (compared to 8.0% for newspaper articles and 3.4% for fiction).
The authors then compared the frequency lists culled from each corpus to identify an overlapping, "stable" core of general service words-a list not demonstrating bias toward any of the four corpora, and thus exhibiting stability across time.
Such disambiguation remains both a theoretical and practical challenge for corpus-based frequency list research.
Imagine, finally, the frequency range he is currently interested in is between 35 and 40 words per million words and, as he browses the frequency list for good words to use, he comes across an adjective and a verbenormous and staining -that he thinks he can use because they both occur 37 times in the Brown corpus (and are even equally long) so he notes them down for later use and goes on.
However, chances are that this choice of words is highly problematic: While both words are equally long and equally frequent in one and the same corpus, they could hardly be more different with regard to the topic of this chapter, their dispersion, which probably makes them useless for the above-mentioned hypothetical purposes, controlled experimentation, vocabulary testing, or vocabulary lists.
Also, any kind of more advanced corpus statistic -for instance, association measures (see Chap. 7) or key words statistics (see Chap. 6) is ultimately based on the observation of, and computations based upon, such frequencies.
The area of research/application where dispersion has gained most ground is that of corpus-based dictionaries and vocabulary lists.
The first one explores D-values of a set of words in the British National Corpus, which, for the purpose of testing what effect the numbers of corpus parts n one assumes, was divided into n = 10, 1000, and 1000 equal-sized parts; crucially, the words explored were words for which theoretical considerations would lead an analysis to expect fairly different D-values, contrasting words such as at, all, or time (which should be distributed fairly evenly) with words such as erm, ah, and urgh (which, given their preponderance in spoken data, should be distributed fairly unevenly).
Their main conclusion of the first case study is that "D values based on 1,000 corpus parts completely fail to discriminate among words with uniform versus skewed distributions in naturalistic data" (p. 454).
In their second case study, Biber et al. created different data sets with, therefore, known distributions of target words across different numbers of corpus parts, but the bottom line of this more controlled case study is in fact the same as that of the first.
Their maybe most extreme, and thus worrying, result is that the exact same distribution of a target word -a uniform distribution across 10% of a corpus -can result in a D value of 0.0 when the computation is based on a corpus split into 10 parts, versus a D value of 0.905 when the computation is based on a corpus split into 1000 parts.
For example, it is more likely that a word will occur in 6 out of 10 corpus parts than for that same word to occur in 600 out of 1000 corpus parts.
The values for 1-DP seem to reflect this fact, resulting in consistently lower values when computations are based on a large number of corpus parts.
In summary, DP is clearly more effective than D at discriminating between uniform versus skewed distributions in a corpus, especially when it is computed based on a large number of corpus-parts.
Another is how dispersion measures relate to issues outside of corpus linguistics such as, again, psycholinguisticallyor cognitively-informed approaches.
For several decades now, corpus linguists have discussed dozens of association measures that are used to rank-order, for instance, collocations by the attraction of their constituent words.
Also, even just in the sixth frequency band, the extreme range values that are observed are 85 / 905 = 9.4% vs. 733 / 905 = 81% of the corpus files, i.e. huge differences between words that in a less careful study that ignores dispersion would simply be considered 'similar in frequency'.
This is obvious from the visual scatter in both plots, but also just from simple math: If a researcher reports an adjusted frequency of 35 for a word, one does not know whether that word occurs 35 perfectly evenly distributed times in the corpus (i.e., frequency = 35 and, say, Juilland's D = 1) or whether it occurs 350 very unevenly distributed times in the corpus (i.e., frequency = 350 and, say, Juilland's D = 0.1).
Second, most existing dispersion measures require a division of the corpus into parts and the decision of how to do this is not trivial.
While ready-made corpus tools such as WordSmith Tools or AntConc might assume for the user that the corpus parts to be used are the n (a user-defined number) equally-sized parts a corpus can be divided into or the separate files of the corpus, this may actually not be what is required for a certain study if, for instance, sub-divisions in files are to be considered as well (as might be useful for some files in the BNC) or when groupings of files into (sub-)registers are what is of interest.
To mention a few concrete examples, WordSmith Tools offers a dispersion plot as well as range and Juilland's D-values (without explicitly stating that that is in fact the statistic that is provided) while AntConc offers a version of a dispersion plot separately for each file of a corpus, which is often not what one needs.
Beyond the world of the corpus linguistic researcher, frequency lists can be used directly or indirectly to support language learners by providing a way to focus on the more frequent words in a text, or suggesting priorities for language teachers when preparing lesson materials.
For example, if frequency counts are derived from a large representative corpus such as the British National Corpus (BNC), we may reasonably claim that high frequency words in the corpus may also have a similarly high usage in the language.
Hence, as described in Chap. 4, and more particularly in Chap. 5, we need to pay careful attention to dispersion or range measures, which can help us estimate, for instance, how widely represented a word is across the various texts, domains, or genres within a written corpus, or across speakers within a spoken corpus.
As a first step, two frequency sorted word lists are prepared, one from the corpus being studied (the 'target') and one from a reference corpus.
However, depending on the research question and aims, a suitable comparison set may also be used, e.g. from a corpus representing a different variety, time, or genre.
Rather than using two different corpora entirely, some researchers use various subcorpora from the same (reference) corpus for the 'target' and 'reference' sets, and this approach is further exemplified in the two representative studies summarised below.
The resulting sorted list contains two kinds of keyword: positive (those which are unusually frequent in the target corpus relative to the reference corpus) and negative (words which are unusually infrequent in the target corpus).
Posts were inspected to extract only those written by people with cancer (as opposed to family members, carers, or those experiencing symptoms that may be associated with cancer), which resulted in a final corpus comprising 12,757 posts and 1,629,370 words.
Often, stylistic, grammatical, or syntactical features of the target corpus are highlighted through keyness comparison with a general reference corpus.
Qualitative analysis of the interview corpus also supported findings from previous studies: women were more likely to claim to seek social support on the Internet, whereas men said that they use the Internet to look for information.
The use of WordSmith's 'keyness measure' was used to rank results, with the 'top 300' skimmed from each corpus for further analysis.
In small corpora (such as the Romeo and Juliet corpus), many content words of interest would necessarily be low-frequency.
In this chapter so far, we have described the origins and motivations for the development of the keywords method in corpus linguistics, and shown two representative studies using the technique.
Usually, corpus software tools tokenise words by identifying boundaries with white space characters and removing any punctuation characters from the start and end of words.
Most corpus tools avoid the capitalisation issue completely by forcing all characters to upper-or lowercase, but this can cause issues with different meanings, e.g. "Polish" versus "polish".
If this were not identified in advance as a semantically meaningful chunk meaning 'to ridicule or parody', then separate word counts for 'send' and 'up' would be observed and merged with the other occurrences of those words in the corpus, potentially incorrectly inflating their frequencies.
For these 6 Analysing Keyword Lists 133 languages, it might be the case that comparing surface forms of words from the corpus works less well than comparing lemmas, because the frequencies are too widely dispersed across different word forms (in a similar way to historical spelling variants) to be comparable.
Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specific research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identification of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results.
In the future, we recommend investigating the use of statistical power calculations in corpus linguistics.
This might help us answer the perennial question, 'How big should my corpus be?' and help researchers determine comparability and the relative sizes of sub-corpora defined by metadata such as socio-linguistic variables.
As a result, nearly all concordancers and corpus linguistic tools will offer some assistance in the calculation of keyness.
Selection of a reference corpus will impact the results, and some care should be taken to select an appropriate 'benchmark' to highlight differences aligned with a given research question.
Those wishing to answer more general questions (e.g. 'aboutness') may choose to make use of a general reference corpus.
In the latter, these counts would be kept separate; ‚Ä¢ to account for possible effects of the evenness of dispersion of a collocation within the corpus, each item was also quantified with a DP value (see Chap. 5).
Finally, the dispersion of a collocation across a reference corpus had only a weak relationship with knowledge (more widely spread collocations were better recognized), and this relationship was significant only in the BNC.
Retrieving all cases of this construction from the syntactically parsed ICE-GB corpus, Hampe and Sch√∂nefeld use the Fisher-Yates exact test to identify verbs which are associated with it.
For three of the verbsencourage, support, and fear -use in one of the searched constructions is rare (less than 1% of occurrences of the verb) (continued) 7 Analyzing Co-occurrence Data 151 and not listed as a possible form in the corpus-based Collins Cobuild English Language Dictionary.
In this section, we are discussing a few areas that we feel should be on corpus linguists' radar; they involve.
To reiterate, while corpus-linguistic research into the association between elements has produced dozens of AMs, the frequencies of their use is as Zipfiandistributed as that of words: While there is still a lively discussion of which measure(s) is/are most useful for which specific purpose, a mere handful of (symmetric) measures are used in the vast majority of studies.
In a research context, in contrast, especially when researchers want to make a claim for exhaustive data retrieval and/or when the sequencing of attestations in the corpus matters for the research question at hand, one would only delete concordance lines that contain false hits.
Imagine, for example, that you have a (untagged) corpus from which you want to retrieve all mentions of former and current Presidents of the United States.
The fact that the terms bogus and genuine appear as collocates of refugee(s) in the corpus suggests that this occurs quite frequently.
Many quantitative corpus analyses are based on concordance data (though not necessarily all: one could think of, for example, a study that is based on frequency or collocation lists instead, see Chaps.
There is a growing strand of research that explores the efficacy of so-called datadriven learning (DDL) approaches, which give students access to large amounts of authentic language data (Frankenberg-Garcia et al. 2011), and corpus-based materials naturally lend themselves to use in such an approach.
A common phrase in this corpus was 'see your doctor', which implied that journalists placed trust in doctors (as long as they were not foreign).
For one, it can be time consuming, particularly if we are using a large corpus or searching on a frequent item.
The inspection of dozens of alphabetically sorted concordance lines enables patterns to emerge from a corpus that an analyst would be less likely to find from simply reading whole texts or scanning word lists.
Secondly, we only list monolingual concordancers, i.e. tools that let the user examine text from one corpus representing one language.
Stubbs, M. 2001. Words and phrases: corpus studies of lexical semantics.
This chapter discusses the important role of programming in corpus linguistics.
It then offers some strong counter arguments for why an understanding of the basic concepts of programming is essential to any corpus researcher hoping to do cutting-edge work.
Next, the chapter explains the basic building blocks of all software programs, and then provides a number of criteria that can be used to assess the suitability of a programming language for a particular corpus linguistics project.
In the first case study, basic programming concepts are applied in the development of simple programs that can load, clean, and process large batches of corpus files.
In the second case study, these same concepts are applied in the development of a more advanced program that can replicate and, in some ways, improve on the tools and functions found in ready-built corpus tools.
The chapter finishes with a critical assessment and discussion of future developments in corpus linguistics programming.
All modern programming languages can be used to develop programs that will fulfill the needs of corpus linguists, whether they are MA students, PhD students or seasoned experts in the field.
However, some languages are more suited to the tasks commonly carried out by corpus linguists, i.e., reading large text files into memory, processing text strings, counting tokens, calculating statistical relationships, formatting data outputs, and storing results for use with other tools or for later use.
In order to decide which language is most suitable for a particular corpus linguistics programming task, there are at least four important factors to consider at the outset.
In contrast, languages such as Java, Perl, Python, Ruby, and R have features tailored for both web and desktop environments, making them a common choice for many corpus tools.
In this paper, the authors aim to determine which keyness measure best identifies words that are distinctive to the target domain(s) present in a corpus (cf. Chap. 6).
To achieve this goal, they compare various keyness measures that are primarily based on the frequency of words in a corpus together with their own "text dispersion keyness" measure, which is based on the number of texts in which a word appears.
Although the majority of examples presented in these case studies mimic the functionality of existing ready-built corpus tools, they will generally perform much faster because they are designed to carry out a specific task and they remove the overhead of creating and updating an interface.
It should also be noted that they can be easily extended or adapted to carry out tasks that are quite difficult or impossible to achieve with the most popular tools used in corpus linguistics today.
These features have resulted in Python becoming one of the most popular languages used in corpus linguistics work today.
We can extend Script 3 in a different way to process an entire corpus.
To do this, we create a new function ("show corpus") that finds the paths of all the corpus files in a folder (e.g. "target_corpus"), calls the "get_file_content" function on each file path, and then prints out the output of each file (lines 21-28).
For this case study, imagine that a toy corpus that comprises just three UTF-8encoded, plain-text files needs to be processed.
Then, we only need to adapt the "show_corpus" function from Script 5 to process each file and count all the words in the corpus.
The main, "create_kwic_concordance" function is designed to accept five parameters: (1) the corpus location, (2) an option to ignore case when searching, (3) the search term, (4) a context size that defines how many tokens to the left and right of the search term will be shown in the KWIC results, and (5) a file path for the results file (lines 15-20).
Finally, it performs the main action of the function, i.e., locating search term hits in the corpus files (via "finditer") and generating KWIC results for each of them (lines 35-51).
It can create the nearly 70,000 KWIC results for the word "the" in the 1-million-word Brown corpus (a notoriously slow search) in under 1 sec on a modest computer.
In this case, we could use the MyConc class as a foundation for a more complete corpus toolkit that could be released as an open source project allowing it to be used and extended by others.
Fortunately, corpus linguists who are interested in further developing their programming skills have an abundance of learning resources available to them.
Rather, they should explain what they want, e.g. an ordered list of important words in the corpus.
For end users to make appropriate use of a corpus it is instrumental that they understand how it has been compiled.
Because of the limitations of the historical data available, and because research goals are very diverse, there is no such thing as an ideal historical corpus.
A corpus that has used this type of data to the best advantage is the Corpus of Early English Correspondence, compiled by Terttu Nevalainen, Helena Raumolin-Brunberg and a team of collaborators (Raumolin-Brunberg and Nevalainen 2007).
Not only does the corpus contain letters from the sixteenth to eighteenth century, it also covers four major English regions, as well as giving information on the social rank of the letter writers and how they relate to their addressees.
The compilers eventually arrived at a very elaborate coding scheme to describe the letter writers in their corpus, using 27 different parameters, some with very open information.
For example, given a rich historical record and a long tradition of high-quality text editions, it is perfectly possible to create a sizeable corpus of Old French, as shown by the FRANTEXT database.
Yet it is impossible to create a corpus of Old French that is comparable in any straightforward way to a corpus of Present-day French.
Similarly, it is possible to create a sizeable corpus of Latin, with a time-depth of over 2000 years, as shown by the 13-million-word LatinISE corpus, built by Barbara McGillivray.
However, even despite efforts to preserve genre balance, any diachronic comparison across this timespan has to take into account that the language used in such a corpus changes from a relatively standardized written language with more or less direct roots in a contemporary vernacular to a functionally impoverished and much more writing-dependent language used mainly in religious contexts and as a lingua franca among European scholars and scientists.
Especially where the body of historical data is finite, disparate and severely biased, or where a corpus is to be used to study change over very long time periods, this is a perfectly defendable strategy.
In a diachronic corpus of narrative fiction this inevitably affects the frequencies of specific grammatical patterns associated with either descriptive or more action-driven narrative passages.
Turning from the level of the texts that make up a corpus to the internal properties of those texts, perhaps the most fundamental question compilers and users of diachronic corpora must ask is to what extent they can rely on methods devised for the annotation and analysis of contemporary data in handling data from older periods. Older texts are in principle somewhat alien.
Added to this is a layer of inadvertent 'noise' created along the way as a corpus text travels from historical manuscript or print to digital edition.
For this reason, rich annotation is best seen only as a means to facilitate querying a corpus.
That is, it is not meant as a final analysis of all the sentences in the corpus but as a means to retrieve with greater ease data that is of potential research interest and that is otherwise nearly impossible to collect.
As regards precision, results collected automatically from the corpus should be manually checked to make sure they actually include what the researcher is looking for.
Methods of corpus interrogation will be affected by how linguistic organization is conceived.
The tenet that lexicon and grammar form an integrated continuum of form-meaning pairing has gained ground to the point of probably forming the majority position in current corpus linguistics.
In this section, we will discuss a number of heuristic techniques that can be used to interrogate the syntagmatic-paradigmatic organization across the whole lexicogrammatical continuum, and we will illustrate their potential relevance to diachronic corpus linguistics.
It contains some of the oldest French texts from 950 up to the present-day and, with currently about 300 million words of text, it decidedly qualifies as a large corpus.
It started life, however, as a corpus for lexicographic research which explains its great time-depth and wide coverage in terms of genres.
Part of the corpus has been part-of-speech tagged.
One striking feature is that the corpus comes with various predefined subcorpora, varying in size or in the period that is represented, so as to meet different research needs.
Indeed, the corpus has such a flexible online interface that it allows the user to dynamically select a working corpus precisely tailored to their specific objectives.
Another distinguishing feature is that it is open to contributions by third parties, who can submit new material for inclusion in the corpus.
The York-Toronto-Helsinki Parsed Corpus of Old English Prose (YCOE) is a 1.5-million-word syntactically annotated corpus representing Old English prose, with texts dating from before 850 up to 1150.
With its 24.4 million words it is a sizeable corpus, but what really sets it apart is the kind of text material it contains, with published transcripts of the spoken interactions in court.
By controlling the situation in which language is produced corpus compilers increase the probability that the phenomena they are interested in are actually included in the corpus.
Especially rare linguistic phenomena might not occur in sufficient numbers in a corpus that contains data U. Gut exclusively produced in uncontrolled situations.
The researcher's control over the raw data production moreover influences the degree of variation that is represented in the corpus.
It is important for corpus users to be aware of such potential differences in the orthographic transcriptions as they may lead to the missing of tokens in a word-based corpus search: in a search for 'because', for instance, transcribed forms such as 'coz' and 'cos' will not be found.
However, U. Gut the interoperability between tools is still one of the major challenges for spoken corpus use and re-use across linguistic subdisciplines as discussed in Sect. 11.3.
As is true for any corpora, when the metadata is provided in a separate file without using standoff techniques (e.g. IDs pointing from annotations to corresponding speaker metadata), it requires great manual effort to make use of it for automatic corpus searches (see also Chap. 3).
As existing spoken corpora vary greatly in the type and number of annotations as well as their data format, possibilities for corpus search diverge significantly.
English discourse particles -evidence from a corpus.
This is an example of an innovative study using the corpus-based method to gain new insights into the linguistic subdiscipline of pragmatics.
She analysed the use of the English discourse particles now, oh/ah, just, sort of, actually, and some phrases such as and that sort of thing in the London-Lund corpus and compared this to the Lancaster-Oslo/Bergen Corpus of written English and the COLT Corpus.
This is an example of a study based on a corpus with integrated linear annotations.
Yet, even well documented spoken corpora are often not immediately (re-)usable to the wider research community, especially if the intended linguistic use is not the original one of the corpus compilers.
Adding new annotations to a spoken corpus, however, can still constitute a major challenge for the simple lack of suitable tools although great advances have been made in the last decade in terms of the interoperability of the major tools in use for spoken corpus construction: ELAN, Praat, EXMARaLDA and ANVIL (see Sect. 11.4 below) now all have import and export functions for their respective file formats so that it is possible to add new annotations with one of these tools to a spoken corpus that was compiled with another tool (for this Transformer by Oliver Ehmer can also be used).
Yet, the conversion of linear corpus annotations into multi-layered ones still constitutes an unsolved challenge.
Some international initiatives such as CLARIN in Europe have the aim of overcoming this challenge by providing an infrastructure for corpus users that includes easy-to-use standardised tools.
While an increasing number of corpus compilers are eager to make their spoken corpora available to the research community, technological and ethical difficulties have to be met as discussed below.
Furthermore, new strategies for corpus dissemination are being proposed to ensure long-term access to spoken corpora.
As an alternative, corpora can be stored in external data services such as clouds with corpus access for example by compressed media streaming.
Typically, privacy laws require corpus compilers to ask for the formal written authorisation by each speaker to be recorded for the corpus and to allow the transcription, sharing and re-use of his or her data.
Only when corpus compilers have received the written consent of the speakers recorded for the corpus that their data can be used for research and be disseminated can corpora be used and shared.
Moreover, privacy rights require corpus compilers to anonymise the disseminated data (cf. Chap. 1): while this is easily achieved in the transcriptions, where references to people and places can be removed, complete anonymization in audio files, i.e. the changing of the voice quality, would run counter and make impossible many research purposes of the corpus.
In some national laws the speaker can withdraw his or her consent at any later point 11 Spoken Corpora 251 in time, which poses serious challenges for corpus dissemination.
Changing legislation in these areas might pose further difficulties for corpus-based research in the future.
It is important to point out from the outset that the term parallel corpus is to some extent ambiguous, because it is sometimes used to refer to comparable original texts in two or more languages, especially texts that 258 M.-A. Lefer belong to comparable genres or text types and deal with similar topics (e.g. Italian and German newspaper articles about migration or English and Portuguese medical research articles).
In cross-linguistic research, it is of paramount importance to know, for instance, whether a given text was translated from Spanish into German or vice versa, because corpus studies have shown that translation direction influences translation choices, and hence the linguistic make-up of translated text (e.g. Dupont and Zufferey 2017).
Delaere and De Sutter (2017), for example, rely on an English-to-Dutch parallel corpus to find out whether the English loanwords found in translated Dutch stem from their corresponding trigger words in the English source texts.
Obtaining detailed metadata is another challenge facing anyone wishing to compile a parallel corpus.
As is often the case in corpus-based translation studies, parallel data are used with a view to identifying the source-text items/structures that have triggered the use of a given item/structure in the translations (in this case, the presence of a trigger term in the English source texts, such as unit, job, or team).
Their study convincingly illustrates the need to adopt multifactorial research designs in corpus-based translation studies, as these make it possible to go beyond the monofactorial designs where, typically, only the "translation status" variable is considered (translated vs. non-translated).
Second, whenever possible, we should go beyond the inclusion of translated novels, news, and international organizations' legal and administrative texts, and strive for the inclusion of more genres and text types, especially those that are dominant in today's translation market, to which corpus compilers have had limited access to date, for obvious reasons of confidentiality and/or copyright clearance.
In such a corpus, data are collected from the same subjects at different time intervals, so as to reflect the development of their language skills over time.
Ideally, a language development corpus presents an ecologically valid and representative picture of the linguistic development of language learners.
Since recording over several years is time-consuming, a staggered design of several longitudinal studies of children including different age spans is often applied (cf. e.g. the Chintang corpus in Box 1).
The number of children is one of the most important decisions in the design of a study, having the potential both to make data statistically (more) robust and to increase the amount of work put into a corpus ad infinitum.
This point is crucial considering how additional target children multiply the amount of time and money required to compile a corpus with minimum annotation standards.
Transcription is often the bottleneck of corpus development.
In order to be able to spot developments even when the corpus is not (yet) fully transcribed, it is useful to transcribe data according to a systematic "zoom-in" pattern, where one starts with the first and the last recording of a child, then transcribes the recording in the middle between the two, then the two recordings in the middle of the stretches thus defined, and so on.
Any corpus relies on metadata to correlate the speech of the child and her surroundings with social variables.
Below an example from the Chintang corpus is given (in its native format, Toolbox).
Such results can only be obtained by dense corpus studies allowing us to detect relatively rare phenomena such as passive constructions.
Comparability is sometimes complicated by rather technical reasons such as different choices of file formats, corpus formats and syntax, and coding schemes.
Thus, more standardization, also linking to standards existing in other corners of corpus linguistics, seems an indispensable step for exploiting the full range of developmental data that we have.
CHILDES also provides a number of tools for corpus development including transcription and annotation programs.
When considering the term 'web as corpus', the first question we must ask is whether the web can actually be classed as a corpus according to the criteria set out in Chap. 1.
From the 1 million word Brown corpus of the 1960s to the 100 million word BNC of the 1990s, the resources used by researchers in the field have conventionally been of known (usually finite) size.
An example will illustrate the limitations we face if we attempt to treat the web as a corpus using conventional search engines.
Since the late 1990s when linguists began to turn to the web as a corpus, search engines have actually become less advanced in terms of the options they offer.
Although WebCorp Live offers advantages over direct use of commercial search engines (and is still widely used for this reason), it does not solve the underlying problems of the web as corpus approach.
Quantitative analysis -one of the core activities of corpus linguistic research -is not possible as we do not know the total size of the web 'corpus' held on the search engines' servers.
Few researchers would now claim that the web is a corpus in any meaningful sense, but the web as corpus approach can still be fruitful for certain kinds of research and it is particularly useful for introducing newcomers to the field.
The first step in the process is to supply a list of 'seed' words from which the corpus will be grown by the software.
The BootCaT manual gives a simple example for the building of a domain-specific corpus on dogs, with the seeds dog, Fido, food hygiene, leash, breeds, and pet.
When building general reference corpora from the web we need to choose seed words that are likely to appear across a wide range of topics, but there has been some debate about exactly what constitute good seeds for a corpus of this type.
Whatever seeds are chosen, this is only the first step in the process of building a corpus from the web.
In all but the most basic examples, it is likely that the researcher will want to expand the corpus beyond the initial set of seeds.
When the crawl is eventually complete, several other steps are usually carried out to 'clean up' the downloaded web documents before they are added to a corpus.
As such features are often repeated verbatim across multiple pages on a single website, it is desirable to remove them to prevent the words they contain from becoming unnaturally frequent in the resulting corpus.
The exact choice of filters is dependent on the intended nature of the corpus and research aims, but size and language filters are amongst the most common.
Size filters are designed to remove very short and very long documents from the corpus.
The assumption is that very short documents are unlikely to contain much connected prose, while very long documents are likely to skew the corpus unnaturally.
As a result, any web-derived corpus is likely to include documents that are either exact duplicates of one another or are very similar.
After these steps have been performed, the corpus can then be tokenised, lemmatised and part-of-speech tagged using the standard approaches described in Chap. 2.
An alternative approach to large indiscriminate crawls is to focus on specific websites, thus limiting the size of the textual universe and increasing the chances of building a representative corpus.
This is not true of all Web 2.0 texts though, and corpus linguists have had to devise new methods to access the language of social media sites.
Options include the FireAnt package designed specifically for corpus linguists, as well as general purpose software such as IFTTT, Import.io, and TAGS.
The last of these, standing for Twitter Archiving Google Sheet, automatically downloads tweets matching particular search parameters into a spreadsheet which can subsequently be used to build a corpus.
The building of bespoke corpora from the web usually requires considerable technical skill and, thus, may not be an option for all corpus linguists, especially beginners.
Until relatively recently, corpus-based studies have tended to focus almost exclusively on the English language, and on British and American English in particular.
Although all texts were downloaded from the web using the 'web for corpus' approach, COCA was designed in such a way that it excludes web-native genres like blogs and social media.
GloWbE was constructed using 'web for corpus' techniques, seeded through search engines queries.
Of particular interest is the sub-corpus from WordPress and Blogger, which includes 95 million words of blog posts and a further 86 million words of reader comments on those posts.
The crawling for the WordPress and Blogger sub-corpus of the BBC took place without relying on commercial search engines.
When each post on each blog was processed, links to other WordPress and Blogger blogs were extracted and added to the crawling list, thus widening the scope of the corpus.
Such research has also identified a shift in use of the blog format from its original 'online diary' focus, meaning that it is now possible to capture a wide variety of topics and communicative intentions in a corpus made up exclusively of blogs.
The web may not be a corpus in a conventional sense but, as we have seen, it can be a valuable corpus surrogate or, increasingly, a source of texts for the building of corpora.
As we have moved towards the 'web for corpus' approach, a new challenge has emerged concerning the legality of distributing corpora crawled from the web.
The solutions adopted by corpus compilers vary, from limiting the context shown through the search interface (Mark Davies' corpora) to releasing corpora for download with the sentences shuffled into a random order (COW corpora).
This is still very much a grey area, with different laws applying across the world, and anyone planning to distribute a web-derived corpus should seek local advice.
There have been attempts in the past but most have proved to be unsustainable as academic funding models do not allow the continuous expansion of disk storage space required for regular web crawling or the bandwidth required to allow multiple users to carry out complex searches on a large corpus simultaneously.
A corpus is a principled collection of language data taken from real-life contexts.
This dependency is due to the fact that our ability to analyse and examine complex and extensive multimodal corpora in a systematic and accurate way (particularly when integrating traditional corpus-based methods into the approach), is reliant on the availability of appropriate technology.
As a 'pilot' study, the paper with a corpus only comprising six recordings of circa 7 min each, problematizes the collection, annotation and analysis of multimodal corpora for research.
The corpus was compiled in 2005 and remains one of the largest freely accessible multimodal corpora in existence.
It will be interesting to see whether some of the major insights generated within monomodal, or text-based, corpus linguistics will be replicated once we are able to truly analyse multimodality at scale.
As Transana is primarily a tool for qualitative analysis, its provisions for, for example, frequency-based or numerical analysis (as utilized by corpus linguists) is somewhat limited.
This study is situated mainly within the MDA paradigm, so it focuses on small-scale corpora of situated discourse, utilizing discourse analytic methods as the initial point of departure, but discusses how such an approach can be extended with the integration of corpus methods.
R and RStudio are the two software tools that will be used in the rest of the handbook to describe a set of statistical techniques available to corpus linguists.
A representative corpus of speech is collected, a set of phonetic variables is defined, and the number of times each speaker uses each of these variables is recorded, thereby building up a body of data.
Phonetic data was abstracted from the Diachronic Electronic Corpus of Tyneside English (DECTE), a digital corpus of audio-recorded and transcribed speech from Tyneside in North-East England.
The usual data creation approach of abstracting and counting the collocates of target words from a corpus was modified by including only the covarying collexemes of target words, that is, words which occur in a defined slot of the same construction as the target word (see Chap. 7).
The conclusions made with these methods are therefore valid for the corpus only.
Indeed, we may well observe different tendencies in another corpus of British English.
The diary examples were extracted from the LiveJournal corpus, developed by Dirk Speelman (University of Leuven).
The aim of this work is to spot the patterns of linguistic variation among registers in a corpus of English texts.
First, the corpus is tagged for linguistic features.
He also observes that the most complex prepositions (i.e. prepositions that consist of three words and more) are over-represented in the corpus of Indian English.
To compare the semantic profiles of the prepositions, the preferred and dispreferred nominal collocates of the prepositions are examined in the FrWaC corpus.
We search them for the linguistic variable of our interest, such as swearwords, and we observe that the male corpus includes an average of 16 swearwords and the female corpus 14.
To evaluate the evidence (corpus findings, e.g. the means of 16 and 14 for men and women, respectively) we employ a particular statistical test.
The data used consists of two corpora: a male corpus (10 speakers) and a female corpus (10 speakers).
We, however, need to critically evaluate the assumptions of the tests before applying them; in the context of corpus linguistic analyses, the assumption of independence of observations is of utmost importance (see Sect. 20.2.2).
This article offers a critical view on using frequency data in corpus linguistics in the context of language learning; this critique is, however, applicable to other contexts as well.
In doing so, this chapter showcases applications of regression techniques in corpus-linguistic research.
Below, we will use it to analyze a corpus based measure, namely the type-token ratio (TTR) of different texts that have the same overall length.
For example, if a corpus-based analysis contains the length of a phrase as an independent variable, and we introduce the pronominality of that phrase as a second independent variable, these two variables might correlate significantly, since full lexical phrases are by necessity longer than single pronouns.
In order to investigate whether iconicity of sequence influences the positioning of English adverbial clauses, Diessel retrieved 600 complex sentences with the conjunctions when, after, before, once, and until from the ICE-GB corpus.
Let us assume that we conduct a corpus-based analysis of how speakers address each other in conversation.
It is also argued that the observed tendencies -such as short-before-long and animate referents firstare in line with synchronic corpus-based and experimental findings about general cognitive principles underlying the framework of probabilistic grammar.
He also argues that by not doing so, corpus linguists risk violating fundamental assumptions about the independence of the error terms in models.
The grouping factors derived from the structure of the corpus are mode (only two levels), register (five levels), and subregister (13 levels).
The paper thus shows that it is not appropriate to ignore lexical grouping factors and grouping factors derived from the corpus structure, especially as both are easy to annotate automatically.
For example, s(CorpusTime, Speaker, bs = "fs", m = 1) requests wiggly curves for log odds as a function of CorpusTime for all speakers in the corpus.
In corpus linguistics, bootstrapping has also been proposed as a method for quantifying the degree of homogeneity in a corpus sample, for validation of statistical results, and as a methodological step in random decision forests, an advanced classification method.
We argue in this chapter that bootstrapping is underused in corpus linguistics, and that quantitative corpus linguists would do well to add this tool to their repertoire.
We address several applications of bootstrapping, including the measurement of sample estimate accuracy, the validation of statistical models, the estimation of corpus homogeneity, and random forests.
We then present an overview of several applications of bootstrapping that have been used in prior corpus linguistics research.
We conclude with a summary of the chapter and some comments on potential applications of bootstrapping in corpus linguistics that have yet to be attempted.
For example, we can calculate the mean rate of occurrence for nouns in a single corpus sample of size n.
Explanations and examples of these applications of boostrapping in corpus linguistics are included in Sect. 24.2.2.
This application has been more widely used by corpus linguistics researchers than the previous two applications.
While it is not found in a large number of studies, its use seems to be on the rise to answer questions in corpus-based variationist linguistics.
We believe that the applications of bootstrapping in corpus linguistics we have discussed in the previous four sections are just a beginning for corpus linguistics.
There are many applications of the principles and methods of bootstrapping that need to be further explored by corpus researchers.
Bootstrapping could be used as a method for evaluating the linguistic representativeness of a corpus (cf. Chap. 1).
Although the use of bootstrapping methods in corpus linguistics is still in its infancy, writing this book chapter has given us reason to be optimistic about the future of bootstrapping in corpus linguistics.
During the process of reviewing the corpus-based research that uses bootstrapping, we have seen no evidence of bootstrapping results being misinterpreted or misrepresented.
So while these authors have ample anecdotal evidence to suggest that bootstrapping methods need to be better understood, generally, it seems that their application in corpus linguistics research has gotten off to a great start.
The high quality of bootstrapping methods in corpus linguistic research may actually be due in part to the slow and cautious pace at which they have been adopted.
It is far more important that corpus researchers apply and interpret bootstrapping appropriately, than that they use it a lot.
Having said that, we hope to see both of these things happening in corpus linguistic research.
In this section we demonstrate how to bootstrap corpus-based data using R.
Conditional inference trees (CITs) and conditional random forests (CRFs) are gaining popularity in corpus linguistics.
This happens quite often in corpus-linguistic research.
The data come from a corpus of spoken York English at the turn of the twenty-first century.
In addition, some frequency information was collected from the GloWbE corpus.
For more general info about how to report the results of a quantitative corpus-based study, see also Chap. 26.
Such situations are very common in corpus linguistics.
In this chapter, we focus on how to write the 'Methods' and 'Results' sections of a quantitative corpus linguistic paper since the 'Introduction' and 'Discussion' sections are so phenomenon-dependent that, apart from the general guidelines above, they defy easy discipline-specific characterization.
However, the 'Methods' and 'Results' sections of many corpus-linguistic papers share some important commonalities that we believe can be usefully summarized for less experienced writers and/or beginning corpus researchers.
First, it should start with a detailed description of the corpus used.
If relevant, this section should also specify which version of the corpus and what types of annotations available with the corpus were used to answer the research questions.
As for the former, if answering the research questions requires an unannotated corpus to be automatically tagged or parsed, details about the tool used will need to be provided.
This also requires the author to describe how, for instance, false hits, i.e. instances of something in a corpus that fits the structural description or regular expression used for data retrieval, but that turn out to not actually be instances of the phenomenon in question, were identified.
For instance, in a shallowly-parsed corpus, one might search for instances of V NP NP with the goal of retrieving ditransitive constructions such as He gave him a book, but that search might also return instances of object complementation such as He called him a liar, which would have to be filtered out.
Again, each corpus processing tool used at the retrieval stage should be listed, described and properly referred to.
Importantly, the exact search expressions should be reported and the settings used should be specified (e.g. list of word separators, minimum frequency or dispersion threshold for word lists; cf. Part II for the settings typically associated with different corpus methods).
The 'Methods' section of a corpus-linguistic paper also needs to contain information on how the notions/concepts considered relevant for the analysis were operationalized as variables and how annotations were added with regard to the variables that may affect the linguistic phenomenon under study.
As is obvious from the above, a lot of corpus-linguistic work does not discuss all their methodological aspects in sufficient detail, but in order to at least begin to approach the ideal of reproducibility, it is essential that all this information be provided at a sufficient level of detail.
At each step we consider and discuss both best practices as well as some of the many challenges unique to meta-analyzing research in corpus linguistics.
This information can be considered comparable to metadata for texts in a corpus or to demographic data in a study with human participants.
Imagine further that a number of corpus-based studies had examined this issue using different corpora, with each study yielding a result expressed in occurrences per million words.
This chapter has sought to lay a foundation of meta-analysis for corpus linguistsfor both those who might produce meta-analyses themselves and to improve the field's understanding of such studies.
Nevertheless, it is not often the case that the findings of a meta-analysis will provide conclusive answers to questions of interest, particularly in a young field such as corpus 27 Meta-analyzing Corpus Linguistic Research 679 linguistics.
A third major challenge to meta-analysis in corpus linguistics and throughout many other fields is that of publication bias.
In this chapter, we have argued that meta-analysis should be more widely applied within corpus linguistics as a method of synthesizing and empirically reviewing previous corpus-based research.
In order to encourage and enable the application of meta-analysis in corpus linguistics, we have also introduced and demonstrated the main procedures involved including how to (1) search for primary studies, (2) develop and implement a coding scheme, (3) aggregate effect sizes using different R packages to obtain estimates of the relationship in question as well as estimates of variance and of different moderator effects.
The importance of a corpus is acknowledged because it contributes to making new findings, helps in utilizing data in applications, provides scopes for modifying old observations, generates opportunities for formulating new theories, and prepares new fields for applying language data in direct service to society.
The introduction of a corpus in language study adds a new dimension to linguistics.
The underlying theoretical idea of corpus linguistics is quite broad.
Digital corpus is now a known thing to us and we have come to a common agreement to understand what counts as a 'corpus', what are its characteristics, how it can be built and classified, how it can be annotated and processed, and how it can be analyzed and utilized.
This confirms the importance of the description and analysis of data and application of corpus in diverse spheres.
A corpus should primarily contain simple plain texts.
Data and information should be retrieved from a corpus.
Modern technology allows one to store a corpus in such a manner that one can easily retrieve data from it.
This puts a corpus ahead of generative linguistic study.
A corpus, unless it is a fixed one, should grow regularly.
This allows a corpus to reflect on the linguistic changes that take place in a language over time.
By regular addition of synchronic data, a corpus attains a diachronic dimension.
This becomes useful at subsequent stages of corpus management and reference.
The utility of a corpus is increased by an elegant arrangement of texts in an archive.
It is known to us that corpora can be of many types with regard to texts, languages, modes of data sampling, methods of corpus creation, manners of text annotation, and goals of text utilization, etc.
For instance, one corpus may contain samples of written texts, the other one may contain samples of spoken texts, and the third one may contain transcripted spoken texts.
Similarly, a corpus may contain samples of present-day use of a language, while the other may contain samples from old texts; a corpus may be monolingual with a collection of data from a single language, while another may be bilingual or multilingual by including texts from two or more languages; texts included in a corpus may be collected from one source, two sources or many sources of a particular field or across fields.
This implies that numerous factors control the content, type, and use of a corpus as well as numerous ways that one can employ to classify corpora.
Each corpus developer is, however, free to adopt unique methods in language representation, text collection, and text application.
There are various issues relating to the design, generation, and management of a corpus.
Development of a speech corpus involves issues and factors like purpose of use, selection of speakers, choice of settings, size of a corpus, use of recording instruments, manner of data sampling, manner of data elicitation, nature of transcription, types of data encoding, management of audio files, editing of input data, processing of spoken texts, annotation of speech, analysis of speech corpus, etc.
In the case of a speech corpus, it is related to the amount of audio data (in MB or GB) or duration of a speech (in hours and minutes).
In the case of a speech corpus, on the other hand, it is linked with the total number of sentences, unique words (types), and total words (tokens) in a corpus.
It also depends on the decision of how many text categories are kept in a corpus, how many text samples are put in each category, and how many words are kept in each sample.
Although size does matter on the scale of the 'reliability' of a corpus, in reality, a corpus, however big it may be, is a small representation of the vast amount of speech and text varieties that are produced in a language.
In the early 1960s, when computer technology was not conducive to collecting a large amount of language data, a corpus of one million words was considered good enough (e.g., Brown Corpus, Lancaster-Oslo-Bergen (LOB) Corpus, KCIE (Kolhapur Corpus of Indian English)).
Therefore, the bigger the size of a corpus, the more is its utility, faithfulness, and reliability.
A large corpus does not guarantee a balanced representation of all varieties of texts of a language, if not desired so.
A small corpus with wider text variations is a much better representative than a large corpus with fewer text variations.
A large yet less varied corpus cannot be used for the generalization of a language.
A corpus is truly 'representative' when findings from it are generalized to a language or a part of it.
Therefore, rather than focussing on the quantity of data in a corpus, it is always sensible to emphasize varieties of data.
The size of a corpus should be set against the diversity of texts to achieve proper representation.
In any corpus, the greater the number of unique text samples, the higher the reliability of the analysis of linguistic variables.
The Brown, LOB corpus, and Survey of English Usage (SEU), for instance, contain a wide variety of text types, and therefore, they are considered much better representatives of English.
The general argument is that texts produced by native language users should get priority over texts produced by nonnative users in a general and monitor corpus.
The non-native use of texts in a general corpus can have an adverse effect on linguistic analysis of data and information.
One of the reasons behind building a general corpus is to enable scholars to analyze 'standards' to see what does occur and what does not than looking into non-standard interpolations.
The primary idea is that we should have a general corpus that includes 'benchmarked' and 'standard' data so that we can collect information about how accepted standards are commonly used in mainstream linguistic activities.
Based on a general corpus, we like to produce texts and reference materials that will guide in spelling, pronunciation, grammar (i.e., syntax), meaning and usage.
In the case of a special corpus, we like to include text samples produced by non-native users as the aim of this corpus is to highlight linguistic peculiarities that are typical to non-native users.
There is no fixed target user for a general corpus, as such.
In the case of a special corpus, the identification of target users is important.
Since each research has a specific goal, a special corpus has to be designed accordingly.
For instance, a person working on developing a tool for machine translation (MT) requires a parallel corpus than a general one.
Similarly, a person working on comparative studies between two or more languages requires a multilingual comparable corpus than a monitor one.
It is necessary to decide if a corpus should contain all types of written text samples or specific sets of text samples.
A general corpus usually includes standard text samples as it aims to identify what are central and common features of a language.
Therefore, we can furnish a general corpus with standard text samples of contemporary writings and a calculated and proportional representation of texts can suffice our requirement.
On the other hand, in case of a monitor corpus, which tries to represent all possible text varieties, we may include all non-standard and ordinary text samples along with standard and established text samples.
A heterogeneous monitor corpus includes text samples from all subject areas and the individuality of the source of a text and its writer are well attested.
Thus, a monitor corpus becomes a worthy representation of texts that are ever produced in a language.
Diversity is a useful safeguard for a monitor corpus against skewed representation.
The task of sampling of texts has to be done with collected text materials based on the character of a corpus.
Since there are several methods for data sampling to ensure the optimum representation of texts in a corpus, we may pre-define the kind of data we want to study before we define a sampling procedure.
The application of the random sampling method usually saves a corpus from being skewed and less representative.
Alternatively, we may apply selective sampling methods that are used in the Brown Corpus and the LOB corpus or consider more suitable methods of text representation keeping in mind the goal and purpose of a corpus.
The management of a corpus is a complex and tedious task.
Once a corpus is built and stored in an archive, we need robust schemes for regular maintenance, upgradation, and augmentation of the resource.
We have to be comfortable with the application of modern toolkits on corpus as such devices help us execute many useful tasks on a corpus.
The processes of corpus sanitation start when a corpus is made ready for use.
There are many linguistic and technical issues relating to corpus sanitation (e.g., text normalization, orthographic error correction, spelling error correction, real word-error correction, grammatical error correction, punctuation error removal, and tokenization).
With regard to the use of a corpus for academic purposes, there are also problems with ethical 'right, wrong, legal or illegal'.
As long as it is not directly used for commercial purposes, one can utilize a corpus.
However, when one uses a corpus to produce tools, systems, and resources and commercialize these, one has to face copyright problems.
In direct commercialization of corpus, one should seek permission from legal copyright holders.
The processing of a corpus starts after a corpus is generated and normalized.
There are various ways of processing (e.g., data analytics, concordance, collocation, keyword marking, local-word-grouping, lexical clustering, lemmatization, morphological processing, sentence processing, and named entity marking), which are applied to a corpus with appropriate technological supports.
I briefly discuss here a few well-known corpus processing techniques for understanding these technical issues.
Studies in mathematical linguistics, computational linguistics, corpus linguistics, applied linguistics, forensic linguistics, stylometrics, and other domains require statistical and quantitative results from a corpus.
A corpus often becomes a subject of quantitative and qualitative statistical analysis for addressing empirical and theoretical queries.
In qualitative statistical analysis, on the other hand, we look at multidimensional analysis of observed phenomena in a corpus and try to find factors behind the occurrence of a feature or an element in a text.
Both types of analysis have something to contribute to corpus-based language study.
Numerical sorting is one of the most straightforward approaches to working with quantitative data of a corpus.
Here items are classified according to a particular scheme, and an arithmetical count is made on the number of items that belong to each class in the scheme within a corpus.
Anyone studying a corpus may like to know the frequency and patterns of use of each item in it.
Since a corpus contains text samples either from a written or spoken discourse, annotation is done by adopting a value-encoding process, which attaches interpretative information at different levels of a text.
Analysis of corpus texts shows that apart from pure intralinguistic information, a corpus also carries several kinds of extralinguistic information.
Both from a theoretical and application point of view, corpus linguistics is a powerful method, which is empirical, scientific, realistic, authentic, open-ended, verifiable, and reliable.
Within mainstream descriptive linguistics, a corpus is a useful resource based on which faithful language description can be made.
Due to many such advantages, criticisms against corpus linguistics, although partly succeeded at the initial stage, have failed to stop corpus linguistics.
Parsed versions of part of this corpus are now available and included in the Penn-Helsinki Parsed Corpus of Middle English and the Penn-Helsinki Parsed Corpus of Early Modern English.
The Dictionary of Old English Corpus is a three-millionword corpus containing all surviving Old English texts.
But while printed texts can be easily included in a corpus, spoken texts still have to be manually transcribed: no voice recognition software can accurately produce a transcription because of the complexities of spoken language, particularly spontaneous conversation with its numerous hesitations, incomplete sentences, and reformulations.
This is why corpora such as the Santa Barbara Corpus of Spoken American English, which is approximately 249,000 words in length, required a team of transcribers to create the corpus.
In a corpus of spoken dialogues, for instance, it is necessary to include tags that identify who is speaking or which sections of speaker turns contain overlapping speech.
In a tagged corpus, various kinds of lexical categories, such as proper nouns or modal auxiliaries, can be easily retrieved.
In a purely lexical corpus (i.e. a corpus containing just the text), only individual words (or sequences of words) can be searched for.
But even in a lexical corpus, the numerous concordancing programs that are currently available can greatly facilitate searches.
This brief overview of the current state of corpus-based research is explored in detail in this second edition of English Corpus Linguistics.
It describes how linguistic corpora have played an important role in providing corpus linguists with linguistic evidence to support the claims they make in the particular analyses of language that they conduct.
The chapter opens with a discussion of how to define a corpus, and then traces the history of corpus linguistics, noting, for instance, that corpus-based research began as early as the fifteenth century, when biblical concordances were created based on passages from the Bible.
Current conceptions of corpus linguistics started with the creation of the Quirk Corpus (which contained print samples of spoken and written English) in 1955 at the Survey of English Usage at University College London.
But as corpus-based research began to expand during this period, its emergence during the dominance of generative grammar in the 1960s created quite a controversy, since to some linguists, corpus linguistics reeked of behaviorism.
But as much research in corpus linguistics has demonstrated, empirical data can both enhance and support the claims made in linguistic analyses.
The chapter concludes with a description of the many different areas of linguistics (e.g. lexicography and sociolinguistics) that have benefited from the use of linguistic corpora, followed by a linguistic analysis illustrating that corpus-based methodology as well as the theory of construction grammar can provide evidence that appositives in English are a type of construction.
Subsequent sections discuss other topics relevant to planning the building of a corpus, such as defining exactly what a corpus is.
Other sections of the chapter focus on additional issues relevant to planning the construction of a corpus, such as how to determine the appropriate size of a corpus and the length of particular texts that the corpus will contain (complete texts versus shorter samples from each text, e.g. 2,000 words); how to select the particular genres to be included in a corpus (e.g. press reportage, technical writing, spontaneous conversations, scripted speech); and how to ensure that the writers or speakers analyzed are balanced for such issues as gender, ethnicity, and age.
This chapter focuses on the process of creating and annotating a corpus.
Other stages of building a corpus are also discussed, ranging from the administrative (how to keep records of texts that have been collected) to the practical, such as the various ways to transcribe recordings of speech.
The analysis of Trump Speak also contains a discussion of how to use concordancing programs to obtain, for instance, information on the frequency of specific constructions in a corpus as well as relevant examples that can be used in subsequent research conducted on a particular topic.
In short, the goal of the second edition of English Corpus Linguistics is to provide a comprehensive description of all aspects of corpus linguistics, ranging from how to build a corpus to how to analyze a finished corpus.
In addition, sample corpus analyses are included to illustrate the many theoretical and practical applications that linguistic corpora have.
This corpus established a methodology for corpus creation and analysis that has continued until the present.
Ironically, the corpus was created around the time when generative grammar completely changed the shape of linguistics.
This corpus contained one million words of edited written American English divided into 500 samples representing different types of writing (such as fiction, technical prose, and newspaper reportage).
Each sample was 2,000 words in length, enabling valid comparisons between the different registers in the corpus.
But as corpus linguistics developed alongside the dominant paradigm of generative grammar, obvious differences and disputes resulted.
Although arriving at a definition of a linguistic corpus may seem like a fairly straightforward process, it is actually a more complicated undertaking than it initially appears.
This corpus contains 5,800 sentence pairs.
Because corpora are fairly diverse in size, structure, and composition, the TEI provides a general definition of a corpus.
Several points mentioned in the guidelines are directly relevant to whether or not the Microsoft Paraphrase Corpus (MPC) fits the definition of a corpus.
The MPC fits this criterion, as the kinds of language samples to be included in the corpus were carefully planned prior to the collection of data.
Second, a corpus can contain either complete texts (e.g. a collection of newspaper articles) or parts of texts (e.g. 500-word samples from various newspaper articles).
The MPC satisfies this criterion too: each sentence in the corpus was selected from some larger text of which it is a part.
A third TEI guideline for defining a corpus is more problematic for the MPC.
For a corpus to be "representative," it must provide an adequate representation of whatever linguistic data is included in the corpus.
For instance, a general corpus of newspaper editorials would have to cover all the different types of editorials that exist, such as oped pieces and editorials produced by an editorial board.
For a general corpus of editorials to be "balanced," the editorials would have to be taken from the various sources in which the editorials are found, such as newspapers, magazines, perhaps even blogs.
A corpus of spontaneous conversations would need to contain unrehearsed conversations between two or more individuals in the types of settings in which such conversations are held, such as the family dinner table, a party, a chat between two friends, and so forth.
The more a corpus satisfies these four criteria, the more prototypical it would be.
Obviously, the MPC would be a less prototypical corpus as well.
While it is available in a machine-readable format, it is too short to be representative of the types of sentences occurring in news stories, and whether the random sampling of sentences produced a balanced corpus is questionable.
Because the sentences included in the corpus were taken from news stories, the sentences did occur in a natural communicative setting.
Balanced corpora like Brown are of most value to individuals whose interests are primarily linguistic and who want to use a corpus for purposes of linguistic description and analysis.
The most ambitious pre-electronic corpus, the Quirk Corpus, served as a model for the modern-day electronic corpus.
Although it now exists in digital form, the Quirk Corpus was originally an entirely "print" corpus.
But of all the early electronic corpora, the first computerized corpus of written English, the Brown Corpus (described earlier), was really the corpus that ushered in the modern-day era of corpus linguistics.
Compared with present-day corpora, this corpus is relatively small (one million words).
However, for the period when it was created (early 1960s), it was a huge undertaking because of the challenges of computerizing a corpus in an era when mainframe computers were the only computational devices available.
Using the corpus was a two-step process.
The early influences on corpus linguistics discussed in this section do not exhaust the many other factors that affected the current state of corpus linguistics.
As a consequence, even though the creators of the Brown Corpus, W. Nelson Francis and Henry Kuƒçera, are now regarded as pioneers and visionaries in the Corpus Linguistics community, in the 1960s their efforts to create a machine-readable corpus of English were not warmly accepted by many members of the linguistics community.
This attitude was largely a consequence of the conflict between what Chomskyan and corpus linguists considered "sufficient" evidence for linguistic analysis.
In short, corpus linguists were grouped into the same category as the structuralists -"behaviorists"that Chomsky had criticized in the early 1950s as he developed his theory of generative grammar.
Unlike generative grammarians, corpus linguists see complexity and variation as inherent in language, and in their discussions of language they place a very high priority on descriptive adequacy, not explanatory adequacy.
Consequently, corpus linguists are very skeptical of the highly abstract and decontextualized discussions of language promoted by generative grammarians, largely because such discussions are too far removed from actual language usage.
Corpus linguists also challenge the clear distinction made in generative grammar between competence and performance, and the notion that corpus analyses are overly descriptive rather than theoretical.
But while corpora are certainly essential linguistic resources, it is important to realize that no corpus, regardless of its size, will contain every relevant example of a particular linguistic construction or be able to provide a fully complete picture of how the construction is used.
At best, a corpus can provide only a "snapshot" of language usage.
But despite the difference in length, each corpus contains many of the same registers, such as fiction, press reportage, and academic writing.
The BNC is a fixed corpus: its structure hasn't been changed since its creation in the 1990s, though a successor corpus, BNC2014, is now in progress (cf.
Each corpus has been both lexically tagged and syntactically parsed.
Users of the corpus will therefore be able to study the development over time of both individual words as well as a variety of different syntactic structures.
The first computer corpus, the Brown Corpus (a general-purpose corpus), contained various kinds of writing, such as press reportage, fiction, learned, and popular writing.
In contrast, the MICASE Corpus is a more specialized corpus that contains various types of spoken English used in academic contexts (e.g. advising sessions, colloquia) at the University of Michigan.
Moreover, once such a corpus is created, it is less straightforward to study sociolinguistic variables than it is to study genre variation.
To study press reportage, for instance, it is only necessary to take from a given corpus all samples of press reportage, and to study within this sub-corpus whatever one wishes to focus on.
To study variation by gender in, say, spontaneous dialogues, on the other hand, it becomes necessary to extract from a series of conversations in a corpus what is spoken by males as opposed to femalesa much more complicated undertaking, since a given conversation may consist of speaker turns by males and females distributed randomly throughout a conversation, and separating out who is speaking when is neither a simple nor straightforward computational task.
But despite the difficulties of creating a truly balanced corpus, designers of the BNC made concerted efforts to produce a corpus that was balanced for such variables as age and gender, and that was created so that information on these variables could be extracted by various kinds of software programs.
Included within the spoken part of the BNC is a sub-corpus known as the Corpus of London Teenage English (COLT).
This part of the corpus contains a valid sampling of the English spoken by teenagers from various socioeconomic classes living in differing boroughs of London.
In the British component of the International Corpus of English (ICE-GB), ethnographic information on speakers and writers is stored in a database, and a text analysis program designed to analyze the corpus, ICECUP (the International Corpus of English Corpus Utility Program) can draw upon information in this database to search by, for instance, age or gender.
In using SARA to gather dialectal information from the BNC, the analyst would want to spot check the ethnographic information on individuals included in the corpus to ensure that this information accurately reflects the dialect group in which the individuals are classified.
In studying variation by gender in ICE-GB, the analyst will want to review the results with caution, since this corpus does not contain a balanced sample of males and females.
Obviously, if the goal of a lexical analysis is to create a dictionary, the examination of a small corpus will not give the lexicographer complete information concerning the range of vocabulary that exists in English and the varying meanings that these vocabulary items will have.
Using a relatively inexpensive piece of software called a concordancing program, the lexicographer can go through the stages of dictionary production described above, and instead of spending hours and weeks obtaining information on words, can obtain this information automatically from a computerized corpus.
In a matter of seconds, a concordancing program can count the frequency of words in a corpus and rank them from most frequent to least frequent.
Such linkages between theory and practice are important because they, on the one hand, avoid the oft-made criticism of corpus linguists that they are mainly "bean counters"linguists who fail to make connections between theory and usagewith too little attention paid to the theoretical underpinnings of the data they are describing.
Instead, linguistic theories can be used to explain why particular frequencies and examples actually exist in a corpus: in other words, to discuss how theory and data interact.
This is why corpus linguistics has grown as an area of linguistics and why many people now are using linguistic corpora for many different purposes.
The first step in building a corpus is to decide what the ultimate purpose of the corpus will be.
This decision is important because it will determine exactly what the corpus will look like: what types of texts will be included in it (e.g. spoken, written, both), how long the individual texts will be (e.g. full length, text excerpts), how detailed the annotation will need to be (e.g. word-class tagging or a purely lexical corpus with minimal annotation), and so forth.
For instance, if the corpus is to be used primarily for grammatical analysis (e.g. the analysis of relative clauses or the structure of noun phrases), the corpus can consist simply of text excerpts rather than complete texts, and will minimally need part-of-speech tags.
On the other hand, if the corpus is intended to permit the study of discourse features, then it will have to contain lengthier texts, and some system of tagging to describe the various features of discourse structure.
To explore the process of planning a corpus, this chapter first provides an overview of the methodological assumptions that guided the compilation of two general-purpose corporathe British National Corpus (BNC) and the Corpus of Contemporary American English (COCA)and two more specialized corporathe Corpus of Early English Correspondence (CEEC) and the International Corpus of Learner English (ICLE).
The remainder of the chapter then focuses more specifically on the individual methodological considerations (e.g. ensuring that a corpus is "balanced") that anyone planning to create a corpus needs to address.
Consequently, fewer female writers were included in the corpus than male writers.
Like the BNC and COCA, ICLE is lexically tagged and comes with a concordancing program that has been customized to search for specific tags or combinations of tags included in the corpus and to also study effects on usage such as the age, gender, mother tongue, and so forth of the writers whose texts have been included in the corpus.
The remainder of the chapter explores these considerations in greater detail, focusing on such topics as the factors determining, for instance, how lengthy a corpus should be, what kinds of texts should be included in a corpus, and other issues relevant to the design of linguistic corpora.
With a corpus such as the BNC, we know precisely what kinds and types of English are being analyzed.
A more recent corpus, the Corpus of Web-Based Global English, is 1.9 billion words in length.
First generation corpora, such as Brown and LOB, were relatively short (each of one million words in length), mainly because of the logistical difficulties that computerizing a corpus created.
In contrast, those creating second generation corpora have had the advantage of numerous technological advances that have automated the process of corpus creation.
More recently, so many texts are available in electronic formats that with very little work, they can easily be adapted for inclusion in a corpus.
These logistical realities explain why 90 percent of the BNC (a second-generation corpus) contains written texts and only 10 percent spoken texts.
Ultimately, the length of a corpus is best determined by its intended use.
Unfortunately, such calculations presuppose that one knows precisely what linguistic constructions will be studied in a corpus.
The ultimate length of a corpus might therefore be better determined not by focusing too intently on the overall length of the corpus but by focusing more on the internal structure of the corpus: the range of genres one wishes to include in the corpus, the length and number of individual text samples required to validly represent the genres that will make up the corpus, and the demographic characteristics of the individuals whose speech and writing will be chosen for inclusion in the corpus.
But in striving for breadth of coverage, some compromises had to be made in each corpus.
For instance, while the spoken part of the ICE contains legal cross-examinations and legal presentations, the written part of the corpus contains no written legal English.
The reason that a carefully selected range of genres is not important in this corpus is that the corpus is not intended to permit genre studies but to, for instance, "train" taggers and parsers to analyze English: to present them with a sufficient amount of data so that they can "learn" the structure of numerous constructions and thus produce a more accurate analysis of the parts of speech and syntactic structures present in English.
Consequently, there exist multi-purpose corpora, such as the Helsinki Corpus, which contains a range of different genres (sermons, travelogues, fiction, drama, etc.), as well as specialized corpora, such as the previously mentioned Corpus of Early English Correspondence, a corpus of letters written during the middle English period, with the original CEEC containing letters from late middle English and early modern English and a later version of the corpus (CEECE) letters from the eighteenth century.
An early corpus of spoken British English, the London-Lund Corpus, contains 5,000-word samples.
Of course, just because only text samples are included in a corpus does not mean that sections of texts ought to be randomly selected for inclusion in a corpus.
For both the spoken and written parts of the corpus, not all samples are exactly 2,000 words: a sample is not broken off in mid-sentence but at a point (often over or just under the 2,000-word limit) where a natural break occurs.
But even though it is possible to include coherent text samples in a corpus, creators and users of corpora simply have to acknowledge that corpora are not always suitable for many types of discourse studies, and that those wishing to carry out such studies will simply have to assemble their own corpora for their own personal use.
In including short samples from many different texts, corpus compilers are assuming that it is better to include more texts from many different speakers and writers than fewer texts from a smaller number of speakers and writers.
And there is some evidence to suggest that this is the appropriate approach to take in creating a corpus.
For corpus linguists, this involves determining how many text samples need to be included in a corpus to ensure that valid generalizations can be made about the genre, and what range of individuals need to be selected so that the text samples included in a corpus provide a valid representation of the population supplying the texts.
There are linguistic factors that need to be considered in determining the number of samples of a genre to include in a corpus, considerations that are quite independent of general sampling issues.
Although it is quite easy to determine the relative importance of spontaneous dialogues in English, it is far more difficult to go through every potential genre to be included in a corpus and rank its relative importance and frequency to determine how much of the genre should be included in the corpus.
In addition, speakers would be recorded using language in a variety of different contexts, such as conversing over the phone, lecturing in a classroom, giving a sermon, and telling a story (www.linguistics.ucsb.edu/research/santa-barbara-corpus).
The idea behind this sampling frame was to ensure that the corpus contained samples of speech representing the ways that people speak in different regions of the country and in different contexts.
The previous sections provided descriptions of various corpora of English, and the many methodological issues that one must address both in the creation and analysis of a corpus.
But one issue that has not been discussed so far concerns the linguistic backgrounds of those whose speech or writing has been included in a corpus.
Note that this description does not necessarily exclude bilingual speakers from the corpus.
It simply assures that individuals whose speech will be included in the corpus have had exposure to English over a significant period.
The BNC is defined as "a monolingual British English corpus: it comprises text samples which are substantially the product of speakers of British English" (www.natcorp.ox.ac.uk/docs/URG/BNCdes.html#spodes).
This restriction allows for a certain degree of flexibility as well, as it would permit a variety of different speakers and writers of British English to be represented in the corpus.
However, this does not necessarily mean that the text must be excluded from the corpus, since there is annotation that can be included in a corpus indicating that certain sections of a sample are "extra-corpus" material; that is, material not considered part of the corpus for purposes of word counts, generating KWIC (key word in context), and so forth.
In general, when selecting individuals whose texts will be included in a corpus, it is important to consider the implications that their gender, age, and level of education will have on the ultimate composition of the corpus.
For the spoken parts of a corpus, several additional variables need to be considered: the dialects the individuals speak, the contexts in which they speak, and the relationships they have with those they are speaking with.
The potential influences that these variables have on a corpus are summarized in the following categories.
Information on gender (which is defined biologically in corpus linguistics) can be requested on a biographical form, and in written texts, one can often tell the gender of an individual by his or her first name.
Achieving gender balance in a corpus involves more than simply ensuring that half the speakers and writers in a corpus are female and half male.
To introduce more writing by females into a corpus of an earlier period distorts the linguistic reality of the period.
Consequently, to adequately reflect gender differences in language usage, it is best to include in a corpus a variety of different types of conversations involving males and females: women speaking only with other women, men speaking only with other men, two women speaking with a single man, two women and two men speaking, and so forth.
To summarize, there is no one way to deal with all the variables affecting the gender balance of a corpus.
It is also important to consider the extent to which a corpus should contain a range of dialects, both social and regional, that exist in any language.
In short, there are numerous dialects in the United States, and to attempt to include representative samplings of each of these dialects in the spoken part of a corpus is nothing short of a methodological nightmare.
The previous sections have discussed several methodological issues that need to be considered as one plans and creates a corpus.
But as corpora have grown larger, it has become a much more complicated undertaking to ensure that a corpus is both balanced and representative.
For instance, at 1.9 billion words in length, the Corpus of Global Web-Based English is so lengthy that it would be impossible to determine not just the content of the corpus but the distribution of such variables as the gender of contributors, their ages, and so forth.
To create a valid and representative corpus, it is important, as this chapter has shown, to carefully plan the construction of a corpus before the collection of data even begins.
This process is guided by the ultimate use of the corpus.
If one is planning to create a multi-purpose corpus, for instance, it will be important to consider the types of genres to be included in the corpus; the length not just of the corpus but of the samples to be included in it; the proportion of speech versus writing that will be included; the educational level, gender, and dialect backgrounds of speakers and writers included in the corpus; and the types of contexts from which samples will be taken.
The process of annotating a corpus involves running software that can (1) tag a corpus (add part-of-speech tags to all of the words in the corpus, such as nouns, prepositions, and verbs), or (2) parse a corpus (add markup that identifies larger structures, such as verb phrases, prepositional phrases, and adverbials).
Grammatical markup is inserted when a corpus is tagged or parsed.
Metadata is a key component of any corpus: users need to know precisely what is in a corpus.
Linguistic annotation varies from corpus to corpus as well.
While it is quite common for corpora to be lexically tagged, parsing a corpus is a much more complicated process.
Even though the processes of collecting and encoding data for inclusion in a corpus are described above as separate processes, in many senses they are closely connected: After a conversation is recorded, for instance, it may prove more efficient to transcribe it immediately, since whoever made the recording will be available either to transcribe the conversation or to answer questions about the recording to aid in its transcription.
In these cases, changes are natural and inevitable and, if they are carefully made, the integrity of the corpus will not be compromised.
After the project began, it was discovered that not all the regional groups involved in the project would be able to collect all of the text categories originally planned for inclusion in the corpus.
Given the wealth of speech that exists, as well as the logistical difficulties involved in recording and transcribing it, collecting data for the spoken part of a corpus is much more labor-intensive than collecting written samples.
It is therefore imperative both to inform individuals that their speech is being recorded and to obtain written permission from them to use their speech in a corpus.
However, there are ways to increase the probability that that the speech included in a corpus will be natural and realistic.
Thus, less speech is needed to reach these necessary numbers of words for the particular corpus being created.
In earlier corpora, most corpus compilers used analog recorders and cassette tapes, since such recorders were small and unobtrusive and cassette tapes were inexpensive.
Those compiling spoken corpora should therefore expect to gather much more speech than they will actually use to compensate for all the recordings they make that contain imperfections preventing their use in the ultimate corpus being created.
While most of the recordings for a corpus will be obtained by recording individuals with microphones, many corpora will contain sections of broadcast speech.
Therefore, the time saved in using second-party transcripts is worth the tolerance of a certain level of error, provided that some checking is done to ensure overall accuracy in the transcripts included in a corpus.
Thus, including something in a corpus without getting explicit permission from the copyright holder could involve copyright infringement.
With the Corpus of Contemporary American English (COCA), though, workarounds were implemented that allowed users full access to the corpus without violating copyright law.
Additionally, the corpus can be used only for academic research (www.english-corpora.org/copyright.asp).
If attempts are made to secure permission to use copyrighted material in a corpus, it is best to over collect the number of texts to include in each part of the corpus: A written text may be gathered for possible inclusion in a corpus, and its author (or publisher) may not give permission for its use, or (as was a common experience gathering written texts for inclusion in the American component of ICE) contact is made requesting permission but no reply is ever received.
Because of the difficulties in obtaining permission to use copyrighted materials, most corpus compilers have found themselves collecting far more written material than they are able to obtain permission to use: for ICE-USA, permission had been obtained to use only about 25 percent of the written texts initially considered for inclusion in the corpus.
Therefore, if a corpus is to be used only for non-profit academic research, this should be clearly stated in any letter of inquiry or email requesting permission to use copyrighted material and many publishers will sometimes waive fees.
However, if there will be any commercial use of the corpus, special arrangements will have to be made both with publishers supplying copyrighted texts and those making use of the corpus.
First of all, each text was assigned a number that designated a specific genre in the corpus in which the sample would be potentially included.
The particular information collected will very much depend on the kind of corpus being created and the variables that future users of the corpus will want to investigate.
Because ICE-USA is a general-purpose corpus, only fairly general ethnographic information was obtained from contributors: their age, gender, occupation, and a listing of the places where they had lived over the course of their lives.
Other corpora have kept different information on individuals, relevant to the particular corpus being created.
Ethnographic information is important because those using the ultimate corpus that is created might wish to investigate whether gender, for instance, affects conversational style, or whether younger individuals speak differently than older individuals.
Therefore, it is unrealistic to expect that ethnographic information will be available for every writer and speaker in a corpus.
After all the above information is collected, it can be very useful to enter it into a database, which will allow the progress of the corpus to be tracked.
Creating a corpus is a huge undertaking, and after texts are collected, it is very easy to file them away and forget about them.
It is therefore crucial to the success of any corpus undertaking that accurate information be kept about each text to be considered for inclusion in the corpus.
First, because texts to be included in a corpus will be edited with some kind of text editing program, it may be tempting to save computerized texts in a file format used by a word processing program (such as files with the extension .doc in Microsoft Word).
When creating a corpus, it is easiest to save individual texts in separate files stored in directories that reflect the hierarchical structure of the corpus.
This does not commit one to distributing a corpus in this format: the ICAME CD-ROM (2nd ed.) allows users to work with an entire corpus saved in a single file.
Each ICE component consists of texts in two main directoriesone containing all the spoken texts included in the corpus, the other all the written texts.
While this numbering system is unique to ICE components, a similar system can be developed for any corpus project.
Other directories can be created to fit the needs of the research team building a particular corpus.
For instance, a "draft" directory is useful for early stages of corpus development and can contain spoken texts that are in the process of being transcribed or written texts that have been computerized but not proofread.
Once a draft version of a text contains "metadata" and "textual markup", it can then be placed into a "lexical (pending proofreading)" directory to indicate that the text will be ready for use as a lexical version of the corpus once it has been proofread.
The first stage involves proofreading each individual sample in a corpus with samples being placed into a "proofread 1" directory.
A second round of proofreading is done after completion of the entire corpus so that the corpus can be viewed as a whole.
The segments of speech that overlap need to be marked so that the eventual user of the corpus knows which parts overlap in the event that he or she wishes to study overlapping speech.
But there are caveats to the process of creating a corpus outlined in this section.
Instead, spoken samples in this corpus consist entirely of transcriptions of numerous radio and television programs that were created by a thirdparty.
This kind of detail is included because creators of this corpus attached a high value to the importance of intonation in speech.
The advantage is that very detailed information about a spoken text is provided to the user, thus ensuring that a broad range of studies can be conducted on the corpus without any doubt about the authenticity of the data.
Metadata and Textual Markup are more descriptive, noting, respectively, what is in a corpus and where, for instance, paragraph boundaries in a written corpus occur or speakers pause in a spoken corpus.
Linguistic Annotation is more grammatical in nature, providing linguistic information about the various structures occurring within a particular corpus.
For instance, if a corpus is lexically tagged, each word in the corpus is assigned a part-of-speech designation, such as noun, verb, preposition, and so forth.
In contrast, if a corpus is syntactically parsed, various types of grammatical information is provided, such as which structures are noun phrases, verb phrases, subordinate clauses, and imperative sentences.
Metadata and textual markup are important because they can narrow the range of possible choices when a corpus is searched.
There is also TEI-conformant Textual Markup to describe features occurring within a particular corpus.
However, this will have the unfortunate consequence of skewing a lexical analysis of the corpus in which this utterance occurs, since all instances of the, police, and boat will be counted twice.
Attempting to transcribe speech of this nature in a purely linear manner is not only difficult but potentially misleading to future users of the corpus, especially if they have access only to the transcription of the conversation, and not the recording.
But while many features of writing can be annotated in a written corpus with TEI-conformant markup, annotating all of these features may prove to be unnecessary.
While many users of a spoken corpus will potentially be interested in analyzing overlapping speech, there will likely be very little interest in studying italicized words, for instance, in a written corpus.
Consequently, much of what could potentially be annotated in a written text is likely to be of marginal interest to future users of the corpus.
Nowadays, however, so many written texts exist in digital formats that they can be easily adapted for inclusion in a corpus.
Moreover, manuscripts can be illegible in sections, requiring the corpus creator to reconstruct what the writer might have written.
This corpus contains texts representing three periods in the development of English: Old English (850-1150), Middle English (1150-1500), and Early Modern English (1500-1710).
Various registers are also included in the corpus, such as law, philosophy, history, and fiction.
In 2011, a TEI-XML version of the corpus was released.
This represented the first attempt to create a historical corpus conforming to the standards of TEI.
All of these programs are designed to assign various lexical tags to every word in a corpus.
While this seems like a very high accuracy rate, in a large corpus, the remaining 3 percent of words with multiple tags can constitute a very sizable number of words.
While word-class annotation is very well established in corpus linguistics, there are other types of annotation as well.
For instance, semantic tagging involves annotating a corpus with markup that specifies various features of meaning.
This corpus contains 400,000 words of spoken English from ICE-GB and an additional 400,000 spoken words from the London-Lund Corpus, a corpus that was based on texts recorded between 1960 and 1980.
Because of the complexity of parsing a corpus, there are relatively few corpora parsed in as much detail as ICE-GB and the DCPSE.
Written texts, in contrast, are now widely available in digital formats and can easily be incorporated in a corpus after permission has been received to use a given text.
While lexically tagging corpora with part-of-speech information can now be done quickly and quite accurately, parsing a corpus is a much more difficult undertaking, since the level of accuracy decreases when identifying structures such as noun phrases or imperative sentences.
The process of analyzing a completed corpus is in many respects similar to the process of creating a corpus.
The major difference between creating and analyzing a corpus, however, is that while the creator of a corpus has the option of adjusting what is included in the corpus to compensate for any complications that arise during the creation of the corpus, the corpus analyst is confronted with a fixed corpus, and has to decide whether to continue with an analysis if the corpus is not entirely suitable for analysis, or find a new corpus altogether.
This chapter describes the process of analyzing a completed corpus.
For instance, many of the corpus-based reference grammars of English, such as Quirk et al.'s A Comprehensive Grammar of the English Language, are more qualitative, as they draw upon linguistic corpora for authentic examples to illustrate the many points of English grammar discussed throughout the grammar.
Consequently, his style of speaking has drawn considerable interest from corpus linguists.
While the Web has increasingly become a corpus used for linguistic analysis, the other two sources of dataan archive of tweets and a collection of Trump's speeches and interviewsare specific to this particular analysis.
As sources of data became computerized, concordancing programs were created that allow for various constructions (e.g. words or phrases) to be automatically retrieved from a corpus.
The results of searches can also help in establishing trends in a corpus.
The concordancing program included with the Trump Twitter Archive is only able to retrieve individual strings of words from a corpus of Trump's tweets.
For instance, the expression talk about occurred 6 times in the corpus.
The corpora available range from a corpus of American English, the Corpus of Contemporary American English (COCA), to the Coronavirus Corpus, a corpus containing texts retrieved from the Web containing discussions of the Covid-19 virus.
The size of each corpus constantly changes because all corpora featured on the site are monitor corpora: corpora to which new texts are added on a regular basis.
However, if a corpus is parsed (i.e. contains larger structures such as phrases, clauses, and sentences that are annotated), it is possible to retrieve through searches much larger structures, such as phrases and clauses.
One lexically tagged and grammatically parsed corpus is ICE-GB, the British component of the International Corpus of English.
Each individual word in the corpus is assigned a lexical tag (e.g. noun, verb, preposition, etc.).
While corpus linguistics has very little to say to those with interests in, for instance, generative grammar, it is of great value to those who are interested in studying pragmatics: how language is used.
The next section explores in greater detail the differences between qualitative and quantitative corpus analyses.
Qualitative corpus analyses, as Hasko (2020: 952) comments, have a long tradition, dating back to the pre-electronic era.
The linguist Randolph Quirk created what is now known as the Quirk Corpus, a corpus of spoken and written British English collected between 1955 and 1985.
First, to adequately represent the various kinds of written American English, a wide range of different types of written English were selected for inclusion in the corpus.
For instance, the bulk of the corpus contains various kinds of informative prose, including press reportage, editorials, and reviews; government documents; differing types of learned writing; learned writing from, for instance, the humanities and social sciences.
In addition to informative writing, the corpus contains differing types of imaginative prose, such as general fiction and science fiction.
Since neither informative nor imaginative writing is homogeneous, the corpus contained 2,000-word samples from a range of different articles, periodicals, books, and so forth to provide as broad a range as possible of different authors, books, periodicals, and so forth.
Had lengthier samples been used, the style of a particular author or periodical, for instance, would have been over-represented in the corpus and thus have potentially skewed the results of studies done on the corpus.
For instance, the London/Oslo/Bergen (LOB) Corpus contains one million words of edited written British English as well as the same genres and length of samples (2,000 words) as the Brown Corpus with only very minor differences: In several cases, the number of samples within a given genre vary: popular lore contains 44 samples, while in the Brown corpus the same genre contains 48 samples (cf.
A select overview of research conducted on this corpus makes this point very clear.
The Brown Corpus was also the first corpus to be lexically tagged.
Exploring a corpus qualitatively allows the analyst to provide descriptive information about the results that cannot be presented strictly quantitatively.
Before a multi-dimensional analysis can be performed, a corpus needs to be both lexically tagged and then analyzed by a particular statistical method termed factor analysis.
As was noted in Chapter 3, a corpus that is lexically tagged contains part-of-speech information for each word in the corpus.
Since this era, advances in corpus linguistics have resulted in the creation of many different types of corpora containing authentic texts ranging from spontaneous conversations to technical writing in the social and natural sciences.
In ECL1, it is noted that "as more and more corpora have been created, we have gained considerable knowledge of how to construct a corpus that is balanced and representative and that will yield reliable grammatical information" (138).
It requires considerable resources to create balanced corpora, such as the BNC, whereas COCA contains texts downloaded from websites, requiring much less effort than building a corpus such as the BNC.
Lexically tagging a corpus has become quite routine, particularly because the accuracy of current tagging programs is quite high.
Parsing a corpus is a more complicated undertaking, since larger structures, such as phrases and clauses, must be identified.
Consequently, more post-editing has to be done after a corpus has been parsed.
For instance, there are concordancing programs that can be used to identify and retrieve various grammatical constructions in a corpus, such as particular words or phrases.
And if a corpus has been lexically tagged, concordancing programs can retrieve various grammatical structures: lexical items, such as nouns or verbs; phrases, such as noun phrases or verb phrases; and clauses, such as relative clauses or adverbial clauses.
In particular, we can expect an increase in accuracy for certain corpus tools, such as improvement in the accuracy of software used to parse and lexically tag corpora, and the increased accuracy of tools, such as concordancing programs, used to retrieve constructions from corpora.
In the book, it is noted that these two approaches are not necessarily mutually exclusive: the generative linguist, for instance, could very well work with examples that are corpus-based rather than examples that are made up.
Section 1.4 contains a discussion of how the corpus methodology has many different applications.
Select one of the areas described in this section and briefly discuss how corpus methodology has changed the way that research is done in the area.
The COCA is a web-based corpus.
One point that is made in the chapters is that fewer letters written by women are in the corpus than letters written by men.
The idea of the Web as a corpus was initially a controversial notion, largely because at the time, more traditional corpora were the norm, and analyzing the Web, with its seemingly endless size and unknown content, contradicted the whole idea of what a corpus was thought to be.
For instance, would you want the corpus to contain a specific section of a newspaper (e.g. editorials).
As is noted in the chapter, collecting and transcribing spoken language is one of the more labor-intensive parts of creating a corpus.
But including spoken language in a corpus is very important because spoken language is the essence of human language, particularly spontaneous conversation.
Section 4.5 discusses how corpus analyses can be quantitative, qualitative, or a combination of the two (sometimes termed mixed methods).
After you have selected a corpus, you will need to create an account to use the corpus.
Corpus of Global Web-Based English (GloWbE): a 1.9 billionword corpus containing samples of English from 20 different countries in which English is used.
Following the works of American linguist Noam Chomsky, it changed from the essentially rationalist discipline it had been since the middle of the 20th Century, and gradually (re)opened up the empirical approaches represented by corpus linguistics and experimental linguistics.
The development of corpus linguistics has led to the creation of new methods for collecting and analyzing linguistic data, which were made possible thanks to the development of computers and the arrival of the Internet.
While it is true that corpus linguistics has imposed itself in an incontestable manner in the English-speaking world and that a significant proportion of French-speaking researchers currently use these methods, the teaching of corpus linguistics still remains marginalized in France.
Therefore, this book also aims to highlight the vitality and richness of corpus studies devoted to French, as well as identify the most important resources which have been developed for this language, in the hope of making a contribution to the rise of this discipline for the study of French.
For example, it is possible to collect a series of newspaper articles and make a corpus of them in order to study the specificities of the journalistic genre.
Working with corpus linguistics therefore implies being in contact with linguistic data in the form of texts, and also in the form of recordings, videos or any other sample containing language.
For most of the questions addressed by corpus linguistics, it would be impossible to search through a paper database, and that is why having computerized corpora becomes essential.
The problem of manual tracking and counting of occurrences is all the more acute since corpus linguistics is often based on large amounts of data which have not been drawn from a single book, in view of observing the multiple occurrences of a certain linguistic phenomenon and thus apprehending its specificities.
We will discuss this topic in Chapter 6, which is devoted to the methodological principles underlying the construction of a corpus.
For the moment, the important point to bear in mind is that corpus linguistics often resorts to a quantitative methodology (see section 1.5) so as to be able to generalize the conclusions observed on the basis of a linguistic sample to the whole of the language, or belonging to a particular language register.
As we will see in the following chapters, corpus linguistics may be of use in all areas of linguistics, for instance in fundamental (see Chapter 2) or applied (see Chapter 3) linguistics.
But corpus linguistics is also used for conducting research in fundamental areas of linguistics such as the study of syntax, since it makes it possible to identify the types of syntactic structures used in different languages.
Finally, thanks to the existence of a corpus of oral data, corpus linguistics also makes it possible to answer questions related to phonology and sociolinguistics.
For example, in order to determine the geographical area of diffusion of a certain pronunciation trait, it is necessary to know where each speaker having contributed to the corpus came from.
To sum up, in this section, we have defined corpus linguistics as an empirical discipline, which observes and analyzes quantitative language samples gathered in a computerized format.
In the following sections, we will discuss in depth the different central points of the definition, indicated in bold, in order to better understand the theoretical and methodological anchoring of corpus linguistics.
If sentences following such a syntactic structure never or almost never appear in the corpus, linguists might conclude that this sentence is only rarely used in English.
According to Chomsky, another problem related to corpus linguistics stems from the fact that corpora are not representative of the language as a whole.
He illustrates this problem in an extreme way, by picking the case of an aphasic speaker recorded in a corpus.
Linguists analyzing this corpus would draw totally incorrect conclusions about the language in question, since this person does not represent the linguistic competence of a typical speaker.
It is for this very same reason that it is impossible to conclude that a word simply does not exist in a language just because it is absent from a corpus.
It could simply never have been pronounced in such particular context, while it could exist in other language registers or have been mentioned by other speakers not included in the corpus.
This limitation has led to Chomsky's third criticism of corpora, namely the fact that a corpus can never contain the whole of a language and that, therefore, the above-mentioned biases are not solvable.
According to him, this problem is all the more serious because even if a corpus were of a very large size and included a representative portion of the language, it would not be fully analyzable by linguists, given the fact that it is impossible to manually analyze the content of billions of sentences.
We will return to this in section 1.4, devoted to the connections between computer science and corpus linguistics.
For example, good practice for building a corpus is to accurately document the type of language it contains.
It is nonetheless true that a corpus can only show that which it contains, and therefore the absence of evidence that a word or a structure exists in a corpus cannot constitute definitive proof of their absence from the language.
Thus, for certain research questions relating to rare or hardly observable phenomena in a corpus, it might be advisable to complement research with another empirical method, namely with the experimental method.
As we will see later in this chapter, this method shares the use of a quantitative methodology with corpus linguistics.
By resorting to many different speakers and including them in reference corpora of speakers coming from different geographical areas, corpus linguistics makes it possible to respond to this problem in a much more satisfactory way.
It is for this reason that the approach that we will adopt in this book corresponds to a corpus-based approach, considering these as available tools for linguists to be able to test their hypotheses.
Even if works related to corpus linguistics have existed for a long time (such as the indexing of the Bible by theologians or the file-based construction of dictionaries by scholars like Antoine Fureti√®re in French or Samuel Johnson in English), this discipline was only able to properly take off after the arrival of computing.
Nevertheless, looking for elements in a corpus, even a computerized one, by using a simple word processing tool is rather inconvenient.
It is for this reason that other computing tools, specifically devoted to corpus linguistics, have been developed.
In the case of corpora containing texts as well as their translation, certain tools called aligners make it possible to align the content of the corpus sentence by sentence.
That being done, bilingual concordancers search directly for the occurrences of a word in one of the two languages of the corpus, and simultaneously extract the matching sentence in the other language.
Then, in Chapter 7, we will also see that in order to answer certain research questions, it is necessary to annotate the content of a corpus.
If we only look up the word since in the corpus, we will also find occurrences which do not correspond to the use of this word as a causal adverb, but to its use as a preposition, for example in "I haven't seen Mary since Christmas".
This search can be greatly simplified if the corpus has been annotated by determining, for each word, its grammatical category.
For this study, a good starting point would be to look for relative pronouns such as who or which in order to find occurrences of relative sentences in the corpus.
In order to find only the occurrences of who and which as relative pronouns, we should use a corpus in which the syntactic structure of each sentence has been analyzed in such a way that we can assign a grammatical function to each word and group them into syntactic constituents.
The arrival of these tools has greatly accelerated research in corpus linguistics.
But corpus linguistics was not only developed thanks to the creation of such tools.
Until the 1980s, a corpus of a million words was considered to be a very large corpus.
For instance, the first reference corpora (such as the Brown corpus developed for American English in the early 1960s) were about this size.
The need to use a large amount of data and the desire to quantify the presence of linguistic elements in a corpus corresponds to a quantitative research methodology.
We will also need to make sure that the corpus collected in this way includes approximately the same speaking time or the same number of words pronounced by men and women.
For example, if we want to know whether learners of French as a foreign language at an advanced level are able to use collocations as native speakers do (collocations such as "prendre une d√©cision" -to make a decision -or "pleuvoir √† verse" -to pour with rain), we can search for occurrences of these expressions in text corpora produced by learners and compare the number of times these expressions appear -and their frequency -in a corpus of similar textual productions made by native speakers.
In summary, a corpus can be analyzed using a quantitative or qualitative methodology.
While we acknowledge the use and importance of combining these two approaches, in the rest of the book we will focus on the quantitative approach to corpus linguistics, which poses its own theoretical and methodological challenges.
On the one hand, corpus linguistics focuses on data observation as found in collections of texts, recordings, etc.
The corpus only shows you the result of the speakers' production, but not what led to these results.
The study of linguistic productions in a corpus and the manipulation of experimental variables both have their advantages and disadvantages.
On the one hand, corpus linguistics has the advantage of favoring the observation of natural data, that is, those which are not influenced by an experimental context.
A corpus of journalistic texts includes real productions by journalists, which are not produced for the purpose of being observed.
Finally, once a corpus has been created, it can be used for numerous research questions without requiring any additional time or financial costs.
Experimental studies also have definite advantages over corpus studies.
Second, while an experimental paradigm can be developed to test almost any kind of phenomenon, there are some rare linguistic phenomena which may be absent or too little represented in a corpus to be examined in this way.
It only means that they did not have an opportunity to produce them in the corpus.
To conclude, corpus studies and experimental studies can often be used in a complementary way, and, when put together, they represent powerful tools for answering a good number of research questions.
The first distinction we can make among all the existing corpora is the one that classifies them into a sample corpus and a monitor corpus.
A typical example of this type of data is the corpus that contains newspaper archives or parliamentary debates.
It is evidently impossible to collect a sample of the whole language, but in the same way that a general language dictionary aims to describe the common lexicon of a language, the general corpus seeks to offer a global image, including the main textual genres found in language.
In order to study one of these areas specifically, it is preferable to resort to a specialized corpus.
Finally, by default, a general corpus includes examples of the variety considered as a language standard, or one of its main varieties.
Another distinction that can be made regarding the types of existing corpora relates to the type of processing carried out on the linguistic data of the corpus.
For example, it is possible to build a comparable corpus of parliamentary debates in France and the UK.
Such a corpus would make it possible to compare the ways of speaking in a similar context in two languages and two different cultures.
In this chapter, we have defined corpus linguistics as an empirical discipline, that is, based on the observation of real data.
We have also seen that corpus linguistics often resorts to a quantitative methodology, studying a large sample of data which is representative of the phenomenon studied, with the aim of generalizing the observations to the whole of the language or to a language's register.
We have shown that the main difference between corpus linguistics and experimental linguistics is the way in which empirical data are collected.
In the case of corpus linguistics, data are collected in a natural context and then observed, whereas in the case of experimental linguistics, one or more causes are manipulated within a controlled context in order to observe their effects.
That being said, good practices in corpus linguistics and experimental linguistics require obtaining information about participants beforehand, which can eventually eliminate this type of bias.
Typically, researchers verify that the people who contribute to a French corpus are native French speakers.
The starting point for research is the corpus itself, in order to be able to infer usage rules from its content.
On the other hand, this type of research is corpus-based, because it starts from a hypothesis (e.g. "passive sentences tend to be used more frequently with state verbs"), and seeks to verify it in the corpus, which, in that way, only works as an analysis tool.
These tools have been developed for simplifying searches within a corpus.
These tools also help us create a list of all the words in the corpus, sorted by frequency.
While one corpus can be compared to another reference corpus, these tools also make it possible to extract a list of keywords that are specific to the corpus studied.
This corpus should be specific to the population of French-speaking Switzerland.
Finally, this corpus should contain a synchronic type of data, corresponding to the current pronunciation, rather than to its diachronic evolution.
Finally, the chosen corpus should include productions made by adult native speakers.
This corpus should contain original texts in French and their translation in English.
It should be a synchronic corpus, corresponding to current uses of the language.
Some morphologists also consider these lists as a form of corpus, because they make it possible to gather large amounts of data.
Nevertheless, these data do not necessarily represent a corpus stricto sensu, since they do not contain extracts of language produced naturally.
For some research questions, these lists can be very useful, whereas in other cases, it is necessary to resort to a real corpus containing linguistic productions in their context of use.
First, in order to assess the productivity of a morphological rule, or even to assess word formation, that is, whether certain words made up from derivation or morphological composition exist, a corpus can offer much more information than the one found in a dictionary.
Verwimp and Lahousse's study could be carried out on a transcribed spoken corpus in the absence of any form of syntactic notation, since the structure the authors were looking for matched a clearly identified lexical pattern.
For this type of case, the use of a syntactically annotated corpus becomes compulsory.
In fact, most of the time, the study of lexicon can be done directly on a raw corpus, or in some cases, on a corpus that has been annotated with part-ofspeech tagging.
By means of corpus studies, it is also possible to define lexical fields and to study meaning relations between words, such as synonymy and anonymity.
The discourse elements that are best suited for a corpus quantitative analysis are those easily identified on the basis of raw data.
In order to verify these hypotheses, they used a written corpus (Le Soir newspaper) and a spoken corpus (Valibel database), both representing the variety of French spoken in Belgium.
The authors then manually classified 50 occurrences of each connective found in the spoken and written modes as either objective or subjective, that is, a total of 200 occurrences, randomly chosen from the corpus.
For these phenomena, it is necessary to use data annotation and this annotation must be done manually on the basis of an observation of the entire corpus.
In order to study the distribution and prevalence of this type of structure in the French spoken in Parisian suburbs, all the occurrences of indirect questions were collected in a small oral corpus of approximately 350,000 words.
They observed that this structure was much less used in this second corpus, with only two occurrences by male speakers of Moroccan origin.
Each of the messages in the corpus was coded according to the three linguistic variables to be studied, namely the message's length, the presence of opening and closing elements and its main function.
Finally, all the corpus studies illustrated in this chapter were carried out on corpora of very diverse shape and size.
In Chapter 5, we will discuss in more depth the data that are available in French to the public for carrying out corpus studies.
A selection of occurrences of the vowels to be studied, representing the different periods, should then be looked up in the corpus.
Conversely, phenomena such as the structuring of information within discourse and the analysis of global coherence are much more difficult to annotate on a large scale since they require an entirely manual processing of the corpus.
The main constraint posed by many pragmatic phenomena such as the study of speech acts and implicatures is related to the fact that the occurrences of these phenomena cannot be identified on the basis of lexical markers which can be automatically searched for in the corpus.
In order to identify the occurrences of pragmatic phenomena which are not related to words, it is therefore necessary to scan the entire corpus manually.
When the sociological information about the speakers in a corpus is known, it is very easy to use this for comparing the productions of different categories of speakers such as men and women, people from different age groups, from different socio-economic backgrounds, etc.
These data are typically not part of the corpus itself, but are listed in the metadata (see Chapter 6).
Typically, a longitudinal corpus includes a recording of a few hours, gathered every one to three weeks.
However, when a certain linguistic phenomenon is not found in the corpus, it does not necessarily mean that the child or patient recorded in the corpus does not have the competence to produce such types of words or sentences.
As we will later explore in the case studies, when studying children with atypical development, it may be advisable to analyze a corpus of children with typical development (or suffering from a different pathology which does not involve the same linguistic deficits) in parallel, so as to favor the comparison between different linguistic productions in similar contexts.
To do this, in the spoken corpus InterFra, Forsberg Lundell et al. (2014) defined three groups of French non-native speakers whose mother tongue was Swedish.
Buysse (2020) compared the oral productions of French-speaking and Dutch-speaking learners of English in the LINDSEI corpus with the productions of native English speakers in the LOCNEC corpus, a corpus which was compiled to be comparable with the LINSDEI (see Chapter 4 for a definition of the concept of comparability).
Regardless of the stylistic genre targeted, the use of quantitative methods linked to corpus linguistics has led to many advances in lexicography and has been one of its main application areas.
From the point of view of functional words, texts intended for children in the corpus mainly refer to the question of space, whereas the texts intended for adults focus primarily on the temporal dimension.
For example, for meaning (3), the Nouveau Petit Robert mentioned "a woman's companion", whereas the corpus showed that this use also extends to homosexual couples.
This is why literary scholars working in stylistics have started to acknowledge the interest of incorporating quantitative methods from corpus linguistics into the analysis of literary texts, so as to have a more objective starting point for identifying interesting themes and relevant excerpts, which should be the subject of a book's qualitative analysis.
As we will see below, in the field of stylistics, corpus linguistics tools do not seek to replace qualitative analysis, but only aim to guide the choice of themes and excerpts to analyze.
McIntyre and Walker (2010) built a corpus of 200,000 words from blockbuster film scripts.
The results indicated that male characters have a much longer speaking time than the female characters, more than four times more words in the corpus.
Likewise, the topics men and women talk about (identified from the keywords of the corpus) also differ.
A search for this same connective in the COBUILD corpus of spoken English showed a frequency of one occurrence every 500 words, which matches the use by witnesses rather than the police.
Even more strikingly, the structure of the type then I appeared 10 times more frequently in the COBUILD corpus that the I then construction, repeatedly employed in the declaration.
In this chapter, we have shown how corpus linguistics can be of use in different areas of applied linguistics.
If the corpus has been annotated, as is the case of many corpora in the CHILDES database, other analyses become possible.
Above all, it is fundamental that any child recorded in the corpus has been diagnosed with ASD according to the diagnostic tests recognized in the literature, rather than following the mere indication of the pediatrician.
That being said, as we discussed in Chapter 2, it is difficult to identify certain pragmatic uses automatically by means of a corpus search, because these uses are not transparently linked with the linguistic form used.
In many cases, it is necessary to analyze the corpus manually so as to find interesting occurrences.
For example, one of the occurrences of the word colline (hill) in the Le Monde corpus (year 2012) is "Sur la colline, tout le monde est all√© voter" (On the hill, everyone went voting).
There are many other similar cases in the corpus.
A priori, it is possible to create a comparable corpus for any language pair, provided that digitized texts of a comparable nature are available in each language.
The situation has changed a great deal since the 1990s, thanks to the arrival of corpus linguistics, which made it possible to empirically compare linguistic systems.
The data provided by these contrastive corpus-based studies are not only useful in theoretical linguistics for understanding how languages work, but may be helpful for other applications, notably for the development of tools such as bilingual dictionaries (see section 4.6).
For the English-French pair (remember that the TED corpus is unidirectional), these variations enabled the authors to compare the impact of this variable on the translations under scrutiny.
The comparison revealed that translation choices were systematically less varied in the TED corpus than in other corpora.
As in the case of contrastive studies, translation studies has benefited from the availability of multilingual corpora, as well as theoretical and methodological advances in corpus linguistics.
This hypothesis has been partly confirmed through corpus studies, performed on a single language pair and limited to one translation direction.
In this way, we can see how corpus linguistics can provide translators with tools that may help them adapt their translation choices on the basis of a better identification of the recurrent linguistic properties at work in a text.
They confirmed that some of the meanings frequently found in the corpus could not be adequately translated by their "equivalent".
For example, 75% of the occurrences of plus au moins in the corpus should have been translated using expressions such as pretty much or somewhat in English, rather than using the expression more or less.
Indeed, such a corpus makes it possible to establish the degree of mutual correspondences between these connectives, by counting the number of times that they can be translated by each other.
For instance, the use of these connectives could be compared only in the source language section of the parallel corpus.
A parallel corpus, containing translations, would not be able to meet these two objectives.
First of all, this study should be carried out by means of a parallel corpus, in order to determine to what extent these two pronouns are translation equivalents or not.
More specifically, a bi-directional parallel corpus should be used, since the equivalences are often variable depending on the direction of translation.
This analysis of translations should be supplemented by a study on comparable corpora, made up of the two original language sections from the parallel corpus.
The number of words used per sentence can also be calculated, even on a corpus that has not been subjected to syntactic annotations.
Links to websites providing online access to corpora, as well as corpus consultation tools, are listed at the end of the chapter.
For example, it is very common for research teams to offer the corpus compiled during their research projects as a tool available to the general public, once the project is finished.
An important element to take into account when downloading a corpus from the Internet is its encoding format (this not to be confused with the compression format, when applicable, that needs to be handled by means of specific archiving tools).
Another element to take into consideration before deciding to download an entire corpus is its size.
For example, the online interface on the OFROM corpus (see section 5.3) only offers the possibility of looking for productions by women or men, but for the moment does not indicate the total number of participants of each genre, or the number of words produced by each of them.
In the same way, the consultation interface for the CLAPI corpus (see section 5.3 for a description) does not currently provide information about the total number of words included in the portion of the corpus that is available for online consultation.
In many cases, this piece of information can be obtained by contacting the corpus creators.
For example, this is the case of the Belgian Valibel database of spoken French (see section 5.3) or the SMS corpus in Switzerland's national languages, collected by the universities of Zurich and Neuch√¢tel (see section 5.5).
After purchase, Le Monde newspaper corpus (see section 5.2) can be downloaded via corpora distribution sites such as the European Language Resources Association or ELRA, the Linguistic Data Consortium or LDC, or distributed in a CD-Rom format, such as the French SMS corpus collected in Belgium.
In all cases, it is necessary to study the rights of use indicated by the respective sites before starting to compile the corpus.
We will discuss this issue in more detail in Chapter 6, which is devoted to presenting the basic principles of corpus creation.
More specifically, when researchers reuse a corpus created by other teams, they must mention in their publication the Internet link where the data were downloaded or retrieved from.
Very often, the authors of a corpus provide a bibliographic reference where they describe their corpus or the name of the team who compiled it.
Unlike many European languages, French still does not have a reference corpus, a representative sample of the French language in general, similar to the British National Corpus that exists for British English, one of the pioneers in the genre.
For the time being, it is not possible for linguists to observe how the French language works through the study of a single corpus.
In the field of journalism, the most exhaustive resource is undoubtedly the Le Monde corpus, which contains the newspaper's archives for the period 1987-2012, representing a total of nearly 1,200,000 articles, corresponding to almost 20 million words per year.
The Le Monde corpus is a valuable tool not only for exploring the French journalistic style but also for studying recent developments in the language, thanks to its data spanning 25 years.
Unfortunately, this corpus is not available for free and must be purchased via the ELRA platform.
In the more specialized journalistic genre, the Sciences Humaines corpus produced by ATILF (Analyse et Traitement Informatique de la Langue Fran√ßaise [Computer Processing and Analysis of the French Language]) in Nancy includes 125 linguistic articles from the Sciences Humaines journal.
Despite its modest size, this corpus makes it possible to study the specificities of the journalistic style when applied to a particular field.
The corpus is now available via the Ortolang platform, where it can be downloaded for free.
As for the literary genre, the Frantext corpus brings together many literary texts ranging from ancient to modern French, in a corpus which totals more than 250 million words.
Since 2018, a new version has made it possible to consult the corpus by means of an improved interface, facilitating the search for regular expressions.
The corpus has been lemmatized and tagged into grammatical categories, which also helps in refining the search criteria.
This version of the corpus is available online but requires a paid subscription.
A portion of the corpus, including works from the 18th to the 20th Century, can be downloaded for free from the Ortolang website.
The main corpus, BFM 2016, includes 153 texts, corresponding to more than 4 million words.
This database also provides access to the Corpus repr√©sentatif des premiers texts fran√ßais or CORPTEF, which brings together texts from the 9th to the 12th Centuries, and to the Passage du latin au fran√ßais corpus or PALAFRALAT, which aims to document the linguistic transitions between Latin and French.
Finally, the Corpus Fran√ßais de l'universit√© de Leipzig, which is not actually a corpus stricto sensu as it contains a set of isolated sentences rather than whole texts, brings together different sources such as newspapers and web pages, as well as entries from the participative encyclopedia, Wikipedia.
The data collection mode makes this corpus unsuitable for many types of research but provides a very useful interface for lexical searches, offering the possibility of looking for simple or compound words and having access to all the occurrences within the context, with an indication of the source for each occurrence.
For each request, the interface returns an indication of the frequency rank of the word looked up in the corpus.
The Traitement des corpus oraux en fran√ßais project or TCOF from the ATILF laboratory brings together corpora collected between the 1980s and 1990s, and later enriched in the 2000s.
The portion of the corpus available to the public not only includes interactions between adults and children, but also interactions between adults only.
This corpus was designed to document less commonly taught languages or regional varieties of widespread languages.
The other corpora include children up to the age of 7, and only one of them (VionColas corpus) includes children up to 11 years old.
The CHILDES database also offers a section on bilingualism, including five corpora for which one of the languages is French, the other language is Portuguese (Almeida corpus), Dutch (Amsterdam corpus), Russian (Bailleul corpus) and English (GNP and Watkins corpora).
The FoudonReboul corpus is a longitudinal corpus of eight children with autism spectrum disorder (ASD), recorded between the ages of 4 and 9 years.
The Nadig corpus also includes 28 French-speaking children diagnosed with ASD, aged between 3 and 7 years.
Le Normand corpus includes seven children diagnosed with epilepsy and specific language impairment (SLI), recorded between the ages of 4 and 5 years.
This is the case of the EMA √©crits scolaires corpus (Bor√© and Elalouf 2017), a longitudinal corpus bringing together the text productions of primary and middle school children.
A portion of the corpus contains narrative texts produced by CP and CE1 students (6-7 years old), while the other section is made up of a series of argumentative texts produced by CE2 and CM1 students (8-9 years old).
The corpus includes images of handwritten texts, raw transcriptions in text format and an annotated transcription, also in text format.
This corpus can be downloaded from the Ortolang platform.
The corpus is made up of 11 sub-sections containing at least 10 texts each, produced under similar conditions, namely by students of the same level.
This corpus can be downloaded from the Ortolang platform.
The corpus amounts to approximately 100,000 words but only part is publicly available.
This portion of the corpus comprises a longitudinal section, where each learner has produced four texts.
The cross-sectional portion of the corpus includes 136 texts written based on the same image description task, by learners from the initial level to the advanced level, and by a control group of native speakers.
The Dire autrement corpus, created in Canada by Marie-Jos√©e Hamel and Jasmina Milicevic, contains texts mainly produced by English-speaking learners.
The corpus is available on request from the authors.
Learner levels vary depending on the corpus.
The Interfra corpus created by Inge Bartning and Fanny Forsberg Lundell focuses on Swedish learners of French at different levels.
The corpus contains interviews, narrations based on videos, and images.
The first part of the corpus includes French learners who have been exposed to the language within the context of schooling.
A second portion of the corpus focuses on advanced learners who have all lived in France, for a period ranging from 1-2 years to more than 30 years (see Chapter 3, section 3.3 for a study based on these data).
The corpus is fully available to the public and can be viewed free of charge via an online interface.
The corpus contains conversations during spoken exams and in informal contexts, and amounts to approximately 15,000 words, 9,500 of which were produced by learners.
The corpus has been transcribed in CHAT format and can be downloaded or viewed online.
The parallel portion of the corpus, Multilingual Parallel Corpus, contains texts translated into nine European languages: German, English, Danish, Spanish, French, Italian, Greek, Dutch and Portuguese.
The comparable portion of the corpus contains articles from financial newspapers of the early 1990s, in six languages: German, English, Spanish, French, Italian and Dutch.
Given the fact that the Europarl corpus was mainly compiled to serve as training material for machine translation systems, the difference in status between the original and translated languages is of little importance.
This corpus includes spontaneous language, prepared speeches and written texts.
The version of the corpus which is available free of charge online includes 1,300,000 aligned sentences or fragments, amounting to approximately 2 million words per language.
Representative of another type of language register, TED conferences have recently made it possible to create a large parallel corpus.
Another limitation of this corpus is that it is unidirectional.
Actually, TED Talks are always made in English; as a result, English is the only source language, contrary to the Europarl corpus, where all languages are alternately source and target.
In the TED Talks corpus, the only variations concern the numerous target languages.
In total, this corpus includes 200 articles which correspond to approximately 400,000 words.
The corpus can be queried online.
For instance, in the Litt√©racie avanc√©e corpus described above, a search for the French word avis (opinion) by means of a concordancer indicates that the students have used this word 61 times.
For example, in the Litt√©racie avanc√©e corpus, the five most frequent cooccurrences to the right of the word avis are avis sur, which appears 11 times and then avis et (six times), avis de (five times), avis divergent (four times) and avis des (three times).
The search for co-occurrences to the left indicates that the most frequent elements found in the corpus are: leur avis (18 times), les avis (seven times), d'avis (six times), l'avis (six times) and son avis (five times).
Finally, some concordancers can be used to extract a list of keywords in a corpus by comparing them with a reference corpus (see Chapter 6).
More specifically, the concordancer determines which of the words are used significantly more (or even less used in the case of negative keywords) in the search corpus compared to the reference corpus.
Another possibility is to take a portion of a corpus and compare it to the rest of the corpus, or to compare a corpus with another similar corpus.
For example, in the Litt√©racie avanc√©e corpus, we can identify the keywords which specifically match student reports, compared with other types of academic work such as dissertations, by comparing this sub-section of the corpus to the others.
In the case of the noun Sala√ºn, its presence in the keywords of the corpus can be explained by the fact that Sala√ºn was a general delegate of a road prevention association who had taken part in an interview that students had to discuss in one of their assignments.
In another register, a comparison of the Le Monde corpus from the year 2011 with that of the year 1987 generates keywords in the form of common nouns such as euros, economy, Internet and international and proper nouns like Sarkozy, Hollande, Obama, Aubry and Merkel.
In a nutshell, the keyword list of a corpus is very useful to identify its main topics, provided that the comparison with the reference corpus is appropriate.
In sum, concordancers make it possible to analyze recurrent properties in a corpus, such as its frequent words, its collocations and its keywords from a quantitative point of view, something which is not possible to infer from simply reading texts. This is why they represent essential tools for grasping the quantitative properties of a corpus.
So, to begin with, it is necessary to convert the files included in the corpus to text format.
The Clusters and Collocates tabs help you to identify the collocations spotted in the corpus, as well as the most likely collocations.
AntConc has another feature which offers the possibility of generating a list of all the words in the corpus sorted by frequency via the Word List tab.
Finally, AntConc makes it possible to create a list of keywords from the corpus based on the comparison with a reference corpus.
CLAN offers the possibility of formulating queries on the CHILDES corpus for studying many aspects of children's language.
A request in CLAN should always start by specifying the name of the command to be performed, followed by the search elements, the file or file's name where the information should be retrieved from, and whether it should be related to the whole corpus or only narrowed to some files.
One of the most useful CLAN commands is the combo command, which helps you to look up words or word sequences produced by specific speakers in the corpus.
If the corpus has been annotated, this command also makes it possible to search for grammatical categories, speech acts or even errors.
Let us take a look at an utterance from the York corpus (De Cat and Plunkett 2002): *CHI:tu me l'as donn√©.
We have observed that, despite the absence of a reference corpus, numerous more specific corpora are available, which can be combined to carry out research in many areas of linguistics, as we will see in the subsequent exercises offered.
Using the interface provided on the website of the Corpus fran√ßais de l'universit√© de Leipzig, what is the frequency order in this corpus of the words maison, chalet, immeuble, bungalow.
Compare with the results for Max in the same corpus.
The word immeuble is the second most frequent word, with 20,133 occurrences, making it the 6,633th most frequent word in the corpus and corresponding to frequency class number 12.
The third most frequent word is chalet, with 8,184 occurrences in the corpus, corresponding to frequency rank number 13,609 and frequency class number 13.
The word bungalow is the least frequent word, with 1,100 occurrences in the corpus, corresponding to frequency rank number 53,542 and frequency class number 16.
The proper nouns associated with chalet in the corpus show one of the limitations of the collocation analysis.
Indeed, in the corpus, several articles referred to Roman Polanski's residence, which made these associations very strong, but these words do not obviously collocate in everyday language.
The 10 most frequent content words are the following: This list illustrates the fact that the most frequent words in a corpus are those belonging to functional categories such as prepositions and determiners.
We can also observe that the frequency of words in a corpus decreases rapidly.
The most frequent word in the corpus, that is, de, has 4,461 occurrences, whereas the 32nd word has only 499 occurrences, representing almost 10 times fewer occurrences.
In addition, from frequency rank number 4,077 onwards, the words in the corpus only have one occurrence.
In the corpus of spoken French from Quebec, there is no occurrence of the verb d√©tester produced by men, versus 16 occurrences produced by women.
The most frequent word produced by Max at the start of the corpus at 1 year and 9 months old was also no, with 24 occurrences.
At 3 years and 2 months old, at the end of the corpus, the most frequent word was I, with 48 occurrences.
His MLU ranged from 1.17 at the start of the corpus to 3.78 at the end.
At the end of the corpus, Anne produced 230 word types against 182 for Max, which tends to confirm that her language was more advanced.
The English noun issue was used 1,957 times in the TED conference corpus.
First, we will discuss some facts that need to be considered before deciding to create a new corpus and highlight the advantages of reusing existing data whenever possible.
Then, we will address various important methodological concerns for creating a corpus, in particular questions related to the size and representativeness of samples, and will explain simple methods for data sampling and coding.
We will finally see that the task of creating a corpus carries with it a certain number of ethical and legal issues which must be dealt with.
As we will see throughout the chapter, creating a corpus is a challenging task and presents many difficulties.
Choosing the right texts to be included in a corpus should also be the object of careful reflection, since any kind of analysis carried out on data that are not representative of the target genre (see section 6.2) could be largely invalid.
When it comes to creating a reference corpus, the data collection phase is so time-consuming that it can only be tackled by a group of experts.
Becoming involved in a corpus creation project individually is realistic only in the case of specialized corpora, for example, if the task is narrowed to a specific language register or a regional variety, that is, a project of a smaller size.
These need to be collected in the form of audio files which are later transcribed to become analyzable with corpus searching tools.
For example, for investigating the expression of subjectivity in journalistic discourse, a corpus entirely made up of editorials would not be appropriate, since this is only a sub-section of the genre, which incidentally is more likely to contain markers of subjectivity than other sub-genres, as dispatches for instance.
In this case, two scenarios are possible: either the conclusions of the study will be limited to the editorial style, or the corpus should be diversified in view of including other types of journalistic texts.
The problem we have just mentioned involves a key methodological point for corpus studies, which is the representativeness of data.
Suffice it to say that the characteristics of a corpus may be more or less appropriate for answering a research question.
Currently, some corpora such as the Google Books corpus (see Chapter 5) and the FrenchWeb 2012 corpus (available on Sketch Engine), collected from the Internet, contain several billion words.
For a long time, the rule of thumb for collecting a corpus was that it should be as large as possible.
The logic behind this principle was that the larger the corpus, the more likely it would contain occurrences of rare linguistic phenomena.
In fact, a large corpus is not always suitable for addressing all kinds of research questions.
The question of the optimal size for a corpus primarily depends on the nature of the linguistic phenomenon to be studied.
The more frequent a linguistic phenomenon, the better it can be studied on the basis of a small corpus.
A corpus representing a specific genre can be relatively small, whereas general corpora need to be much larger.
As we discussed in Chapter 1, a corpus does not simply represent a collection of randomly chosen texts.
The question of representativeness is therefore essential so that a corpus can be used for answering a research question.
The same applies to the compilation of a corpus.
In order to be a representative, a reference corpus should contain a balanced set of samples covering the main stylistic genres, both in the spoken and written modes.
The main issue is to determine the criteria according to which it is advisable to classify the elements included in the corpus to ensure its representativeness.
For example, it would be rather inappropriate to try to study the production of speech acts in the legal context by choosing a corpus exclusively based on a number of performative verbs such as demand, condemn, order that it contains, since this criterion would influence the results found in the analysis afterwards.
However, this study would require the assembly of a corpus which tangibly represents the legal language, such as court decisions, because these writings properly match the targeted field of study, that is, the legal language.
The spoken section of a corpus should reconcile a variety of choices.
For a literary corpus, for example, works from different authors should be included.
In the case of monitor corpora, while the new data added at regular intervals may improve their representativeness, they nonetheless pose balancing problems between the different portions of the corpus, an aspect we will discuss in the following section.
Unless we are working on a very specific corpus like the Bible or the complete works of an author, most of the time, it is actually impossible to include all the texts or all the recordings belonging to the genre to be studied in a corpus.
The preparation of samples to be included in the corpus poses two important methodological questions: on the one hand, the appropriate size for each sample, and, on the other hand, how to balance the portions of the corpus in such a way that the result is truly representative of the genre.
In order to understand the difficulties of corpus balancing, we will give an example.
To be representative, a spoken French corpus should include speakers from different regions, different ages, both male and female.
If 95% of the corpus is made up of Parisian speakers aged between 20 and 30 years, such a corpus will not be a representative sample of the population, even if this means including the other speakers among the remaining 5%.
A corpus of spoken French should therefore include a similar proportion of male/female speakers, from different age groups and different regions.
For example, a corpus created for studying the pronunciation of vowels in Paris and Marseille and which has not been compiled representing a balanced sample of different age groups cannot be used carelessly for studying the evolution in the pronunciation of vowels between generations.
However, the creators of large corpora have agreed that it is very difficult to fulfill all the criteria to achieve a perfectly balanced corpus.
This issue prevents the inclusion of recently published texts in a corpus that have not yet fallen into the public domain.
In the end, balancing a corpus is never a perfect task.
Each of these properties should then be mirrored by the corpus sample proportionally to its prevalence in the population.
For example, in order to create a corpus of French, speakers from different regions should be included in the sample.
Therefore, geographic region becomes a relevant trait for the sampling frame of a spoken French corpus.
In these cases, the classification is often based on common sense or on the pragmatism of the corpus designer, depending on the importance of subcategories for addressing the questions that the corpus is supposed to help study.
Now, let us move on to the question of which samples to include in the corpus.
A first technique involves choosing the samples completely at random, the idea being that out of the total number of samples in the corpus, the most frequent characteristics will eventually stand out on their own.
However, we cannot take for granted that this method yields a balanced sampling, especially in the case of a small corpus.
For example, if the sampling frame for a corpus of French spoken in Switzerland includes different criteria such as gender, age, socio-economic level or place of residence, an equivalent number of samples should be chosen to match each selected criterion.
The more generic the corpus, the more samples will be needed, whereas for a more specialized corpus, fewer samples are necessary in order to provide representative data.
On the one hand, it is preferable to create a corpus including language samples which represent a coherent whole, rather than isolated sentences.
In Chapter 7, we will see that errors can be systematically annotated in a corpus, in the same way as syntactic or semantic information is provided.
No matter the way of acquiring data, an important point is to save the corpus files into a format which can then be used by a concordancer.
Therefore, all newly created files for a corpus should be directly saved into text format.
In the case of corpus studies, this is not a problem, however, since the files will not be read by humans, but processed by a concordancer.
This might eventually become a problem with a corpus, including thousands of different files.
The Sketch Engine corpus creation and management platform, discussed in Chapter 5, also automatically transforms the format of files downloaded from the Web.
The corpus created in this way can be directly analyzed using the tools provided by the platform, for example, for retrieving concordances, word lists or keywords.
If the corpus has been created manually rather than through the use of a platform enabling automatic data download, a practical but important question concerns the number of files that should be created.
A practical solution is to code these characteristics directly onto the file names: this is why these names are another important element that should be taken into account when creating the corpus.
For example, it is possible to name files only by using a number, each representing a criterion used when compiling the corpus.
Following this procedure, the sample corresponding to the first 18-year-old male participant from Marseille registered in the corpus would be saved in a file called "221001.txt".
We have pointed out that corpus files should contain plain text, in order to facilitate data analysis.
This "piece of extra information concerning the data" included in the corpus is what we call metadata.
For this information to be made available for future users of the corpus without separating it from the rest of each sample portion, it should be possible to include it in the files, but in such a way that it is not taken into account by a concordancer when counting the words of the corpus.
The conventions used can be explained in the corpus documentation or may follow a more widely recognized standard.
Audio recordings make it almost impossible to grasp this type of information, which later have to be added to the corpus so that the interactions can be understood and analyzed by the experts who will listen to these recordings.
In the case of audiovisual recordings, a larger share of contextual information will be captured and should not be explicitly added to the corpus (although some kind of codification may later be required to perform specific analyses).
Due to the difficulty of collecting and transcribing spoken data, the question of the amount of data needed for creating a corpus is even more acute than for written data.
But in this case too, there is no ideal size for a spoken corpus.
The importance of this information depends on the questions that the corpus is expected to answer.
For example, in the CLAPI corpus, the contributions from the different participants are presented one below the other.
Creating a corpus involves using (or even sharing with other researchers) language samples produced by third parties.
In the case of a spoken corpus in particular, it is essential for participants to know that they are being recorded and that their data will later be used for linguistic analyses.
For this, the creators of a corpus must hand a document to their future participants clearly explaining who the data will be accessible to and how it will be used.
If a participant later refuses to have their data distributed, removing them from the corpus may pose many difficulties.
The right to anonymity of the persons mentioned in the corpus represents another important ethical problem.
In general, we should also be aware of the fact that distributing a corpus implicitly amounts to disseminating the ideas contained inside its texts.
This solution can be realistic when creating a corpus drawn from a limited number of sources.
Another way of dealing with the copyright problem during corpus distribution would be to allow users to search for concordances in the corpus, but not to visualize it in its entirety.
In this chapter, we have discussed the main elements to consider when creating a new corpus.
For a start, we mentioned that corpus creation is a long and complicated process.
Then, we saw that the important methodological trait to be respected when creating a corpus is datarepresentativeness.
The representativeness of a corpus also depends on its balance and the choice of samples it contains.
We then addressed some concrete problems, related to data coding and transcription into a corpus, and concluded that these questions needed to be resolved before starting the data collection phase.
Finally, we saw that the creation of a corpus poses several ethical and legal questions which should be carefully considered, since distributing data amounts to disseminating information that belongs to and concerns third parties, whose rights must be respected.
The young speakers included in the corpus should proportionally represent the two genders and have different socio-economic profiles.
Finally, the corpus should contain young speakers recorded under similar conditions in order to avoid any context-related bias.
Another study could aim to compare these uses across different speech styles, which should then be represented in the corpus in a balanced manner.
First, the corpus should contain French literature, which restricts the literary subject to works written in original French, rather than translations.
The difficult point to assemble this corpus relates to the way of balancing its content between different literary genres.
Let us assume that the corpus targets a size of 200,000 words per genre, in order to have a representative sample of each of them.
Conversely, since poems are a very short genre and more marginal in terms of the amount of texts published, a possible decision would be to limit the share of poems to a smaller percentage of the corpus, for example, to limit poetry to 50,000 words.
The first question to ask is whether a sample is representative of the genre it embodies in the corpus.
For example, before deciding to include an interview with a young Brussels resident in a corpus of French spoken in Belgium, we should make sure that this person has not recently moved to Belgium from another country, and that they properly reflect the linguistic specificities that the corpus is supposed to embody.
The second important question concerns the size of the sample that will be included in the corpus.
As we recalled above, it is not always optimal to include entire texts in a corpus when these are very long.
Besides, it is also necessary to determine which metadata will be associated with each corpus sample.
A final and very important point is to ensure that copyright is respected, either because the text is copyright-free or because the author has provided written consent granting permission to include their text in the corpus.
The keywords in the corpus include proper nouns such as Edison, Fun√®s, Fernandel, Gabin and Reynaud and also content words like cin√©phile, cin√©ma and cr√©dits.
These collocations make perfect sense in view of the search terms used for creating the corpus.
The metadata of such a corpus should at least include information of where the recording took place, its date, the context in which it happened, the conversation topic, the number of participants, the gender of the person recorded, his/her age and profession.
No element in the corpus should make it possible to identify any participant.
Depending on the purpose of the corpus, this agreement should also consider data sharing with third parties.
In Chapter 6, we discussed the importance of associating metadata with corpus files.
In this chapter, we will explore how to insert other types of information into a corpus as linguistic annotations.
To begin with, we will see that annotating a corpus is a way of adding value to it by widening the field of questions that it will make it possible to investigate.
We will then review the different types of annotations we can add to a corpus, briefly present some tools for performing some annotations automatically or for making manual annotations easier.
Let us imagine, for example, that we wish to know how often French nouns such as ferme and car appear in a corpus.
A simple search for their occurrences in a raw data corpus will provide a biased answer, since these words also have other uses apart from being nouns, and these will be included among the search results (ferme is also a conjugated form of the verb fermer and car is also a coordinating conjunction).
Such manual sorting tasks can actually be avoided if the words in the corpus have been previously annotated into grammatical categories.
In this way, they make it possible to look for such phenomena automatically, without having to listen to all the audio files in the corpus again.
Words are often the linguistic element the most subjected to annotations in a corpus.
Lemmatizing a corpus is an essential process for studying lexicon, since annotation makes it possible to look for occurrences without having to detail all the morphological variants it may adopt.
Finally, words can be annotated into semantic categories, for example, the word tennis can be annotated as a part of the sports category, later making it easier to quickly go through the content of a corpus, and to disambiguate polysemic words in context.
These annotations imply a global processing of the corpus.
Conversely, many other annotations made on corpus studies are devoted to a single type of element to be studied.
For example, a corpus that is annotated into grammatical categories with the aim of being used as a reference tool for beginner language learners will have to use a simplified categorization of grammatical categories.
For example, for annotating speech acts in a corpus, a tag like question could be interpreted very differently depending on the annotators, if no explanation is added.
Once the tag set has been defined, the corpus processing phase can begin.
The first step is to identify which occurrences will be annotated in the corpus.
For some phenomena such as the annotation of morphosyntactic categories or speech acts, every word or sentence in the corpus will be involved.
However, for other phenomena, the annotation will only refer to very precise elements in the corpus.
Annotations covering the entire corpus are often made using computing tools, which we will describe in the next section.
In the rest of this section, we will focus on a detailed annotation of specific elements in the corpus.
For example, if an initial distinction proves impossible to discern sensibly in the corpus by the annotators, it should be abandoned.
It will therefore be up to each researcher to identify the tool best suited to his corpus and his/her research question.
The Sketch Engine platform, which we presented in Chapter 6, automatically performs this tagging process when a new corpus is created and this can be done for several languages (see below).
The list of tags used for a corpus is presented on the site.
However, Sketch Engine makes it possible to tag a corpus in French as well as in many other languages.
Once annotated, the elements of the corpus can be searched by their POS tags by means of a concordancer.
This research specifies that all forms of the lemma acteur will be retrieved from the corpus when a word tagged as an adjective appears to its right.
For example, Brat is an online tool that makes it possible to annotate not only entities in a corpus, but also the relations between them.
We can thus annotate events described in a corpus, as well as the links between the various participants in these events.
XML can also be used for encoding the metadata of a corpus (see Chapter 6, section 6.4).
For annotations of a specific linguistic phenomenon (in the situation presented at the end of section 7.2), one solution would be to retrieve the relevant data from the corpus, and then to annotate them separately.
It was by means of this type of comparison that the annotations offered for the Google Books corpus in section 7.2 were considered as suitable tagging and parsing systems.
Residual errors are either corrected manually or in the case of very large corpora such as the Google Books corpus, they are left as such, because their prevalence is low enough not to significantly bias the results of a research.
The annotation manual should also list the rules that have been applied to certain borderline or ambiguous cases in the corpus in order to deal with them systematically.
For example, it should specify the manner in which the corpus has been segmented into words, sentences, utterances or discourse segments.
Finally, when automatic processing tools have been used for preparing and (helping to) annotating the data in the corpus, they must be clearly indicated.
In this chapter, we introduced the notion of linguistic annotations and discussed the different ways in which these annotations can be incorporated into a corpus.
Annotated corpora, however, make it possible to look for elements in the corpus related to syntactic or semantic annotations, for example.
The second advantage of annotated corpora is that annotations can be reused later and thus add value to a corpus.
Finally, the annotation of a corpus represents a transparent process when it is well documented, which enables an objective evaluation of the data by the scientific community, contrary to other methodological choices which are often less transparent.
Here in the table below is a list of 20 occurrences drawn from the corpus with two possible annotations.
Then, we will present two types of descriptive statistics that, respectively, make it possible to measure lexical diversity (the type/token ratio), and to calculate lexical dispersion in a corpus.
The first step for analyzing quantitative data is to describe the content of a corpus.
For example, this step could involve calculating the number of sentences in the passive voice used in a corpus of journal articles.
Another measure of central tendency is the median, placed at the middle of the different values found in the corpus, so that the data set is divided into the lower half and the upper half.
Imagine a study aiming to compare the use of passive sentences in spoken and written modes, and finding 67 occurrences of passive sentences in the spoken corpus versus 3,372 in the written corpus.
If these figures come from corpora of different sizes, for example a written corpus of 3,179,546 words and a spoken corpus of 573,484 words, they cannot be compared directly.
As a matter of fact, a larger corpus increases the probability of finding more occurrences of a phenomenon.
If a word appears five times in a corpus of 15,000 words, it would not be wise to conclude that this word would have a frequency of 500 occurrences in a corpus of 1,500,000 words.
For this reason, extrapolating a frequency, obtained on the basis of a small corpus, to a much larger scale does not offer a correct representation of what was actually observed.
In general, it is appropriate to choose a base of normalization close to the size of the smallest corpus examined.
In the text of an article, the two frequency elements can be reported as follows: "There were 94 occurrences of parce que in the corpus, corresponding to a relative frequency of 3.61 occurrences every 10,000 words".
There is yet another distinction that needs to be applied to the concept of word, since it is fundamental for corpus linguistics.
Other times, however, we refer to a word not to designate the total number of character strings, but the number of different words that a corpus contains.
For example, in the spoken corpus of French from French-speaking Switzerland OFROM (see Chapter 5), the type/token ratio is 0.02, whereas in the French section of the children's literature corpus ByCoGliJe, it is equal to 0.04.
The problem with the type/token ratio is that it is very sensitive to the size of a corpus (for this reason, the type/token value 9/12 = 0.25 obtained for sentence (1) is in no way representative).
Indeed, the larger the corpus, the more the words end up repeating themselves and, therefore, the ratio decreases all the more.
For example, the type/token ratio in the Sciences Humaines written corpus that contains around 260,000 word occurrences is 0.06, but it drops to 0.007 for the 2012 installment of the Le Monde corpus, which has over 26 million word occurrences.
However, in order to estimate the importance of a word in a corpus, frequency is not the only element to take into account.
For example, in a corpus containing scientific articles, the word linguistics may appear relatively frequently, due to the fact that a portion of the corpus is devoted to this field.
However, this word will probably never be used in other texts in the corpus covering different fields, unlike words such as analysis, hypothesis or conclusion, which will probably be used in all scientific fields.
In a small corpus of texts produced by 20 French-speaking students, the word nonobstant appears more frequently than in other language registers such as journalistic texts, but this higher frequency is due to the propensity of a particular student to use this word very frequently.
This parameter reflects the way in which word occurrences are distributed across the different portions of the corpus.
There are different ways to calculate lexical dispersion in a corpus.
One of the simplest ones is to count the number of portions of the corpus in which the word is present.
For example, for a corpus made up of texts written by 20 students, if the word nonobstant is used by three of them, we can say that its dispersion is 3/20, or that this word is found in 15% of the corpus.
Although this measure may be useful in some cases, it is not entirely satisfactory, since it does not reflect the proportion in which this word has been used throughout the three portions of the corpus.
If, in this case, one of the students produces 80% of the occurrences and the other two students produce 10% each, it would be inappropriate to conclude that the distribution is homogeneous in the 15% of the corpus where this word appears.
In order to find the difference in proportions, we then have to subtract the expected proportion from the observed proportion for each portion of the corpus.
All of the figures obtained for each portion of the corpus are then added, and divided by 2.
Suppose we want to know whether the connective car was used consistently by the 10 students who compiled a dossier in the Litt√©racie avanc√©e corpus (L2_DOS_SORB sub-corpus).
In summary, in addition to frequency indications, it is important to provide information about lexical dispersion in a corpus, either by simply calculating the percentage of texts in which a word is used, or by reporting a dispersion measurement, such as the deviation of proportions we have just described.
The primary goal of research in corpus linguistics is to be able to draw conclusions in relation to a linguistic phenomenon, not only at the level of the observed data sample, but also to generalize it to all data of the same type.
Likewise, the gender of participants in a corpus, or their geographical origin, are also variables.
Another example of a nominal variable, this time an explanatory one, could be the region of origin of the speakers of a spoken corpus, for example, Paris, Geneva, Brussels, Montreal.
For example, if the participants in a corpus are classified into age groups such as group 1 (18-25 years), group 2 (26-40 years) and group 3 (41-60 years) for sociolinguistic analyses, the age variable is an ordinal one.
Linguistic variables involving the relative frequency of some kind of phenomenon in a corpus are also scalar variables.
For example, let us imagine that we have collected information concerning the nominal variable mother tongue from the speakers in a corpus, and that we have 36 French-speaking people and 40 German speakers.
To carry out this test, let us set aside the values of the canton of Geneva, which is under-represented in the corpus, with only 75,598 words (against more than 100,000 in all the other cantons).
We have also described the different forms that variables in a corpus may adopt, and highlighted the fact that statistical tests must be adapted to the nature of the variables.
We have emphasized that frequency is not the only indicator of the prevalence of a word in a corpus and that its dispersion across the different portions of the corpus is also very important.
We also showed how to measure the lexical richness of a corpus using the type/token ratio, and presented the limitations of such a measurement.
This corpus includes four original works in French and their translation into English, as well as four original works in English and their translation into French.
Throughout this work, we have presented the different facets of corpus linguistics, both from the point of view of the theoretical questions to which this discipline provides answers and of its methodological foundations.
Indeed, corpus-based studies aim to test empirical hypotheses that are often formulated from the results of other studies, either more theoretical or empirical in nature, which have already been published in the scientific literature.
In addition, many journals specializing in different areas of linguistics, such as Journal of Pragmatics, Journal of Sociolinguistics and Journal of French Language Studies, regularly publish corpus studies.
For example, if the question to be investigated is the assertion that women talk more about their emotions than men, the operationalization of this question will require building or obtaining a lexicon of words related to emotions that can be searched in a corpus, chosen to make it possible to determine whether women actually use these words more than men.
As from the operationalization phase, it is also important to choose the corpus on which the study will be carried out.
If the study makes use of existing corpora, the possibility of investigating a certain question or not, or the manner in which it can be operationalized, depends on the characteristics of the corpus.
For example, to be able to compare the language of women and that of men, this variable must be clearly identifiable in the metadata of a corpus.
Likewise, the stylistic genre of the corpus should be compatible with the question under investigation.
For many questions, the raw data retrieved from a corpus will not be sufficient.
In order to be able to draw quantitative conclusions, it is necessary to carry out inferential statistical tests, which will indicate whether the results obtained on the corpus sample can be generalized to the entire population.
The second section contains a review of relevant previous studies (also called the state of the art), which served as a theoretical basis for the study, or which are inspired on other corpus studies that the current research project will supplement or sometimes call into question.
The detailed presentation of the study begins by describing the corpus used and explaining how data was retrieved from it, as well as their annotation.
In corpus linguistics, the collection and annotation of data commonly involves a relatively balanced combination of computer-aided and manual labour.
It is still common practice, for instance, to first retrieve data representing a particular linguistic phenomenon from an electronic corpus (e.g. by means of a concordancer tool or query script) and subsequently manually categorize the collected examples into different functional-semantic groups (e.g. animate/inanimate; literal/figurative; agent/patient/instrument/ ‚Ä¶).
We will now show that this approach continues to perform strongly when the data is taken from a corpus of specialized language and is annotated at finer levels of granularity (i.e. with more and subtler sense distinctions).
Thus, given its robustness, high reliability, flexibility, and potential for reusability and replicability, it is at least worth considering whether this (semi-)automated data annotation procedure could be what is next for corpus linguistic methodology.
There is, however, more to explore before LLMs can be fully integrated into corpus linguistic research.
However, given that there is, by now, a large number of corpus-linguistic textbooks available, ranging from the very decent to the excellent, a few words seem in order to explain why I feel that it makes sense to publish another one.
The main reason is that I have found, in my many years of teaching corpus linguistics, that most available textbooks are either too general or too specific.
On the other hand, there are textbooks that focus one or more specific corpus-based techniques, discussing very specific phenomena (often the research interests of the textbook authors themselves) using a narrow range of techniques (often involving specific software solutions).
A book that discusses the history and epistemology of corpus linguistics only to the extent necessary to grasp these methodological issues and that presents case studies of a broad range of linguistic phenomena from a coherent methodological perspective.
I then present what I take to be the methodological foundations that distinguish corpus linguistics from other, superficially Preface similar methodological frameworks, and discuss the steps necessary to build concrete research projects on these foundations -formulating the research question, operationalizing the relevant constructs and deriving quantitative predictions, extracting and annotating data, evaluating the results statistically and drawing conclusions.
These case studies are drawn from the vast body of corpus linguistic research literature published over the last thirty years, but they are all methodologically deconstructed and explicitly reconstructed in terms of the methodological framework developed in the first part of the book.
I also provide supplementary online material, including information about the corpora and corpus queries used as well as, in many cases, the full data sets on which the case studies are based.
I hope that the specific perspective taken in this book, along with the case studies and the possibility to study the full data sets, will help both beginning and seasoned researchers gain an understanding of the underlying logic of corpus linguistic research.
Let us define a corpus somewhat crudely as a large collection of authentic text (i.e., samples of language produced in genuine communicative situations), and corpus linguistics as any form of linguistic inquiry based on data derived from such a corpus.
I cannot think of any other scientific discipline whose textbook authors would feel compelled to begin their exposition by defending the use of observational data, and yet corpus linguistics textbooks often do exactly that.
Even the resulting, somewhat weaker statement is quite clearly true, and will remain true no matter how large a corpus we are dealing with.
However, the largest currently publicly available linguistic corpus of British English, the one-hundred-million-word British National Corpus, does not contain a single instance of this construction.
Clearly, it is, for all intents and purposes, impossible to include this variation in its entirety in a given corpus.
Second, just like a corpus, a speaker's linguistic experience is limited to certain language varieties: most English speakers have never been to confession or planned an illegal activity, for example, which means they will lack knowledge of certain linguistic structures typical of these situations.
In other words, the interpretation of a constructed sentence is subjective in the same way that the interpretation of a sentence found in a corpus is subjective.
In fact, interpreting other people's utterances, as we must do in corpus linguistic research, may actually lead to more intersubjectively stable results, as interpreting other people's utterances is a more natural activity than interpreting our own: the former is what we routinely engage in in communicative situations, the latter, while not exactly unnatural, is a rather exceptional activity.
To decide this, we need objective evidence, obtained either by serious experiments (including elicitation experiments) or by corpus-linguistic methods.
This in itself would be surprising if intuited judgments were indeed superior to corpus evidence: after all, the distinction between linguistic behavior and linguistic knowledge is potentially relevant in other areas of linguistic inquiry, too.
If I were willing to speculate, I would consider the possibility that the rejection of corpora and corpus-linguistic methods in (some schools of) grammatical theorizing are based mostly on a desire to avoid having to deal with actual data, which are messy, incomplete and often frustrating, and that the arguments against the use of such data are, essentially, post-hoc rationalizations.
Although corpus-based studies of language structure can look back on a tradition of at least a hundred years, there is no general agreement as to what exactly constitutes corpus linguistics.
As we saw in the preceding chapter, corpora fell out of favor just as linguistics grew into an academic discipline in its own right and as a result, corpus-based studies of language were relegated to the margins of the field.
While the work on corpora and corpus-linguistic methods never ceased, it has returned to a more central place in linguistic methodology only relatively recently.
Thus, I will attempt in this chapter to sketch out a broad, and, I believe, ultimately uncontroversial characterization of corpus linguistics as an instance of the scientific method.
I will develop this proposal by successively considering and dismissing alternative characterizations of corpus linguistics.
The first chapter of this book started with a similar definition, characterizing corpus linguistics as "as any form of linguistic inquiry based on data derived from [...] a corpus", where corpus was defined as "a large collection of authentic text".
In order to distinguish corpus linguistics proper from other observational methods in linguistics, we must first refine this definition of a linguistic corpus; this will be our concern in Section 2.1.
We must then take a closer look at what it means to study language on the basis of a corpus; this will be our concern in Section 2.2.
The term corpus has slightly different meanings in different academic disciplines.
To distinguish this type of collection from other collections of texts, we will refer to it as a linguistic corpus, and the term corpus will always refer to a linguistic corpus in this book unless specified otherwise.
The texts which are collected in a corpus have a reflected reality: they are only real because of the presupposed reality of the discourses of which they are a trace.
First, any text, spoken or written, will lose not only its communicative context (the discourse of which it was originally a part), but also some of its linguistic and paralinguistic properties when it becomes part of a corpus.
Such semi-structured elicitation techniques may also be used where a phenomenon is frequent enough in a typical corpus, but where the researcher wants to vary certain aspects systematically, or where the researcher wants to achieve comparability across speakers or even across languages.
These are sometimes good reasons for eliciting a special-purpose corpus rather than collecting naturally occurring text.
Thus, studies based on such corpora must be regarded as falling somewhere between corpus linguistics and psycholinguistics and they must therefore meet the design criteria of both corpus linguistic and psycholinguistic research designs.
Such a corpus is sometimes referred to as a balanced corpus.
In this case, the corpus will be deliberately skewed so as to contain only samples of the variety under investigation.
However, if we plan to generalize our results to that variety as a whole, the corpus must be representative of that variety.
This is legitimate if the goal is to investigate that particular variety, but if the corpus were meant to represent the standard language in general (which the corpus creators explicitly deny), it would force us to accept a very narrow understanding of standard.
This is true all the more so when we take into account the second level of sampling within these 2.1 The linguistic corpus genres, which uses a mixture of sub-genres (such as reportage or editorial in the press category or novels and short stories in the fiction category), and topic areas (such as Romance, Natural Science or Sports).
Intuitively, there may be a rough correlation in some cases: newspapers publish more reportage than editorials, people (or at least academics like those that built the corpus) generally read more mystery fiction than science fiction, etc.
For example, the copyright registrations for 1961 suggest that the category of periodicals is severely underrepresented relative to the category of books -there are roughly the same number of copyright registrations for the two language varieties, but there are one-and-a-half times as many excerpts from books as from periodicals in the BROWN corpus.
Despite these shortcomings, the BROWN corpus set standards, inspiring a host of corpora of different varieties of English using the same design -for example, the Lancaster-Oslo/Bergen Corpus (LOB) containing British English from 1961, the Freiburg Brown (FROWN) and Freiburg LOB (FLOB) corpora of American and British English respectively from 1991, the Wellington Corpus of Written New Zealand English, and the Kolhapur Corpus (Indian English).
However, if the design had been widely felt to be completely off-target, 2 What is corpus linguistics? researchers would not have used it as a basis for the substantial effort involved in corpus creation.
Linguists would probably agree that the design of the ICE corpora is "more representative" than that of the BNC Baby, which is in turn "more representative" than that of the BROWN corpus and its offspring.
This raises the question as to why corpus creators go to the trouble of attempting to create representative corpora at all, and why some corpora seem to be more successful attempts than others.
It seems to me that, in fact, corpus creators are not striving for representativeness at all.
The impossibility of this task is widely acknowledged in corpus linguistics.
These corpora are certainly impressive in terms of their size, even though they typically contain mere billions rather than trillions of 2 What is corpus linguistics? words.
However, their size is the only argument in their favor, as their creators and their users must not only give up any pretense that they are dealing with a representative corpus, but must contend with a situation in which they have no idea what texts and language varieties the corpus contains and how much of it was produced by speakers of English (or by human beings rather than bots).
These corpora certainly have their uses, but they push the definition of a linguistic corpus in the sense discussed above to their limit.
On the other hand, assuming (as we did above) that language structure and use are not infinitely variable, size will correlate with the representativeness of a corpus at least to some extent with respect to particular linguistic phenomena (especially frequent phenomena, such as general vocabulary, and/or highly productive processes such as derivational morphology and major grammatical structures).
Looking at the published corpus-linguistic literature, my impression is that for most linguistic phenomena that researchers are likely to want to investigate, these corpus sizes seem sufficient.
Let us take this broad range as characterizing a linguistic corpus for practical purposes.
Such differences are important to understand for anyone working with the these corpora, as they will influence the way in which we have to search the corpus (see further Section 4.1.1 below) -before working with a corpus, one should always read the full manual.
The SBCSAE and the LLC cannot easily be combined into a larger corpus, since they mark prosodic features at very different levels of detail.
For example, the word Mme in line 94 is an abbreviation, indicated in the corpus by the sequence \0 preceding it.
Annotations of paralinguistic or linguistic features in a corpus impact its authenticity in complex ways.
On the one hand, including information concerning paralinguistic features makes a corpus more authentic than it would be if this information was simply discarded.
After all, this information represents aspects of the original speech events from which the corpus is derived and is necessary to ensure a reconceptualization of the data that approximates these events as closely as possible.
On the other hand, this information is necessarily biased by the interests and theoretical perspectives of the corpus creators.
By splitting the spoken corpora into intonation units, for example, the creators assume that there are such units 2.1 The linguistic corpus and that they are a relevant category in the study of spoken language.
Researchers using these corpora are then forced to accept the assumptions and decisions of the corpus creators (or they must try to work around them).
These may be recorded in a manual, a separate computerreadable document or directly in the corpus files to which they pertain.
Metadata may also pertain to the structure of the corpus itself, like the file names, line numbers and sentence or utterance ids in the examples cited above.
Crucially, it would cover a procedure in which the linguistic corpus essentially serves as a giant citation file, that the researcher scours, more or less systematically, for examples of a given linguistic phenomenon.
But let us assume, for the moment, that the cross-section of published material read by the editors of Merriam Webster's dictionary counts as a linguistic corpus.
Given this assumption, the procedure described here clearly falls under our definition of corpus linguistics.
However, the method of collecting citations cannot be regarded as a scientific method except for the purpose of proving the existence of a phenomenon, and hence does not constitute corpus linguistics proper.
In other words, corpus-illustrated linguistics simply replaces introspectively invented data with introspectively selected data and thus inherits the fallibility of the introspective method discussed in the previous chapter.
Since overcoming the fallibility of introspective data is one of the central motivations for using corpora in the first place, the analysis of a given phenomenon must not be based on a haphazard sample of instances that the researcher happened to notice while reading or, even worse, by searching the corpus for specific examples.
The best way to achieve this is to draw a complete sample of the phenomenon in question, i.e. to retrieve all instances of it from the corpus (issues of retrieval are discussed in detail in Chapter 4).
As a consequence, it is usually impossible to extract a complete sample of a given phenomenon manually, and this has lead to a widespread use of computers and corpus linguistic software applications in the field.
Definition (Fourth attempt) Corpus linguistics is the complete and systematic investigation of the distribution of linguistic phenomena in a linguistic corpus.
On the one hand, this definition subsumes the previous two definitions: If we assume that corpus linguistics is essentially the study of the distribution of linguistic phenomena in a linguistic corpus, we immediately understand the central role of the techniques described above: (i) KWIC concordances are a way of displaying the distribution of an expression across different syntagmatic contexts; (ii) collocation tables summarize the distribution of lexical items with respect to other lexical items in quantitative terms, and (iii) frequency lists summarize the overall quantitative distribution of lexical items in a given corpus.
This definition is close to the understanding of corpus linguistics that this book will advance, but it must still be narrowed down somewhat.
First, it must not be misunderstood to suggest that studying the distribution of linguistic phenomena is an end in itself in corpus linguistics.
However, while the statistical properties of language are a worthwhile and actively researched area, they are not the primary object of research in corpus linguistics.
Definition (Fourth attempt, linguistic interpretation) Corpus linguistics is the investigation of linguistic research questions based on the complete and systematic analysis of the distribution of linguistic phenomena in a linguistic corpus.
This is a fairly accurate definition, in the sense that it describes the actual practice of a large body of corpus-linguistic research in a way that distinguishes it from similar kinds of research.
It is not suitable as a final characterization of corpus linguistics yet, as the phrase "distribution of linguistic phenomena" is still somewhat vague.
In line with the definition above, we would now try to determine their distribution in a corpus.
That the two words have roughly the same frequency in our corpus, while undeniably a fact about their distribution, is not very enlightening.
If our combined corpus were representative, we could at least conclude that neither of the two words is dominant.
Definition (Final Version) Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus.
The remainder of Part I of this book will expand this definition into a guideline for conducting corpus linguistic research.
In corpus linguistics, the explicandum is typically some aspect of language structure and/or use, while the explicans may be some other aspect of language structure or use (such as the presence or absence of a particular linguistic element, a particular position in a discourse, etc.), or some language external factor (such as the speaker's sex or age, the relationship between speaker and hearer, etc.).
The fourth step consists in collecting data -in the case of corpus linguistics, in retrieving them from a corpus.
Note that in the simple example presented here, the conditional distribution is a matter of all-or-nothing: all instances of windscreen occur in the British part of the corpus and all instances of windshield occur in the American part.
Finally, note that our final definition does distinguish corpus linguistics from other kinds of observational methods, such as text linguistics, discourse analysis, variationist sociolinguistics, etc., but it does so in a way that allows us to recognize the overlaps between these methods.
At the end of the previous chapter, we defined corpus linguistics as "the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus" and briefly discussed the individual steps necessary to conduct research on the basis of this discussion.
Or we could search a corpus for all passages mentioning cars and hope that one of them mentions the forward-facing window; alternatively, we could search for grammatical contexts in which we might expect the word to be used, such as ‚ü® through the NOUN of POSS.PRON car ‚ü© (see Section 4.1 in Chapter 4 on how such a query would have to be constructed).
If we do not find a word in our corpus, this may be because there is no such word in English, or because the word just happens to be absent from our corpus, or because it does occur in the corpus but we missed it.
There are, after all, only a handful of texts in LOB and BROWN that mention either of the two words at all (three in each corpus).
In corpus linguistics (and the social sciences more generally), hypotheses of this type are the exception rather than the norm -we are more likely to deal with statements about tendencies (think Most swans are white or Most examples of windscreen are British English), where the search for counterexamples is not a viable research strategy.
They may, however, inform corpus-based syntactic argumentation (cf.
However, the larger our corpus is (and most corpus-linguistic research requires corpora that are much larger than the four million words used here), the less feasible it becomes to do so.
But linguistic corpora do not (and cannot) contain only well-known authors, and so checking the individual demographic data for every speaker in a corpus may be difficult to impossible.
Let us further assume that we can assign all other uses of pavement in the sample to the reading 'paved surface', and that two of the four examples of sidewalk in the British English corpus are genuine counterexamples.
First, we can rarely say with any certainty whether we are dealing with true counterexamples or whether the apparent counterexamples are due to errors in the construction of the corpus or in our classification.
Thus, we would have to discard it based on our intuition that it constitutes an error (the LOB creators actually mark it as such, but I have argued at length in Chapter 1 why this would defeat the point of using a corpus in the first place), or we would have to accept it as a counterexample to the generalization that singular subjects take singular verbs (which we are unlikely to want to give up based on a single example).
We could argue that we simply have to make sure that there are no errors in the construction of our corpus and that we have to classify all hits correctly as constituting a genuine counterexample or not.
We can (and must) try to minimize errors in our data and our classification, but we can never get rid of them completely (this is true not only in corpus-linguistics but in any discipline).
However, as pointed out above, many (if not most) hypotheses in corpus linguistics do not take the form of universal statements ("All X's are Y", "Z's always do Y", etc.), but in terms of tendencies or preferences ("X's tend to be Y", "Z's prefer Y", etc.).
For example, we defined American English as "the language occurring in the BROWN and FROWN corpora", but we saw that the FROWN corpus contains at least one misclassified text by a British author, and we also saw that it is questionable to assume that all and only speakers of American English produce the language we would want to call "American English" (recall the uses of sidewalk by British speakers).
Similarly, we assumed that it was possible, in principle, to recognize which of several senses of a word (such as pavement) we are dealing with in a given instance from the corpus; we saw that this assumption runs into difficulties very quickly, raising the more general question of how 3.2 Operationalization to categorize instances of linguistic phenomena in corpora.
These sequences seem easy enough to identify in a corpus (or in a list of hits for appropriately constructed queries), so a researcher studying the possessive may not even mention how they defined this construction.
If we accept graphemic or even orthographic representations of language (which corpus linguists do, most of the time), then we also accept some of the definitions that come along with orthography, for example concerning the question what constitutes a word.
At the very least, it is crucial to understand that tagging and other kinds of annotation are the result of applying operational definitions by other researchers and if we use tags or other forms of annotation, we must familiarize ourselves with these definitions by reading the fine manuals that typically accompany the corpus.
Let us begin with a brief discussion of tokenization and part-of-speech (POS) tagging, two phenomena whose operational definitions are typically decided on and applied by the corpus makers and implicitly accepted by the researchers using a corpus.
Ditto tags are a way of tokenizing the corpus at orthographic word boundaries while allowing words to span more than one token.
The first answer is that the theories of tokenization and word classes are (usually) explicitly described in the corpus manual itself or in a guide as to how to apply the tag set.
Obviously, the larger this corpus, the more accurate the probabilities, the more likely that the tagger will be correct.
In other words, particular linguistic phenomena will be severely misrepresented in the results of corpus queries based on automatically assigned tags or parse trees.
Due to the practical and theoretical difficulties of defining and measuring complexity, the vast majority of corpus-based studies operationalize Weight in terms of some measure of Word Length even if they theoretically conceptualize it in terms of complexity.
Most phenomena that are of interest to linguists (and thus, to corpus linguists) require operational definitions that are more heavily dependent on interpretation.
For example, sidewalk is normally spelled as an uninterrupted sequence of the character S or s followed by the characters i, d, e, w, a, l and k, or as an uninterrupted sequence of the characters S, I, D, E, W, A, L and K, so (assuming that the corpus does not contain hyphens inserted at the end of a line when breaking the word across lines), there are just three orthographic forms; also, the word always has the same meaning.
In these cases, the most common operationalization strategy found in corpus linguistics is reference to a dictionary or lexical database.
Of course, in order to turn this into an operational definition, we need to specify a procedure that allows us to assign the hits in our corpus to these categories.
The fact that dictionaries disagree as to whether these are inanimate shows that this is not a straightforward question that calls for a decision before the nouns in a given corpus could be categorized reliably.
We have seen that operational definitions in corpus linguistics may differ substantially in terms of their objectivity.
Next, the predictions must be tested -in the case of corpus linguistics, corpora must be selected and data must be retrieved and annotated, something we will discuss in detail in the next chapter.
As briefly discussed in Chapter 2, concordancing software allows us to query the corpus for a string of characters and displays the result as a list of hits in context.
A study of the word (in either or both of its senses) would obviously require that we look at the lemma PAVEMENT, comprising at least the word forms pavement (singular), pavements (plural) and, depending on how the corpus is prepared, pavement's (possessive).
In Chapter 3, we implicitly treated the second issue as a problem of retrieval, noting in passing that we queried our corpus in such a way as to capture all variants of the lemma PAVEMENT.
Broadly speaking, there are two ways of searching a corpus for a particular linguistic phenomenon: manually (i.e., by reading the texts contained in it, noting down each instance of the phenomenon in question) or automatically (i.e., by using a computer program to run a query on a machine-readable version of the texts).
There is a range of more or less specialized commercial and non-commercial concordancing programs designed specifically for corpus linguistic research, and there are many other software packages that may be repurposed to the task of searching text corpora even though they are not specifically designed for corpuslinguistic research.
The power of software-aided searches depends on two things: first, on the annotation contained in the corpus itself and second, on the pattern-matching capacities of the software used to access them.
In the simplest case (which we 4.1 Retrieval assumed to hold in the examples discussed in the previous chapter), a corpus will contain plain text in a standard orthography and the software will be able to find passages matching a specific string of characters.
If the corpus contains part-of-speech tags, for example, this will allow us to search (within limits) for grammatical structures.
This syntax is very powerful, allowing us to query the corpus for tokens or sequences of tokens at any level of annotation.
In arriving at the definition of corpus linguistics adopted in this book, we stressed the need to investigate linguistic phenomena exhaustively, which we took to mean "taking into account all examples of the phenomenon in question" (cf. Chapter 2).
We can describe the quality of a data set that we have retrieved from a corpus in terms of two measures.
There are two additional measures that are important in other areas of empirical research but do not play a central role in corpus-linguistic data retrieval.
These measures play a role in situations where a negative outcome of a test is relevant (for example, with medical diagnoses); in corpus linguistics, this is generally not the case.
However, unless we carefully search our corpus manually (a possibility I will return to below), there is typically a trade-off between the two.
Let us look at a specific example, the English ditransitive construction, and let us assume that we have an untagged and unparsed corpus.
Obviously, a corpus tagged for parts of speech could improve the precision of our search results somewhat, by excluding cases like (9c-d), but others, like (9a), 4.1 Retrieval could never be excluded, since they are identical to the ditransitive as far as the sequence of parts-of-speech is concerned.
Note that, unlike precision, the recall rate of a query cannot be increased after the data have been extracted from the corpus.
Thus, an important aspect in constructing a query is to annotate a random sample of our corpus manual for the phenomenon we are interested in, and then to check our query against this manual annotation.
In a POS-tagged corpus, we could, for example, search for a sequence of a pronoun and a noun in addition to the sequence pronoun-determiner that we used above, which would give us cases like (12d), or we could search for forms of be followed by a past participle followed by a determiner or noun, which would give us passives like those in (12b).
In the present example with an untagged corpus, for example, there is no additional pattern that seems in any way promising.
As already discussed in Chapter 2, this may sometimes be the only feasible option, either because automatic retrieval is difficult (as in the case of searching for ditransitives in an untagged corpus), or because an automatic retrieval is impossible (e.g., because the phenomenon we are interested in does not have any consistent formal properties, a point we will return to presently).
As discussed above, in the case of words and in at least some cases of grammatical structures, the quality of automatic searches may be increased by using a corpus annotated automatically with part-of-speech tags, phrase tags or even grammatical structures.
Still, an automatically annotated corpus will frequently allow us to define searches whose precision and recall are higher than in the example above.
This is precisely the situation where exhaustive retrieval can only be achieved by a manual corpus search, i.e., by reading the entire corpus and deciding for each word, phrase or clause, whether it constitutes an example of the phenomenon we are looking for.
Obviously, the more completely we can extract our object of research from the corpus, the better.
In some cases, the variables and their values will be provided externally; they may, for example, follow from the structure of the corpus itself, as in the case of british english vs. american english defined as "occurring in the LOB corpus" and "occurring in the BROWN corpus" respectively.
In the simplest case, this consists in accepting the operational definitions used by the makers of a particular corpus (as well as the interpretative judgments made in applying them).
Take the example of british english and american english used in Chapters 3 and 2: If we accept the idea that the LOB corpus contains "British English" we are accepting an interpretation of language varieties that is based on geographical criteria: British English means "the English spoken by people who live (perhaps also: who were born and grew up) in the British Isles".
Or take the example of Sex, one of the demographic speaker variables included in many modern corpora: By accepting the values of this variable, that the corpus provides (typically male and female), we are accepting a specific interpretation of what it means to be "male" or "female".
In some corpora, this may be the interpretation of the speakers themselves (i.e., the corpus creators may have asked the speakers to specify their sex), in other cases this may be the interpretation of the corpus creators (based, for example, on the first names of the speakers or on the assessment of whoever collected the data).
For many speakers in a corpus, these different interpretations will presumably match, so that we can accept whatever interpretation was used as an approximation of our own operation definition of Sex.
But in research projects that are based on a specific understanding of Sex (for example, as a purely biological, a purely social or a purely psychological category), simply accepting the (often unstated) operational definition used by the corpus creators may distort our results substantially.
For example, if we define Word Length in terms of "number of letters", we could write a simple computer program to go through our corpus, count the letters in each word and attach the value as a tag.
We could also, for example, create a list of the 2500 most frequent nouns in English and their Animacy values, and write a program that goes through a tagged corpus and, whenever it encounters a word tagged as a noun, looks up this value and attaches it to the word as a tag.
Unfortunately, corpus linguists have long paid insufficient attention to this (and I include much of my own research in this criticism).
It is high time that this change and that corpus linguists make an honest effort to describe their designs in sufficient detail to make them reproducible (in all senses discussed above).
This is not yet standard in corpus linguistics, but it is a good idea to plan and document your research as though it already were.
We will conclude this chapter with a discussion of a point that may, at first, appear merely practical but that is crucial in carrying out corpus-linguistic research (and that has some methodological repercussions, too): the question of how to store our data and annotation decisions.
There are broadly two ways of doing so: first, in the corpus itself, and second, in a separate database of some sort.
The first option is routinely chosen in the case of automatically annotated variables like Part of Speech, as in the passage from the BROWN corpus cited in Here, the annotation (i.e., the part-of-speech tags) are attached to the data they refer to (i.e., words) by an underscore (recall that alternatively, a vertical format with words in the first and tags in the second column is frequently used, as are various kinds of XML notation).
However, there are cases where it may be more useful to record them in the form of annotations in (a copy of) the original corpus instead, i.e., analogously to automatically added annotations.
Thus, if we are dealing with a variable that is likely to be of general interest, we should consider the possibility of annotating the corpus itself, instead of first extracting the relevant data to a raw data table and annotating them afterwards.
Recall, once again, that at the end of Chapter 2, we defined corpus linguistics as the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus.
In order to illustrate these types of data, let us turn to a linguistic phenomenon that is more complex than the distribution of words across varieties, and closer to the kind of phenomenon actually of interest to corpus linguists: that of the two English possessive constructions introduced in Section 4.2.3 of Chapter 4 above.
A number of factors underlying these preferences have been suggested and investigated using quantitative corpus-linguistic methods.
We could categorize all grammatical expressions of possession in a corpus in terms of the values s-possessive and of-possessive, count them and express the result in terms of absolute or relative frequencies.
For example, the s-possessive occurs 22 193 times in the BROWN corpus (excluding proper names and instances of the double spossessive), and the of -possessive occurs 17 800 times.
We can also calculate their mean frequency (19 996.50), but again, this is not a mean of the two constructions, but of their frequencies in one particular corpus.
Potentially ordinal data are actually frequently treated like nominal data in corpus linguistics (cf. Section 5.3.2), and with complex scales combining a range of different dimensions, this is probably a good idea; but ordinal data also have a useful place in quantitative corpus linguistics.
It should be obvious that we can calculate the mean length of words or other constituents in a corpus, a particular sample, a particular position in a grammatical construction, etc.
Even our definition of corpus linguistics makes reference to this fact when it states that research questions should be framed such that it enables us to answer them by looking at the distribution of linguistic phenomena across different conditions.
This could be done using the measure of referential distance discussed in Section 3.2 of Chapter 3, which (in slightly different versions) is the most frequently used operationalization in corpus linguistics.
Thus, it is always preferable to report the frequencies of all values, and, in fact, I have never come across a corpus-linguistic study reporting modes.
However, as also discussed in Chapter 3, many if not most hypotheses in corpus linguistics have to be formulated in relative terms -like those introduced in Chapter 5.
Given the vast range of corpus-linguistic research designs, these three tests will not always be the ideal choice.
As mentioned in the preceding chapter, nominal data (or data that are best treated like nominal data) are the type of data most frequently encountered in corpus linguistics.
Unfortunately, studies in corpus linguistics (and in the social sciences in general) often fail to report effect sizes, but we can usually calculate them from the data provided, and one should make a habit of doing so.
In the vast majority of corpus linguistic research issues, we will be dealing with designs that are at least bivariate (i.e., that involve the intersection of at least two variables), like the one discussed in the preceding section.
We may, for instance, have annotated an entire corpus for a particular speaker variable (such as sex), and we may now want to know whether the corpus is actually balanced with respect to this variable.
In order to determine whether the BNC can be considered a balanced corpus with respect to Speaker Sex, we can compare this observed distribution of speakers to the expected one more or less exactly in the way described in the previous sections except that we have two alternative ways of calculating the expected frequencies.
Thus, we can say that "the BNC corpus contains a significantly higher proportion of male speakers than expected by chance (ùúí 2 = 272.34, df = 1, ùëù < 0.001)" -in other words, the corpus is not balanced well with respect to the variable Speaker Sex (note that since this is a test of proportions rather than correlations, we cannot calculate a phi value here).
I will leave it as an exercise to the reader to determine whether and in what direction these frequencies differ from what would be expected either under an assumption of equal proportions or given the proportion of female and male speakers in the corpus.
This means that at least some rank values will occur more than once, which is a typical situation for corpus-linguistic research involving ordinal data.
In the context of corpus linguistics, there is one fundamental problem with the t-test in any of its variants: it requires data that follow what is called the normal distribution.
For example, if we wanted to compare the length of heads and modifiers in the s-possessive, we would have two groups that are dependent in that for any data point in one of the groups there is a corresponding data point in the other group that comes from the same corpus example.
Frequently, perhaps even typically, corpus linguistic research questions will be more complex, and we will be confronted with designs where both the dependent and the independent variable will have (or be treated as having) more than two values.
Since we are most likely to deal with nominal variables in corpus linguistics, we will discuss in detail an example where both variables are nominal.
However, there is a potential problem that we need to take into account: the values of the variables Sex and 6.6 Complex research designs Age and their intersections are not necessarily distributed evenly in the subpart of the BNC used here; although the makers of the corpus were careful to include a broad range of speakers of all ages, sexes (and class memberships, ignored in our study), they did not attempt to balance all these demographic variables, let alone their intersections.
This correlation in the corpus of old and male on the one hand and young and female on the other may well be enough to distort the results such that a linguistic behavior typical for female speakers may be wrongly attributed to young speakers (or vice versa), and, correspondingly, a linguistic behavior typical for male speakers may be wrongly interpreted to old speakers (or vice versa).
A very general precaution against this possibility is to make sure that the corpus (or our sample) is balanced with respect to all potentially confounding variables.
In reality, this is difficult to achieve and may in fact be undesirable, since we might, for example, want our corpus (or sample) to reflect the real-world correlation of speaker variables).
There is a range of multivariate statistical methods that are routinely used in corpus linguistics, such as the ANOVA mentioned at the end of the previous chapter for situations where the dependent variable is measured in terms of cardinal numbers, and various versions of logistic regression for situations where the dependent variable is ordinal or nominal.
In other words, taking into account than young men are underrepresented in the corpus compared to old men, there is a clear preference of all men for the of -construction.
The (orthographic) word plays a central role in corpus linguistics.
However, the focus on words is also due to the fact that the results of corpus linguistic research quickly showed that words (individually and in groups) are more interesting and show a more complex behavior than traditional, grammarfocused theories of language assumed.
An area in which this is very obvious, and which has therefore become one of the most heavily researched areas in corpus linguistics, is the way in which words combine to form so-called collocations.
At first, this will seem like a somewhat abrupt shift from the topics and phenomena we have discussed so far -it may not even be immediately obvious how they fit into the definition of corpus linguistics as "the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus", which was presented at the end of Chapter 2.
Note that Firth, although writing well before the advent of corpus linguistics, refers explicitly to frequency as a characteristic of collocations.
However, it is much more likely that we will be interested in large sets of collocate pairs, such as all adjective-noun pairs or even all word pairs in a given corpus.
Better yet, they should invest a few weeks (or, in the worst case, months) to learn a scripting language like Perl, Python or R (the latter being a combination of statistical software and programming environment that is ideal for almost any task that we are likely to come across as corpus linguists).
However, if we were to identify a set of 100 collocations with ùëù-values of 0.001 in a corpus, we are potentially treating all of them as important, even though it is very probable that at least one of them reached this level of significance by chance.
It would be surprising if corpus linguistics was an exception, and indeed, it is not.
Let us use the adjective-noun sequence good example from the LOB corpus (but horse lovers need not fear, we will return to equine animals and their properties below).
As long as we are deriving all our collocation data from the same corpus, this will not make a difference, since the table total will always be the same.
Obviously, the distribution of such pairs or sets with respect to other words in a corpus can provide insights into their similarities and differences.
There are contexts which might be expected to be particularly suitable to particular kinds of lexical relations and which could be used, given a large enough corpus, to identify word pairs in such relations.
However, under the definition of corpus linguistics adopted in this book, Sinclair's observations would be just the first step towards a full analysis.
However, such exact frequencies do not advance the analysis meaningfully: as long as we do not know how frequent a particular phenomenon is in the corpus as a whole, we cannot determine whether it is a characteristic property of the expression under investigation, or just an accidental one.
Stubbs does not present the results of his procedure in detail and the corpus he uses is not accessible anyway, so let us use the BNC again and extract our own data.
We could now calculate the association strength between the verb and each of these nouns to get a better idea of which of them are significant collocates and which just happen to be frequent in the corpus overall.
This allows corpus linguistic methods to be used in uncovering at least some properties of that culture.
Again, this is simpler in the case of morphologically marked and relatively simple grammatical structures, for example, the s-possessive (as defined above) is typically characterized by the sequence ‚ü® [pos="noun"] [word="'s"%c] [pos="adjective"]* [pos="noun"] ‚ü© in corpora containing texts in standard orthography; it can thus be retrieved from a POS-tagged corpus with a fairly high degree of precision and recall.
However, this is of very little help in retrieving transitive verbs even from a POS-tagged corpus, since many noun-phrases following a verb will not be direct objects (Sam slept the whole day) and direct objects do not necessarily follow their verb (Sam, I have not seen); in addition, noun phrases themselves are not trivial to retrieve.
These are, roughly speaking, the words most typical for the collocational framework: when we encounter the framework (in a corpus or in real life), these are the words that are most probable to fill the slot between a and of.
Renouf and Sinclair then point out that the frequency of these items in the collocational framework does not correspond to their frequency in the corpus as a whole, where, for example, man is the most frequent of their twenty words, and lot is only the ninth-most frequent.
Under the definition of corpus linguistics used throughout this book, there should be nothing surprising about the idea of investigating the relationship between words and units of grammatical structure based on association measures: a grammatical structure is just another condition under which we can observe the occurrence of lexical items.
They point out that this sentence will not occur in any given finite corpus, but that this does not allow us to declare it ungrammatical, since it could simply be one of infinitely many sentences that "simply haven't occurred yet".
His focus is on the second hypothesis, which he tests on the LOB corpus by, first, identifying all occurrences of both verbs with both complementation patterns and, second, categorizing them according to whether the verb in the complement clause refers to an activity, a process or a state.
It also demonstrated that such corpus-based studies may result in very specific hypotheses about the function of lexicogrammatical structures that may become the basis for claims about mental representation.
We can avoid these problems by drawing our sample from the corpus itself.
The case study was also meant to provide an example of a corpus-based design where it is more useful to operationalize one of the constructs (Frozenness) as an ordinal, rather than a nominal variable.
In order to determine whether priming occurs and under what conditions, we have to extract a set of potential targets and the directly preceding discourse from a corpus.
These can be extracted relatively straightforwardly even from an untagged corpus using the following queries: The query in (12a) will find all finite forms of the verb be (as non-finite forms cannot occur in tag questions), followed by the negative clitic n't, followed by a pronoun; the query in (12b) will do the same thing for the full form of the particle not, which then follows rather than precedes the pronoun.
Case studies assume that there is an equal amount of male and female speech in the corpus, so the question is what to compare these frequencies against.
Since corpora are the only source for the identification of changes in discourse frequency, this is a question that can only be answered using corpus-linguistic methodology.
It is an interesting question to what extent such a citation database can be treated as a corpus (cf. the extensive discussion in Hoffmann 2004).
This case study demonstrates that very large collections of citations can indeed be used as a corpus, as long as we are investigating phenomena that are likely to occur in citations collected to illustrate other phenomena; the results are very similar to those we get from well-constructed linguistic corpora.
In a written corpus, we can thus query the sequence ‚ü®[word="''"] [pos="pronoun"] [pos="verb"]‚ü© to find the majority of examples of the construction.
Chapter 3 included a discussion of counterexamples and their place in a scientific framework for corpus linguistics.
It does not pose particular problems for corpus-based morphology, which studies structures smaller than the word.
Depending on the tokenization of the corpus, this query might miss cases where the word containing the suffix -ness is the first part of a hyphenated compound, such as usefulness-rating or consciousness-altering; we could alter the query to something like ‚ü®[word=".+ness(es)?(--.+=)?"%c]‚ü© if we believe that including these cases in our sample is crucial.
This is true regardless of whether we look at its token frequency in the corpus as a whole or under specific conditions; if its token frequency turned out to be higher under one condition than under the other, this would point to the association between that condition and one or more of the words containing the affix, rather than between the condition and the affix itself.
For example, the token frequency of the suffix -icle is higher in the BROWN corpus (269 tokens) than in the LOB corpus (225 tokens).
For example, there are 7 types and 9 tokens for mini-in the 1991 British FLOB corpus (two tokens each for mini-bus and mini-series and one each for mini-charter, mini-disc, mini-maestro, mini-roll and mini-submarine), so the TTR is 7 /9 = 0.7779.
As pointed out in connection with the comparison of the TTRs for mini-in the FLOB and the FROWN corpus, we would like to be able to test differences between two (or more) TTRs (and, of course, also two or more HTRs) for statistical significance.
Again, corpus linguistics is a uniquely useful tool to investigate this.
Here, too, corpus linguistics provides useful tools, for example to determine whether the choice between affixes is influenced by syntactic, semantic or phonological properties of stems.
Let us repeat the study with the BROWN corpus.
Again, the overall difference between the two words is significant and the effect is slightly stronger than in the LOB corpus (ùúí 2 = 22.83, df = 3, ùëù < 0.001, ùúô = 0.3413), suggesting a stronger differentiation between them.
In this case, devices are much more frequently referred to as electric and less frequently as electrical than expected, and, as in the LOB corpus, the nouns in the category industry are more frequently referred to as electrical and less frequently as electric than expected (although not significantly so).
Since this is a written corpus, let us define Length in terms of letters and assume that this is a sufficiently close approximation to phonological length.
There are 373 stem types occurring with -ic in the LOB corpus, with a mean length of 7.32 and a sample variance of 5.72; there are 153 stem types occurring with -ical, with a mean length of 6.60 and a sample variance of 4.57.
The following discussion is based on the full data from the fiction subcorpus and a subsample of the newspaper corpus that was arrived at by deleting every second, then every third and finally every 192nd example, ensuring that the hits in the sample are spread through the entire newspaper subcorpus.
If we think of the two samples as subsamples of the same corpus, it is very counterintuitive to do so.
However, the notion "hapax" is only an operational definition for neologisms, based on the hope that the number of hapaxes in a corpus (or sub-corpus) is somehow indicative of the number of productive coinages.
Still, if we want to use this operational definition, we have to stick with it and define hapaxes strictly relative to whatever (sub-)corpus we are dealing with.
This would mean that the hapaxes in any sample would overwhelmingly cease to be hapaxes -the larger our corpus, the fewer hapaxes there will be.
In these cases, quantitative corpus linguistics is essentially a variant of sociolinguistics, differing mainly in that the linguistic phenomena it pays most attention to are not necessarily those most central to sociolinguistic research in general.
In other words, the corpus-linguistic identification of keywords is analogous to the identification of differential collocates, except that it analyses the association of a word W to a particular text (or collection of texts) T in comparison to the language as a whole (as represented by the reference corpus, which is typically a large, balanced corpus).
The tagged version of the BROWN corpus does not contain quotation marks because they have intentionally been stripped from the text.
The with an uppercase T does not occur in the tagged LOB corpus, because case is normalized such that only proper names are capitalized.
There are still some artifacts of corpus construction: the codes F and J are used in BROWN to indicate that letter combinations and formulae have been removed.
There are also personal names that differ across corpora; for example, the name Macmillan occurs 63 times in the LOB corpus but only once in BROWN; this is because in 1961, Harold Macmillan was the British Prime Minister and thus Brits had more reason to mention the name.
Such studies have two nominal variables: Culture (operationalized as "corpus containing language produced by members of the culture") and Area of Life (operationalized as "semantic field").
Next, they look at concordances of the remaining words to determine first, which senses are most frequent and thus most relevant for the observed differences, and second, whether the words are actually distributed across the respective corpus, discarding those 10 Text whose overall frequency is simply due to their frequent occurrence in a single file (since those words would not tell us anything about cultural differences).
For example, they note that there are obvious differences between the types of sports whose vocabulary differentiates between the two corpora (baseball is associated with the BROWN corpus, cricket and rugby with the LOB corpus), reflecting the importance of these sports in the two cultures, but also that general sports vocabulary (athletic, ball, playing, victory) is more often associated with the BROWN corpus, suggesting a greater overall importance of sports in 1961 US-American culture.
With respect to the latter, note that it is also questionable whether one can simply combine a British and an American corpus to represent "Western" culture.
However, these differences very obviously depend on the topics of the conversations included in the corpus.
It is not inconceivable, for example, that male linguists constructing a spoken corpus will record their male colleagues in a university setting and their female spouses in a home setting.
If we are careful with our operational definitions, then, we may actually use corpus-linguistic methods to investigate not (only) the role of words in texts, but the role of their referents in a particular community.
Even taking into consideration the general overrepresentation of men in the corpus, they are overrepresented strongly in reportage and editorials, religious writing and belles-lettres/biographies -all "factual" language varieties, suggesting that actual, existing men are simply thought of as more worthy topics of discussion than actual, existing women.
Referents that are important in a culture are more likely to be talked and written about than those that are not; thus, in a sufficiently large and representative corpus, the frequency of a linguistic item may be taken to represent the importance of its referent in the culture.
The use of the Google Books archive may be criticized because it is not a balanced corpus, but the authors point out that first, it is the largest corpus available and second, books constitute cultural products and thus may not be such a bad choice for studying culture after all.
These are reasonable arguments, but if possible, it seems a good idea to complement any analysis done with Google Books with an analysis of a more rigorously constructed balanced corpus.
I used the 2012 version of the corpus, so the result differs very slightly from theirs.
However, corpus linguists have actually uncovered a number of relationships between words and linguistic phenomena beyond lexicon and grammar without making use of such annotations.
Two broad alternatives have been proposed in corpus linguistics.
Among other things, the corpus-based study of (small set of) source domain words may provide insights into the systematicity of metaphor (cf. esp. Deignan 1999b).
Deignan studies this potential difference systematically based on a sample of more than 1500 hits for flame/s in the Bank of English (a proprietary, non-accessible corpus owned by HarperCollins), from which she manually extracts all 153 metaphorical uses.
The names of decades (such as 1960s or sixties) occur too infrequently with dawn of in this corpus to say anything useful about them, but the names of centuries are frequent enough for a differential collexeme analysis.
Let us test this hypothesis using the Corpus of Historical American English, which includes language from the early nineteenth to the very early twenty-first century -in a large part of the corpus, the twentieth century was thus entirely or partly in the future.
Given how frequently we have compared British and American English in this book, these two varieties may seem an obvious place to start, but the two cultures may be too similar, and the word happiness happens to be too infrequent in the BROWN corpus anyway.
Let us compare British English (the LOB corpus) and Indian English (the Kolhapur corpus constructed along the same categories) instead. .
This is easiest done by using the meta-information supplied by the corpus makers, which includes the category "commerce" as a subcategory of "newspaper" (cf.
For example, Charteris-Black (2005) investigates a corpus of "right-wing communication and media reporting" on immigration, containing speeches, political manifestos and articles from the conservative newspapers Daily Mail and Daily Telegraph.
Charteris-Black's findings are intriguing, but since he does not compare the findings from his corpus of right-wing materials to a neutral or a corresponding left-wing corpus, it remains an open question whether the use of these metaphors indicates a specifically right-wing perspective on immigration.
The BNC contains 1 232 966 words from the Daily Telegraph (all files whose names begin with AH, AJ and AK), which will serve as our right-wing corpus, and 918 159 words from the Guardian (all files whose names begin with A8, A9 or AA, except file AAY), which will serve as our corresponding left-wing (or at least left-leaning) corpus.
Again, metonymy is a vastly under-researched area in corpus linguistics, so much work remains to be done.
In this book, I have focused on corpus linguistics as a methodology, more precisely, as an application of a general observational scientific procedure to large samples of linguistic usage.
The second reason is that I believe that corpus linguistics has a place in any theoretical linguistic framework, as long as that framework has some commitment to modeling linguistic reality.
In these models, the corpus becomes more than just a research tool, it becomes an integral part of a model of linguistic competence (cf. Stefanowitsch 2011).
In less radical usage-based models of language, such as Langacker's, the corpus is not a model of linguistic competence -the latter is seen as a consequence of linguistic input perceived and organized by human minds with a particular structure (such as the capacity for figure-ground categorization).
The corpus is, however, a reasonable model (or at least an operationalization) of this linguistic input.
While an understanding of the principles, approaches, and advantages of using corpora provides the necessary foundational knowledge of this approach to language analysis, there is a technical side to corpus linguistics that is best acquired through practice and experience with corpora.
When "doing" corpus linguistics, students need to gain experience searching a corpus and interpreting the results of their corpus searches so that they can use this information to explain why their analysis and findings are important or relevant.
In this book, we offer multiple opportunities to work on corpus projects by first including an entire chapter dedicated to smaller corpus projects (Chapter 4) and then providing students with information on how to build and analyze their own corpora (Part III).
In order to allow the students to gain experience in "doing" corpus linguistics, we have intentionally limited the corpora and software used in the corpus projects.
There are many different descriptive and theoretical frameworks that are used in corpus linguistics.
We have selected one particular framework to guide the students in their interpretation of their corpus findings.
We recognize the relevance of other descriptive and theoretical agendas but feel that focusing on a single approach provides students with more extended experience interpreting their corpus results and motivating the significance of their findings.
Without knowledge and practice using a specific framework, we have found that students can be quite enamored with the "button-pushing" aspects of corpus linguistics at the expense of interpreting the results of their searches.
The perspective that we will take is, as you can tell from the title of the book, related to an area of language study called corpus linguistics.
One way to understand linguistic analysis and language is through corpus linguistics, which looks at how language is used in certain contexts and how it can vary from context to context.
But, before we do, we would like to briefly describe what we mean by a corpus.
A corpus is a collection of a fairly large number of examples (or, in corpus terms, texts) that share similar contextual or situational characteristics.
In fact, the word equal occurs 22,480 times in the corpus, and the word identical occurs 8,080 times.
In addition to information on collocation and frequency, a corpus will also allow us to examine the extent to which certain types of prescriptive rules are followed.
Let us look at what a corpus might tell us about splitting infinitives.
The Corpus of Contemporary American English shows that we have examples such as to better understand (874 times in the corpus) compared with to understand better (94 times) and to really get (349 times) compared with to get really (151 times).
We have already seen a few examples of what corpus information can tell us.
Now we will consider the defining characteristics of corpus linguistics as they will be used in this book.
In a general sense, a corpus can refer to any collection of texts that serve as the basis for analysis.
A person might, for example, collect examples of news editorials that are on a particular topic and refer to this collection as a "corpus".
In Chapters 3 and 4, we will cover some of the specific directions we can explore in the corpus through software programs that allow for different types of analysis.
It is important to remember that corpus methods do not just involve using computers to find relevant examples; these methods also focus on analyzing and characterizing the examples for a qualitative interpretation.
We then provide a register analytical framework for interpreting corpus findings (Chapter 2).
Part III of the book takes you through the steps of building a corpus (Chapter 5) and then covers some statistical procedures that can be used when interpreting data (Chapters 6 and 7).
Recall that corpus linguistics includes both quantitative and qualitative analysis.
They use a corpus to find out how these individual features vary across contexts/registers.
To do this, they search multiple linguistic variables at the same time in a corpus.
In Chapter 3, we will show you how to search an existing corpus for the linguistic variables you may be interested in (whether lexical or grammatical), and in Chapter 4, we will take you through several projects that will help you learn how to do studies of this kind.
In Chapter 3, you will have the opportunity to do situational analyses in some of the projects, and in Chapter 5 you will do a situational analysis of your own corpus.
It is written for someone doing corpus linguistics perhaps even for the first time and for someone who has limited or no basic statistical knowledge.
In this case, you compare words in one corpus, called target corpus, with words in another, called reference corpus.
Following the online tutorial on YouTube referenced above, identify the press reports sub-corpus in both the American and the British collections (AmE06_PressRep and BE-06_PressRep) and feed them into the tool.
The British press reporting is your target corpus and the American press reporting is your reference corpus.
The higher the keyness value for the words, the more likely that they appear in the target versus the reference corpus.
A corpus with more or longer texts will allow more words in them.
A special computer program is often designed to process the text and to look at sequences of words in a window of text as they emerge in a corpus.
We can explore how frequently particular word combinations (n-grams) occur together in a corpus or how they are distributed across different registers.
In this case, you are not looking for the kinds of n-grams that may be emerging in that corpus; instead, you are just looking for a pre-defined sequence of words (that may, or may not, have been extracted from a previous corpus).
For example, if you type in the word baby, you will see that it occurs over 60,000 times in the COCA corpus.
If you are interested in the sequence a baby, it occurs more than 10,000 times in the same corpus, and if you are interested in the sequence like a baby, you will see that it occurs more than 600 times in COCA.
In contrast, if you don't know what you want to look for ahead of time, but you are interested in the possibilities a corpus may have, you can either design a new computer program for yourself and run your own data, or run the n-gram program already available to you (e.g., through AntConc, a software that we discuss more in Chapter 5).
Each word in the COCA corpus is classified into frequency bands.
In fact, the definite article the is ranked #1 in the corpus with a frequency of 50,017,617.
Words marked with the color blue are among the top 500 most frequently occurring words in the corpus.
The green ones rank as the top 501-3,000 most frequently occurring words in the corpus, and the yellow ones are marked for those that are in the rank of less commonly used vocabulary in the corpus (beyond the rank of 3,000).
It is a great baseline corpus for your own research as well.
The difference between bi-grams and collocations is the fact that bi-grams are identified based on two words that happen to be next to each other in a corpus while collocations are two words co-occurring more frequently than by chance.
When tri-grams occur with a particular frequency (e.g., 20 times) and in one specific register in a corpus, they are often called lexical bundles.
When we call them 4-grams, it does not matter how often they occur in a corpus; they are still called 4-grams.
When you are extracting n-grams from a corpus, you can imagine that your sequences could be endless.
To illustrate this, we ran a lexical bundle search in a corpus of webtexts.
Now, some corpora (e.g., COCA) have POS tags attached to each word, and some corpora (e.g., Michigan Corpus of Academic Spoken English [MICASE], and Michigan Corpus of Undergraduate Student Papers [MICUSP]) do not have that feature in addition to the actual words in a corpus.
This chapter provides you with an opportunity to use readily available corpora to conduct corpus linguistics projects.
In this chapter, you will gain practical experience in using corpus linguistic methodologies and interpreting results.
Although you need to register in order to use the corpus, as we mentioned in the previous chapter, access to the corpora is free of charge.
All of the corpora also use the same search interface so that once you learn how to "ask" for information in one corpus, you can conduct searches in all of the available corpora.
The home page also includes many helpful resources and videos to guide you in the use of the corpus and use of search terms.
First, the corpus sizes range from the 1.9-billion-word GloWbE corpus to the 50-millionword Strathy corpus.
To narrow your search and make an analysis more manageable, you can choose a smaller number of texts to analyze or even create a smaller, virtual corpus.
For Step 3, make sure that you are still using the COCA corpus and have selected "chart" in the search.
Make sure to provide examples from the corpus to support your analysis of their meanings.
Make sure to support your analysis with examples from the corpus.
Make sure to provide examples from the corpus to support your analysis of their meanings.
Comment This chapter will take you through the steps to complete a corpus project.
By reference to a specific research question, you will learn how to build your own corpus and then analyze it using both the register functional analysis approach covered in Chapter 2 and the corpus software programs covered in Chapter 3.
Corpus building not only allows you to answer a specific research question, but it also gives you experience in corpus construction.
Constructing a useful corpus involves a number of steps that are described below.
In some cases, you may use the internet for the texts to include in your corpus.
Additionally, it is important to take into account the country in which the corpus materials are used.
If you are using the corpus for educational purposes and do not plan on selling the corpus or any information that would result from an analysis of the corpus (e.g., in publications), the likelihood of being prosecuted as a copyright violator is usually small.
A worthy research project has a number of different components, including providing a motivation of the significance of the topic, a clear description of the corpus and the methods used in the study, presentation of results, a discussion of the results, and a conclusion that provides a summary and "takeaway message" of the research.
For example, a topic examining gender differences could involve creating a corpus of fitness articles with females as the intended audience and comparing this to a corpus of fitness articles with males as the intended audience.
A project looking at the language of social media posts could be achieved by creating a corpus of Twitter posts and comparing the language used in the posts with written or spoken language found in existing corpora such as the CORE corpus found English-corpus.org.
In this sense, all research questions are framed from a particular perspective by reference to a specific methodology (corpus linguistics).
The corpus that you will be building for your project will likely not be a large general corpus but will be a smaller, "specialized" corpus that is designed to answer the specific research question(s) you are investigating.
Even though smaller, specialized corpora are used for more restricted research purposes than general corpora, adopting a sound set of guidelines to build the corpus is still important.
A welldesigned corpus includes texts that are relevant to the research goals of the study; are saved into a file format that allows different software programs to analyze the texts; and are labeled with enough relevant contextual material so that the different contexts are easily identifiable in the corpus.
The selection of the texts to include in your corpus depends on their suitability and their availability.
Clearly, the texts need to share relevant characteristics (or variables) that meet your selection criteria for inclusion in the corpus.
A project that considers how news writing changes in different time periods would, obviously, require a corpus that includes newspaper articles written at different periods of time.
In order to build a corpus to address this issue, you would need to make sure that there is an adequate number of newspaper articles that have been written at different time periods.
Additionally, a corpus addressing this issue would need to have sub-corpora of relatively equal size.
As illustrated in some of the corpus projects that compared COCA with the BYU-BNC corpus, the unequal sizes of these two corpora did not allow for straight frequency comparisons between the two corpora.
Note that the balance should consider not only the number of texts in each sub-corpus but should also consider the word count of the sub-corpora.
If your specialized corpus also contains subcorpora, then you should ensure the sub-corpora is of fairly equal sizes.
Another issue related to corpus balance in your corpus relates to text types.
A news writing corpus would need to include the various types of news textssports and lifestyle news as well as state, local, national, and international news.
A balanced news writing corpus would either include texts of sports, lifestyle, and general news texts or would select only one of these text types for analysis.
Note that this entire corpus -let us call it A Historical Corpus of American News Writing -would consist of three sub-corpora related to each of the three time periods.
Depending on different research questions, the corpus could also be loaded with all three time periods.
Depending on your research goal and the design of your corpus, you may also want to include specific information on the situational characteristics of each text in individual text files in your corpus.
In a corpus of general song lyrics, you would want to include lyrics from different types of music (rock, rap, country, popular music, etc.) in order to achieve balance in your corpus.
He develops software programs that are extremely useful for corpus linguistic analyses and he makes them freely available (although you are able to make a donation should you choose to do so).
While it is worth finding out about each one of the programs on Anthony's webpage as they are very useful, we will mainly focus on two here, AntWordProfiler for lexical analyses and AntConc for lexical as well as grammatical analyses, as the most pertinent for your use when analyzing your corpus.
Using your own corpus, you should be able to do KWIC searches through the concordance lines, and other types of lexical as much as grammatical analyses in your own texts.
You are using a corpus to look for patterns.
Take 100 instances of complement clauses and mark each for who uses them in terms of age and educational background (hopefully, this information will be available in the corpus).
Let's say we have the average scores for the use of hedges in our corpus of nine texts.
In the example below, we display the use of nouns by teachers and students in the corpus.
You look at a corpus of student presentations that includes nine presentations from each area (a total of 27 texts).
She built a small but balanced corpus of randomly selected 30 class sessions from multiple corpora (MICASE, BASE, and others) where male and female instructors and levels of instruction (undergraduate, graduate) were both represented.
She defined informational focus by adding up the number of normed counts for nouns, attributive adjectives, nominalizations, and prepositions in each of the courses she included in her corpus.
Among the three different types of correlations (Pearson, Spearman Rank Order, and Point-Biserial), Pearson correlation is the most frequently used statistical procedure in corpus studies.
You have a small corpus of presentations comprising two sub-corpora: teacher presentations and student presentations.
We must be careful as to how we select them in the corpus, though -and the best way to do so is to get a set of randomly selected relative clause sentences and continue our classification based on that.
This chapter will guide you through the steps and procedures to actually put the corpus to use and to report on your research findings.
In order to illustrate some of the concepts in doing a situational and linguistic analysis along with a functional interpretation, we will refer to a small corpus of English as Second-Language writers who were asked to produce problem-solution paragraphs in two different conditions.
The texts from this corpus were collected under two different conditions: First, the students were placed into pairs and asked to write a problem-solution paragraph collaboratively.
The corpus was also divided into two sub-corpora: one consisting of collaboratively written essays (N = 51) and another consisting of individual essays (N = 102).
In one sense, the distinction between corpus-driven and corpus-based research methods can be misleading.
At least with respect to a register functional approach, any feature that is identified through a corpus-driven approach will still merit closer scrutiny and analysis in the corpus.
One might argue that this second step can be seen as corpus-based because it identifies features in the corpus that merit further attention.
On the other hand, it might also be argued that the feature was not identified by previous corpus-based research so it is, by definition, not corpus-based.
Sometimes this is not possible, as in the case of the problem-solution corpus described above.
If your project involves comparing the frequency findings of sub-corpora in your corpus (or to another existing corpus), you will need to employ a simple statistical procedure known as "normalization" to ensure that your results are comparable (see a brief mention of this in Chapter 6).
In this small sample, we not only see differences in nouns ("English" is the third most frequent word in the individual corpus and "students" is the fourth most frequent word in the collaborative corpus) but also differences in function words ("in" is the fourth most frequent word in the individual texts and "of" is the fifth most frequent words in the collaborative texts).
Only a closer examination of these features in the corpus can provide us with evidence to support the analysis.
There are many different types of searches that you can do with your corpus.
For example, as mentioned above, an important consideration in understanding the use of any feature (including both word lists and n-grams) relates to the distribution (or dispersion) of a feature in the corpus.
Seeing the distributional patterns can also help in examining whether your findings for a given feature are, in fact, spread in your corpus or are found in a limited number of texts only.
At this point, you have a well-structured corpus that is guided by a well-motivated research question (see Chapter 5, Section 5.2).
As we have discussed many times throughout the book, one can follow a) a corpus-driven or b) a corpus-based (often called corpus-informed) approach to linguistic analysis.
We showed how we can do a corpus-based study with already identified language features, whether doing a lexical study or searching for the use of particular grammatical patterns.
While it is relatively easy to carry out lexical studies with corpus-driven approaches (whether you rely on existing corpora or analyze your own corpus), as available tools allow you to extract lexical patterns from the texts, it is quite difficult to apply corpus-driven approaches to do a full lexico-grammatical analysis of texts.
Some tagging software is available, but there is an increasing need for corpus researchers to gain computational and statistical skills if they want to carry out more in-depth analyses.
If you don't have such skills (yet), perhaps the best solution is to continue doing corpus-based studies.
You can continue to look for lexico-grammatical patterns that you find interesting, or you can carry out corpus-based studies that rely on the results of previous, corpus-driven studies.
Unlike the corpus-driven approaches illustrated in this book, more advanced corpus-driven register studies have identified co-occurring linguistic features that have emerged through corpus analyses.
The purpose of these studies is to investigate how their own texts place on the existing continuum of variation and in this sense have a corpus-informed component to them.
The frequencies of several n-grams in the untagged Brown corpus 3.2.
In addition, corpus analyses have documented the existence of linguistic constructs that are not recognized by current linguistic theories.
Research of this typereferred to as a "corpus-driven" approach -identifies strong tendencies for words and grammatical constructions to pattern together in particular ways, while other theoretically possible combinations rarely occur.
A novice student of linguistics could be excused for believing that corpus linguistics evolved in the past few decades, as a reaction against the dominant practice of intuition-based linguistics in the 1960s and 1970s.
In fact, it can be argued that intuitionbased linguistics developed as a reaction to corpus-based linguistics.
The goals of the CHECL are to complement, but not duplicate, the coverage of existing textbooks and handbooks on corpus linguistics.
There are many excellent textbooks in print, providing thorough introductions to the methods of corpus linguistics, surveys of available corpora, and general reviews of previous research.
As a result, the handbook includes relatively little discussion of topics that have been fully covered in existing textbooks, such as surveys of existing corpora, or methodological discussions of corpus construction and analysis.
Although each chapter includes a broad summary of previous research, the primary focus is on a more detailed description of the most important corpus-based studies in this area, with discussion of what those studies found and why they are especially important.
Part II is organized as a progression of the linguistic levels, beginning with corpus-based analyses of prosodic characteristics, moving on to chapters dealing with lexical characteristics (keywords, collocations, and phraseology), followed by chapters on grammatical features (descriptive grammar, grammatical variation, grammatical change, and the intersection of grammar and lexis), and finally concluding with chapters on the corpus-based study of discourse functions and pragmatics.
Part III, then, is organized in terms of the range of varieties that have been studied from a corpus perspective.
This part begins with chapters on the corpus-based description of spoken English, written academic English, and patterns of variation (synchronic and diachronic) among a wider range of spoken and written registers.
Finally, the CHECL concludes with a major section on applications of corpus-based research.
Part IV of the handbook surveys these major areas of application, including classroom applications, the development of corpus-based pedagogical materials, vocabulary studies, and corpus applications in lexicography and translation.
The types of corpora (and corpus-related resources) that we consider are the following: 1   1. Small 1-5-million-word, first-generation corpora like the Brown Corpus (and others in the Brown "family," such as the LOB, Frown, and FLOB) 2.
In addition, a Brown-based frequency list (for all words in the corpus) would be quite sparse.
And this is especially true when the corpus is created by a small team and with limited resources.
COCA, on the other hand, currently has more than 180,000 texts and the 400-million-word COHA historical corpus has more than 100,000 texts.
As we will see in this section, the best solution may be to take the texts from a text archive or the Web (containing billions of words of data), and then combine this with a robust corpus architecture.
Although the number of collocates between the BNC and COCA (4-5 times as large as the BNC) is striking, in a corpus like enTenTen12 from Sketch Engine (which is 25 times as large as COCA), for some words (e.g. nibble or serenely) there is not nearly as significant a yield in collocates.
This chapter presents an introductory survey of computational tools and methods for corpus construction and analysis.
Corpus tools and methods are now being applied very widely to historical data, learner language, and online varieties (Usenet, Emails, Blogs, and Microblogs), so I also consider the effect of non-standard or "dirty data" on corpus tools and methods, e.g. where spelling variation affects their robustness.
Although the focus in this chapter and the handbook is on tools and methods for English corpus linguistics, I highlight issues of support for other languages and corpora and tools that support multiple languages where they are relevant.
In Section 2.4, I will reflect on the question of whether corpus linguistics is now tooldriven, i.e. whether researchers can only ask the research questions that are supported by the existing tools and methods, and whether other important questions are not tackled due to a lack of tool support.
Improvements in speed and usability of corpus tools are important as well as interoperability between the tools.
However, in this chapter, the scope needs to be limited carefully to computational methods and tools employed for corpus linguistics research.
After that I will reflect on the current state of the art in corpus tools and methods.
Unfortunately, considering the plethora of textbooks in the field, it is the practical aspects of this process that are dealt with least out of the three key phases of corpus-linguistics methodology.
Creating a machine-readable corpus can be a very costly and timeconsuming exercise.
Systems such as Voicewalker was used for the Santa Barbara corpus and SoundScriber was used for compiling the Michigan Corpus of Academic Spoken English (MICASE).
Most corpus tools require plain text versions with optional XML encoding, so where material is sourced in another form, some format conversions will be in order.
Adding annotation allows the researcher to encode linguistic information present in the corpus for later retrieval or extraction using tools described in the next section.
If the text is annotated or corrected by hand then this could form the basis of a training corpus for an automatic tagging system which can then learn from the human annotators in order to attempt to replicate their coding later on larger amounts of data.
Focusing on the first kind for a moment, it would be possible of course to manually annotate texts using any standard word processor, but here it is useful to have software tools that check annotation as it is added, e.g. to ensure that typos in tags or category labels do not occur, and to allow standard mark-up formats (such as XML) to be employed consistently and correctly in the resulting corpus, e.g. as in the Dexter software.
All concordance tools provide for searching by a simple word and some tools permit searching for suffixes, multiple word phrases, regular expressions, part-of-speech tags, other annotation embedded within the corpus, or more complex contextual patterns.
Frequency lists, usually of words, provide a list of all the items in the corpus and a count of how often they occur and in some cases how widely dispersed the items are across multiple sections of a corpus.
The next method in the expanding corpus toolbox is usually referred to in the computational linguistics community as n-grams.
In the corpus linguistics field, it is also known as lexical bundles, recurrent combinations or clusters.
In addition, the keyness method and the n-gram method can be combined in order to highlight key clusters, i.e. repeated sequences whose frequency differs significantly in one corpus compared to a reference corpus.
The final method in the corpus toolbox is collocation.
The five methods described above have all been defined in relation to words contained in a corpus.
They can equally well apply to tags within a corpus, if any levels of annotation have been applied.
It should therefore be clear that a specific piece of corpus software cannot always be pigeonholed into one of these three categories.
Updated versions of corpus software are being delivered on a regular basis; however, the corpus toolkit is in need of a methodological overhaul on a number of fronts.
Following on from the critical reflection about the limitations of computational methods and tools in corpus linguistics, this section will zoom in on one of the standard methods and illustrate some of the potential problems with it for corpus linguists and some possible solutions.
As described in the previous section, the computational n-grams method appears under various guises in corpus linguistics.
First, tools which provide Computer Assisted Qualitative Data Analysis (CAQDAS), such as ATLAS.ti, NVivo, QDA Miner, and Wordstat, incorporate some very similar methods to those described here but are not widely used in corpus linguistics.
However, here the focus has been on the tools and methods used in the field of (English) corpus linguistics.
I uncovered some limitations of the current crop of computational tools and methods and reflected on whether corpus linguistics could be said to be becoming tool-driven.
Methods and tools for corpus linguistics have developed in tandem with the increasing power of computers and so it is to the computational side that I look in order to take a peek into the future of corpus software.
With regard to the former, for instance, corpus linguists have used different association measures to quantify, typically, how much two words are attracted to each other or how much a word is attracted to a grammatical pattern, but critical methodological analysis of the commonly used association measures is relatively rare.
In this overview, I will discuss statistical tools in corpus linguistics.
Section 2 is devoted to the "first group," i.e. statistics directly involving corpus-linguistic tools; Section 3 then turns to the "second group," i.e. statistics that are usually applied to the annotation of concordances.
For example, if words a and b occur 1,000 and 100 times in a corpus, a will be recognized faster than b, but not 1000 / 100 =10 times as fast but maybe log 1000 / log 100 =1.5 times as fast.
For example, the British National Corpus (BNC) has annotation that shows the corpus compilers considered of course, for example, for instance, according to, irrespective of, etc. to be one lexical item each, which means one would count of course, not of and course separately.
However, one cannot use a simple frequency list of an English engineering corpus, because its most frequent words would still be the, of, in, . . . -these are frequent everywhere.
Given the straightforward logic underlying the notion of dispersion, the huge impact it can have, and the fact that dispersion can correlate as strongly as frequency with experimental data (see Gries 2010c), dispersion and corpus homogeneity should be at the top of the to-do list of research on corpus-linguistic statistics.
Perhaps the most important tool in confirmatory statistics in corpus linguistics is, or should be, the generalized linear model and its extensions, a family of regression models, which serve to model a response/ dependent variable as a function of one or more predictors.
Third, a range of other interesting statistics can help corpus linguistics tackle other statistical challenges.
Other fields have had long and intense discussions about these things -corpus linguistics, unfortunately, has not.
The results can then be displayed in the concordance format familiar to corpus linguistics with the search feature (e.g. rise tone or low key) centered in the concordance.
In this section, examples of recent corpus-based studies of prosody are described to provide a sense of the types of current work that are underway.
The term "keyword" has considerable currency outside corpus linguistics, but there it is usually understood in a different sense.
To measure the style of a passage, the frequencies of its linguistic items of different levels must be compared with the corresponding features in another text or corpus which is regarded as a norm and which has a definite relationship with this passage.
For the stylistic analysis of one of Pope's poems, for instance, norms with varying contextual relationships include English eighteenth-century poetry, the corpus of Pope's work, all poems written in English in rhymed pentameter couplets, or, for greater contrast as well as comparison, the poetry of Wordsworth.
The significance test calculates the significance of the difference in frequency between the word in the target data and in the reference corpus.
It is recognized that the notion of exactly what qualifies as key in any study is influenced by the settings and parameters of the program used, and by the comparator texts (in the reference corpus).
Keyness in corpus linguistics is but the first statistical step in the analysis of texts.
Second, corpus-based collocation studies have so far focused on, or indeed have largely been confined to, the English language.
As intuition is usually an unreliable guide to patterns of collocation and semantic prosody, this study takes a corpus-based approach to addressing these research questions.
This observation is confirmed in a larger corpus.
This chapter has sought to provide a critical account of the current debates in corpus-based collocation research.
The review in this section demonstrates that corpus linguistics has enabled large-scale collocation analysis and foregrounded collocation in linguistic research while corpus-based collocation studies over the past decades have uncovered a range of interesting collocational behavior and semantic prosody which have been hidden from intuition and can only be revealed by examining a large amount of attested data simultaneously.
A related consideration is whether a study includes register comparisons: some studies compare phraseological patterns across two or more registers; others focus on phraseological patterns in a single register; while some studies analyze a general corpus and disregard the influence of register altogether.
In the following section, we undertake a survey of some of the most important corpus investigations of phraseology carried out to date, grouped according to the considerations introduced above.
Sections 2.1 and 2.2 contrast studies based on their major methodological approaches, distinguishing between corpus-based studies of pre-selected lexical expressions versus corpus-driven studies to identify the full set of important multi-word sequences in a corpus.
Then, in Section 3 we take a step back to consider the state of the art of phraseological research in corpus linguistics, considering some of the core issues still being debated within the field.
The main distinguishing characteristic of corpus-driven studies of phraseological patterns is that they do not begin with a pre-selected list of theoretically interesting words or phrases.
Rather, the corpus itself is analyzed, inductively and typically automatically, to identify the lexical phrases that are especially noteworthy.
These studies set frequency thresholds and dispersion requirements in order to identify the lexical phrases that are prevalent in the target corpus.
One of the earliest corpus-driven studies of this type was Salem's (1987) analysis of repeated lexical phrases in a corpus of French government resolutions.
In the late 1990s, corpus-driven studies of recurrent lexical phrases in English registers began to appear.
As the review of research in this section has shown, corpus-linguistic studies of extended lexical sequences have addressed the nature of patterned language from a variety of perspectives, under a range of rubrics, and with varying goals and methodologies.
Taken together, corpus investigations over the last few decades have greatly extended our understanding of phraseological lexical sequences in English discourse.
For example, one current issue for the study of phraseology in corpus linguistics concerns the best methods to be used for the identification of the most important lexical phrases in a corpus.
As a result, corpus-driven studies of lexical phrases (both continuous and discontinuous) have shown that lexical patterning is ubiquitous in English, and basic to the discourse structure of both spoken and written texts.
Although the focus of this handbook is on corpus-based investigations of English language texts, a few of the studies reviewed here have addressed the issue of phraseology in other languages.
It is clear that corpus composition must have an influence on the identification of important lexical phrases.
Even if register is controlled, the set of lexical phrases identified in a large corpus (containing more words) will probably be different than the set of phrases identified in a small corpus.
Thus, a corpus with a greater number of different texts is likely to result in a greater number of different lexical phrases than a comparable corpus with fewer texts.
These methodological considerations are generally disregarded in corpus-driven studies of phraseology.
This fully corpus-driven approach is rather resource-intensive, yet is theoretically important because it makes it possible to account for frequent discontinuous sequences of words that are not associated with a moderately frequent lexical bundle.
Because of the computational resources required to process and store all potential discontinuous sequences (frames) in large corpora (c. 10 million words in total) from a strictly corpus-driven approach, we used a MySQL database to store all possible four-word sequences in the two subcorpora (specifically all four-word sequences that did not cross (a) punctuation boundaries in the written corpus, and (b) turn boundaries in the spoken corpus).
The fully corpus-driven approach employed in this case study shows that some of the gaps in previous findings were simply an artifact of the corpusbased methodology at the first stage of the research.
Corpus-driven studies begin by analyzing a corpus to identify the set of important lexical phrases, and then study further the use of those phrases in discourse contexts to interpret the initial results.
In our case study, we illustrated how this distinction is not absolute: some previous studies of discontinuous lexical frames (like Biber 2009 and Ro ¬®mer 2010) are intermediate along this continuum, beginning with a corpus-driven approach to identify a set of continuous lexical sequences, but carrying out the corpus-based investigation of only those sequences, to analyze the extent to which they occur as discontinuous frames.
In contrast, the exploratory investigation that we present in our case study employs a corpus-driven approach to directly identify the important discontinuous frames in speech and writing.
However, these frames can be identified through direct corpus-driven analysis.
All corpus studies of grammar inevitably make use of the evidence of grammatical usage as observed in corpora in order to arrive at some kind of description of what the corpus attests about some area(s) of grammar.
It is sometimes supposed that corpus linguists are content "merely" to "describe" what is found in a corpus, for example by describing the structures found and their frequencies, rather than to show how their findings advance understanding in terms of some theoretical framework(s) of how grammar works.
They finished and released their corpus, the Brown Corpus of edited written English "for use with digital computers," in 1964, before Quirk (concentrating on detailed spoken transcription) had completed as much as a quarter of his.
These two corpora inaugurated the modern age of corpus linguistics, although both suffered from limitations: the Brown Corpus was restricted to written, printed material; and the SEU corpus remained on paper until the mid 1970s, when most of the spoken part of it was computerized by Jan Svartvik and became the London-Lund Corpus (LLC).
Automatic corpus parsing, however, has proved a more difficult nut to crack than POS-tagging.
The conversion of a corpus into a database of syntactic tree structures or a treebank remains a problematic and laborious task, which is associated with error or failure rates far greater than those associated with taggers.
It must be admitted, however, that the analysis of gradience is a laborious affair, and even corpus grammarians have been known to shy away from it.
By thus linking description to theory, Gries argues that his corpus-driven study not only describes, but explains and predicts the way the choice between option (5a) and option (5b) is made.
It seems that the simple goal of "describing" some aspect of English grammar, such as seemed sufficiently ambitious in the early days of corpus linguistics, no longer holds the limelight.
A reference corpus cannot in any sense represent the language, unless it is subdivided into text categories or subcorpora representing a broad range of registers, as in the BNC or the Bank of English.
As corpus grammar provides frequency information, it can hardly be ignored that different subcorpora yield very different frequency profiles associated with their communicative functions -above all, in the contrast between speech and writing.
One of the ways in which corpus linguistics has changed our view of language, I believe, is in the now widespread recognition that grammar and lexis are not separate components of language, but that they interpenetrate.
Thanks to them, descriptive grammar will continue to have a role in corpus linguistics.
Bresnan et al. calculate several logistic regression models that all correctly predict more than 90 percent of the actual dative outcomes in the Switchboard collection of recorded telephone conversations and in a corpus of Wall Street Journal texts.
In addition, the study employs bootstrapping techniques and mixed-effects modeling to investigate issues such as the role of idiolectal differences and the validity of cross-corpus generalizations.
To start out, we clarify briefly what we mean by the terms grammar, grammatical change, and corpus-based analyses.
The hallmark of a corpusbased analysis, as understood in this chapter, is that a grammatical phenomenon is studied in its entirety, such that all relevant examples of a phenomenon are exhaustively retrieved from a corpus.
Israel's study demonstrates that the OED, with its database of precisely dated quotations, is a highly useful resource for corpus linguists.
Wolk et al.'s study exemplifies how diachronic corpus studies can precisely document changes in grammatical structure and simultaneously address issues of speakers' knowledge of language.
The variables and their possible values, including corpus period, are summarized in (10) below; the following paragraphs discuss each variable in turn.
While the configuration of abhorment does occur more often than expected in the third corpus period (o = 75, e = 52.4), this difference is not large enough to be statistically significant, so that the configuration of abhorment is not a type.
By contrast, there are other configurations that are found significantly more often than expected in their respective corpus periods.
In a loose sense, much classical philological work on grammatical change was corpus-based.
As the results of corpus-based research on grammatical change accumulate and as the methods of analysis become more sophisticated, the corpus ceases to be merely a tool and becomes an active ingredient in the further development of usage-based theoretical models.
In order to address this question, historical corpus linguists need to intensify collaborations with researchers in sociolinguistics and psycholinguistics, who have long been concerned with the social and cognitive processes that shape grammar and that ultimately also shape grammatical change.
The Fisher test is used to establish "collostruction strength" by comparing the number of times a word (e.g. tell) occurs in a given construction (e.g. the ditransitive) with the total other occurrences of tell and the total other occurrences of the ditransitive, and the total number of other words and other constructions in the corpus.
In other words, a word+construction combination with a high collostruction strength in a given corpus may actually not occur particularly frequently.
The concept of Pattern Grammar (PG) came out of the COBUILD project, a large-scale lexicographical exercise that was unique when it began, in that the compilers relied on a very large (for its time) corpus of English to identify frequent usages and phraseologies of words.
LGSWE is more explicit about its methodology, which is based on the annotation of a corpus with the categories used in the book.
In comparing his two sentences, Hoey finds that many two-word combinations in Bryson's original sentence occur frequently also in the 100-million-word Guardian corpus.
For example, bus and ride co-occur in the corpus, as do ride and hour, and thirty and hour.
In the sample sentence, in winter is "thematised," and Hoey shows that this phrase when occurring in Theme position collocates in his corpus with the verb be, and with the names of places, again proportionally more frequently than the alternative phrases.
Most forms of traditional non-corpus-assisted discourse analysis have practiced the close-reading (that is, "qualitative analysis") of single texts or a small number of texts in the attempt to highlight both textual structures and also how meanings are conveyed.
In what follows we will attempt to outline ways in which corpus-assisted discourse studies (CADS) can help build upon traditional qualitative linguistic analysis, what "added value" it can bring.
Several corpus techniques, for example, keyword and key-cluster tools, have the specific aim of facilitating comparison.
We can compare between corpora or within a corpus (for example, for different speaker roles such as questioner and responder) and we can compare a specialized corpus to a general (or "heterogeneric") one.
It is sometimes possible, moreover, with the benefit of corpus techniques, to observe how White House messages evolve, how the exact nature of the primings flooding into the discourse changes over time, which provides strong evidence of deliberate attempted linguistic engineering.
We then passed from the concordance to close reading of the co-text around occurrences of Mubarak, a very common process in corpus-assisted discourse studies.
It is hard to see how, without the corpus techniques or some extremely time-consuming substitute for them, any firm, objective statements on these matters could be made.
As in all research, there is much in corpus linguistics that is subjective, including the choice of research question and of the procedures and software to employ, not to mention the interpretation of the output data.
However, in common with other fields, corpus pragmatics investigates the co-textual patterns of a linguistic item or items, which encompasses lexico-grammatical features such as collocation or semantic prosody.
However, where corpus pragmatics' "added value" lies is in its insistence that these patterns be considered in light of the context -the situational, interpersonal, and cultural knowledge that interactional participants share.
Through an iterative process, corpus pragmatics therefore moves beyond important but surface observations of lexico-grammatical patterns to allow a more nuanced interpretation of these patterns taking into consideration who uses them, where they were used, for what purposes, and how this use has changed over time.
In this way, corpus pragmatics has retained in part its original interpretative nature but has endeavored to supply this interpretation with objective supporting evidence.
We contend that the studies critically examined here exemplify many of the strengths of corpus pragmatics.
Although the majority of the research concentrates on pragmatic features of spoken language, we also include studies that highlight the importance of corpus pragmatics to the written context.
The use of personal pronouns to negotiate identity has received some attention in corpus pragmatics.
With the state of the art in corpus-pragmatic research established, we now turn to a more fine-grained discussion of exemplar studies in the areas outlined above.
The final corpus-pragmatic work that we examine here is Ru ¬®hlemann's (2007) unique study of the conversational subcorpus of the BNC.
This study is significant because it analyzes frequent conversational features that previously had not been systematically researched using corpus linguistics.
In modern spoken corpora in general, a wealth of paralinguistic information is tagged, such as coughing or door slamming, much of which is of little importance; however, some of these features, such as laughter, are of significance to corpus pragmatics.
As already mentioned, corpus linguistics has been criticized in relation to its suitability for the study of speech acts.
Moreover, if corpus pragmatics is concerned with the interpretation of meaning in context, another disadvantage associated with the relationship between corpus linguistics and pragmatics is that many larger corpora are impoverished both textually and contextually (Ru ¬®hlemann 2010).
The second corpus, the 12,500-word SettCorp, represents the conversations of a six-member (father, mother, two boys, and two girls) middle-class Irish family.
First, it vividly illustrates, through the application of a corpus-based, bottom-up methodology, the close interrelationship between pragmatic function and context.
Reflecting on the process, we can see that the corpus software aided our analysis but much of it had to be done manually.
Indeed, we speculate that in years to come, much, if not all, pragmatics research will involve corpus linguistics.
However, in the meantime, corpus pragmatics can do more to show the real worth of methodology to the wider field of pragmatics.
It needs to be explained that pragmatic research which truly focuses on language in use has to be corpus-based because there is no convenient way of making speakers say what you want them to say where language is unpredictable, messy, and extended over many turns.
While the study of pragmatic items can be challenging in a corpus, it is eminently possible.
Most of them rely on corpus linguistic methodology, but a few, mainly on the literary side of studies, focus on individual texts or passages and their interpretations.
Indeed, it is no exaggeration to say that corpus linguistics using large computer-readable language data has established itself as the main methodology in historical pragmatics.
It is a difficult question, and my aim in this chapter is to discuss it and review the state of historical pragmatic research using corpus-linguistic methods.
I shall survey the applications of corpus-linguistic methods in historical pragmatics by a selection of articles that illuminate recent trends within the field, demonstrate the range, and indicate future avenues for research.
In the first phase of corpus studies, lexical items served as the point of departure, but corpuslinguistic assessments at present extend to pragmatic units like speech acts, and discourse studies are included in historical pragmatics.
In general, the repertoire of corpus-linguistic studies is fairly broad from corpus-based but mainly qualitative studies to advanced statistical methods and computer programs specially designed for the research question under investigation.
Technical developments have brought along a rich array of corpus-linguistic applications, and corpus compilers have created a good selection of databases for public use.
One of the novel software applications, the Keyword analysis, uses significance tests to distinguish words that are significantly more frequent or significantly less frequent than in a reference corpus; calculations are carried out automatically by the program and it is possible to gain valuable insights into the material that cannot be achieved with qualitative study.
It is easy for the end user to apply the method to their data, but the researcher's own input shows in the selection of the target corpus and an optimal reference corpus and the interpretation of the machine-produced key word lists, which is not simple at all (see below).
As in all corpus-linguistic studies, research can be conducted with the "top-down" method that takes a linguistic feature or grammatical category as its point of departure; this is the deductive method.
In historical pragmatics, contextual mappings with illustrative examples are used to complement the corpus-linguistic assessments.
Historical pragmatics has its roots in philology, and it is against this background that we can most clearly see what new corpus linguistics has brought to historical pragmatics and to historical studies in general.
With corpus linguistics the basis has broadened and the focus has shifted to common features and everyday practices.
The availability of historical corpora and other electronic resources has multiplied from one pioneer corpus, HC, in 1991 to almost anything ever printed readily available as research data.
The Corpus of Early English Correspondence (CEEC) project was the first and the work is still going on with a whole corpus family in the pipeline.
In this section I shall outline some of the most important subfields of historical pragmatics where corpus-linguistic methods have yielded novel insights into central research questions.
Speech act studies represent function-to-form mapping, which is more difficult to deal with than the form-to-functions direction of fit with corpus-linguistic methods.
In some areas, like historical discourse pragmatics, progress has been considerable, but it has also been noticed that all branches of historical pragmatics do not lend themselves easily to corpus studies.
These include T2KSWAL, which contains a variety of registers found in academic settings (e.g. classroom teaching, study groups, and office hours), the Nottingham Health Communication Corpus (NHCC), which consists of interactions between nurses, pharmacists, NHS Direct health advisers, a hospital chaplain, and patients, and the Language in the Workplace Project (LWP) corpus, which contains workplace interactions in a variety of settings (e.g. government departments, meetings, and factory settings).
The BNC is probably the most well-known corpus focused on a national variety.
One monitor corpus that allows us to see changes in spoken corpora and also provides more current spoken data is COCA.
However, as mentioned above, one limitation of this corpus is that it is composed of transcripts from news programs and talk shows and thus the speech is likely to be more scripted or more carefully produced than informal registers of spoken English.
The following sections highlight key advancements within corpus linguistics on the study of speech, starting with characteristics that differentiate speech from writing (Section 2), and then moving to characteristics of particular spoken registers (Section 3), and specific individual features associated with speech (Section 4).
The SEC corpus is coded for these features as well as temporal alignment at the level of the phoneme.
This corpus was created for the study of grammatical variation in dialects rather than phonetic/phonological variation.
Thus, studies that attempt such segmentation currently require a combination of top-down and bottom-up approaches, using models from previous literature but making modifications based on the actual corpus in question.
The previous sections have attempted to illustrate many of the advancements in the use of spoken corpora and the important findings that corpus-linguistics research on spoken corpora has revealed.
These areas are a challenge to be met by the next generation of corpus linguists focusing on spoken corpora.
The motivation for this study is that no previous studies have used corpus linguistic methods to investigate differences in the use of linguistic features by patients and nurses across the phases of an interaction.
The corpus used for this study is the American Nurse-Standardized Patient corpus (ANSP corpus).
This corpus was collected and transcribed in 2012 and includes 50 interactions between registered nurses working at a US hospital and standardized patients (SPs).
Rates of occurrence were computed for the stance features in each text of the corpus.
The case study provided in Section 7 illustrated a common focus of corpus-based studies of writing and increasingly speech, namely stance devices; it also provided an example of a new direction in spoken discourse analysis in its exploration of variation within spoken interactions.
Perhaps most significantly, corpus approaches to academic writing provide insights into disciplinary practices which help explain the mechanisms by which knowledge is socially constructed through language.
Together, this research explicitly contradicts the view that corpus linguistics takes an impoverished, decontextualized view of texts and replaces it with a detailed picture of how students and academics write in different genres and disciplines.
It is these patterns of repetition which corpus analyses seek to uncover.
Another criticism of academic corpus studies is that, until fairly recently, these have been largely text-focused so that features seem rather abstract and disembodied from real users.
Based on the 6.5-million-word British Academic Written English (BAWE) corpus, the study develops a genre classification to identify and describe thirteen major types of assignment according to their purpose, stages, genre networks, and characteristic language features.
The study is also a good example of how corpus techniques can be used to map patterns in a large collection of texts, identifying moves in different genres, comparing frequencies of various language features, and offering detailed description of how individual words and phrases are used.
The descriptions offer important characterizations of academic writing so, for example, Biber found that over 50 percent of all nouns in the written corpus had abstract/process meanings that refer to intangible concepts or processes (system, factor, difficulty).
Perhaps most corpus studies of academic writing have followed what Tognini-Bonelli (2001) calls a corpus-based approach, where the researcher begins with a pre-selected list of potentially productive items and uses the corpus to examine their frequencies and the ways they behave in different contexts.
It also offers a starting point for corpus-driven investigations of academic corpora by generating list of items which can be further explored in more detail using concordance analyses.
So, by checking the frequency of definite, indefinite and "zero" articles in a corpus of bios and then looking at concordance lines for each, we find that professors are far more likely to use naming terms that collocate with definiteness (she is professor of, he is the author of) which serve to uniquely identify them.
Annotation refers to adding linguistic information to a corpus.
While a raw corpus is a highly useful resource, annotation provides an extra layer of information, which can be counted, sorted, and compared.
We need to expand corpus studies into multimodal academic genres where writing is frequently used with graphical and visual semiotic forms, such as academic websites and textbooks.
Comparing the features of target writers' texts with a much larger reference corpus of work in the same discipline can help to determine what is general in the norms of a community and what represents more personal choices.
We can, in other words, see that if a particular word, phrase or usage is common in a corpus of a particular writer's work, then it might be said to be a consistent preference which reveals something of that individual's routine expression of self: of a relatively unreflective performance of identity.
To capture this I compiled a corpus from each of the published single-authored works of two experienced and wellknown applied linguists, Deborah Cameron and John Swales.
My corpus of Cameron's published writing consists of 21 single-authored papers made available by the author.
The Swales corpus was compiled at the Michigan ELI and consists of 14 single-authored papers together with the bulk of his three monographs, representing eighteen years of output and comprising 342,000 words.
These corpora were individually compared with a larger reference corpus representing a spectrum of current published work in applied linguistics and in the same genres as the target texts.
The top content items from the Swales corpus are research, genre, English, academic, writing, non-native speakers of English, and the concept of discourse community which similarly encompass his key areas of contribution.
One example from Cameron's writing is the significantly above-average use of is; which was the fifth most frequent keyword in her corpus.
Self-reference, in fact, occurs 9.1 times per 1,000 words in the Swales corpus compared with 5.2 in the reference corpus, imparting a clear authorial presence of a thoughtful reflective colleague thinking through issues.
It is important to mention, however, that generalizing from a corpus will always be an extrapolation -it provides the evidence for interpretations about how language works.
A well-designed corpus can provide representative samples of registers (see Clancy 2010 on representativeness).
Although always important, this issue is especially important for corpus-based studies, which generally seek to produce more generalizable results than many other approaches and to facilitate comparisons between different corpora.
Some concern methodology within corpus linguistics.
A thorough synthesis that answers this question would be very useful to corpus linguists.
However, corpus-based register studies could have far greater impact than they currently do.
However, the study varied from typical corpus studies in its use of interview data.
In Section 2, an initial survey demonstrates the importance of the register factor in historical corpus linguistics and introduces a number of central matters that arise when the concept of register is applied to diachronic material.
In this section, we discuss how two of the central desiderata in corpus linguistics apply to historical registers: representativeness and comparability.
A synchronic or diachronic corpus is representative to the extent that the study of that corpus can replace the study of the textual universe it represents.
This, however, means that another desideratum of corpus composition must be considered, viz. comparability.
The extension of the corpus-linguistic paradigm to past stages of the English language has increased the attention given to sampling issues in the above regard.
Walker's study is a good example of a diachronic corpus-based investigation where the author needs to consider the special nature of his or her data and carefully assess the reliability of the data sources used.
This chapter has highlighted the growing recognition of the crucial role played by the register parameter in historical corpus linguistics.
Future advances in historical corpus linguistics are likely to have theoretical as well as practical effects on how scholars use registers.
The present chapter begins by situating corpus stylistics within the context of corpus linguistics (Section 1) and computational stylistics (Section 2).
While the purpose of the analysis of texts may vary between corpus linguistics and studies interested in style, the methods, however, can still be similar.
In reviews of historical developments in corpus linguistics, reference is often made to the fact that concordances are not an invention of corpus linguistics, but have been used in the study of literature even before computers existed, for instance, to compile concordances of the Bible or of works of Shakespeare.
While in corpus linguistics concordancing has become a mainstream method, in literary criticism it does not seem to play a major role.
In this sense, corpus stylistics requires engagement with concepts that address properties and interpretations of literary texts.
In the following I want to look at four studies that exemplify principles relevant to corpus stylistics.
Overall, corpus-stylistic methods are characterized through the tension between qualitative and quantitative techniques.
Studies that employ corpus-linguistic methods to demonstrate the working of a method make selective links to literary critical arguments.
The term "corpus stylistics" can be used to emphasize an intrinsic explanatory goal of stylistics that is concerned with the meaning of individual texts.
This focus is closely related to key concerns in corpus linguistics showing that frequent patterns are not necessarily those that language users are aware of.
This relationship is addressed by questions about what linguistic features are best regarded as register, genre or style features, but also by testing models originally designed for the analysis of literary texts on a larger corpus.
With the focus on textual meanings, limitations of the application of corpus methods are highlighted.
This approach results from a combination of corpus-linguistic and literary arguments.
At the same time, corpus linguistic studies show that very frequent clusters (more commonly referred to as "lexical bundles") are associated with discourse functions and so become important textual building blocks.
The concept of "local textual functions" allows a combination of both corpus-linguistic and literary perspectives in the analysis of clusters.
The more specific corpus tools and methods that are employed comprise relatively basic techniques: the retrieval of clusters, key comparisons, concordance searches, and the identification of (significant) collocates.
This functional analysis also takes account of distributions across Dickens's texts and the nineteenth-century reference corpus and hones in on detailed textual examples discussed mainly from an intrinsically explanatory point of view.
Methodologically, the working of the corpus-stylistic circle here means I tried to find links between the patterns that emerged from the corpus and the discussion of related examples or relevant theories in the literature on Dickens.
However, at the same time, this narrow corpus-methodological focus makes it possible to systematically complement the quantitative findings with a detailed qualitative analysis.
Instead of providing a range of methods and linguistic examples to demonstrate the usefulness of corpus stylistics more generally, the study creates a coherent argument for a theoretical approach to characterization in Dickens.
This chapter has shown that corpus approaches to the study of literary style can take various forms.
The more corpus approaches are interested in the literary quality of texts and intrinsic analytical goals, the more these approaches have to become interdisciplinary.
Hence there are obvious points of contact for cognitive-stylistic and corpus-linguistic approaches.
Although far less common, a corpus-based approach to data collection also has several advantages, including allowing for dialectologists to collect large amounts of data from a large number of informants, observe dialect variation across a range of communicative situations, and analyze quantitative linguistic variation in large samples of natural language.
In addition to national variation, one of the more successful applications of the corpus-based approach to dialectology is in the field of historical sociolinguistics.
The corpus that has probably been the subject of the greatest number of studies of social dialect variation is the BNC, which is coded for a variety of demographic information, including age, gender, education level, and class.
First, corpus-based studies have shown that dialect variation exists across a wide range of different varieties of language, including forms of written and standard language, where dialect patterns had never been sought or even believed to exist.
Second, corpus-based studies have shown that dialect variation can involve a much wider range of linguistic variables than is generally analyzed in dialectology and sociolinguistics.
In addition to reviewing previous corpus-based dialect studies, this chapter presents an analysis of dialect variation and change in not contraction in a corpus of American letters to the editor.
In particular, the goal of this analysis is to compare not contraction across years, genders, and regions in the WARD corpus, which was introduced earlier in this chapter.
In total, the WARD corpus contains 26,573,826 words, spread across 159,181 letters, written by 130,659 authors -which is a far larger sample than would be possible if traditional methods for data collection had been applied.
Each letter is also annotated for its date of publication, facilitating the analysis of temporal variation in this corpus.
In order to analyze gender variation in this corpus, the gender of the author of each letter was predicted based on their first name, as listed in the byline of the letter.
Each of these variables were measured as the percentage of contracted not in a given corpus.
For example, if a corpus contains 80 tokens of the words do/does/did followed by the full form of the word not and 20 tokens of these verbs followed by the contracted form of the word not, then the DO not contraction rate in that corpus is 20 percent.
This case study analyzed variation in not contraction across year of publication, gender and region in a corpus of American letters to the editor.
The corpus-based approach to dialectology contrasts with the standard approach, which is based on analyzing language elicited through interviews and questionnaires.
As demonstrated by the studies reviewed in this chapter, the corpus-based approach to dialectology is a growing field of inquiry that allows for new types of research questions to be pursued and new types of dialect variation to be identified.
In particular, a corpus-based approach is especially conducive to the analysis of quantitative grammatical variation, because it allows for large samples of natural language to be collected for analysis, and for dialect variation to be observed and compared across a variety of communicative situations.
Because of these advantages, corpus-based dialect studies have greatly expanded our knowledge of dialect variation, showing that social and regional linguistic variation are far more complex and pervasive than has previously been assumed.
Nevertheless, there are disadvantages to the corpus-based approach as well.
To a large extent, these are reasons why the corpus-based approach is not more popular in dialectology and sociolinguistics today.
When this level of technology is reached, corpus-based dialect studies will become the norm.
English corpus linguistics was kick-started by the compilation of the Brown corpus of written American English (AmE) in 1961.
Thus, corpus comparability needs to be addressed in a critical evaluation of ICE for the study of World Englishes.
The majority of ICE corpora were released without detailed bibliographical background information on individual texts included in the corpus or biographical information for the spontaneous spoken conversations, notable exceptions being ICE-NZ and ICE-IRE.
Finally, a recent trend in corpus-based research of World Englishes is the detailed statistical modeling of variation, often including ENL, ESL, and EFL varieties.
Corpus work is therefore still rare; the databases that have been collected have mostly been small, and are perhaps best counted into the very generic category of "corpus" that in traditional philology was used to describe the language data investigated for a study.
Despite the somewhat bleak overall picture of the amount of corpus work completed hitherto on ELF, the first one-million-word spoken corpus, The Corpus of English as a Lingua Franca in Academic Settings (ELFA, www.helsinki.fi/elfa) was completed in 2008, very soon followed by The Vienna-Oxford International Corpus of English (VOICE, www.univie.ac.at/voice) of the same size.
A basic question facing an ELF corpus is how different ELF is from English as a native language (ENL).
Unlike World English corpora (see Chapter 21 this volume), an ELF corpus does not seek to capture a local or regional variety of English, which would then lend itself to comparisons across others of the same kind.
A corpus of this kind allows us to seek answers to the question of how the situational constraints of lingua franca use shape the language.
Again the POS-tagged VOICE corpus clarifies the situation.
By providing a manually checked, dual POS tag which encodes both form and function (VOICE Project 2013), the XML corpus can be searched for all cases of an "innovative" form, in Cogo and Dewey's terms.
In keeping with the theme of seeking meaningful patterns in ELF above the level of individual words, in Section 2 we report a large-scale study of verb syntax in the ELFA corpus.
This section presents an overview of a recent study and findings on four syntactic features of spoken ELF carried out on a subset of the ELFA corpus.
A subset of the ELFA corpus was used to gain a maximal diversity of L1 backgrounds of the speakers.
The corpora employed in the study were the ELFA corpus for primary data, and the 1.8-million-word MICASE corpus for reference data for its close match in content and construct to ELFA, but collected in native-speaker settings.
Further, a vast majority of the matrix clauses introducing both kinds of embedded inversions are declarative clauses (almost 90% in each corpus), which seems to indicate that it is the ("interrogative-like") matrix verb (not e.g. an interrogative clause) preceding the indirect question that has the strongest effect on the occurrence of the non-standard formulation for both speaker groups.
The article is especially interesting for its discourse-oriented perspective, starting from a rhetorical function rather than from one or two isolated items, as well as for its focus on scientific prose and its plea for more corpus-based English for Academic/Specific Purposes textbooks.
The frequencies of POS categoriesboth major categories such as pronouns and subcategories like personal or indefinite pronouns -were compared in two similar-sized corpora, a corpus of native novice writing, LOCNESS, and the French component of ICLE.
The study is a good example of a fully corpus-driven analysis which allows generalizations to emerge bottom-up from the learner data.
This includes errors (which were the focus of pre-corpus interlanguage studies), but also cases of under-or overuse, i.e. the use of significantly fewer or more instances of a particular item as compared to the reference corpus.
As opposed to earlier LCR studies that did not include any statistics, most current studies now follow the general trend in corpus linguistics by providing some sort of statistical analysis.
CIA studies raised the issue of the norm (native vs. non-native; novice vs. expert) and pointed to the benefit of relying on an explicit corpus-based norm rather than the implicit and intuitive norm that underlies many SLA studies.
Interestingly, this variability also appears in the native corpus, where it is actually the most marked of all (sub)corpora.
This corpus-internal variability should be taken into account when doing LCR.
It may be tempting, in corpus linguistics in general and in LCR in particular, to limit the analysis to a quantitative approach.
It also fails to fully exploit the spoken corpus at hand, since we did not use the audio files, even though these would have been useful, for example, to disambiguate between the DM and non-DM uses of the six bigrams under study.
Hence, gathering texts from a wide swath of academic disciplines, Gardner and Davies extracted a "core" academic list from a corpus of over 120 million words.
Although clearly out of date now and fraught with a number of issues, including the corpus on which it was based, and subjective decisions regarding what should be included or not, there may be a baby in that bathwater.
That meticulous counting method resulted in what was probably a more accurate representation of the nature of the lexis in the corpus from which the 1953 GSL was derived, with counts that reflected separate lexemes, including multi-word expressions.
Of course, a corpus of the size of the BNC cannot be easily analyzed without the use of some kind of specialized software to be able to observe patterns using all the data contained in it.
WordSmith Tools was then used to upload all the texts (over 4,000) and construct what is called an "index" of all the words in the corpus.
An index analysis collects vital information about each word in the corpus (e.g. dispersion, collocation, and so on), and is necessary if one wishes to run an analysis of recurrent word strings.
With the frequency cut-off bands now established, we began the actual extraction process by using WordSmith Tools to interrogate the original indexed list for any and all n-grams (or bundles) between two and four words long repeated in the corpus at least five times.
However, since an item like at first has a frequency of over 5,000 in the corpus, line-by-line searching was not a viable option.
What the random sampling entailed was simply generating a concordance of the potential phrasal expression in question using the entire BNC corpus.
Sinclair's pioneering corpus work was first put into practice lexicographically in the Collins COBUILD English Language Dictionary (CCELD), a monolingual dictionary for learners of English published in 1987.
It was created for lexicographers and computational linguists, using a custom-built corpus of 1.7 billion words uploaded in the Sketch Engine.
The PDEV is an ongoing corpus-driven project in which a procedure called Corpus Pattern Analysis (CPA) is applied to identify the various patterns in which a verb is used and then discover how exactly meanings arise from each of the patterns.
The Oxford Collocations Dictionary for Students of English is the only one of the three collocation dictionaries which is corpus-based.
In the second part of the study, Granger and Lefer (2012) used the English part of the Label France translation corpus to identify frequent translation equivalents of two lexical bundles, i.e. de plus en plus (de) and sur le plan (de), and compare corpus-derived translation equivalents with those found in three French-English bilingual dictionaries: RC, HO, and the Larousse French-English Dictionary (LA).
However, the most frequent equivalent in the translation corpus is increasingly, which is only mentioned in the LA.
There are many different types of English corpora (see Chapter 1 this volume) but the most widely used corpus in lexicography is the large monolingual reference corpus.
Today, lexicographers at Oxford, for example, have at their disposal a corpus of over 2 billion words that represent a range of material from different subject areas (e.g. business, computing, law), regions of the world (e.g. Australian English, Canadian English as well as new varieties such as Hong Kong English), and types of writing (e.g. academic papers, newspapers, novels, blogs).
In terms of quantity, for example, the use of corpus-derived collocation boxes needs to be systematized.
First, corpus tools can be applied to individual texts, in helping decide whether a text is appropriate and what elements to focus on.
Free software such as VocabProfile online (www.lexutor.ca/vp) or AntWordProfiler offline (www.antlab.sci.waseda.ac.jp/) allows a teacher to input a text which is then returned with the lexis color-coded according to the frequency of each word in the BNC or COCA corpus.
The question of course is whether corpus work really lives up to expectations, with benefits sufficient to justify the investment.
Mostly these corpora are explored on computer, only 24 using exclusively or in part printed activities derived from a corpus.
Following this indication that corpus work could help these learners expand their lexicons, a scaled-up version of the project was prepared using two levels of learner, both experimental and control groups, two outcome measures corresponding to experimental and control conditions, and a learning target of 200 new word families per week for twelve weeks (or 2,400 words, roughly the number these learners would need to have a chance of reading for content in English).
The data considered here are drawn from the corpus of 116 individual studies described in the previous section.
In other words, this meta-analysis investigates whether corpus use can have an effect over a wide range of variables, including vocabulary and grammar learning, error correction, lexical retrieval, and translation success.
However, no other variables will be considered at this stage of the meta-analysis, such as participant meta-data (e.g. age, L1, L2, level of proficiency), instructional design (e.g. duration, hands-on or mediated interaction with various corpus types) or experiment design (e.g. immediate or delayed post-test).
In other words, the answers to both our research questions (Is corpus use effective for L2 learners -i.e. does it have a demonstrable effect? Is corpus use efficient for L2 learners -i.e. compared to other forms of learning?) are clearly Yes and Yes, based on the studies available to date.
The evidence suggests that corpus work is now ready to expand beyond the university ESP class, where it has largely been used to date, into mainstream second and foreign language learning -where, of course, its effects can continue to be investigated and the conditions of its success elaborated.
Traditional corpus consultation is in some ways a relatively marginal activity, to be found in few classrooms around the world.
Conrad argued that three changes prompted by corpus-based studies of grammar had "the potential to revolutionize the teaching of grammar" (2000: 549): first, monolithic descriptions of English grammar would be replaced by register-specific descriptions; second, the teaching of grammar would become more integrated with the teaching of vocabulary; and third, the emphasis would shift from structural accuracy to the appropriate conditions of use for alternative grammatical constructions.
Section 1 presents some of the key issues in the writing of corpus-informed materials.
In Section 2, we briefly present eight grammar textbooks (four corpus-informed and four non-corpus-informed); these textbooks are analyzed with a view to finding out the similarities and differences between these two types of materials and to answering the research questions presented in the introduction.
We carry out a more indepth analysis of two of the four research questions listed above, namely how corpus-based research applications are presented, and how (and how much) corpus-based research has been incorporated into materials.
As numerous corpus studies have investigated the passive, we have selected that topic as a candidate for the comparison of corpus-informed versus non-corpus-informed pedagogical materials.
In addition to the corpus-informed books, we will also review four popular noncorpus-informed grammar books at this same upper-intermediate to advanced level.
In this section we analyze the treatment of the passive in the four corpusinformed books and four non-corpus-informed grammar books presented in Section 2.
Our aim in comparing those two types of grammar books is twofold, viz. to find out the similarities and differences between these materials, and see which corpus findings have been included in corpusinformed materials.
This topic has been extensively researched in corpus studies and, therefore, much is known about the use and also the lexical associations of the passive.
The checklist for corpus-informed materials will be presented first and will then be followed by the non-corpus materials one.
We will then provide more details on exactly which type of information is presented in the corpus-informed books, but also how is it presented.
In 2008, Meunier and Gouverneur stated that publishers seem to acknowledge the importance of corpora in ELT but fail to give precise information on how exactly the corpus is used.
What the checklist reveals is that some corpus-based information is rather coherent across the various books (get-passives are rare and are more commonly found in speech; agentless passives are more frequent than passives with by-agent, etc.).
Non-corpus-informed pedagogical grammars fail to include important information on the passive.
Granger's results are, however, disconfirmed for the corpus-informed materials as we obtained a clear Yes for almost 70 percent of the cells on the checklist. There nonetheless remains room for improvement as 11 percent of the cells received a negative evaluation.
Non-corpus-informed materials slightly outperform corpus-informed materials on two fronts: the contextualization of the examples (in 3 cases out of 4, versus 2 out of 4 for corpus-informed books) and the integration of grammar within skills (in 3 cases out of 4, versus 2 out of 4 in corpus-informed books).
In this section, we will carry out a more in-depth and qualitative analysis of one of the features that distinguish corpus-informed from non-corpusinformed ones, i.e. the inclusion of lexical information.
Lexical information is one area where the corpus-informed books have a clear advantage over the non-corpus-informed books.
We then briefly presented a number of corpus-and non-corpus-informed grammar books and carried out a case study on the treatment of the passive in those two types of grammar books.
The comparison revealed that non-corpus-informed materials fail to include important information on the passive.
Our aim in carrying out the case study was not to come up with the conclusion that all grammar textbooks should include corpus-based information on all grammar points.
Whilst corpus-based information on article usage might not add much to the treatment of articles in grammar books, we believe that some areas would really benefit from the inclusion of the results of corpus studies.
Finding exactly the right amount of corpus information (frequency, registers, lexical preferences, etc.) in sufficient detail is probably an elusive goal as each user/learner has different needs.
But, as shown in the case study carried out in this chapter, it should be feasible to draw up a list of core corpus findings worth including in all types of grammar books.
The first part of this chapter surveys the development of corpus-based translation studies (CBTS), from the programmatic proposals in the early 1990s to recent developments and trends.
The application of corpus methodologies to translation research can be traced back to Mona Baker's seminal paper in which she argues that: the most important task that awaits the application of corpus techniques in translation studies . . . is the elucidation of the nature of translated text as a mediated communicative event.
A more complex parallel design is used by √òveras (1998), who searches for shifts in cohesion/coherence in the English-Norwegian Parallel Corpus, a bidirectional corpus including STs in English and their Norwegian TTs and (comparable) STs in Norwegian and their English TTs.
The translational component of the comparable corpus of narrative texts contains a higher proportion of high-frequency words and its list head covers a greater percentage of text with fewer lemmas than the nontranslational component.
Differently from the newspaper corpus, this turns out to be significantly higher, with higher variance, in translated fiction.
The second hypothesis is confirmed for both observations: in the translated component, the most frequent word forms account for a significantly higher percentage of the corpus and the proportion of high-frequency to lowfrequency words is significantly higher.
On the basis of this study and of a previous analysis of newspaper language, Laviosa proposes four "core patterns of lexical use" potentially applying to translated English in general, namely that: Translated texts have a relatively lower percentage of content words versus grammatical words (i.e. their lexical density is lower); the proportion of high frequency words versus low frequency words is relatively higher in translated texts; the list head of a corpus of translated texts accounts for a larger area of the corpus (i.e. the most frequent words are repeated more often); The list head of translated texts contains fewer lemmas.
Laviosa's work from the late 1990s has been extremely influential in paving the way for the construction of translation corpora for languages other than English (e.g. the corpus of translated Finnish at the university of Savonlinna) and for the search for core patterns of translated language.
In a nutshell, bigram types are first obtained from the corpora of translated and non-translated texts used for the study; they are then matched with frequency and Mutual Information data obtained from a reference corpus of English, and ranked according to these data.
The FINREP corpus is a corpus of corporate financial reports, i.e. reports issued annually by companies providing financial information as well as commentaries about operational and financial performance.
The SHARLET corpus covers a prominent subgenre within the corporate financial report, namely that of shareholders' letters.
The intervening function words are used to constrain searches but are not retained in the subsequent phases: only the association between lexical words is tested against the reference corpus.
A method for comparing use of collocations in translated and non-translated texts was applied to two monolingual comparable corpora of texts from the same domain and in the same language (finance/English) but varying in size and genre (a largish corpus of financial reports and a tiny corpus of shareholders' letters).
Relying on frequency data obtained from a large monolingual corpus, it was possible to show that translated financial reports are less collocational than comparable non-translated reports, while translated shareholders' letters seem to go in the opposite direction: they feature stronger collocations than non-translated letters, often resulting from explicitating or normalizing shifts.
Over the years, scholars have noted and discussed the occasionally elusive nature of the challenges and suggested solutions to the problems, tackling broader and abstract notions such as representativeness and balance of corpora, but also more concrete questions like sampling, various levels of annotation, precision and recall of corpus queries, dispersion of search hits across a corpus, statistical evaluation, among other points.
The attitudes of corpus linguists seem to include both a healthy sense of humility as regards what is possible to achieve, as well as an understanding of the need to continuously remind the scholarly community of the core concerns in the field.
In many ways we can then say that the challenges do not only relate to corpora and their use themselves, but also to how we educate new corpus users and inform them about the fundamental concepts and concerns.
Yet beginning corpus users might benefit from learning about what may be regarded as tacit knowledge in corpus linguistics, and even the more advanced scholar may encounter issues new to them that have been addressed earlier.
While the chapters mostly concentrate on English corpora, many of the themes are relevant from a general corpus-linguistic point of view.
Notable improvements could also be gained by the increased collaboration between traditional corpus linguists and scholars in the fields of computational linguistics and NLP, of which there are already encouraging examples.
In Chapter 5, the question of the useability of databases for corpus linguistic purposes is addressed by Turo Hiltunen, who discusses issues relating to the use of materials in the British Library Newspapers database.
Perspectives on the matter vary between scholarly approaches: the criteria for determining what is "good data" -for example, considerations on the accessibility and processing of the data -can be quite different in Digital Humanities and corpus linguistics.
As a way forward in trying to find solutions to improve the adaptability of databases for linguistic research, Hiltunen calls for more interdisciplinary discussions and collaboration between corpus linguistics, Digital Humanities, and related disciplines, which are still too far from each other because of the disparities in their research agendas.
The differences between how literary genres are categorized largely reflect the different viewpoints and goals between the fields, with corpus linguistic categorizations aiming to focus on the different communicative purposes of texts, whereas the descriptions of genres by literature scholars are based more on content and stylistic concerns.
When Professor Matti Rissanen wrote a short article entitled "Three problems connected with the use of diachronic corpora" for the ICAME Journal in 1989, the landscape of corpus linguistics looked quite different from today.
At the time, Rissanen was leading a group of scholars responsible for the compilation of the first diachronic corpus of English, the Helsinki Corpus of English Texts.
The Helsinki Corpus, which was to be published in 1991, would revolutionize the historical research of English for decades to come, and its importance for historical corpus linguistics cannot be overstated.
Against this background, it is interesting to note that Rissanen felt compelled to discuss some potential pitfalls related to historical corpus linguistics two years prior to the publication of the Helsinki Corpus.
Furthermore, there are some new trends in corpus-based research that Rissanen could not have anticipated in his article, and these introduce challenges of their own.
In this paper, we revisit Rissanen's ideas in order to identify some of the present-day pitfalls in historical corpus linguistics.
While the original formulation of the philologist's dilemma is no less relevant for scholars engaged in, say, historical sociolinguistics, we argue that in more grammatically oriented studies that make use of big data, increasing attention should be paid to the corpus itself: to its sampling principles and layers of annotation.
Each of the following sections is intended to illustrate one general problem associated with historical corpus linguistics by discussing individual studies that we have carried out previously.
Section 3 focuses on some pitfalls related to big corpora, and our examples in this section concern both the reliability of the semi-automated sampling of such resources and the comparability of the research results when new genres are introduced to the corpus.
In (7) to (11), taken from COHA's sister corpus, the Corpus of Contemporary American English (COCA), all the "nouns" in boldface are used in a descriptive and non-referential function (i.e., similarly to key and fun in Examples (1) to (4) above), and yet none of them has been annotated as an adjective in the corpus.
Moreover, because the corpus contains private letters from a period before English was standardized, there is a great deal of spelling variation in the corpus.
Here, it is not our intention to argue that the texts included in big-data corpora should be sampled manually; indeed, when the corpus includes samples from tens or even hundreds of thousands of individual texts, it is obviously not feasible to check all the data sources individually.
We consequently decided to investigate the frequency of sentence-initial as well in COHA, a corpus that provided us with enough material to examine its diachronic development in recent American English.
We were only concerned about the low frequency of the form: in the last period studied , there were 32 tokens of the sentence-initial usage in COHA, which meant that our results might have been tarnished by errors made in the sampling of the corpus.
Indeed, our initial enthusiasm was quickly dampened by the observation that the connective use of as well was dispersed very unevenly in the corpus.
While most of the corpus consists of novels, the most recent periods in the corpus include increasing amounts of drama, short stories and movie scripts, for example.
When we restricted our corpus queries to novels alone, which is possible if one uses the downloadable version of COHA rather than the online interface, the gender difference disappeared.
On the other hand, the history of many fiction genres, including drama and short stories, is certainly longer than the coverage of COHA, so their proportions could have been more optimally balanced in the corpus; considering the automated sampling of COHA, their better representation in the most recent periods probably reflects their increased availability in digital form.
Our main argument was that the recent trends and advances in the field, such as the publication of big-data corpora, the increased reliance on statistical approaches to linguistic data, and the exploitation of various kinds of metadata, require that the linguist should have a detailed understanding of the structure of the corpus, its sampling procedure, and the principles followed in the construction of the metadata.
We also suggest that these challenges are ultimately not so different from the pitfalls discussed by Rissanen in his "philologist's dilemma": just like in the early days of historical corpus linguistics, today's challenges pertain to the linguist's knowledge and understanding of their data.
However, because the focus of research has in many cases shifted from small, carefully compiled corpora to bigdata resources, the nature of the "data" and methods with which historical corpus linguists typically work has changed.
Consequently, we suggest that it would be prudent to think about the principle of "knowing one's data" from a new perspective: as the sheer size of the modern mega-corpora prevents scholars from engaging with either all or the majority of the original source texts in as much detail as in the early days of historical corpus linguistics, they should strive for an intimate understanding of the historical corpora as mediators of the original texts.
On the other hand, the linguist can compensate for this to some degree by acquiring a thorough understanding of the corpus and its description.
Many corpora nowadays still do not adequately enable corpus users to set up queries that would exclude items appearing in names when needed to improve precision of the searches.
With new corpora containing billions of words and allowing complex searches by making use of various search functions and different types of linguistic anno-tation, novice corpus users may find it difficult to question the relevance of their search results adequately and carefully.
Section 3 presents small-scale studies of corpus searches involving words that may be frequently used as named entities, or parts of named entities.
With a total frequency of 3,700, and a normalized frequency of 37.6 instances per million words, it can be regarded as a high-frequency word in the corpus.
However, a closer look into the instances of lifespan informs us that all the 3,700 hits are found in only 83 texts, which suggests the word probably has a very uneven dispersion across the corpus.
The differences seen in the rates of samurai in named entities potentially also reflect some cultural differences and the interests of the writers represented in the corpus: the named entities including samurai in the GB and US sections included more references to drama films (The Seven Samurai, The Last Samurai), the Asian sections featured references to food-related items, computer software (Market Samurai, a keyword research tool), action toy figures (Samurai Predator AC-01).
In other words, the potential of finding unwanted or irrelevant items may in some cases vary between different sections of the corpus.
However, out of the total number of hits of royal in the corpus -14,628 hits in 1,825 texts -it appears with a capital initial as often as 10,237 times in 1,603 texts).
In other words, in the case of both royal and regal, substantial proportions of the occurrences of the words in the corpus appear in proper names, and it would be crucial to exclude such instances if we wanted to study of the use of the words as non-frozen elements.
Some corpus interfaces such as the one at English-Corpora.org allow users to compare the collocates of two words, and the collocates which are more typically used with the compared words are listed in two separate columns in a ranked order, based on a score that takes into account the frequencies and comparative ratios of the two words in the corpus, the frequencies and ratios of different combinations of the words and their collocates, as well as the frequencies of the individual collocating words across the corpus.
Ultimately names also contribute to the word count of a corpus, and it might also be beneficial to exclude named entities from automated calculations in cases where names -or other types of items in a corpus which may be irrelevant to what one is trying to assess by the calculations -do not reflect the active linguistic choices of the writers or speakers represented in the corpus.
As has been observed in instances such as the noun lifespan in the British National Corpus, and the adjectival phrase absolutely fabulous in British and Irish sections of the News on the Web corpus, sometimes the irrelevant tokens may far outnumber the relevant ones.
As a lot of work has been done on named entity recognition in the field of computational linguistics, a key to address the problems raised in commonly used, large corpora in corpus linguistics would be to increase the collaboration between the areas, and to integrate the systems and practices developed in NLP and computational linguistics into corpora such as the BNC.
LCR as a field only emerged and became visible in the course of the 1990s in the context of the popularization of corpus linguistics at large but has rapidly evolved and grown in scope and sophistication over the past four decades.
This bias can be introduced either indirectly by the topic of the task in that, for example, a large batch of essays on the topic of friendship in a corpus will lead to a disproportionally high frequencies of words and phrases that belong to the lexical field of friendship.
In addition, ICE uses two more tags relevant to the present discussion: <foreign> (for a "word or sequence of words that is foreign and non-naturalised") and <indig> (marking words which are "non-English but are indigenous to the country in which the corpus is being compiled").
When compiling a new corpus, the task instructions and the input material should be neutral with regard to the elicitation of specific structure(s).
The aim of this chapter is to reflect on some of the pitfalls involved in using the British Library Newspapers database as a corpus and consider some ways in which they can be avoided.
I argue that the specific issues encountered with this database are due to fundamentally different ideas of what might constitute "good data" in the fields of Digital Humanities and corpus linguistics, respectively, and this in turn leads to different expectations regarding how the data is made available to researchers (e.g., Mehl 2021).
To use large, digitised archives as data for corpus linguistic research, it is essential to critically think about the ways in which they have been collected and processed, as well as what kinds of software tools are available to search them.
In Section 4 I summarise the main findings and discuss their implications to the corpus linguistic study of historical newspaper discourse.
Very similar discourse is often found in corpus linguistic literature in support of a corpus-based approach.
However, to discuss possible pitfalls associated with research at the interface of Digital Humanities and corpus linguistics, it is necessary to briefly review some basic assumptions associated with these respective areas of study.
It is clear that many of the points made in the previous section apply equally well to corpus linguistics, where research is based on large amounts of language data which have been processed and turned into linguistic corpora.
Even the same metaphors are used: Such similarities are of course not surprising, given that both Digital Humanities and corpus linguistics deal with textual data.
However, there is one major difference that sets corpus linguistics apart from the Digital Humanities, and this relates to the definition of the term corpus.
The argument that I want to make in this chapter is that in order to take advantage of the synergy between corpus linguistics and Digital Humanities, it is often necessary to critically reflect on the digital materials, how they have been collected, processed, and made available.
In particular, it is often the case that the default workflow for accessing a specific archive is not ideal to tackle corpus linguistic research questions, which may make it difficult to compare results to previous work.
To corpus linguists, however, the database also presents multiple issues, which need to be addressed to make full use of it.
While such an "individual-document approach" (McEnery & Hardie 2011: 232) might indeed be appropriate for some purposes, corpus linguists would usually be interested in looking at a concordance to identify patterns in the use of the search term or calculate and compare its normalised frequency across different sections of the database.
This issue, too, can be illustrated with the help of the same sample corpus from Hiltunen (2021), Corpus A, which comprises complete issues of the four newspapers sampled at 10-year intervals.
This ensures the extracted sample corpus is reasonably clean and can therefore be expected to provide reliable output for standard corpus linguistic tasks.
After identifying and reviewing four major pitfalls and suggesting possible ways of avoiding them, it is possible to offer some preliminary conclusions about the usefulness of the British Library Newspapers database for corpus linguistic research.
As a result, the British Library Newspapers database is an attractive alternative for the corpus-linguistic analysis of historical newspaper prose.
The replication crisis in linguistics is highly relevant to corpus-based research: Many corpus studies are not directly replicable as the data on which they are based are not readily available.
In a best-case scenario, it involves the open availability of the entire corpus the researcher draws on, as well as sharing of concordances, annotations, and analysis scripts (if applicable).
Section 4 concludes the paper by bringing the two strands of the discussion together by showing how Open Corpus Linguistics can contribute to overcome several widely discussed problems in corpus linguistics.
This can also be advantageous with regard to the "mystery of vanishing reliability", that is, the phenomenon that each datapoint tends to become less reliable the more parameters (in corpus-linguistic terms, annotations) one adds.
As such, the "mystery of vanishing reliability" can be reframed in terms of a lack of extensibility: Being in control over the compilation of a corpus allows us to easily extend the database if necessary.
For example, it could turn out that an annotation set used for a corpus is based on false assumptions.
Thus, it is tremendously helpful if a corpus is extensible, that is, existing annotations can be amended or improved and new ones can be added, both by the original creators and by people who reuse the data.
In the previous section, I argued that open research practices can provide (partial) solutions to common corpus-linguistic problems.
After all, the problems discussed here are not specifically corpus-linguistic ones.
Given that the information was public at the time of crawling, one could make a case that including it in the corpus is unproblematic, but it is quite easy to imagine scenarios in which the publication of data crawled from the web can lead to legally or ethically challenging situations.
In some cases, it can therefore be useful to publish a corpus in password-protected form, even though generally, the ideal should of course be maximal accessibility.
If we need full texts, my suggestion is that we should always consider using an openly available corpus like the BNC or the Open American National Corpus first.
In such cases, we might have to either fall back on commercial corpora or compile our own corpus.
In the preceding sections, I argued for adopting open research practices in corpus linguistics and examined a number of potential problems that such an endeavor entails.
Such a scenario also provides some flexibility in working with pre-compiled corpora, as we are not at the mercy of the corpus creators with regard to the composition of the corpus.
The latter is even true if we work with commercial corpora but make our concordances and analysis scripts available, as is increasingly common in corpus linguistics.
The choice of corpus has to be guided, first and foremost, by the research question.
Examples for synchronic English data include the Open American National Corpus for spoken and written American English, as well as the ENCOW corpus for World Englishes as used on the web.
The BNC, which is a paradigm example of an open corpus, hardly needs to be mentioned as it is already widely used.
If possible, try to create a corpus that can be published freely under an open license.
If you cannot make the full texts freely available for copyright reasons, try to make the corpus as accessible as possible, for example, by allowing queries via flexible search engines such as NoSketchEngine or CQPweb and by publishing word and lemma lists (and ideally, n-gram lists), or by publishing it in password-protected form via a repository that allows for closed-access corpora (e.g., CLARIN).
This is probably quite common -not only in corpus linguistics but also in other domains, for example, when it comes to publishing programming scripts.
Everybody who has ever worked in corpus linguistics knows that no corpus will ever be perfect, and that the quality of a corpus depends less on the competence of the researchers involved than on the resources they were able to put into it.
As such, nobody will blame you for releasing a corpus that is still more of a raw diamond.
Fortunately, this basic problem has a simple mathematical solution, which commonly forms the basis of typical quantitative corpus-linguistic inquiries.
Second, extremely short texts cannot be easily compared with other texts using many typical quantitative corpus-linguistic methods, such as normalization, because the normalization results become meaningless as the length of the text becomes increasingly short.
In order to talk about these problems, in this chapter, I use the words "long" and "short" to refer to texts which are and are not long enough for typical quantitative corpus-linguistic analysis, respectively, though the line between the two is of course fuzzy.
Most genres traditionally studied in quantitative corpus linguistics tend to comprise of longer texts, compared to the extremely short text lengths of up to only a few dozen words which are overwhelmingly common on, for example, social media.
But these texts also tend to be long enough for reasonable quantitative corpus-linguistic analysis.
The question of influence and significance also comes back to the question of availability, since it is of course more difficult to collect a large and representative corpus of post cards or shopping lists than of newspaper articles.
But the easy access to textual data online has allowed quantitative corpus linguists to cast a wider net in terms of their research topics than ever before.
Platforms which do limit posting length, such as Twitter (now X), usually limit the maximum length, confining all of their content into the range of short texts, which is mathematically difficult to work with in quantitative corpus linguistics.
If the aim is to be able to include as much of the data in our analysis as possible, this approach is at its most reasonable when there are only a small number of outliers under the chosen length limit, as the exclusion of a handful of outliers do not affect the overall results from a good-sized corpus very much.
It could even be argued that clear outliers do not even represent the varieties of interest in the corpus particularly well, and that therefore it would even be beneficial to exclude them.
However, the larger the proportion of the texts in the corpus which fall under the chosen length limit, the more problematic the exclusion method becomes.
However, since all texts in a normal-sized corpus are rarely close enough to each other in length, as a typical workaround, the ratio is calculated for a set number of words (such as 400 words) taken from the beginning of each text.
This chapter highlights genre categorizations as a pitfall at the intersection of corpus linguistics and literature and problematizes the use of the genre category tag from the perspectives afforded by both fields.
While the reference corpus did offer sub-categorization of the fiction component of the corpus, aligning those sub-categories with Hemingway's oeuvre is not a straightforward task.
This allows for the corpus to be used for comparisons between the authors short stories and novels, as well as comparisons between different authors considered to belong within the same group.
However, the corpus can only be used for questions based on comparisons within the Lost Generation in a broad sense, and it does not allow the user to perform comparisons along other lines of inquiry, for instance, carrying out comparisons between different genres of fiction.
To approach the potential genre category pitfall, this paper will first consider the position of corpus linguists and literature scholars based on the previous sections.
For those stuck between linguistics and literature, it becomes a decision regarding the intended audience of the corpus.
Our analyses rely on a particular type of linguistic data -a large, custombuilt corpus of tweets -as well as a recent computational approach to modeling lexical semantic phenomena -neural word embeddings.
For a given lexical item, our method produces computational representations of its occurrences in the corpus and splits them into groups based on semantic similarity, prioritizing those that are expected to reflect the influence of French.
In addition to the preprocessing decisions applied to the original corpus, we introduced additional filtering for the experiments presented in this paper.
First, we removed the content posted before 2016 in order to limit the likelihood of picking up diachronic effects; the tweets in the original corpus date back to 2006.
We begin by examining positive contributions of our computational system, focusing on the help it provided in distinguishing between conventional and contact-related uses based on documented patterns from the corpus.
They are generally used in a span of French text within a codeswitched tweet where most tokens are in English; this explains why the tweets were tagged as written in English and retained in the corpus.
Codeswitching is overall rare in our corpus, but its relative frequency is considerably higher in Montreal, as can be expected given the prevalence of bilingual speakers in the city.
Dotted lines show the 10th and 20th percentile for the bilingualism score, for all users in the corpus important influence of bilingualism.
That said, our analysis identified clear trends regarding the use of semantic shifts based on a large amount of data, further confirming the potential that corpus-based analyses have in understanding the patterns behind complex linguistic behaviors.
However, given the advent of technology and corpus linguistics, it is now possible to study and analyse these patterns of usage.
In this paper, a look will be taken at the area of corpus linguistics.
Firstly, a brief outline of what corpus linguistics is will be given.
Therefore, it may be advisable for the researcher to make a back-up copy of the corpus before taking the step of tagging it.
These files would be immediately ready for inclusion in a specialised corpus for both individual classes and a group of classes.
This would allow the teacher to tailor future lessons to the needs of the students as the corpus would help highlight any common or frequent errors and, hopefully, aid in discovering in why this type of error was made.
The corpus could also be student specific, which would greatly enhance feedback that a teacher gives.
Creating a corpus for a communication course would, naturally, be more time consuming, but would also offer the same potential benefits.
However, it would be quite difficult to create the type of student specific corpus mentioned above.
Especially over the last ten years or so, corpus linguists have begun to take this (in some sense obvious) fact into consideration and have followed the general development in linguistics towards more and more sophisticated quantitative methods.
One might have expected corpus linguistics to spearhead this development because, following the above logic, corpus linguistics is inherently quantitative.
Returning to corpus linguistics, it is instructive to compare its methodological challenges and trajectories to those of psycholinguistics.
On the one hand, corpus-linguistic observational data are typically much messier and unbalanced than psycholinguistic experimental data because many confounding and moderator variables that psycholinguists can control for (by randomising, blocking, etc.) plague corpus-linguistic analyses.
The general goal of this paper is to help to increase the number of corpus linguists who recognise the problems of the approaches on the left and, thus, decide to move towards the approaches on the right.
This has an extremely important consequence: both in psycholinguistics and in corpus linguistics, the vast majority of datasets violate the assumptions of the simplest test that an analyst might normally think of first -namely, that the data points are independent of one another.
As a consequence, the results that the majority of corpus-linguistic studies report are likely to be very anti-conservative (i.e., too likely to return a significant result) and imprecise (because the results are tainted to an unknown degree by idiosyncrasies from which one can, and should not, generalise) and, just to acknowledge that quite openly, this also applies potentially to several earlier studies of mine.
Thus, the hopefully not too high-flying goal of this paper is to become for corpus linguistics what the abovequoted papers have become for psycholinguistics: a first go-to resource that explains to corpus linguists what (generalised) linear mixed-effects/multilevel modelling ((G)LMM/MLM) has to offer and that provides them with a concrete example and some instructions on how to perform such analyses.
Second, I am not using a logistic/multinomial model, (i.e., a model with a categorical response), for this example even though such models are more typical of corpus-linguistic applications.
However, it is at least incomplete, if not inappropriate, because it pretends that the 2,321 data points are all independent of one another, which we know they are not: they exhibit inter-relations because they were produced by fewer than 2,321 speakers, because of the lexical items in the verb-particle constructions, and because of the levels of corpus sampling.
Then we fit a first model that, as above, contains all fixed effects -LOGLENGTH, TYPE, and their interaction LOGLENGTH:TYPE -as well as varying intercepts for each level of corpus sampling -(1|LEVEL1), (1|LEVEL2), (1|LEVEL3) -and for each verb (1|VERB) and each particle (1|PARTICLE).
Given the logic outlined in Sections 2.2 and 2.4, that means that the baseline probability of CONSTRUCTION: V-Part-DO can be different for each of these random effects; in other words, the model allows every corpus part (at each level of corpus organisation), every verb and every particle to have a different baseline 'preference' for CONSTRUCTION: V-Part-DO.
We generate a sixth we have now identified the optimal random-effects structure, which turns out to be much more complex than corpus-linguistic studies usually assume.
Given the fact that corpus-linguistic data are much more unbalanced and messier than experimental data, it is time that corpus linguists avail themselves of that same family of methods.
These techniques include frequency profiling: listing all of the words (types) in the corpus and how frequently they occur, and concordancing: listing each occurrence of a word (token) in a corpus along with the surrounding context.
The n-gram technique (also called clusters or lexical bundles) counts and lists repeated sequences of consecutive words in order to show fixed patterns within a corpus.
Although this iterative process is often not reported in final publications, it is evident from the many textbook descriptions of corpus linguistics.
Although they are not necessarily viewed as such, some existing techniques in corpus linguistics can be considered as visualisations.
First and foremost, the concordance view with one word of interest aligned vertically in the middle of the text and the left and right context justified in the middle, is a way of visualising the patterns of the context of a particular word, and is the main way that corpus linguists engage with corpora.
Very few of the existing techniques are tailored for the specific methods in corpus linguistics, and in addition, the existing corpus visualisations do not scale to large bodies of texts, a key requirement to tackle the growing size of corpora.
All these reasons call for new visualisation techniques, or at least the adaptation of existing ones, in order to specifically address the particular needs of corpus linguistics in terms of scalability, and support for iterative exploration.
Our case studies cover three of the five main methods in the corpus linguistic methodology: frequency lists, key words, and collocations.
In the second case study, we propose to use interactive visualisation techniques to improve the interpretation and exploration of the collocation method in corpus linguistics.
The exploration could proceed as described for the structural use case above but would now be extended to cover other levels of linguistic annotation assuming that they were represented in the corpus.
A prototypical example of this would be a Twitter corpus that has been collected over a number of months or years.
This combination across three dimensions will therefore allow a user to explore the corpus on many different interconnected levels and visualisations.
We have highlighted tools and techniques that are already used in corpus linguistics that can be considered as visualisation: concordances, concgrams, collocate clouds, and described new methods of collocational networks and exploratory language analysis in social networks.
The development of an annotated corpus is a very time-consuming process.
The remainder of this paper is organized as follows: Section 2 describes our proposal, and Section 3 contains information regarding studies based on corpus compiled with this tool, as well as the description of future lines of action.
The next step is to assign each corpus a set of independent labels.
They can be made using a set of predefined labels, such as outof-domain, positive, negative, neutral, do-notknow-do-not-answer or define a new set of tags for the corpus.
The corpus labeling process can be carried out manually by the same user or allow access to the platform to a set of annotators and to supervise their work.
This is useful, for example, when a classification has been made on a scale of very negative, negative, neutral, positive, and very positive type values, but one may want to combine the results to return the corpus grouped into positive, neutral, and negative.
One of the advantages of this approach is that corpus can be exported by consensus: since the same tweet can be classified by different annotators, the number of tweets to export can be limited and retrieve those tweets that have achieved strong consensus among annotators.
Thus, subsets of the corpus comprising the documents with common agreement can be retrieved, and the rest of the documents can be analyzed.
In this study, we have presented UMUCor-pusClassifier, a NLP tool that assists in the compilation and annotation of linguistic corpus.
This book is a practical introduction to statistical procedures in corpus linguistics, a discipline that uses computers to analyse language, organized according to linguistic topics.
The book is intended for anyone interested in corpus linguistics and quantitative analysis of language.
The book reflects the needs of students and researchers who are looking for a practical guidebook on corpus statistics, which is grounded in the current literature in the field and reflects the best practice.
Another scientific requirement corpus linguists follow in principle is replicability of results.
In order for the results to be replicable, corpus linguists need to make their choice of corpora and analytical techniques transparent.
If corpus linguistics wants to retain its scientific status, it should not be content with statements such as 'this feature was found in a large corpus that is, however, not available'.
Mastering these terms will make reading of the rest of the book, and many papers in corpus linguistics, much easier.
Dataset is a series of corpus-based findings that can be statistically analysed.
Linguistic variables capture frequencies of linguistic features of interest in the corpus.
A rogue value can be caused, for instance, by mistyping data in a spreadsheet or by a tagging error in the corpus.
In corpus linguistics the most useful ones are: mean, median and 20% trimmed mean.
To put it very simply, the null hypothesis states that there is nothing special going on in the corpus or corpora we analyse, e.g. there is no difference between two (sub)corpora.
Based on the p-value (i.e. the probability value of the observation in the corpus by chance alone) we decide whether to reject the null hypothesis.
If the p-value is equal to or is larger than 0.05 (or 5%) we conclude that there is not enough evidence in the corpus to reject the null hypothesis.
In the example of a sociolinguistic research looking at the use of swearwords by men and women we would get a p-value that would give us the probability of seeing the observed or even more extreme difference between the two groups if the null hypothesis were true, that is, if there really was no difference in the population and the difference observed in the corpus (sample) was merely due to chance as the result of a sampling error.
In corpus linguistics the term 'representative' is often used to describe a corpus.
To make the sampling manageable, corpus designers often start with a set of categories within which they aim to collect an unbiased sample.
Each Brown family corpus thus consists of approximately one million words of written English (500 √ó 2,000).
If a corpus samples only certain sections, e.g. by taking the first or the last 2,000 words of each text, these sections will be overrepresented.
This is connected to a so-called 'whelk problem' (see Section 2.4) when some infrequent lexical items become dramatically overrepresented in the corpus.
Topic bias is a problem especially in small corpora where each individual text forms a relatively large proportion of the corpus.
In most cases, corpus designers should therefore consciously select texts on a range of topics.
When selecting the sampling frame we are often predisposed to see as salient the text types that have traditionally been included in language corpora such as those in the Brown family sampling frame (see ‚Ä¢ Legal considerations bias: corpus designers often face the problem of copy- right; this is especially the case when corpus creators want to share their corpora with other researchers.
This practicality consideration may lead to creating a corpus that overrepresents easily obtainable texts.
For instance, if we want to create a corpus of classroom writing and ask students to volunteer and contribute their texts, we may end up with texts from highly motivated students that will not reflect the written production of the class as such.
However, in some specific cases, a corpus can include the whole population.
For the investigation of grammatical structures such as passives, which are generally fairly common, even a small corpus (e.g. one million words or less) can be sufficient.
From this example, we can derive a general rule: unless the corpus represents the whole population, the absence of evidence is not the evidence of absence.
In other words, if an expression does not appear in a corpus, this doesn't mean that this expression is non-existent.
If our corpus represents a sample rather than the population (which is typically the case), we should consider the amount of evidence we have in the sample.
In other words, we can use inferential statistics to enquire whether the observed effects and differences between (sub)corpora can be generalized to the population, i.e. all language that the corpus or corpora represent.
When analysing corpora, we also need to think about the type and format of the data that needs to be obtained from the corpus in order to answer the research question.
Next, I offered to check her hypothesis in the BNC, a corpus that samples both academic writing and fiction.
So the corpus is already fairly old.
When we ask about word types we are asking about how many different word forms there are in the text/corpus.
Let us now think about how the different notions of a 'word' (type, lemma and lexeme) influence the kind of analysis we can carry out in corpus linguistics.
When talking about words in corpus linguistics, we need to specify if we mean tokens (running words), types, lemmas or lexemes.
This means that, on average, there are over 61,000 instances of the definite article for every million tokens in the corpus.
In fact, in all corpora of written English you can expect the definite article to be at the top of the wordlist, with an absolute frequency roughly equivalent to 6% of the overall number of tokens in the corpus.
The relative frequency can therefore be considered as the mean frequency ('mean' is a statistical term for a type of average; see Section 1.3)that is, the mean of the frequencies of the word in hypothetical samples of x tokens from the corpus, where x is the basis for normalization (in this case one million).
For example, imagine a very small corpus that contains 11,000 tokens, including the type homeostasis which appears only once in the corpus.
When a rare word such as homeostasis occurs just once in a small corpus, it is highly unlikely that it would occur at the mathematically equivalent 90 times in a million words.
The actual frequencies of words in a real corpus will differ to a certain extent from those predicted by this model.
The practical implications of Zipf's law for corpus linguistics are as follows: we have to learn to critically evaluate the amount of evidence we have for our claims.
The relative frequency needs to be normalized to the appropriate basis that is similar in size to the corpus or its parts (subcorpora or texts) that we are interested in.
However, to fully describe a word or phrase in a corpus, we need to introduce another concept, namely dispersion.
To illustrate this, imagine a different word (w 1 ) which has the same absolute frequency in the example corpus as word w (i.e. 50), but a very different distribution, namely 46, 1, 1, 0, 1, and 1 in the six corpus parts.
When we calculate the range 2 of w 1 , we'll get the same number as for word w (i.e. 5 out of 6 or 83.3%), although the large majority of all occurrences of w 1 are in only one part (Part 1) whereas word w is, in comparison, more evenly spread out across the corpus.
The difference is that the sum of squared distances is divided not by the total number of corpus parts but by the total number of corpus parts minus 1 (the reason for this is connected with the notion of 'degrees of freedom' explained in Section 6.3).
Standard deviation always needs to be considered in relation to the mean (the relative frequency of the word in the corpus overall).
The coefficient of variation (CV) describes the amount of variation relative to the mean relative frequency of a word or phrase in the corpus.
However, a true conversion to percentage can be achieved by considering the maximum possible variation in a given corpus.
We know that the maximum value of CV depends on the number of corpus parts and can be calculated as ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi no: of corpus parts √Ä 1 p .
The maximum level of variation would be reached if the word occurred only in one part of the corpus.
While CV tells us about the amount of variation in the corpus (larger CV means more variation in the frequencies), Juilland's D tells us about homogeneity of the distribution (larger Juilland's D means a more even distribution and less variation).
The expected proportions are calculated by taking one-by-one the sizes of the corpus parts (number of tokens) and dividing them by the total number of tokens in the corpus; this is to establish their proportional contribution to the overall size of the corpus.
The assumption is that if a word or phrase is evenly distributed in the corpus it should follow the proportional distribution calculated in this step, hence the expected (or baseline) distribution.
The observed proportions are calculated by taking, again one-by-one, the absolute frequencies of the word or phrase of interest in the corpus parts and dividing these by the absolute frequency of the word or phrase in the whole corpus.
This is done to establish how much proportionally each part of the corpus contributes to the overall frequency of the word or phrase.
This is done for all of the distances between two occurrences of the word in the corpus.
Although the equation looks complicated, the idea behind ARF is fairly simple: we notionally divide (this is not done physically but as part of the calculation) the corpus into x parts of the same size (v).
This means that the number of the notional parts, and their length (v in equation (2.20)), depend on the frequency of the word in the corpus.
Then, in addition, to make this procedure robust, we repeat the process for every possible beginning-point in the corpus (think about the corpus as a circle rather than a line) and calculate the mean of all the reduced frequency values that we get via the procedure described above.
Both words occur five times in the corpus, i.e. with absolute frequency of 5.
The first word (w 1 ) occurs only in one text, at corpus positions 1 -5 -10 -15 and 20, while the second word (w 2 ) is evenly distributed at positions 1 -200,000 -400,000 -600,000 and 800,000.
Then we calculate the distances between the individual occurrences of the word (w 1 ) in the corpusto do this we need to use the corpus positions.
Note especially how distance 1 is calculated: it is the distance between the last and the first occurrence of w 1 in the corpus.
This means that because w 2 is evenly distributed throughout the corpus, its frequency should not be reducedwe should continue to consider each instance as a separate occurrence.
The frequency list based on lemmas confirms our initial assumption (based on Zipf's law) about the number of words with the frequency of 30 and over: there are 3,196 such lemmas in the corpus.
Although this lemma occurs in a majority of corpus parts (Range 10) it has a very low Juilland's D (0.2).
This signifies a very uneven distribution in the 15 corpus genres.
Indeed, a closer inspection of this word confirms that flood occurs mainly in one genre-based part of the corpus representing the official documents.
After this initial exploration, to answer the research question about the prominent topics in British public discourse we need to look at the weather terms in the context of other words (lemmas) in the corpus.
It is a well-known fact in corpus linguistics that words occur in combinations that we call collocations.
The disadvantage of this approach is that the top collocates will typically be function words co-occurring with the node merely by the virtue of their frequency anywhere in the corpus.
Some collocation measures such as MI highlight rare exclusivity of the collocational relationship, favouring collocates which occur almost exclusively in the company of the node, even though this may be only once or twice in the entire corpus.
Frequency refers to the number of instances in which a node and collocate occur together in a corpus.
Typically, a reference corpus is larger than or similar in size to the corpus of interest so as to provide a large enough amount of evidence about word frequencies (see question 2 below).
Generally speaking, the larger and the more similar the reference corpus is to the corpus of interest the more reliable and focused the comparison is.
To illustrate the point of the relative heterogeneity of any two corpora, let's compare two excerpts taken from a corpus of American (AmE06) and a corpus of British (BE06) English respectively, each excerpt consisting of exactly 100 words.
We should also consider which words would get highlighted as keywords had we chosen a different reference corpus.
When comparing two corpora, it often happens that a word that occurs frequently in one corpus is absent from the other corpus.
In practice, some corpus tools let us set the minimum cut-off limits for the frequencies of words in C and R before considering them in the keyword procedure.
Let's also assume that AmE06 is our corpus of interest (C) and BE06 is the reference corpus (R).
However, as we have seen, the keyword list is a result of multiple decisions in the process starting with the selection of the reference corpus and finishing with the choice of the particular statistic.
The outcomes of the keyword procedure are influenced by three crucial parameters: (i) the selection of the reference corpus, (ii) implementation of minimum frequency cut-off points and (iii) the choice of the statistical measure.
This, however, needs to be complemented with a careful description of the corpus of interest C in the Data subsection of the Method section.
Such situations also usually lend themselves to automatic methods; in a part-of-speech-tagged corpus, active and passive constructions can be searched for and counted automatically.
Following the coding scheme described above, two independent raters coded a random sample of 100 concordance lines from a total of 1,053 containing the word religion in the corpus.
When extracting keywords for one of the newspapers, the comments of the readers from the other newspaper acted as a reference corpus in order to highlight words specific to the Guardian or Daily Mail readership.
While the keywords in the GU corpus appear more neutral and related to the theoretical aspects of the immigration debate (economic, argument, debate), the keywords found in the DM corpus predominately point to negative aspects of immigration (homeless, benefits, police, squatters).
In corpora, this assumption is usually violated to some extent due to the nature of language, where linguistic features are interconnected, and also due to corpus sampling that is done at the level of texts, not individual linguistic features.
This can be done by taking a random subsample of linguistic features from the corpus.
Second, we should let the readers know how the data was obtained (e.g. a random subsample of all occurrences of the linguistic features from a corpus) and coded as well as whether any part of the data was double coded (if so, inter-rater agreement statistic needs to be reported).
This study is based on BE06, a balanced corpus of contemporary British English, and AmE06, a balanced corpus of contemporary American English.
Each corpus consists of four major written genres (general prose, fiction, newspapers and academic writing).
Before performing the statistical analysis (step 2), we need to identify a large number of linguistic variables in the texts of our corpus.
The corpus needs to include different registers (e.g. informal speech, news reporting, academic writing, popular fiction etc.) and we need to identify relevant variables, that is, those that can distinguish between the registers in our corpus.
The degrees of freedom is a complex concept, which is used when dealing with calculations based on a sample (corpus) rather than a population; in corpus linguistics, this is the case most of the time.
There are two women in the corpus (Dee Dee Myers and Dana Perino) and six men.
It is not possible to use traditional gender-distinguishing words such as lovely (preferred by women in informal British speech), because in the whole corpus (over 6 million words) there are only 20 occurrences of lovely.
Broadly speaking, the most important question that we have to deal with when comparing two corpora is whether the frequencies of the relevant variables are significantly larger or smaller in corpus 1 than in corpus 2.
Obligatory: Obtaining (relative) frequencies from the corpus of the linguistic variable of interest for each of the periods (e.g. years, decades etc.) covered by the analysis.
On a rainy Lancaster afternoon, I start searching the EEBO corpus.
Without a reliable description of the data, generalizing from the sample (corpus) to the population (language as such) using sophisticated statistical tests and p-values lacks grounding.
Most of the methodological thinking (not only in corpus linguistics but other disciplines as well) has been biased towards looking for differences and ignoring similarities.
Among other things, we need to select a suitable corpus, an effective analytical technique and an appropriate interpretation of the results.
The growing demand in corpus linguistics for statistical sophistication and the lack of appropriate resources for beginner and intermediate users of statistics can thus easily lead to frustration.
Every summer, a large number of students from different parts of the world come to Lancaster to learn about corpora and statistics during a week of Lancaster summer schools in corpus linguistics.
Robustness is typically operationalized in terms of text coverage -the extent to which words on the list account for the total running words in a corpus.
A list of 500 words that accounts for 15% of the running words in a target corpus would have more pedagogical value than a similarly purposed list accounting for only 5% of a corpus, as that increased coverage suggests increased impact.
Such evaluation can also demonstrate that a list is indeed specialized if it provides higher coverage of a specialized corpus than of a general corpus.
This assumption requires reflection on what is meant by corpus representativeness.
Notable differences between lists would suggest limitations to their generalizability, and, ultimately, to the representativeness of the corpus upon which they were based.
In answering the question "How big is big enough? " for a corpus to be representative, the lexical diversity of a target domain would seem to play some role.
A corpus of 100-articles would suffice for meeting a hypothetical 85% threshold for a list of 750 words, and that level of reliability could be achieved even for a full 1,000 words with corpora of ‚â• 200-articles.
In some sense at least, this book is an introduction to corpus linguistics.
Predictably, I think the answer is still "yes" and "yes, even a second edition," and the reason is that this introduction is radically different from every other introduction to corpus linguistics out there.
However, this degree of power does come at a cost: In the beginning, it is undoubtedly more difficult to do things with R than with ready-made (free or commercial) concordancing software that has been written specifically for corpus-linguistic applications.
For example, if you want to generate a frequency list of a corpus or a concordance of a word in a corpus with R, you must write a small script or a little bit of code in a programming language, which is the technical way of saying you write lines of text that are instructions to R.
There is a variety of very good reasons for this, some of them related to corpus linguistics, some more general.
In fact, nearly all corpus-linguistic tasks in my own research are done with (somewhat adjusted) scripts or small snippets of code from this book.
In fact, R may even be faster than competing applications: For example, some concordance programs read in the corpus files once before they are processed and then again for performing the actual task -R requires only one pass and may, therefore, outperform some competitors in terms of processing time.
This is a good question, and I myself used Perl for corpus processing before I turned to R.
The main one is probably that, in addition text-processing capabilities, R offers a large number of ready-made functions for the statistical evaluation and graphical representation of data, which allows you to perform just about all corpus-linguistic tasks within only one programming environment.
For instance, ready-made concordance tools often have slightly different settings that specify what 'a word' is, which means you can get different results if you have different programs perform the same search on the same corpus.
Worse, developers might even discontinue the development of a tool altogether -and let us not even consider how sorry the state of the discipline of corpus linguistics would be if a majority of its practitioners was dependent on not even a handful of ready-made corpus tools and websites that allow you to search a corpus online.
For instance, many ready-made corpus tools can only offer the functionality they aim to provide for corpora with particular formats, and then can only provide a small number of kinds of output.
This does not mean, however, that corpus linguists only deal with raw text files -quite the contrary: some corpora are shipped with sophisticated retrieval software that makes it possible to look for precisely defined lexical, syntactic, or other patterns.
These meet the criterion of having been produced in a natural setting because journalists write the article to be published in newspapers and to communicate something to their readers, not because they want to fill a linguist's corpus.
Similarly, if I obtained permission to record all of a particular person's conversations in one week, then hopefully, while the person and his interlocutors usually are aware of their conversations being recorded, I will obtain authentic conversations rather than conversations produced only for the sake of my corpus.
Thus, the notion of corpus is really a rather diverse one.
However, because of their availability and size, many corpus linguists use them as resources, and as long as one bears their limitations in mind in terms of representativity etc., there is little reason not to.
Occasionally, people refer to such collections as error corpora, but we will not use the term corpus for these.
Finally, because of the scarcity of speech errors, usually all speech errors perceived (in a particular amount of time) are included into the corpus, whereas, at least usually and ideally, corpus compilers are more picky and select the material to be included with an eye to the criteria of representativity and balancedness outlined above.
There are a few distinctions you should be familiar with if only to be able to find the right corpus for what you want to investigate.
Examples include translations from EU Parliament debates into the 23 languages of the European Union, or the Canadian Hansard corpus, containing Canadian Parliament debates in English and French.
Again, ideally, a parallel corpus does not just have the translations in different languages, but has the translations sentence-aligned, such that for every sentence in language L 1 , you can automatically retrieve its translation in the languages L 2 to L n .
Static corpora have a fixed size (e.g., the Brown corpus, the LOB corpus, the British National Corpus), whereas dynamic corpora do not since they may be constantly extended with new material (e.g., the Bank of English).
The final distinction I would like to mention at least briefly involves the encoding of the corpus files.
Given especially the predominance of work on English in corpus linguistics, until rather recently many corpora came in the so-called ASCII (American Standard Code for Information Interchange) character encoding, an encoding scheme that encodes 2 7 = 128 characters as numbers and that is largely based on the Western alphabet.
While we will see below that corpus linguistics has a lot to offer to the analyst, it is worth pointing out that, strictly speaking at least, the only thing corpora can provide is information on frequencies.
The assumption underlying basically all corpus-based analyses, however, is that formal differences reflect functional differences: Different frequencies of (co-)occurrences of formal elements are supposed to reflect functional regularities, where functional is understood here in a very broad sense as anything -be it semantic, discourse-pragmatic, etc. -that is intended to perform a particular communicative function.
The most basic corpus-linguistic tool is the frequency list.
You generate a frequency list when you want to know how often something -usually words -occur in a corpus.
Thus, a frequency list of a corpus is usually a two-column table with all words occurring in the corpus in one column and the frequency with which they occur in the corpus in the other column.
In this parlance, a frequency list lists the types in one column and their token frequencies in the other; often you will find the expression type frequency referring to the number of different types attested in a corpus (or in a 'slot' such as a syntactically defined slot in a grammatical construction).
A characteristic of words that is often, but not necessarily, correlated with their frequencies is their dispersion: words that are very similar in terms of their overall frequency in a corpus may be very differently distributed in a corpus.
This method, probably the most widespread corpus-linguistic tool, is the concordance.
It is only fair to mention, however, that (1) error collections have proven extremely useful in spite of what, from a strict corpus linguistic perspective, may be considered shortcomings, and that (2) compilers of corpora of lesser-spoken languages such as typologists investigating languages with few written records suffer from just the same data scarcity problems.
Strictly speaking, a concordance does not have to list every occurrence of a word in a corpus.
For corpus linguists, it is of course important to know that vectors can also contain character strings -the only difference to numbers is that the character strings have to be put either between double or single quotes.
Since this function is central to very many corpus loading operations to be discussed below, we will discuss it and a variety of its arguments in some detail.
This is an important issue since corpus files these days will typically be in Unicode encoding, specifically UTF-8.
I hope you can already foresee that we will use table a lot to generate frequency lists of corpora: once a corpus is stored in R such that every word is a vector element, using table is all it takes.
Your corpus has 4,049 files, so you know you will have one results vector for all files' lengths in words (with 4,049 slots), and another results vector for all files' lengths in sentences (with 4,049 slots) (see Section 5.2.4 for a similar application).
Sometimes, names such as qwe or asd will be used because they are superfast and virtually error-free to type on an American QWERTY keyboard, but in the long run, i.e., for your real research projects, names such as sentences.without.tags or cleaned.corpus.file are infinitely more revealing than the kind of names (e.g., qwe or aa) I have mostly used so far.
However, for corpus linguists it is more important to know that R also offers a variety of sophisticated pattern-matching tools for the processing of character strings.
A little example of something I came across in an undergrad corpus linguistics course was that we generated a concordance of in and found that in one of the SGML-annotated files of the BNC that we looked at, the word in was tagged as VBZ, which means "third person singular form of to be".
There is no alternative to knowing your corpora, this cannot be done more easily, and any concordance programs that come with more refined search options also require you to thoroughly consider the format of the corpus files even if their interface 'hides' such decisions behind clickable buttons with smiling corpus linguists on them, in settings, or in .ini files.
This can be extremely useful, for instance, when you know where in a line of annotation, say, the three-character identifier of a speaker is located (as may be the case in CHAT files from the CHILDES database) or when you want to access the line numbers in some version of the Brown corpus, which are always the first eight characters of each line, etc. (see, for example, Sections 5.4.5 and 5.4.9).
The third component lists the percentage of corpus parts containing at least one match, which here amounts to 100 percent.
The fourth component was the main reason to write this function: It returns for each match the corpus element in which it was found but also separates the match from its preceding and subsequent contexts with a tabstop (so that, if you print the content of exact. matches.2(...) [[4]] into a. txt or. csv file, you get a nice three-column output with the context preceding a match in a first column, the match in a second, and the context following a match in a third.
If you set the argument lines.around to a number greater than zero, then you increase the preceding and subsequent context by that number of corpus elements.
If you set the argument characters.around to a number greater than zero, then the preceding and subsequent contexts will be as many characters (as opposed to corpus elements/lines).
Imagine, for example, your data contain dates in the American-English format, with the month preceding the day (i.e., Christmas would be written as "12/25/2016") and you want to reverse the order of the month and the day so that your data could be merged with corpus files that already use this ordering.
This may not seem like a great nuisance to you but, before you go on reading, look at the last word and think about in what way this may be problematic for further corpus-linguistic application.
Just as most corpus-linguistic work has been done on English, this chapter has so far also been rather Anglo/ASCII-centric.
The bad news for Windows users is that, with R for Windows at least, it is often not as straightforward to handle Unicode as it is with R for MacOS X or R for Linux, which is mainly due to the fact that Mac OS X and Linux are UTF-8 operating systems so they play well with input files that have the same encoding, which is one of many reasons (see Section 3.8.2 for another one) why I recommend doing corpus processing with R on a Linux machine (and it takes very little time to install a long-term support version of, for instance, Kubuntu or Linux Mint on a desktop computer).
This is of course useful if your corpus files come in a particular directory structure.
In the long run, linguistic theorising without a firm grip on statistical methods will lead corpus linguistics into a dead-end street.
As to the former assumption, the subjects and objects were randomly drawn from a corpus so there is no relation between the data points.
The same is actually true of corpus frequencies.
Here's a little made-up example of a corpus with three files: the files make up 50 percent, 30 percent, and 20 percent of the corpus, and of all occurrences of the word in question, 70 percent, 20 percent and 10 percent are in the corresponding files.
Then, DP is computed like this: Simplifying a bit, DP ranges from 0 (a word is perfectly evenly distributed in the corpus, i.e., in accordance with the sizes of the corpus files) to 1 (a word is completely unevenly distributed in the corpus).
We need to define a vector words that contains the words for which we want to compute DP values and then also a vector search.expressions that changes words into regular expressions we can search for in the corpus files (using the annotation of the corpus).
Recall from Section 3.6.3, this is a script in which we know the dimensions of the output in advance: If we have three words and 4,049 corpus files, we know, for instance, that the vector corresponding to sizes.of.files.in.words above will need to have 4,049 slots, and we know that the list that collects the three words' frequencies will need three components each with 4,049 slots.
Obviously, we need to be able to define the corpus files we want to search, which means it will be useful to use rchoose.dir to define the directory containing the corpus files; also, we will need to be able to retrieve all the file names from that directory using dir.
The right panel is a similar plot but it bins the words in the corpus (here into ten equally large parts), and again we can see that there are a lot of occurrences of "Perl" in the last 10 percent slice of the corpus.
For the right panel, we will define a number of corpus parts we want (here ten) so that the script can easily be changed to accommodate different divisions of the corpus into parts.
For the former, our table contains the frequencies of "perl" in each corpus part (in the column for TRUEs), which we can divide by the overall frequency of "perl" in the file to get percentages (to be stored in a vector called obs.percs), and we can use the function rowSums to compute the corpus part sizes in percentage in a vector exp.percs (which should all be really close to 10 percent, given how we split the corpus up into ten parts above), from which we can compute DP.
Finally, we use the function barplot to plot the observed percentages of "perl" in the corpus parts and customize the plot.
Obviously, we need to be able to define the corpus files we want to search, which means we use rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory.
As usual, we use rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory.
Thus, each of these vectors needs to have as many empty elements as there are files in corpus.files.
Obviously, we need rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory.
During each iteration, we will use grep to find all corpus sentences.
In this section, we will discuss a script that computes different measures of keyness for words occurring in a target corpus by comparing their frequencies in that target corpus to that of a reference corpus and quantifying the relative attraction of each word to the target corpus by means of a keyness statistic.
Finally, before exiting this conditional expression, we delete the multi-word units from the corpus sentences so that their constituent words are not counted again.
In the first loop, we identify all cases of must + V so that, at the end of it, we know all verb types ever occurring after must, and then we can look for all occurrences of all of them in the whole corpus within the second loop.
Also, we know how many different verbs occur after must (440) and what those are, so after creating a collector vector for the cells a + c, we do a second loop over the corpus files where now we determine the frequencies of these verbs after modals in general, not just after must) by looking into this loop's current.mpis.
We use rchoose.dir and dir to define the corpus files and dir.create to create an output directory.
However, this section adds an additional challenge: The corpus from which we want to retrieve sequences of two adjectives is not tagged.
After the loop, we'll pick the most frequent n adjectives (something like n = 2,000 for the learner data case study and n = 5,000 for the Brown corpus case study) occurring in it and tag all occurrences of these forms in the untagged corpus files, and then we will retrieve sequences of two adjective tags and whatever they tag from these corpus files; with the ICLE corpus, we will actually save the tagged corpus files before we search them, with the Brown corpus example, we'll tag the files and immediately search them while they are still in memory.
Let us begin by discussing how we compute type-token ratios and vocabulary-growth curves using a small vector tokens as a 'corpus', something that I always recommend to get started on a new project: Create a data set realistic enough in its make-up but small enough to be seen on one screen, and start developing your code with that.
But we want to add one little twist to the discussion: A vocabulary-growth curve is dependent -to some degree at least -on the exact order of the words in the corpus.
But if you compute vocabularygrowth values on a corpus consisting of many files, then the order of the files is typically arbitrary and makes the vocabulary-growth curve you plot a bit dependent on the usually unmotivated order of files that the corpus comes in.
Thus an interesting approach to deal with this is to plot not just one vocabulary-growth curve for the corpus one is interested in but, say, 100 vocabulary-growth curves, one for each of 100 versions of the corpus in which the words have been randomly reshuffled, which is what we will add here.
We use a for-loop to scan and tolower the corpus files, and grep to get the sentences.
In the second part, we use character and numeric to create empty collector vectors to merge the hyphenated forms and their frequencies from all over the corpus, a for-loop to load each frequency list file, and, if there are hyphenated forms in the file, we merge them with subsetting, incrementing the vector counter on each iteration (as in Section 5.2.8).
It involves the notion of lexical frequency profiles, which is a corpus-based approach in applied linguistics concerned with the amount and the kind of vocabulary second/foreign-language learners use in their speaking and writing (see Laufer & Nation 1995 for an introduction; Meara 2005 for a critique; and Laufer 2005 for a response).
The first chi-squared test tests the first scenario, the second chi-squared test tests the second, because it determines whether the frequencies in the Brown corpus differ from those expected from LOB, but not also vice versa.
In this assignment and the next, we turn to a completely different corpus format.
Most widespread corpus-linguistic software applications require that all information concerning, say, one particular sentence is on one line.
After that we use rchoose.files to define Eve's corpus files and the function vector (with and without mode="list") to define collector structures.
We then use two nested for-loops to compute wilcox.tests on every combination of elements of lus, and store their negative log 10 p-values in a large 20 √ó 20 matrix called tests (20 because we have 20 corpus files).
In very basic cases, the language of the corpus is rearranged so that a reader is presented with an altered and focused view.
Two examples will be given here, both using the Bank of English corpus (HarperCollins Publishers and the University of Birmingham).
The first was to generate new dimensions from a corpus of articles from a single academic journal (Global Environmental Change).
The second was to avoid a prior division of our corpus into registers.
Linguists normally look askance at 'bag of words' studies, but I do think they offer interesting new ways of carrying out 'corpus-driven' research.
One of our aims in that project was to find bottom-up and novel ways to explore a corpus whose contents were diverse but not in known ways.
What the lists give us, first of all, is a sense of the 'aboutness' of the corpus.
It throws open the notion of 'aboutness' and uses a data-driven way of organising the content of a large corpus.
One of the key questions for Corpus Linguistics is how a corpus might satisfactorily be 'analysed'.
In general, the investigation is top-down, in the sense that a question is asked of the corpus and means devised to find the answer to the question.
The implementation of a corpus in the field of linguistics has grown rapidly over the decades.
The essence of this review is on the modern corpus and corpus-based research conducted by researchers affiliated with different common-section institutions all over the world.
When a researcher taking part in corpus-based research is recognized, his/her names are employed as key terms to deal for research offspring in basic scholastic data.
In a certain study titled "Corpus Linguistics and its Applications in Higher Education" by Fuster M√°rquez & Clavel Arroitia (2010), they are set out to depict implied essentials of corpus linguistics and its progress in relevance to theoretical linguistics and its implementations in modern teaching pursuits.
Finally, they resume problems in connection with the application of corpus linguistics in the classroom since knowledge of the restrictions of corpus linguistics is necessary for its coming prosperity.
Finally, two studies concentrated on improving systems of learning to reinforce learning depending on regarding corpus.
In addition, Lukin et al., (2017) worked toward showing a new corpus, PersonaBank, composed of 108 specific stories from weblogs that have been commented with their story intention schemas, a profound picture of the fabula of a story.
It is also investigated how the corpus can be employed in implementations that tell the story differently using various styles of telling, co-telling, or like a content planner.
In this book titled Corpus Linguistics, Context, and Culture by Wiegand & Mahlberg (2019) Corpus Linguistics, Context and Culture explain the possibility of corpus linguistic methods for discussing language manners across a domain of contexts.
Two studies have made the use of corpus linguistic research to reinforce the capacity and efficiency of discourse analysis.
The variety of topics, the copra volume employed and languages studied to provide us a powerful tone that corpus and corpus-based research is a lifelike domain of research.
Answering either of these questions involves answering the other, as the notion of a 'corpus' in contemporary linguistics is inextricably tied up with the ideas and techniques of corpus linguistics.
The term corpus (plural corpora) is simply the normal word in Latin for 'body', as in a body of text.
Describing corpus linguistics as a 'field' or subdiscipline of linguistics is a statement which requires qualification, however.
For this reason, many corpus linguists prefer to describe it as a 'methodology'.
But for others in the field, the insights into language that arise from the use of the corpus are so radically different from the traditional understanding of how language works that they consider corpus linguistics to constitute an independent field of study or theory of language.
Advances in information technology put computer power sufficient for the analysis of larger and larger bodies of data within the reach of any researcher who cares to use a corpus.
As a result, in the twenty-first century, corpus linguistics has become -in the words of Tony McEnery -a 'killer method' in linguistics, applied to a hugely diverse array of types of linguistic research.
This is, in short, why corpus linguistics matters.
A very large set of questions in corpus linguistics can be answered without going beyond standard, widely available corpora.
A good example of such a corpus is the British National Corpus (BNC).
But there are other types of research question for which no standard corpus is available.
In this case, the first step for the researcher is to build the corpus on which the analysis will be based.
For instance, many corpus-based analyses of media discourse are based on examining media texts that address some particular topic.
If I am interested in how local newspapers in Britain discussed the issue of crime in the years 2010 to 2013, I will almost certainly need to build a corpus myself for this specific purpose.
The virtue of working with a DIY corpus is that the design of the data can be tailored directly to the specific research question at hand.
The virtue of working with a standard published corpus is that the corpus serves as a common basis between different researchers -allowing results to be tested and replicated by other scholars, for instance.
Both approaches have their place in different kinds of corpus-based study.
However, corpus researchers need an understanding of the issues that arise in corpus construction even if they never build any corpora of their own.
This is because we have to understand the contents, design and structure of the corpora that we use, in order to select a corpus appropriate to what we want to research, in order to make appropriate use of that corpus, and in order to treat our results critically in terms of how far they may be extrapolated beyond the specific corpus at hand.
The key concepts here are the notions of the corpus as a sample and of balance and representativeness.
Any corpus is fundamentally a sample.
The very fact that the corpus is a sample means that we need to think carefully about how our sample relates to the population, which means we need a detailed and critical awareness of the process of sampling that has been used in corpus construction.
For example, it would be possible to compile a complete corpus of Old English, because only a limited number of documents in Old English have survived.
An important difference between sampling in corpus linguistics and in other sciences is that a corpus is never a random sample of the population.
It is possible to select the texts of a corpus randomly from a population of texts of interest.
Most of the analyses we will want to undertake with our corpus will be centred on a much smaller linguistic unit, such as the sentence, the clause or, perhaps most commonly, the word.
However, even if the texts of the corpus have been selected randomly, the sentences and words are not random.
This means that the probability of a given word occurring at some point in a text is influenced by the words that have occurred before and thus the words in a corpus are not random.
Second, when we analyse a corpus we must always be aware of dispersionlooking out for the difference, for instance, between a word that occurs 100 times in our corpus where all 100 instances are bunched close together in one text, and a word with the same 100 occurrences that are spread out across the whole corpus.
Third, in the process of corpus construction we have to think about the relationship between the population and the sample in a somewhat different way to how this is done in fields where sampling can be done on a random basis.
We must consider the issue of how representative the corpus is of that language or variety, based on the decisions that were made about what to include and what to exclude in that corpus.
It relates to the relative proportions of different types of data within a corpus.
A corpus is considered balanced if its subsections are correctly sized relative to one another.
For instance, in the case of speech versus writing, we might propose a corpus that is 50 per cent spoken and 50 per cent written data (this is, in fact, the design of the International Corpus of English, ICE).
As with representativeness, the issues around balance are fraught with difficultly, but the critical thing for the user of any corpus is to be actively aware of how that corpus is balanced, to think carefully about what this means for the generalisability of results based on that corpus.
The plain-text words of the corpus are sometimes called the raw data of the corpus.
However, raw data is not necessarily all that the corpus contains.
There are three types of additional information that can be added to a corpus, namely metadata, markup and annotation.
Annotation, by contrast, encodes into the corpus the results of some linguistic analysis which we have undertaken on the corpus text.
The third thing which can be added to the raw text of a corpus is metadata.
There are several ways to store metadata for a corpus, but one popular way is to insert a chunk of descriptive information at the beginning of each corpus text in a fixed, specified format: this metadata chunk is called the header of the text.
First, it allows us to trace the corpus evidence we see back to its source.
If we are examining a series of examples extracted by searching a corpus, for instance, it is often very useful to be able to see, via the metadata, precisely what text each example comes from: knowing, for example, whether a particular instance comes from a novel or from a newspaper report can be a valuable aid to interpretation.
The second function is that metadata allows us to isolate and compare different sections of a corpus.
If, for instance, we have a corpus containing both writing and speech, we may on occasion wish to search just within the spoken texts, or to compare the results of some analysis in the written data versus the spoken data.
Markup has similar applications in that it allows analyses to take account of the structure of the corpus texts.
For instance, in a spoken corpus, who said what is often critical to an analysis: only the markup of utterance boundaries and speaker identities allows us to know this.
But the same can apply to written data -we might be interested in differences in how a word is used paragraph-initially versus medially or finally, for instance, and this can only be automated if the corpus has paragraph breaks marked up.
To annotate a corpus is to insert codes into the running text to represent a linguistic analysis.
Much research has been done to automate the process of adding the most common forms of analysis, so that it is not necessary for a human being to read through the whole corpus and manually insert the tags.
The underlying principle of semantic tagging is the same as POS tagging, namely, the assignment to every word in a text or corpus of labels representing categories within a scheme of analysis.
Nevertheless, semantic tags can be extremely useful, especially for detecting patterns in a corpus that affect groups of semantically-related words which are individually very rare.
Thus, computer searches of a corpus can be based on grouping together all the separate inflectional forms of a single word.
When a corpus has been tagged, the fundamental techniques of corpus analysisfrequency counts and concordance searches, which will be introduced at length in the next section -can be undertaken at levels of analysis higher than the orthographic word-form.
Part of the reason for this is that, as a practical matter, both can be inserted into the corpus using the same encoding system of tags inserted into the running text.
The current standard method for adding tags to a corpus is by means of XML (the eXtensible Markup Language).
The two most basic forms of data which we can extract from a corpus are the concordance and the frequency list.
Thus, what possibilities we have for the analysis of a corpus is inevitably a function of the affordances of the available software tools.
Some programs (e.g. WordSmith, AntConc) run from the researcher's desktop computer; this means that the user can analyse any corpus they have available locally using these programs.
Other tools operate as client-server systems over the internet: the user accesses a remote computer via a client program, typically a web browser, and uses search software that actually runs on a remote server machine where some set of corpus resources and analysis systems has been made available.
Client-server tools currently in wide use include SketchEngine, Wmatrix and CQPweb; some of these allow users to upload their own data to the corpus server, while others restrict users to a static set of available corpora.
In discussing the corpus frequency of words, it is useful to introduce the distinction between word types and word tokens.
A token is a single instance of any word at a particular place in the corpus; a type is an individual word-form, which can occur many times in the corpus: thus one type is usually instantiated by multiple tokens.
So for instance, if we see on a frequency list for an English corpus that the is the most frequent word, occurring 60,000 times, then we could say that the single type the is represented in this corpus by 60,000 tokens. Frequency lists can also be compiled for any annotation that is present in the corpus, and we can talk about types and tokens of annotation tags in the same way.
Any given type will be more frequent in a bigger corpus; for that reason, corpus frequencies are often given in relative terms, as frequencies per thousand words or per million words.
Relative frequencies of types are calculated by taking the raw frequency, dividing it by the number of tokens in the corpus overall, and multiplying by the normalisation factor (one thousand, one million or whatever).
Speaking meaningfully about corpus frequencies is not straightforward.
Simply stating that the occurs forty times per thousand words in a corpus tells us nothing of linguistic interest, whether about the word the or about the corpus in question.
We would find, for instance, that in a written English corpus the will probably be the most frequent word, but in conversational English the personal pronouns I and you are likely to top the list.
To be interpreted meaningfully, then, it must therefore nearly always be combined with some form of qualitative analysis -that is, an analysis that involves the linguist interacting with the actual discourse within the corpus and its structure and/or meaning.
As we saw above, a concordance is the result of a corpus search: all the examples in a corpus matching some specified search pattern, together with some preceding and following context.
However, if tags are present, they can be used instead of or as well as a word pattern -for instance, to search for can as a noun versus can as a verb, or to search for all adverbs in the corpus.
Some corpus software allows the use of the extremely sophisticated system of wildcards called regular expressions.
It is especially important not to look at just the beginning of a concordance, as these early concordance lines may present examples from the early part of the corpus, which is not necessarily representative of the corpus as a whole.
A key items analysis is performed as follows: first, frequency lists are created for two texts or corpora -either two corpora which we wish to contrast, or a single text of interest and a generic reference corpus.
In fact, for many analysts, a primary virtue of a key items list is that it provides a way in to identifying which words or categories in a corpus will be of interest for more detailed study.
The idea that collocation is of high or indeed primary importance to understanding the meaning of a word or other linguistic form originates with J.R. Firth and is very important in the neo-Firthian school of corpus linguistics; but collocation analysis itself is a commonplace of most or all approaches to corpus linguistics.
One is to look at sequences of words which recur in a fixed order with high frequency in the corpus -these sequences are variously referred to as n-grams, clusters or lexical bundles.
It should be noted that this is fundamentally rather similar to the procedure of a key items analysis, except that we are comparing the vicinity of the node to the rest of the corpus, rather than comparing two different corpora, and some of the same considerations apply.
Unfortunately, there is simply no space here to give a full account of all the areas of linguistics, and of other fields of study, where the methods of corpus linguistics have been productively applied.
This is exactly what the methodology of corpus linguistics offers to researchers in disciplines beyond linguistics.
For several decades now, corpus linguistics has been among the fastest-growing methodological disciplines in linguistics.
By that I do not only mean that corpus linguists need to use more different statistical tests (while that is generally true, the choice of a particular test is of course mostly dictated by the particular research question), but also that there needs to be a growing awareness that some choices that corpus linguists traditionally make may be pro blematic and would benefit from a different perspective.
In addition, I shall briefly comment on the underutilized notion of dispersion, that is, a measure that quantifies how evenly distributed elements are in a corpus, and thus also relates to the notion of corpus homogeneity.
If one computes the MI of in spite of in the untagged Brown corpus by comparing the observed frequency of in spite of of 54 against an expected frequency based on complete independence, MI becomes an extremely high value of 12.25.
Thus, corpus linguistics needs to explore more adequate and conservative ways to extend AMs to n-grams.
However, associations in general and associative learning are certainly not (always) symmetric, which is why, ideally, corpus linguistics would explore the use of directional AMs.
In sum, there are compelling arguments to include type frequencies from theoretical considerations as well as from neighboring disciplines such as psycholinguistics or computational linguistics, and there are promising first results within corpus linguistics proper, but more exploration is definitely required.
Thus, there are multiple levels of corpus organization at which effects may be located, but these levels are typically not all tested.
Second and more interestingly, the above two models were fitted with user-defined orthogonal contrasts-something else that happens way too rarely in corpus linguistics-to see easily (i) whether the speakers of an unknown sex are different from those where the sex is known, and (ii) whether female and male speakers behave differently.
It is therefore imperative that corpus linguists follow the lead of recent developments in psycholinguistics and make mixed-effects/multi-level modeling a central analytical tool: without it, we will never know how much of an effect is interesting, and how much is just due to particular speakers sampled in a corpus.
However, at this point in time, quantitative corpus linguistics is becoming more and more established also in specific linguistic subdisciplines, which raise their own, more specialized problems.
Thus, VNC can contribute to the (methodologically already quite sophisticated) domain of quantitative dialectology by helping to identify structures in corpus-linguistically described regions of a country or other larger regions that can then be interpreted against the background of other empirical or theoretical work.
Given the increasing availability of historical corpora and regionally-stratified corpora, this method may therefore be a useful addition to the corpus-linguistic toolkit.
However, this is no time to rest on our laurels-now that corpus linguistics has become mainstream, and that's a good thing, we too must continue to refine our methods just as other fields have to.
Many areas in psycholinguistics and computational linguistics have made interesting discoveries, have developed useful tools, have adopted great methods from neighboring fields, but corpus linguistics is unfortunately not leading the pack and must take care not to lose momentum either in terms of its own evolution or in terms of how it helps to shape linguistics as a whole.
The present paper is an attempt to provide a snapshot of current problems, both in corpus linguistics in general and in selected hot topic areas, as well as to provide ideas and (first) suggestions about how to cope with these problems; I hope it will succeed as a call to (methodological) arms, and thus trigger developments that will help our field advance once more.
Secondly, the corpus should be created by considering a specific idea.
This means that the researcher should have a clear mind about what to do with the corpus being collected.
In addition to these aspects, the corpus needs to be in electronic form since it will be almost impossible to cope with such vast data without the help of technology.
Finally, the corpus should serve as a linguistic study.
As mentioned above, without narrowing down the scope of a corpus, it is impossible to create a corpus representing everything related to all areas of language and texts appearing in the language.
No matter the purpose of a corpus collection, all corpora need to provide metadata to ensure that other scholars can also benefit from the corpus.
Otherwise, the corpus itself, without the metadata, will be useless since the researcher will not be able to know the background information of the corpus.
Descriptive metadata provides classificatory information derived from internal or external properties of the corpus components.
Administrative metadata provides documentary information about the corpus itself, such as its title, availability, revision status, etc.
Due to these levels, it was claimed that corpus linguists give more importance to descriptive adequacy while generative grammarians focus on explanatory adequacy.
Longman Grammar of Spoken and Written English (1999) by Biber et al. can be taken as an example to illustrate how the corpus is used.
When the aim is to trace the historical change of a language, a corpus divided into periods should be used.
Also, the effects of demographic information on the use of specific linguistic constructions could be explored with the help of a corpus designed as a historical or diachronic corpus.
To use corpora for contrastive analysis and translation theory, researchers need to use a parallel corpus, meaning that the same text has been translated into various languages.
CHILDES (Child Language Data Exchange System) can be a good corpus for researchers focusing on acquisition.
Learner corpus is used to find out the strengths and weaknesses of the language learners.
In addition to the genre-based specification, a researcher may restrict the corpus to a time frame, a social setting, or a given topic.
A general corpus comprises texts represented by various types, including written or spoken language.
Compared to a specialized corpus, a general corpus is usually much larger.
Since it can be used to produce reference materials, it is sometimes called a reference corpus.
The Corpus of American Contemporary English is an example of a general corpus.
A historical or diachronic corpus is a collection of texts from different periods.
A monitor corpus is a corpus allowing researchers to track current changes synchronically in a particular language.
If a corpus is divided into word samples representing different types (or genres) of the language, it can be labeled as a 'balanced' or 'representative' corpus.
The researcher needs to be flexible in terms of restructuring the corpus creation process.
If a written text is planned to be used in a corpus, researchers should take the required permissions from the author of the text, which will prevent copyright issues that might be faced in the future.
Even though the authors allow researchers to use the text in their corpus, some of the owners of the writing texts may not allow researchers to share the corpus with other researchers worldwide.
If you want to conduct research with this corpus, you have to go to the University of Oslo to reach the corpus.
Due to this fact, it is suggested that using electronic texts in a corpus may facilitate the researcher's duty.
They should keep the speech transcriptions and the written data in ASCII (text) format as it is the standard format used in corpus linguistics.
Stanford Parser, for example, is widely used for the syntactic level of analyses of the corpus.
Once the corpus is created and annotated, the most crucial part will be using the corpus for analysis.
After deciding on the research question(s), the researcher should test the corpus to determine whether it can answer particular research questions.
If the corpus is not suitable for the research questions, the corpus should be changed or retagged by the researcher.
After testing the appropriateness of the corpus for the research, the researcher needs to find suitable software tools to conduct the study, code the results and then analyze them through appropriate statistical tests.
The more specific concern of corpus linguistics is to account for these decisions by systematically investigating related variants and conditions on their choice.
We will turn to the role of corpus linguistics in sociolinguistics in Chapter 9.
We give examples of corpus linguistic research in Chapter 4, showing that the corpus linguistic approach is possible for many levels of linguistic analysis and diverse languages.
We hope this textbook provides a good first foundation for corpus linguistics and generates your excitement about this large and diverse field.
In linguistics, a corpus is a collection of texts that serves as the empirical basis for the study of natural languages.
Moreover, the texts in a corpus must be machine-readable so that they can be collated and investigated with the help of computers.
Corpora are not defined in terms of specific properties of texts, so texts can be of any type and may exhibit a large range of diverse properties (cf. Chapter 3 on corpus composition).
In corpus linguistics, a text is a record of any stretch of language as produced on a specific occasion at some point in time, at some place, and as received or receivable at some place and time.
This means that spoken and signed texts are not immediately available for inclusion in a corpus, but are transcribed, that is, what is being said or signed is written down according to specific conventions, for example, the conventions of the International Phonetic Association (IPA), which are in turn based on specific writing systems.
In corpus linguistics a fundamental unit of reference is the wordform.
We will explain in Chapter 5 how you can design smart corpus queries that will do searches of alternate forms like this.
Go back to the COCA and the Brown corpus.
For instance, the COCA treats all clitics as separate wordforms whereas the Brown corpus (and other corpora developed in that tradition) treat clitics plus their host as a wordform.
This difference does not necessarily reflect the analytic view of corpus compilers, but can often be due to technical conditions.
It is therefore important that we always ensure we understand how the text of a corpus is internally structured, and to critically relate these to our linguistic analyses and categories.
Type frequencies are then lower than token frequencies, and the type/token ratio can reveal interesting insights into properties of texts, essentially telling us how lexically variable a text or corpus is.
Different corpora follow different procedures here; for example, the Brown corpus follows the space-delimiter as we have done, counting aren't etc. as single tokens whereas the COCA treats such forms as two tokens (which means that in COCA you have to search for a two-word expression are n't in order to find instances of aren't).
Identifying tokens correctly and consistently in a corpus is another aspect of pre-processing and annotation of corpora.
In corpus linguistics the procedure identifying and marking the boundaries of word tokens is called tokenisation.
Let's say we have a small corpus of Matukar Panau 4 that has been parsed and glossed.
Although Matukar Panau is an agglutinating language, we see that by token frequency, most words are monomorphemic or bimorphemic in an 8,961-word annotated corpus.
An important notion in corpus linguistics is that of context.
Of particular prominence in corpus linguistics is the contexts of words, and related to this the corpus linguistic concepts of collocations.
We can search for a specific lexeme in a corpus and determine its collocates, that is, a list of lexemes that co-occurs with it.
It should be noted that we should generally be sceptical of an all too clearcut conception of linguistic levels, and it is corpus-linguistic research that has advanced our understanding of interactions between, for instance, syntax, morphology and phonology in the area of clitics (some examples of which we observed above) and affixation.
Hint: the number bigram tokens in a corpus/string will always be one less than the number of lexeme tokens.
Using COCA and the Brown corpus, find collocates for duckling (only in COCA) and farmer.
Include part-of-speech (PoS) filters to find the most common adjective and determiner w-1 and the most common verb and noun w+1 in each corpus.
The central premise of corpus linguistics is that all of the language-internal andexternal contextual features are relevant to the way that people use language.
Corpus linguists deal with a small fraction of all texts that have been produced in a language, so any corpus is always only a small sample drawn from the real-world population of texts (cf. 8.2.1).
Metadata is what distinguishes a corpus from a random collection of texts by giving it explicit structure: minimally any user of a corpus will know what amount of text data is contained in it, what characteristics texts have, for example, whether they are written for a newspaper or spoken during a conversation with friends or colleagues, who produced the text, and so on.
A key concern in corpus linguistics is to explain the variable use of wordforms and other structures in dependence on both types of contextual factors.
The basic concepts explained in this chapter are explained in virtually any textbook on corpus linguistics.
The first question we can ask about the content of a corpus is how much text it contains: its size.
What size a given corpus has will depend to a major extent on the kinds of texts included and the resources required to compile these into structured collections.
The current corpus (at time of writing) contains texts from 22,388,141 web pages from 94,391 websites.
The smallest corpus in the list is CORE.
The corpus contains 48,569 texts -which are equivalent to web pages herecomprising 52,933,543 wordform tokens.
Corpus size is often directly dependent on limitations on resources and other more practical considerations, a point we will return to in Chapter 6 on corpus-building.
When discussing sizes of spoken-language corpora within documentary linguistics, the time length of primary audio and/or video data is often cited (see Thieberger 2006:7 on the corpus of Nafsan (formally South Efate)).
This is obviously not immediately comparable to corpus sizes given in word tokens since time length does not relate directly to text size: texts may differ in the amount and length of pausing and the speech rate of speakers, so that texts of similar duration in time may exhibit very different numbers of wordform tokens.
The reason why composition is guided by these situational rather than linguistic characteristics is that the latter cannot be known before a corpus has been compiled and investigated: Corpus composition and corpus types you can select for texts based on the situational fact that they involve more than one speaker, but you cannot select texts with a particular proportion of first-and secondperson pronouns.
One means to represent different situationally defined text types evenly in a given corpus is to aim for balance.
In this view, a corpus consisting entirely of traditional narratives from a specific indigenous 'orature' is as much a corpus as a super-varied one covering a wide range of situational characteristics, but they will be amenable to different research projects.
In our view, none of the parameters of authenticity, routineness, and spontaneity constitutes a demarcation criterion for corpus text and/or corpus linguistics.
A corpus can also contain elicited texts, including even lists of elicited sentences, as long as all contextual information is preserved.
The latter will allow corpus analysts to evaluate attestations of particular structures as being elicited, with such and such context, etc., and this may have specific implications for their linguistic analysis as well.
Excluding such data from corpus linguistics will in a sense also deprive us of the possibility to thoroughly evaluate structures vis-√†-vis certain production conditions, for example, the production of elicited texts not based on established routines.
A corpus can be representative of all the possible linguistic features of a language (covering all possible structures that are part of language user's competence), or it can be representative of all the external or situational variables of different texts that are produced in a given language.
We, therefore, take a melioristic approach where we outline ways in which corpora can fare better or worse in regards to representativeness, focusing on the previously mentioned corpus parameters.
We have seen above that the creators of the Brown corpus (and other subsequent corpora following the same design principles) sampled written English texts according to their intuitions about the proportions of different text types.
We do not share views here (occasionally uttered in corpus linguistics literature) that findings from any given corpus merely apply to this corpus and nothing beyond.
Whether a corpus is adequate in terms of its representative depends on the research question(s) at hand.
While there is no clear-cut limit, we can state that a corpus is more representative if it achieves higher saturation.
One can test this by taking a corpus, establishing a catalogue of all structures from certain domains (sounds, words, sentence structures, etc.) attested, and then seeing to what extent the addition of any further text will expand the inventory of these structures.
If the degree of expansion is low, then the original corpus was already nearly saturated and hence reasonably representative.
In addition to the corpus text data, modern corpora are closely interlinked with two further types of data, namely raw data and metadata.
Metadata are data about the corpus files and the structure of the corpus as a whole and the compilation process (including design decisions), as well as data about the situational characteristics of texts.
In corpora, it encompasses properties of all data types, as well as data about the corpus as a whole and its creation process.
Imagine a corpus that is overall fairly representative in terms of size and composition, but lacking metadata.
Metadata is also relevant to what kind of research we can do with a given corpus.
This differentiation of corpus types goes hand in hand with specific research goals as we will see.
One sub-type is a reference corpus: a general corpus taken to be a standard reference for corpus-based research on a given language.
An example is DeReKo -Deutsches Referenzkorpus 'German Reference Corpus' , whose purpose is to make available a large corpus of German amenable to a large variety of research questions.
Several special corpora contain only specific types of text, which could also be included in a general corpus.
The same applies to artificial corpora of experimentally elicited texts: even where participants produce texts narrating the exact same content under the same experimental conditions, as with the Pear Film experiment, it is vital for the corpus to cover speakers with different demographic features, as the corpora are meant to represent the behavioural reaction to the stimulus characteristic of the language community as a whole.
Dynamic corpora grow over time as more and more texts are included (monitor corpora are a type of dynamic corpus).
The Contemporary Historical Corpus of American English (COHA) is an example of a diachronic corpus.
A crucial characteristic of a diachronic corpus is that the texts it contains are comparable across periods.
While this is a fairly trivial reflection of groups of languages with little specific relevance for corpus linguistics, a grouping that is more important for our purposes is that between wellstudied and lesser-studied languages.
This is relevant because it comes with various preconditions, and leads to different goals for corpus linguistic work.
Lesser-studied languages typically lack these things, and here corpus linguistics often means to build corpora in the first instance, and these corpora will then often be relatively small, which then comes with various restrictions as to their usability in research.
Corpora with so-called interlinear morphemic glossing which captures the meaning and/or grammatical functions of morpheme tokens in a corpus are sometimes labelled interlinearised corpora.
Therefore, the metadata of a corpus will include a list of correspondences between the chosen phonetic alphabet and the IPA.
In practice, more specialised annotation is restricted to special corpora created for specific research purposes, but some larger corpora fall into these particular categories; for instance, COCA is a tagged corpus.
Understanding the choice between possible variants is one of the main foci of corpus linguistics, especially variationist corpus linguistics.
In the rest of Chapter 4, we will describe example studies from all of these levels as well as corpus studies of sign and gesture.
Some kinds of research questions are easy to explore with a basic corpus.
Even if a corpus is basic, simply a string of wordforms with no other annotation, it is easy to search for individual words since usually words are a kind of string of characters, separated by white space.
There are many corpus studies that compare lexemes, especially near-synonyms and purported synonyms.
It should be immediately striking that there are many different behavioural profiles but also that there are not 851 distinct profiles, meaning there is consistency across the corpus in terms of not only the syntactic uses of run, but also the words it co-occurs with.
Many corpus studies take a similar approach in looking at words or domains in the lexicon and comparing uses.
The decision of the classification for each of these tokens from the corpora does require some subjective decision-making from the researcher, as is the case in many corpus studies.
These kinds of corpora take a lot of work to produce, much more so if (portions of) the corpus have been manually inspected, checked for accuracy, and hand-corrected.
An existing Quechuan corpus (cf. Bakker and Hekking 2012) of 80,000 words was used to identify 54 wordforms with -ero~-era, 47 of which were complex loan words with this suffix.
As with many of the levels of usage we have described here, certain annotations help corpus linguists look for the particular kinds of phenomena relevant to their studies of discourse.
This study is a good example of how interesting and important information can quickly and easily be gleaned from even a small corpus.
The corpus approach also helped see multiple real interactions (at least from the side of the health professionals) to assess the patterns in this institutional setting.
The authors ran analyses on just under 20,000 FTOs from the Switchboard corpus and then ranked the importance of the variables in relation to each other.
Overall the authors were able to take advantage of a well-annotated corpus and apply a fitting quantitative analysis to show that factors from both processing and conversational norms interact and have an effect on conversational interactions.
This is both the exciting and frustrating part of corpus linguistics.
Frequency lies at the heart of corpus linguistics.
The primary aim of corpus linguistics is to understand linguistic patterns and explore how and why they occur.
To obtain frequencies or examples, one must engage with the corpus or 'query' it.
Most corpus linguists will not read through an entire corpus.
Specialised corpus software and web interfaces also have means of finding examples and often have ways to save those examples to a new document or spreadsheet for further analysis.
It is worthwhile to explore various tools whether you are new to corpus linguistics or are a seasoned veteran.
Additionally, it has corpus search functions that can be used over one or more files to find example strings (words, phrases, parts of words, etc.) in context.
In many early corpus linguistics works, you will find frequency tables as a primary account of data.
In corpus linguistics, we worry about both type frequency and token frequency (cf. 2.2.3) that can tell us different things about our corpora.
Token frequency -a count of all instances of something in a corpus.
So, in a corpus of a million words, there will be a million word tokens.
This is the case for any corpus.
The idea is that the less meaningful words are excluded, giving someone more insight into the corpus.
Usually, these kinds of keywords are lexical items (nouns, adjectives, verbs) that give us an idea of the topics in the corpus.
However, the keywords could be, say, pronouns if one corpus is conversational and another is from monologic or written sources.
This is a visualisation of where a word or collocate occurs in a corpus.
Two words with the same frequency might occur often only in a handful of texts, or more consistently across the entire corpus.
For instance, we might see the end most often at the end of a corpus of children's stories and rarely at the beginning or in the middle of the texts in that corpus.
An early trailblazer of corpus linguistics is George Kingsley Zipf.
There are many more and some strains of corpus linguistic research favour different measurements, either due to historical development of the sub-field or due to specific research goals.
It is usually normalised by the corpus frequency (note that in scientific notation, e is used for very large or very small numbers.
Some corpus programs will highlight collocates of a word of interest with different colours depending on their parts of speech.
In a large corpus, you will see many bigrams that occur more than once.
Write three 5-gram sequences in English that you think may have a chance of being repeated more than once in a corpus.
Hand-tagging an entire corpus, even a small one, is a monumental effort.
Therefore, rules are often used to describe to a computer when to label a word with a particular category or another and then a tagger is run over the corpus.
Usually a tagger is trained on a smaller hand-annotated sample and then applied (tested) on a larger corpus.
Tags are often somewhere 'in the background' , so if you are using an interface to query your corpus, you may not see the tags, although they will constrain your results.
If you are using any tagged corpus, it is good to look through a portion of the actual tagged data before you start your searches so that you can adapt your search to what is really available in the corpus, not just what you expect or hope to be available.
For many languages there are no tagged corpora available, so to find grammatical phenomena, corpus linguists may have to rely on string searchers.
In corpus linguistics, it is also often used to measure the strength of association between phonemes or between a word and a construction it can occur in.
Often, especially at more junior stages of your career, you will not be in a position to build an entirely new, large corpus of a language, especially not if that language has a long research tradition and hence a large academic community.
We will present the issues of this chapter from a practical perspective, assuming that we are the compilers of a corpus.
Such corpora may draw on pre-existing texts, including those contained in a larger, general corpus, or include specifically collected texts, for example, texts elicited during controlled experiments.
The latter term is to be understood here in a broad and non-technical sense, meaning simply that in order for a range of texts to form a corpus they need to be compiled in some form and accessible in some way.
A very first step for a corpus builder is to identify texts that are relevant for the envisaged corpus and should be considered for selection or collection.
These two sets of factors actually create a tension between the ideal representative corpus and a deviation thereof.
In a final step, we need to make the corpus accessible to the scientific community in order to fulfil the scientific imperative of accountability and to enable further scientific developments based thereupon.
A further feature desirable from a scientific point of view may be modifiability or manipulability, that is, we may want to offer the opportunity to add further texts to the corpus as appropriate in given research contexts or to modify the corpus composition in other ways (e.g. by removing some texts, etc.).
These modes differ from written texts in that the raw data is not readily amenable to inclusion in our corpus.
In particular they need to be annotated, at least transcribed, and it is the transcription that will eventually resemble our corpus text.
Furthermore, the corpus text should be linked to the recording media of the primary spoken/signed text in a time-aligned format.
Since mainstream corpus linguistics focuses on English and other Western European languages, the Latinbased script used in these languages is most common, and relevant corpus software and query mechanisms are based on it.
For languages with other scripts, for example, Cyrillic scripts in many languages of Eastern Europe and Central Asia or various scripts of East Asian languages (Mandarin, Japanese, etc.) corpus builders will either need to use encoding such as Unicode (cf. 5.11) or add a layer of transliteration to the corpus text.
Finally, the shaping of any specific corpus-building project will ultimately depend on its purposes.
According to our design criteria and our generalised corpus-building scheme, we select and collect texts carefully following considerations of representativeness.
Although the texts are accessible, there are copyright restrictions in both cases, which limits the availability of published texts for corpus-building enterprises severely.
Often private texts will, therefore, never make it into a corpus.
Yet, their inclusion in the corpus does serve the community' s interest.
Specific scientific requirements -corpus linguists may need to manipulate, randomise, or control for different situations, often particularly relevant in experimental or comparative research designs.
As pointed out above, spoken and signed texts need to be transcribed before they can be included in a corpus.
And where the corpus text is in a language that is not known to the scientific community it needs to be translated to a more widely known language.
This written form is what we treat as the corpus text since this is what can be searched and further annotated.
Like the SBC, a corpus created with ELAN will have two components, namely, a set of media files with the raw audio and/or audio-visual data and a corresponding set of ELAN document files (a special kind of XML file).
Second, subsequent corpus users can much more easily re-evaluate the accuracy of annotations and potentially modify them or add further annotation detail, as required for any research agenda.
If the corpus-building effort is for a language that has been documented by different people or even by the same person over several years, there may be multiple orthographies that will require standardisation.
The general rule of course is that this should be a language that all potential corpus users understand.
For many LD corpora, it may be advisable to create multiple versions in different languages so that the corpus be accessible to people of relevant regions.
Once we have decided what to include in our corpus, we have to actually put together the different texts in such a way that we can analyse all data in essentially the same way concerning any particular research question.
Moreover, you should consider differences in format: if data remain in different formats that cannot be combined for a search, then multiple searches will have to be used for corpus queries.
Also possible is compiling the corpus in multiple formats.
Alongside corpus text data we have to integrate metadata.
Each text that is part of the corpus will also have metadata such as the date and place it was recorded, who spoke or signed, who else was present, and what media files are associated with the text.
Metadata can be integrated into a corpus in various ways.
If you are writing your own computer scripts to build your corpus, you can integrate (or merge in) your metadata, bringing in all the metadata for each participant and all the metadata for each file for every token in the corpus.
Because of this, corpus compilers will often use special characters in a special way so that metadata is regarded as such.
There are always reasons why it may be difficult to get data published, and of course, this step also requires some resources if the corpus is to be presented in a well-structured and well-designed way.
The main point really is that interested users have access to the data and can understand how it can be used, which in turn requires metadata on the corpus as a whole.
As mentioned above, Multi-CAST is available in various formats, and all data can be downloaded from the corpus website.
A corpus website should also have all relevant information on the corpus and its various texts or in this case subcorpora.
Part of this information about the corpus is a citation so that it can be referenced, as we do in this book.
First of all, they should be linked to the corpus text data and their metadata so that corpus users can evaluate how annotators arrived at their annotation decisions; at the same time the annotations are ideally extractable in order to be analysed outside the corpus infrastructure itself, for example, by programmes for statistical analysis.
Moreover, this enables users of the corpus to plan the construction of their queries targeting these annotations.
Our main purpose here is to explain the basic implementation of corpus annotations and how they add value to a corpus by enhancing its amenability to a wider range of research questions.
Section 7.3 is devoted to comparative corpus annotations in the context of corpus-based typology.
In principle this can be done by using the symbols of the IPA to render the corpus text.
In corpus linguistic practice, however, annotators have often resorted to alternative renditions of IPA conventions.
It is applied to a corpus text which has already been annotated and translated, as required.
The annotations are called 'tags' because they are appended to corpus words, as shown in example (7.5) from the Brown corpus.
Let's consider relevant instances of our case example like in the Brown corpus, given in (7.6): Tagging of corpora is done with a clearly defined and confined inventory of tags (a controlled vocabulary) that is called a tagset.
The Brown corpus comes with its own tagset comprising 87 different tags.
The latter relates to production and has been investigated with corpus-linguistic methods.
For some of the annotations discussed thus far, corpus linguists have developed dedicated comparative perspectives.
The GRAID annotations alone enable a number of corpus queries on the annotations as presented here, and as they appear in the ELAN files.
This can be very simple, like counting all the words in a corpus or a specific word within a corpus, but can also become very complicated.
This chapter will focus on a few kinds of statistical analyses that are often used in corpus linguistics and will try to get you in a position to turn a research question into something specific and testable.
This chapter will explain some of the most frequently used quantitative techniques in corpus linguistics today, such as logistic mixed-effects regression, as well as focus on some newer techniques becoming more popular, such as classification trees (and random forests) which are types of recursive partitioning.
There, we concentrate on our strength: variationist corpus linguistics.
There are many means to evaluate corpus-level variation (such as Kullback-Leibler divergence) in order to compare corpora with each other.
In corpus linguistics, sampling is rarely truly random (a random person from a population, or a random text from all existing texts), and is more often a convenience sample (who we can record, what we can find on the web, which newspapers we have access to, etc.) (cf. Chapters 6 and 10).
Interval variables are uncommon in corpus linguistics.
Zero is meaningful for ratio variables (zero token counts of parsimonious in a corpus signal an absence of that word in the corpus).
This can mean that they disproportionately affect analyses, giving us something we cannot trust, or it may mean that they were spurious data points in the first place (Did a student in fact not sit for the test, or did he completely misunderstand the assignment? Did something go wrong with the corpus-building process and result in something funky that needs to be thrown out?).
From a parsed and glossed sub-corpus 2 of 8,961 words, verbs and nouns make up 5,375 word tokens.
It is helpful to know the relationship between variables in your corpus generally.
Therefore, much of the statistics we see for corpus linguistic data are multivariate rather than univariate as in Section 8.3.
We will also discuss a newer method that is becoming popular in corpus linguistics and linguistics more widely, called recursive partitioning, which includes analyses like classification trees and random forests.
A corpus is useful to observe the variation of these properties in constellation with each other and see how each of these independent variables tend to affect our dependent variable.
When you look at journal articles reporting on corpus linguistic studies where researchers are using multiple regression, you will see many ways to describe it.
Also common in corpus linguistics is generalized linear regression.
Logistic regression models are easier to fit and easier to interpret than multinomial regression, and are what you will see most commonly in multivariate quantitative corpus linguistics.
Sociolinguistics is a subfield of linguistics where corpus studies are a possible methodology, alongside surveys and experiments and more.
Other kinds of corpus linguistics are interested in variation between many kinds of register and genre, and often in the differences between spoken and written modes.
A corpus approach can be used then, in conjunction with a sociolinguistic question, to describe the distribution of the variants across such external contextual features.
As an activity, compare multiple English corpora, such as the COCA or COHA, or look at multiple genres within one corpus.
These are generally captured in corpus linguistics as part of the external contextual features.
We have seen how essentially corpus-linguistic investigations can be levelled to address sociolinguistic questions.
This explains in part why documentarians' work differs from that of classical descriptive and typological linguists in its primary focus on data collection rather than analysis and comparison, and creating a corpus is part of a documentation project.
Video recordings also allow us to document sign languages; see Chapter 4.3 for some details on their corpus-based investigation.
Language documentation shares with corpus linguistics the basic goal of representativeness.
We discussed in Chapter 3.1 how the size and composition of a corpus reflect in various degrees its representativeness and saturation.
Here, a corpus, especially a small one, can be specific to one genre or even one person.
Researchers must simply be upfront about what makes up the corpus and be aware that not all corpora are appropriate for grand generalisations about a language.
In Good for a genre-specific corpus, and a starting point for some research questions.
These are the result of linguistic analysis and as such not appropriate for a corpus, but are good to be included in the apparatus once texts are collected.
Yet, the building of an LD-based corpus faces particular challenges through the typically severer limitations of resources and the fact that potential academic users of the corpus have typically no prior knowledge.
Making an LD corpus accessible to a broader scientific community is a key consideration.
In Chapters 6 and 7 we have discussed various layers of annotation and how they add value to a corpus.
In this way the corpus text is searchable and users can grasp the meaning of specific passages.
All that may change in the course of documentation and analysis, and hence, a growing understanding of the data at hand are the annotations that make up the core of the corpus.
For a corpus then, it is perfectly normal to have multiple versions reflecting different stages of understanding, as well as for different purposes.
In a small corpus, particularly of an under-researched language, function words have an advantage because they are so frequent, and it may be interesting to determine the full range of their contexts.
As we have described in previous chapters, one of the main focuses of corpus linguistic research is variation.
Below is a list of the nine most frequent words from the 2020 Matukar Panau corpus (150,740 words).
In this corpus, there are 14,021 different words (types).
In a list of the ten most frequent words of a large English corpus, all of the words will be function words.
As with function words, grammatical marking can be a fruitful area of study even in a small corpus because it tends to be obligatory and frequent, although this obviously depends on the language of study.
A related question is the variable realisation of objects as either pronoun or zero, and in 8.4 we have presented such a corpus-based study for Vera' a in some detail.
Name several advantages and disadvantages of using a small, genre-specific corpus, and list possible research questions that could be answered with a small genre-specific corpus.
While grammatical and functional information can be found often even in a small corpus, it is not the only thing linguists want to study.
If that information is recorded in a machine searchable way, like through FLEX or other programs, then that material can become an annotated corpus.
If the annotations are not changed throughout the corpus, that can cause issues later on.
A researcher should then balance the efforts and rewards of making changes throughout an entire corpus; keeping some glosses underspecified, as described above, is a relevant technique here.
Constructions, however, can be difficult to search for unless they have a particular feature or string that can be searched for, or unless additional coding or annotation has already gone into the corpus, for example, in the form of GRAID annotations mentioned in 10.3.1 that enable searches for different types of clause constructions including the distinction between intransitive, (mono-) transitive, and ditransitive clause constructions.
A small corpus, unless designed to contain particular data, will be unhelpful in investigating the behaviour of particular infrequent words or collocations.
Any rara (rare phenomena) are unlikely to be found in a small corpus, or if found, will be infrequent.
Additionally, many measures of frequency and predictability that require bigram/ n-gram information will be skewed heavily by the data available in a small corpus.
Because the corpus is large, this does not necessarily have a large impact on overall patterns.
However, with a small corpus, there is probably a lot less of this "junk" to throw out.
A small spoken or signed corpus, therefore, can still be a good representation of how people use their language.
When working with corpora based on language documentations, corpus linguists need to work with what they have, and this may often require flexibility.
This means that we cannot introduce you here to a set of fully fledged corpus studies of typological distributions in the way that we did, for example, in Chapter 9.
Although some of these regularities had been suggested a long time ago, corpus linguistic approaches are capable of discovering regularities that have not been dealt with in classic typological research.
Needless to say, corpus linguistic studies of this sort cannot explain why languages opt for one or the other type of subject and object encoding, and the explanation for this lies in deep and intricate histories of language evolution with many stages of development, each a story worth telling in itself.
Original text corpora are thus an option where the phenomenon of interest can be considered ubiquitous enough to be represented sufficiently in the corpus (cf. Seifart In Press).
These considerations are likely to explain why the Sakapultek corpus is among the very few corpora that show the postulated discourse-ergative pattern.
Hence, corpus-based typologists need to carefully evaluate the specific properties of the corpora they use vis-√†-vis the phenomena they are researching. .
Corpus linguistics takes real samples that are used to generalise about language, and corpus-based typology helps us to confirm (or confront the idea) that these assumptions are applicable for a more diverse sample.
Given the recent overarching knowledge-building practices and the methodological roles of corpus linguistics, it is necessary to review how a certain body of knowledge has been created according to the common denominator of corpus linguistics.
In the current study, we analyzed co-cited documents published in corpus linguistics during the past twenty years.
A corpus is a body of systematically gathered texts or transcribed speech to represent a particular function of a language that can serve as the basis for linguistic analysis and description.
As has been observed, corpus linguistics is a fairly new and rapidly growing discipline.
Given the considerable interest in utilizing the corpus linguistic approach, in addition to the dynamic and interdisciplinary nature of current studies involving partnerships among disciplines, a comprehensive and systematic overview of the development of and relationships among individual research in the fields of corpus linguistics is called for.
Thus, the pressing academic quest is to review past achievements as well as future directions of corpus linguistics.
Two sources of important citation data, most-cited journal titles and the individual publications, were analyzed to understand citation patterns in corpus linguistics.
Every journal title has its own aims and scope of publication; therefore, the analysis of the cited journal titles identified trends in how corpus linguistics research has been communicated within the research community.
English for Specific Purposes and TESOL Quarterly, founded in 1980 and 1967, respectively, appeared after the turn of the century and have been consistently cited by the corpus linguistics papers.
Journal of English for Academic Purposes, along with English for Specific Purposes and Cognitive Linguistics, is another example of a specialized journal actively being cited by corpus linguistics papers.
During the earliest period of the current research scope, between 1997 and 2001, the research topics related to new perspectives on grammar were the central issue in corpus linguistics.
Between 2002 and 2006, although researchers still cited corpus-based grammar references for their studies (e.g., Cambridge Grammar of the English Language), one group of researchers made use of newly developed datasets, both large or small, such as The CHILDES Corpus, Wordnet, and A New Academic Word List.
Another group of researchers, possibly novices in the discipline, cited general references to corpus linguistics, such as Introduction to Corpus Linguistics, Corpora in Applied Linguistics, and Foundations of Statistical Natural Language.
An alternative explanation for the emergence of the introductory references of corpus linguistics would be because the period was the optimal time for establishing corpus linguistics as a part of linguistics after a "hodgepodge" multi-directional development of corpus linguistics.
Research papers and books have been published to introduce corpus linguistics and define how to use it or build a small corpus for pedagogic purposes.
In the late 2000s and early 2010s, studies about exploiting corpus were conducted concerning referencingcompiled corpus for language learning, especially in academic writing.
For instance, students' attitudes or reactions were examined regarding consultation with corpus while they were writing.
Considering the relationship among most sited publications and the salient academic research themes, it seems that the corpus linguistics has become a linchpin of certain academic disciplines.
COCA is not a published article; rather, it is a web-based corpus complied of a 520 million-word database from newspapers, magazines, fiction, and other academic text documents.
The present study explored the research trends in corpus linguistics over the past 20 years.
Furthermore, journals that first ranked on the list after the early or late 2000s and remained dominant in corpus linguistics until now were English for Specific Purposes, TESOL Quarterly, International Journal of Corpus Linguistics, and Cognitive Linguistics.
Corpus linguists' most-cited publications which served as the foundations of corpus linguistics were constantly referenced.
Meanwhile, publications by Mike Scott, Ken Hyland, and John Swales were first found in the middle time spans; their publications on corpus tools and discourse analysis in ESP or EAP were frequently cited.
The pattern of cited publications also suggests that corpus linguistics has been specialized and branched out.
In the years before 2000 and the early 2000s, corpus linguistics tended to be studied by using enormous empirical datasets.
During that time, corpora enabled researchers to conduct studies on grammar using corpus-based parsers, such as the Penn Treebank and its parts of speech tags.
Most recently (i.e., since the late 2000s), corpus tools were more commonly used by various groups, including not only researchers, but also language teachers and students, who finally had direct access to corpus.
Another recent trend in the corpus linguistics research, according to the current study, was the emergence of large web-based corpus (e.g., COCA).
It will do so in a way that should not only provide you with the technical skills for such an analysis for your own research purposes, but also raise your awareness of how corpus evidence can be used in order to develop a better understanding of the forms and functions of language.
The book is aimed at a variety of readers, ranging mainly from linguistics students at senior undergraduate, Masters, or even PhD levels who are still unfamiliar with corpus linguistics, to language teachers or textbook developers who want to create or employ more real-life teaching materials. As many of the techniques we'll be dealing with here also allow us to investigate issues of style in both literary and non-literary text, and much of the data we'll initially use actually consists of fictional works because these are easier to obtain and often don't cause any copyright as that of morphology, or even phonology.
I've chosen extracts from these three particular texts and period for a number of reasons: a) their authors all died more than 70 years ago so the texts are in the public domain; in other words, there are no copyright issues, even when quoting longer passages; b) they are included in corpus compilations; and c) they not only illustrate register/genre differences but also how the conventions for these may change over time, as can be seen, for example, in the spelling of to-day in the final extract.
In the same vein, it's also important to understand that once we actually have extracted some relevant data from a corpus, this is rarely ever the 'final product'.
Furthermore, as real corpus linguistics is not just about getting some 'impressive' numbers but should in fact allow you to gain real insights into different aspects of language, you should always try to relate your results to what you know from established theories and other methods used in linguistics, or even other related disciplines, such as for example sociology, psychology, etc., as far as they may be relevant to answering your research questions.
This is also why the solutions to, and discussions of, the exercises may often represent those parts of the book that cover some of the more theoretical aspects of corpus linguistics, aspects that you'll hopefully be able to master once you've acquired the more practical tools of the trade.
As this textbook is more practical in nature than other textbooks on corpus linguistics, at the end of almost all chapters, I've also added a section entitled 'Sources and Further Reading'.
Complications may also arise if the character set is not directly supported by the computer the corpus is viewed on.
This may for example happen when the corpus is in a language that uses a different alphabet from the standard Western European ones that are supported on all computers by default.
Another way to store a corpus is to save it into a database.
We'll experience the advantages of this when we set up/work with accounts for access to some web-based corpus interfaces, such as BNCweb or COCA.
When creating a spoken corpus, one also needs to think about whether an orthographic representation of the text will be sufficient, whether the corpus should be represented in phonetic transcription, or whether it should support annotation on various different levels (see Chapter 11).
An example of a domain-specific literary corpus would be the collected works of an author, which can be used to investigate the style of this particular author, or even to verify disputes about the authorship of a piece of literature where this may be contentious.
Essentially, the fact that the BNC is a mega corpus can easily be seen in the sheer number of words it contains.
One additional highly important attribute of the corpus is the fact that it contains a very large amount of meta-information, i.e. information about who produced which type of document(s) and in which contexts.
As we've seen before, the very first electronic corpus, the Brown Corpus of written American English, contained 500 (written) text samples of 2,000+ words, and its size and composition set a standard for the compilation of new corpora for many years.
For a long time after the publication of the corpus in 1964, many people thought that 1 million words of text represented a general-enough sample to provide sufficient information about the makeup of the English language.
This may depend on a variety of factors, such as availability, the potential for obtaining permission for copyrighted data, how many people are actually working on creating the corpus, etc.
On the other hand, maybe keeping the amount of spoken data in the BNC relatively low was actually not too bad an idea, since transcribing spoken language is an expensive and time-consuming business, and one where corpus compilers often take too many 'shortcuts'.
In order to do so, in the past corpus compilers have frequently resorted to using samples of approximately 2,000 words from different texts and genres to achieve balance, but both that number and 'cutting out' part of a text appear fairly arbitrary, and may in fact make such samples somewhat unrepresentative of the texts they're supposed to represent as a whole.
In other countries around the world, the situation may either be handled in a more relaxed or, in contrast, even harsher way, so it's always advisable to enquire about the exact copyright situation of the country in question, especially if you later want to make your corpus available to other researchers around the world.
Some countries also recognise the concept of fair use, which allows relatively insubstantial parts of copyrighted materials to be used for purposes such as research, education, review, etc. (Wikipedia: Fair Use), although, in practice, this will probably not allow you to include sufficient amounts of text or other materials in your corpus.
Now that you already have a pretty good idea of what types of features you might encounter in a text and how this relates to aspects of its organisation, as well as issues that may affect its analysis, let's move on to finding and processing some data in electronic form as a first step in laying the foundation for creating your own corpus and analysing it.
Exercise 6 has hopefully already alerted you to the fact that it isn't easily possible to just use any document that we can read in some way on the computer equally well as a source for our corpus-linguistic analysis, simply because it contains text.
Header elements may for instance be the page title (contained in the <title>‚Ä¶</title> tag, where the forward slash indicates the end of the tag in the closing part), which then appears in the title bar of your browser, or possibly meta-information (<meta>‚Ä¶</meta>), such as the text (content) type or encoding/character set (charset), as well as style (sheet) information/references (<style>‚Ä¶</style> or <link rel="stylesheet" href="./corpus.css" type="text/css" />, in our case) responsible for some of the page formatting.
However, what you've just seen on the exercise page is not only an issue in corpus linguistics, but actually represents a much more prevalent problem on the internet.
Use your browser's find functionality to look for the Uppsala Student English corpus (USE) under the 'Corpora' tab, then click on the id (2457) on the left.
Look at the information presented on the page, especially about availability, and then download the .zip archive of the corpus from the link provided there.
Type in 'corpus linguistics filetype:doc', and hit 'Enter'.
As the alternative representations listed in the previous sentence show, there may be multiple ways of representing the same thing, and you should not only find a consistent way of representing these features, but also document their meaning, so that other potential users of your corpus will be able to understand exactly what they represent.
On the other hand, it's usually important to make some personal information, such as the informant's age, sex, provenance, level of education, etc., available to users of the corpus, in order to allow them to conduct research of a more sociolinguistic nature.
However, ideally, describing the editing process shouldn't be the only thing you do in this respect; you may also want to retain a certain amount of meta-information about the compilation of your corpus, especially if your plan is to share the data with other people.
As I stated before, because many of the steps you may need to take in order to produce your corpus may frequently involve making changes to the original data, it's advisable to document the steps you've taken in your preparation as much as possible, to allow both yourself and any other potential users of your corpus to understand the exact nature of the data.
This corpus manual will usually be in PDF format, and from here you can always refer to any additional files for reference if necessary.
This is one of the 'beauties' (albeit also one of the pitfalls) of doing corpus linguistic analysis because it allows us to identify language features that we may never have expected to find, thus providing inspiration for further and deeper research into the regularities and irregularities of language (structure), which generally go hand-in-hand.
Once you're finished, you can just load the resulting file in AntConc instead of the original corpus, and filter out or sort the relevant entries in another concordance.
Both of these mistakes basically may cause you to lose valuable time that could be spent on actually analysing your data, thus 'throwing away' one of the most important advantages of using corpus linguistics as a methodology, which is that it allows you to save a significant amount of time finding a large amount of potentially relevant and interesting data quickly.
However, if you persist, I can promise you that soon you'll not only be able to understand the value of regexes for achieving even highly complex tasks in linguistic analysis very efficiently, but will also learn to hone your analysis skills related to morphology and morpho-syntax, something you'll definitely be able to profit from in your work in corpus linguistics, as these two areas often form the basis for further linguistic analyses.
Of course, depending on your corpus, you may also find some rather unexpected words that have nothing whatsoever to do with the verb want; for instance, because my test corpus for trying out regexes contains more 'archaic' language, I also found the adjective wanton, as well as some other constructions, this way.
Luckily, these are problems that have already been overcome to some extent by the advent of web-based interfaces to these mega corpora, which, even if they may not allow us to do everything we might want to do with such a corpus, already provide many facilities for investigating the data in relatively complex ways that will probably satisfy the needs of most researchers.
As the results are now in random order, if we do want to know which particular category of the corpus (or 'genre') the individual result was found in, we need to check the category details.
To ensure that you don't get biased results, based on the order of the texts in the corpus, first go to the 'User settings' from the main query page and change the 'Default display order of concordances' to 'random order' and click on .
This will return you to the basic single-corpus COCA interface.
You may now be tempted to investigate further why you can find these American spelling variants in the corpus, and of course the easiest option for this is to click on the link for 'colour' in the frequency breakdown to get to the concordance for the results, and then checking the meta-information for each result.
In order to develop this understanding thoroughly, as so often in corpus linguistics we need to look at this task from at least two different angles, a theoretical and a practical one.
Now, while of course it's generally not possible for us to directly change the design of any corpus tools we may be using to allow us to deal with this issue, we at least ought to bear this 'handicap' in mind in many of our analyses, and see whether at least some of the tools allow us to avoid any of these problems, or whether we may be able to find a way to work around certain issues by manipulating our data ourselves in simple ways.
Now that you know about some of the issues that may affect our counting of words in a text/corpus, we can start looking at frequency lists properly.
The most useful output format in many areas of corpus linguistics for such a list is generally to have a list that is first sorted according to the frequency in descending order, that is, with the most common words occurring first, and then 'alphabetically' if the same number of tokens occurs for more than one type.
Yet, in order to ascertain the 'spokenness' of a text/corpus until such time, we may often want to handle contractions as single units.
Almost all texts, apart from maybe certain text types including telegrams and recipes, tend to have a rather high occurrence of high frequency function words, something we've just seen during our first explorations of frequency lists. Since these words don't actually tell us much about the lexical richness or the content of a text/corpus, anyway, they're often regarded as redundant and thus lists that exclude them, at least in theory, ought to help us develop better insights into the nature of any text or corpus under investigation.
Attempts at creating such pre-fabricated word lists for EAP from corpus materials have already been made in the past.
To do the latter on a subcorpus you've created, you can simply select your corpus from the BNCweb start page from the dropdown list next to where it reads 'Restrictions'.
However, unless you need to prune the reference list extensively, it can certainly allow you to identify some key terms much more quickly, and may therefore be seen as an alternative way of looking at single-word lists for identifying genredependent or semantic features of a corpus.
In addition, the ability to highlight negative keywords in AntConc may also allow you to investigate under-use of specific vocabulary relatively easily, for instance when comparing learner data with that produced by native speakers, etc., an option that, obviously, a pure frequency list of only the source corpus is unable to provide.
Next, place the cursor in the first empty cell in the rel_g column, 3 across from the word about, type =, then click in the cell below freq_g that contains the frequency for this word, type /, and click on the number that contains the total for the tokens in the general corpus (Excel), or type n_general (Calc), then press the enter key.
Repeat this step for the frequency in the newspaper corpus, ensuring that the formula bar reads =D2/n_newspapers.
Click in the relative frequency cell for about in the general corpus.
When you release the mouse button, the spreadsheet application will automatically have calculated and filled in all the relative frequencies for the general corpus.
Repeat the same process for the newspaper corpus.
This is why we should usually ideally also report the raw frequency and the corpus sizes along with any normed counts, which will then enable fellow researchers to judge our results fully.
On top of that, being a monitor corpus, it also makes it possible to track potential changes across different periods in the same way.
Again, clicking on the item and investigating the concordance lines will soon tell us that s isn't only used to mark a particular speaker, but of course also represents the clitic (contraction) forms of is (as in that's) and us (as in let's), although there are no possessive markers in the corpus.
And even if this number represents only 0.01% of all the words in the written parts of the BNC, the number of potential errors, which appear mainly due to tokenisation errors, is staggering, particularly when considering that this affects only one of the parts of speech represented in the corpus.
For the latter, you could opt for only investigating the speech of men or women, or speakers from the same or different social classes, or create subcorpora for both, and then compare the language in some way, which makes it possible to carry out corpus-based sociolinguistic analyses on the data, as do the options for restricting speakers to particular education levels, etc.
Provided that you don't forget to change the option for creating a new corpus, there should be no issues in completing this exercise and creating a suitable frequency list.
When you look at the results, you should also be able to notice that, similar to the results we had for the Trains corpus, most of the high-frequency function words get sorted to the top, as well as first and second person pronouns, and fillers like erm, etc.
The idea behind seeing such words types as key is of course based on the notion that non-shared items are always key for a particular corpus, which may not necessarily be the case, even though they do help us narrow down the options for identifying true keywords without the use of statistics.
The same applies to sorting the data by frequency if we wanted to compare raw frequencies for whatever reason, or simply identify non-occurring types in either corpus.
As I said before, there are now a number of options for sorting according to different fields, for instance comparing the ranks in one corpus against another or, perhaps more importantly, seeing whether certain words dominate to some extent in one corpus in comparison.
If you sort our data in descending order, all the types that only occur in the first corpus will automatically appear at the top, due to the division by 0 error I referred to in the instructions.
This will then be followed by all instances where the relative frequency is higher in the first (general) corpus, and you can easily identify these 'dominant' words due to the fact that they'll have a ratio above 1.
Looking at the words that exclusively occur in the general corpus, we can see some interesting types, namely concerning and regarding, that have been classified as prepositions despite the fact that they don't look like typical prepositions because they are in fact ing-forms, that is, they clearly still retain some verbal character.
The most striking difference in type frequency for types occurring in both subcorpora is that for upon, which occurs more than 10 times as frequently in the general corpus (ratio 10.604).
This would seem to be corroborated by the fact that among is somewhat underrepresented in the general corpus (ratio 0.712), which, however, exclusively has the alternative, and more formal form, amongst instead, as well as the relatively high level of occurrences of within (ratio 2.727) as an alternative to the less formal in.
Although it's of course somewhat easier to confine our corpus-based investigation to single lexical items, single words and their frequencies aren't the only interesting things we may want to analyse with the help of corpora.
These choices we have when using language can really only be investigated through finding ways of expressing this flexibility on the paradigmatic and syntagmatic axes in our corpus searches.
Unfortunately, though, the BYU interface won't allow us to search for these, throwing the following error message "All of the "slots" in your multi-word search string occur more than 10,000,000 times in the corpus (e.g. [n * ] [be], or [j * ] [nn * ]).
The 'n' here theoretically means 'any number of', but usually n-grams are restricted to relatively short sequences, such as bi-or tri-grams, because finding n-grams can be very expensive computationally as long lists have to be kept in memory, and their size grows with increasing length of a text/corpus, and each increment of n.
Thus, if you're working on a relatively sizable corpus, be prepared to wait for a few minutes for n-gram calculations to finish.
This will sort the results based on how many of the documents in the corpus the n-gram occurs in, that is, the dispersion, in descending order.
So far, the composition of our small 'corpus' has been fairly heterogeneous, which has had a clear effect on our results in making it difficult to identify any interesting recurring 'themes'.
However, if we change the composition of the corpus to make it more homogeneous, this ought to change very quickly.
The more limited the topic contained in the corpus, the easier it'll become to identify the latter.
Unfortunately, though, there's no facility for creating n-gram lists, probably because these could potentially get very large, working with such a big corpus.
However, if you switch the display from random to corpus order, you'll notice that, apparently, not all u-units are in fact retrieved because the KWIC display actually starts with the second unit.
Furthermore, instead of revealing interesting combinations of content words, you'll often find more grammatical constructions or combinations of function + content words, especially if the corpus is not very homogeneous, as in our case.
Other things that will crop up frequently are bits of information related to the plays that form part of this 'corpus', such as references to the author, to acts and scenes within the plays.
In terms of the composition of the corpus and its relation to the individual clusters, you'll hopefully notice very quickly that, with collocates occurring on the right, our results contain a relatively high number of proper names.
However, this very heterogeneity could skew the results of our general collocational statistics rather strongly, especially in such a small corpus.
In BNCweb, even being able to do this is only possible because the whole corpus is marked up for s-units.
At the level of collocations, the very fact that punctuation occurs with a relatively high frequency in any orthographically transcribed corpus like the BNC almost guarantees that it'll be treated as collocating with genuine word types, something that simply doesn't make sense because the semantics and pragmatics of punctuation are very different from, and completely incomparable to, those of ordinary words.
At the same time, the high frequency of punctuation tokens will affect the calculations of relative and normed reported frequencies throughout the whole corpus, which will again have an effect on the calculations for collocations, too.
The first one basically gave you a means of designing your own ways of classifying your data in a sensible manner, which then enables you to use the corpus-based analysis methods we've discussed throughout the course in order to extract and count relevant information.
And one always ought to remember that, even though one might never know how a corpus may potentially be used by others, not all types of annotation need to be included in all corpora, simply because this may be possible or relatively easily achievable.
At the same time, whenever appropriate, I've also tried to point out other potential applications for such data, for example, in the development of teaching materials/textbooks, grammars, or direct application in the classroom, but of course such a list will always be incomplete as there are too many applications of corpus linguistics to be listed exhaustively.
In the next major section (Chapters 5-10), we then investigated various techniques for analysing language data using established methods of corpus linguistics.
Throughout this section, we encountered various issues with tools and methods, again partly illustrating the effects of data where flaws in the basic compilation of the corpus may cause potential errors in the result, but partly also pointing out potential shortcomings in the particular tools at our disposal.
I've therefore had to restrict my discussion of relevant topics in corpus linguistics to what I consider the most essential ones for beginners.
There are, however, also many other applications of corpus linguistics I've been unable to cover, but which may be of interest to you now that you've mastered the basics, and which I'd like to mention here briefly.
Diachronic/historical corpus linguistics is an important area that aims at answering many of the as yet unanswered questions in language history, including differences in style, vocabulary, grammar, and even pragmatics.
The marriage of corpus linguistics and social science seems, initially, straightforward.
We will begin by looking at epistemology, arguing that this is a major driver of corpus linguistics' integration with, or separation from, the social sciences.
In doing so, we will outline our own epistemology and suggest a route that corpus linguistics may take in debates around epistemology in the social sciences.
The route proposed should, in our view, maximise corpus linguistics' engagement with disciplines across the social sciences.
We will see that such variation can militate for, or against, interaction with corpus linguistics.
Throughout our discussion of epistemological concerns, we will note debates within corpus linguistics that echo these debates in the social sciences.
The paper then narrows in focus to look at a group of related areas in the social sciences that might have much to offer corpus linguistics.
Following from that, we consider how data processing procedures in corpus linguisticsin terms of corpus mark-up, annotation and exploitationappear to be converging, to an extent, with those in (especially qualitative) social science.
In terms of quantitative analyses, we observe how there is much in common between corpus linguistics and the social sciences anyway.
Likewise, we also consider how social science theory is exerting influence on studies in corpus linguistics.
We conclude by reflecting on the nature of evidence, falsification and corroboration in corpus use in the social sciences.
While the terminology of the philosophy of science may be slightly alien to corpus linguists, then, the concepts are not.
Moreover, it is advisable for corpus linguists to be cognisant of such concepts and the debates surrounding them, as these are likely to colour the perception that social scientists form of corpus linguistics.
Indeed, in the literature we can see objections to corpus linguistics which follow the anti-positivist critique, most notably with reference to context and subjectivity.
It, therefore, taken by corpus linguistics, position and that of the anti-positivists.
The alignment is not complete; howeverwhile the critique in favour of introspection denies the utility of corpus linguistics on similar grounds to those of the anti-positivists, the scientia rationalis approach of the Chomskyans is clearly strongly aligned to a logical form of positivism.
It would be ideal to be able to align corpus linguistics and the social sciences and identify those areas which align and those which do not.
All of this complicates the formation of bridges between corpus linguistics and the social sciences.
This has consequences for corpus linguisticsthose areas which routinely draw upon corpus approaches, for example CADS (see Nartey and Mwinlaaru 2019, for an overview), the broad area of teaching and language corpora (e.g. Flowerdew and Brezina 2017) and corpus approaches to language and cognition (e.g. Gries and Stefanowitsch 2006) may find their work more broadly engaged with across the social sciences.
Importantly, the degree to which the engagement of social scientists with corpus linguistic research will occur varies, once more, according to epistemology.
The area of the social sciences that arguably comes closest to some of the methodological concerns of corpus linguistics is demographic studies and the related area of social surveys.
The problems faced by such researchers are similar to those faced by corpus linguiststhey often wish to characterise a population which is far too large to encompass fully.
However, for corpus linguists, such datasets can provide some of the crucial context that would allow them to contextualise their observations.
For example, that demographers will have the answer to how to build a perfectly representative spoken corpus.
An obvious area where a fruitful cross-fertilisation can occur between corpus linguistics and the social sciences relates to data processing and theory.
There have almost been shadow developments occurring between corpus linguistics and the social sciences in this area.
The most obvious area in which the social sciences have shadowed developments in corpus linguistics is corpus markup.
The emphasis of corpus software packages tends to be on quantitative exploration and automated annotation.
NVivo has some strengths that corpus linguists should consider seriously.
By the same token, where it exceeds the abilities of some corpus tools, it is not necessarily the case that those corpus tools are lacking; they were simply developed for a different set of users.
Used together, standard corpus tools and packages such as NVivo could represent a powerful combination for users interested in building, manually annotating and exploiting corpora.
This is a question that is shared by linguistics and other areas of the social sciences, so it is perhaps understandable that annotation schemes are another area where there has been a flow of ideas between corpus linguistics and the social sciences.
Each time the epistemological position of a researcher guides them to explore a theory through a corpus, the corpus plays a crucial role.
However, there are also active barriers to interaction rooted in epistemology that are as intransigent and, in fairness, as principled as some of those that exist within linguistics which have stopped some linguists from using corpus methods.
Over the years, corpus linguistics has made progress in its home discipline by being results-focused and by spawning new theories that use the data made available by corpus analyses.
The work produced by such researchers often aligns much more strongly with naturalism than corpus linguistics does.
Firstly, corpus linguists need to be clear about their own epistemologies.
If they are not, it is very easy to bracket corpus linguistics together with approaches to language data which, very often, are free of any serious reflection upon the nature of language in the social world.
Secondly, corpus linguists need to be clear when marking this distinction.
Accordingly, corpus linguistics is bound to be, and has been, used as one methodological approach amongst many in studies which use mixed methods and orient to triangulation.
In being clear about what makes corpus linguistics distinct and what it has to offer, the role of corpus linguistics as offering one further methodological tool in the researcher's toolbox in the social sciences Corpus linguistics and social sciences will be easier to make, and the engagement of corpus linguistics with the social sciences will be more easily facilitated.
Throughout the nineteenth and much of the twentieth centuries corpus linguistics was exclusively paper-based.
Digital electronic technology has been a boon the corpus linguistics community, but in linguistics, as in life, it's possible to have too much of a good thing.
The overload in corpus linguistics is symptomatic of a more general trend.
What is offered instead are detailed descriptions of a relatively small selection of methods which are likely to be of most use to corpus linguists, where usefulness is judged on criteria of intuitive accessibility, theoretically and empirically demonstrated effectiveness, and availability of software implementations for practical application.
In corpus-based linguistics the research domain is some collection of natural language utterances.
The main locations in this area represented in the corpus are the cities of Newcastle upon Tyne on the north side of the river Tyne and Gatesheadon the south side, but its geographical reach is currently being extended to include speakers from other regions in the North-East such as County Durham, Northumberland, and Sunderland.
The 63 interviews that comprise the DECTE corpus differ substantially in length and so, consequently, do the phonetic transcriptions of them.
There is a limit to the effectiveness of normalization, however, and it has to do with the probabilities with which the linguistic features of interest occur in the corpus.
Instead, the aim is to provide an overview of the main approaches to the problem and a discussion of the methods that are most often used and / or seem to the author to be most intuitively accessible and effective for corpus linguists.
There is a problem with id f : it says that terms which occur only once in a corpus are the most important for document classification.
To answer it, the cluster results are supplemented with social data associated with the speakers in the DECTE corpus.
As such, the following review sets a tractable limit on the literature it surveys, and that limit is the literature specific to corpus linguistics.
The Introduction defined corpus linguistics for the purposes of the present discussion as a methodology in the service of the science of language.
The review is in two main parts: the first part outlines work on quantitative methods in corpus linguistics to serve as a context for the second, which deals with the cluster analytic work specifically.
The hypothesis generation methodology described in the foregoing chapters is intended as a contribution to corpus linguistics, whose remit the Introduction described as development of methodologies for creating collections of natural language speech and text, abstracting data from them, and analysing those data with the aim of generating or testing hypotheses about the structure of language and its use in the world.
The literature draws a distinction between corpus-driven and corpusbased linguistics, where the former is taken to be a scientific paradigm in the sense that behaviourist and generative linguistics are paradigms, that is, ontologies in terms of which linguistic theories can be stated, and the latter to be a methodology, that is, a collection of techniques for the formulation and testing of hypotheses within some already-existing paradigm.
Despite its relatively well-developed state, corpus linguistic methodology has historically been unevenly distributed across the linguistics research community as a whole.
Because many modern-day corpus linguists have been trained as linguists, not statisticians, it is not surprising that they have been reluctant to use statistics in their studies.
Many corpus linguists come from a tradition that has provided them with ample background in linguistic theory and the techniques of linguistic description but little experience of statistics.
As they begin doing analyses of corpora they find themselves practising their linguistic tradition in the realm of numbers, the discipline of statistics, which many corpus linguists find foreign and intimidating.
As a consequence, many corpus linguists have chosen not to do any statistical analysis, and work instead with frequency counts.
In the foregoing search for structure in the DECTE corpus the initial selection of phonetic variables was subjective, but the data representation and the transformation methods used to refine the selection were generic, as were the clustering methods used to analyze the result.
The data and clustering methodology was, moreover, specified precisely enough for anyone with access to the DECTE corpus to check the results of the analysis.
Over the last 20 or so years, multifactorial modeling has taken much of corpus linguistics by storm.
Compared to, say, the 1980s or even the 1990s, there is now a huge and constantly growing number of studies in corpus linguistics that tackle phenomena of interest with methods that allow researchers to study the effect of multiple independent variables, or predictors, on the dependent variable, or response, of interest.
One final comment regarding these data: The use of the above data set is not to imply that a data set like this is typical for corpus-linguistic data in general or for tree-based analyses of such data in particular.
This is already an interesting finding given how widespread the consensus among corpus linguists is that tree-based approaches are good at detecting interactions: in this case, all approaches return a three-way interaction when a two-way interaction is all that would be required/desired.
On the whole, the picture that has emerged so far is somewhat sobering because trees on both the small and the large data sets never returned an optimal tree, optimal in terms of accuracy, parsimony, and effect interpretations simply because one strong predictor chosen for the first split may overpower everything else, something which can of course very easily happen in Zipfian-distributed corpus-linguistic data.
These issues are especially problematic given the otherwise positive trend that corpus-linguistic data and methods are now informing linguistic theorizing more than they have for a long time.
This was done by filtering out those tweets which did not have the English (en) assigned by Twitter's machine language detection, as time zone and language features, which are used to infer locations. which is annotated in the tweet's metadata.
An example of metadata based on a questionnaire from the International Corpus of Learner English (ICLE) is summarized in Fig.
Various types of metadata can be placed in a separate file or in a 'header', so that a computer script or web-based tool for example will be able to use the information in systematic ways when counting frequencies, searching for or displaying relevant data.
With better metadata about individual texts and speakers, we will be in a better position to understand the data, not only to correlate metadata to variation, but also to see more precisely how corpora differ in the case of comparison.
Corpus enrichement is an important way forward, and this applies not only to metadata but also to linguistic annotation.
The architecture chosen for a certain corpus refers to the conceptual division of different types of objects contained in a corpus, such as texts, annotations and metadata.
As presented in Chaps. 1 and 2, annotations and metadata are sometimes relatively simple: markup can be added to set them apart from the text and control their inventories.
The fundamentals of corpus architecture begin with conceptualizing corpora as collections of documents, possibly arranged in subcorpora, and often carrying metadata.
Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specific research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identification of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results.
This might help us answer the perennial question, 'How big should my corpus be?' and help researchers determine comparability and the relative sizes of sub-corpora defined by metadata such as socio-linguistic variables.
In providing all this information, the compilers clearly chose to collect as much metadata as possible.
While several international initiatives have attempted to standardise metadata formats (see Broeder and van Uytvanck 2014 for an overview), its divergent use across existing spoken corpora is pronounced.
As is true for any corpora, when the metadata is provided in a separate file without using standoff techniques (e.g. IDs pointing from annotations to corresponding speaker metadata), it requires great manual effort to make use of it for automatic corpus searches (see also Chap. 3).
Equally, it would be of great benefit to the research community if the metadata for already existing spoken corpora could still be made available, for example for some of the older ICE corpora.
Obtaining detailed metadata is another challenge facing anyone wishing to compile a parallel corpus.
First, it is high time we started collecting richer metadata, notably in terms of SL/TL, source and target texts, translator, translation task, and editorial intervention.
Any corpus relies on metadata to correlate the speech of the child and her surroundings with social variables.
The most important metadata are the recording date, the identity, birth date, and sex of participants, and the role a participant takes in a given recording (a cover term for kinship relations such as "mother" and functional roles such as "recorder").
This both makes it easier to type in the speaker of an utterance during transcription and largely anonymizes the data (given that the metadata are stored in a different file).
Moreover, longitudinal designs and developmental questions require tracking participants and collecting metadata such as full names, addresses, and birth dates, which makes the raw data quite the opposite of anonymous.
This information can be considered comparable to metadata for texts in a corpus or to demographic data in a study with human participants.
It is to be stored as metadata in a header file.
It involves diverse activities like storage, metadata generation, archiving, augmentation, upgradation, processing, retrieval, dissemination, and other works.
Once a draft version of a text contains "metadata" and "textual markup", it can then be placed into a "lexical (pending proofreading)" directory to indicate that the text will be ready for use as a lexical version of the corpus once it has been proofread.
We will review the main types of existing corpora at the end of this chapter, and discuss the issue of metadata in Chapter 6.
These data are typically not part of the corpus itself, but are listed in the metadata (see Chapter 6).
Some corpora contain language samples produced by learners of different mother tongues, and this information can be found in the metadata (see Chapter 6, section 6.4).
In the case of children with ASD, it is important to write down the different indications regarding the linguistic profile of the recorded children and to include them in the metadata, since autism represents a broad spectrum of skills and deficits which could lead to comparing children with very different linguistic and cognitive profiles.
Another limitation to the use of online corpora available for consultation is that they only offer limited access to their metadata (see Chapter 6).
This "piece of extra information concerning the data" included in the corpus is what we call metadata.
In some cases, the abundance of metadata requires the use of a precise syntax for tags, based on the conventions of computer languages for XML or SGML coding.
The header section contains metadata, a description of the file, the encoding, the text profile, mainly the language, the context or participants, and even a history of its revisions.
Among other things, the metadata appearing in the header should offer information about the main features of the participants: degree of relatedness (parents, friends, colleagues, strangers, etc.), the context in which the conversation took place, the manner in which the recording was captured, etc.
Besides, it is also necessary to determine which metadata will be associated with each corpus sample.
The metadata of such a corpus should at least include information of where the recording took place, its date, the context in which it happened, the conversation topic, the number of participants, the gender of the person recorded, his/her age and profession.
In Chapter 6, we discussed the importance of associating metadata with corpus files.
This tool makes it possible to perform a time-aligned annotation with audio or video files, to insert metadata and to have access to the results of the annotation in different formats.
XML can also be used for encoding the metadata of a corpus (see Chapter 6, section 6.4).
For example, to be able to compare the language of women and that of men, this variable must be clearly identifiable in the metadata of a corpus.
The study also demonstrates that the distribution of particular metaphorical expressions across varieties, which can easily be determined in corpora that contain the relevant metadata, may shed light on the function of those expressions (and of metaphor in general).
Consider first the issue of accuracy in document-level metadata.
The Brown Corpus is composed of just 500 texts, and it is very easy to achieve 100 percent accuracy in terms of metadata.
We have already discussed some of the challenges inherent in the creation of large corpora, in terms of accurate metadata and word-level annotation.
For each status update or post that comes through, they will have accompanying metadata that show the gender, general age range, and approximate geographical location of the author.
The authors examine a set of case studies, highlighting issues like part-of-speech annotation, miscategorization challenges, and problems related to metadata and digitized texts in historical databases, such as the 10.5-billion-word Eighteenth Century Collections Online, a database which was originally not compiled for linguistic purposes.
Here, we discuss issues related to the reliability of the database with respect to OCR (Optical Character Recognition) accuracy as well as problems pertaining to the balance of the database and the metadata associated with it.
Our main argument was that the recent trends and advances in the field, such as the publication of big-data corpora, the increased reliance on statistical approaches to linguistic data, and the exploitation of various kinds of metadata, require that the linguist should have a detailed understanding of the structure of the corpus, its sampling procedure, and the principles followed in the construction of the metadata.
The OCR confidence value corresponding to each text can be conveniently accessed with Octavo, like any other metadata.
In the simplest case, the texts are combined based on some metadata in which we are interested in our analysis.
As a rule of thumb, however, the highest number of matching or similar metadata field values might be a good starting point.
Raising the level of analysis might be considered a special case of the principled metadata-based combining approach.
The line between raising the level of analysis and the more general approach of principled metadata-based combining becomes blurred, however, in the case of, for example, combining Twitter tweets with their replies together to form individual texts.
In order to protect the privacy of tweet authors, we only reproduce textual content without any metadata.
There are three types of additional information that can be added to a corpus, namely metadata, markup and annotation.
The third thing which can be added to the raw text of a corpus is metadata.
There are several ways to store metadata for a corpus, but one popular way is to insert a chunk of descriptive information at the beginning of each corpus text in a fixed, specified format: this metadata chunk is called the header of the text.
If we are examining a series of examples extracted by searching a corpus, for instance, it is often very useful to be able to see, via the metadata, precisely what text each example comes from: knowing, for example, whether a particular instance comes from a novel or from a newspaper report can be a valuable aid to interpretation.
The second function is that metadata allows us to isolate and compare different sections of a corpus.
Different XML tags are used for markup, metadata and annotation.
No matter the purpose of a corpus collection, all corpora need to provide metadata to ensure that other scholars can also benefit from the corpus.
Otherwise, the corpus itself, without the metadata, will be useless since the researcher will not be able to know the background information of the corpus.
Descriptive metadata provides classificatory information derived from internal or external properties of the corpus components.
Administrative metadata provides documentary information about the corpus itself, such as its title, availability, revision status, etc.
The first step of annotation is called the structural markup, which gives descriptive information about the text; thus, structural markup is used to create the text's metadata.
In addition to the corpus text data, modern corpora are closely interlinked with two further types of data, namely raw data and metadata.
In addition to these, metadata is also collected about the situational features of texts (cf. 2.2.4).
Imagine a corpus that is overall fairly representative in terms of size and composition, but lacking metadata.
For instance, to evaluate whether a language has been changing over time requires metadata not only on the time of text production but also on the demographic features of people like their age.
Without metadata, we cannot test whether differences between any of these categories are meaningful.
Therefore, the metadata of a corpus will include a list of correspondences between the chosen phonetic alphabet and the IPA.
Alongside corpus text data we have to integrate metadata.
Participant metadata, data about each individual, including names, ages, genders, languages they use, and perhaps, occupation, education level, and other characteristics.
Each text that is part of the corpus will also have metadata such as the date and place it was recorded, who spoke or signed, who else was present, and what media files are associated with the text.
One of the simplest ways to record metadata is as a simple table, which can be stored in a comma or tab-delimited value format (CSV or TSV).
If you are writing your own computer scripts to build your corpus, you can integrate (or merge in) your metadata, bringing in all the metadata for each participant and all the metadata for each file for every token in the corpus.
For language archives, one will either have to fill in metadata forms, for instance, the metadata catalogue of PARADISEC (catalog.paradisec.org.au/), or adhere to a standard metadata format, for instance in the DoBeS (tla.mpi.nl/project/dobes/) or the ELAR (soas. ac.uk/elar/) archive.
This gives standardised metadata results, among other things, in the data appearing on the archive' s website in a structured way.
In fact, if metadata is included at the top or bottom of the file, it may be read as text rather than meta-information about the text.
Because of this, corpus compilers will often use special characters in a special way so that metadata is regarded as such.
Some researchers will include basic metadata in file names (i.e. Megan_19991103_monologue_Canberra) and use the names, often in combination with a file folder structure, to organise their data.
The main point really is that interested users have access to the data and can understand how it can be used, which in turn requires metadata on the corpus as a whole.
First of all, they should be linked to the corpus text data and their metadata so that corpus users can evaluate how annotators arrived at their annotation decisions; at the same time the annotations are ideally extractable in order to be analysed outside the corpus infrastructure itself, for example, by programmes for statistical analysis.
To be able to assess these factors properly, good metadata is needed about the language users as well as good metadata or annotations about each specific context of use.
A major remaining challenge is the lack of sufficiently rich corpora from diverse languages that contain the relevant primary data as well as metadata.
Large corpora will often contain a fair number of errors, missing metadata, and incorrectly coded information.
Corpus linguistics has long championed, and pioneered, markup schemes to permit the systematic encoding of metadata and interpretative analyses within corpora.
UDPipe uses two models that facilitate the tagging process and improve the overall accuracy by employing different classification feature sets.
One commonly used tagset is CLAWS (Constituent Likelihood Automatic Word-tagging System), available in different versions (e.g., CLAWS 5 contains just over 60 tags, while CLAWS 7 contains over 160).
The CLAWS tagger, for example, assigns a hyphenated POS tag, referred to as an "ambiguity tag", when its tagging algorithm is unable to unambiguously assign a single POS to a word.
These tags, of which there are many, are assigned as part of an "idiom tagging" step after the initial assignment of POS tags and involve matching the sequences of POS-tagged words in the corpus against templates of multiword sequences.
It would be unrealistic, therefore, to expect that different POS-tagging algorithms for the same language will produce identical results.
In the most straightforward case, it may be possible to use existing annotation software to automatically apply a given set of POS tagging conventions to corpus source materials.
While this discussion has assumed that appropriate tagsets and tagging models exist for the language or variety represented in your corpus, in many cases, existing sets of POS categories may not be suitable for the data, or may not exist at all.
In situations like these, it is possible to train some kinds of annotation software to apply new tagging conventions to your corpus sources.
Creating a new POS tagging model is often an iterative process: small samples of text are manually annotated and used as training data to create a provisional POS tagger, which then automatically annotates more texts.
In (9a), there is ellipsis of were before the past participle placed in the second conjunct and since the passive tag attaches only to the be auxiliary in this tagging system, the passive structure in the second conjunct is not identified automatically.
The complexity of the corpus architecture results from its annotations: as the data is collected, student annotators iteratively apply a large number of annotation schemes to their data using different formats and tools, including document structure in TEI XML, POS tagging, syntactic parsing, entity and coreference annotations and discourse parses in Rhetorical Structure Theory.
Second, it relies on the error tagging of Falko to identify grammatical and ungrammatical uses of complex verbs and to determine error types.
These annotation steps are often referred to as "tagging" or "coding" in the literature.
Thus, POS tagging should be done simultaneously with or after glossing.
For instance, word-class tagging has become much more accurate.
This decision is important because it will determine exactly what the corpus will look like: what types of texts will be included in it (e.g. spoken, written, both), how long the individual texts will be (e.g. full length, text excerpts), how detailed the annotation will need to be (e.g. word-class tagging or a purely lexical corpus with minimal annotation), and so forth.
On the other hand, if the corpus is intended to permit the study of discourse features, then it will have to contain lengthier texts, and some system of tagging to describe the various features of discourse structure.
Over the years, a number of different tagging programs have been developed to insert a variety of different tagsets.
For instance, semantic tagging involves annotating a corpus with markup that specifies various features of meaning.
While lexically tagging corpora with part-of-speech information can now be done quickly and quite accurately, parsing a corpus is a much more difficult undertaking, since the level of accuracy decreases when identifying structures such as noun phrases or imperative sentences.
Of course, different tagging programs will have different terminology and provide varying levels of detail about the items being tagged.
Lexical tagging is crucial because it will enable the factor analysis program to determine where the various parts of speech occur: e.g. first person pronouns in more interactive texts; passive verbs in more informational texts.
Lexically tagging a corpus has become quite routine, particularly because the accuracy of current tagging programs is quite high.
In fact, most of the time, the study of lexicon can be done directly on a raw corpus, or in some cases, on a corpus that has been annotated with part-ofspeech tagging.
As we will see in Chapter 7, TEI tagging can also be used for making more detailed annotations than dividing the text into sentences.
The Sketch Engine platform, which we presented in Chapter 6, automatically performs this tagging process when a new corpus is created and this can be done for several languages (see below).
It was by means of this type of comparison that the annotations offered for the Google Books corpus in section 7.2 were considered as suitable tagging and parsing systems.
The reason for considering recall and precision at the same time (e.g. with the F1 score) is as follows, illustrated with the example of noun tagging.
At the very least, it is crucial to understand that tagging and other kinds of annotation are the result of applying operational definitions by other researchers and if we use tags or other forms of annotation, we must familiarize ourselves with these definitions by reading the fine manuals that typically accompany the corpus.
Let us begin with a brief discussion of tokenization and part-of-speech (POS) tagging, two phenomena whose operational definitions are typically decided on and applied by the corpus makers and implicitly accepted by the researchers using a corpus.
However, POS tagging is not usually done by skilled, experienced annotators, bringing us to the second, completely different way in which POS tags are based on operational definitions.
This can lead to considerable distortions in the tagging of specific words 3.2 Operationalization and grammatical constructions.
Second, such cases demonstrate vividly why the two operational definitions of parts of speechby tagging guide line and by tagger -are fundamentally different: no human annotator, even one with a very sketchy tagging guideline, would produce the annotation in (23).
As discussed in Section 3.2.2.1 of Chapter 3, this brings with it its own problems, as automatic tagging and grammatical parsing are far from perfect.
Some tagging software is available, but there is an increasing need for corpus researchers to gain computational and statistical skills if they want to carry out more in-depth analyses.
If the text is annotated or corrected by hand then this could form the basis of a training corpus for an automatic tagging system which can then learn from the human annotators in order to attempt to replicate their coding later on larger amounts of data.
In both these cases, automatic tagging tools have been shown to be less accurate and robust.
In particular, spelling variation causes problems for POS tagging, concordancing, keywords, n-grams, and collocation techniques.
Beyond POS-tagging, the most important level of annotation for grammar is obviously the syntactic level, which allows the investigator the means to extract tokens of particular constituent structure configurations.
Automatic corpus parsing, however, has proved a more difficult nut to crack than POS-tagging.
The early tagging systems of the Brown and LOB corpora made no provision for discourse markers.
As corpora become more sophisticated in areas such as their pragmatic tagging, it is predicted that they cannot be ignored as research tools by those researching pragmatics.
Corpus tagging involves the same pros and cons as with other speech data and non-standard data.
The situation is somewhat easier if we need to distinguish between proper nouns and common nouns, as far as we can rely on the accuracy of the tagging.
The guidelines used for the earliest English corpora compiled for linguistic analysis such as the Brown Corpus already observed the problems in the tagging of proper names.
The tagging of some of the most well-known named entities in the BNCweb and the Corpus of Contemporary American English (COCA; Davies 2008-; which both use the CLAWS tagger), such as the White House (the official residence of the president of the United States), seems to take account of the use of capital initials, and mostly White in that name is tagged as a proper noun (_np or _NP0).
Although for the most part, white in references to the White House is tagged as a proper noun, there are also some instances in BNCweb and COCA where it is tagged as an adjective; particularly in COCA, there are altogether 1,497 hits (searched August 27th, 2022) with this tagging, showing various combinations of capital or lowercase spelling of initial letters of white and house (mostly referring to the building in Washington D.C.).
Lists of high-frequency names are often used when designing automatic taggers, which might account for the fact that with names of lower frequencies, the tagging of white in names appears less systematic.
Considering the names in which royal with a capital initial is tagged as an adjective instead of a proper noun, the types of entities in question is reflected in the tagging, as noted in the BNC2 manual; in the names of institutions or charters, royal is an adjective, in the names of locations it is treated as a proper name.
Therefore, it may be advisable for the researcher to make a back-up copy of the corpus before taking the step of tagging it.
A rogue value can be caused, for instance, by mistyping data in a spreadsheet or by a tagging error in the corpus.
Similarly, identification of lexemes, although desirable, involves semantic tagging and semantic disambiguation, which again introduces a certain percentage of errors (even higher than part-of-speech, or POS, tagging) and, moreover, cannot be done fully automatically.
The success of MD also directly depends on the reliability of the automatic identification of linguistic variables (tagging) in corpora.
Second, you cannot simply replace word boundaries "\\b" with tags because then you get tags before and after the words: Let us first look at the simple tagging example from above.
Part-of-speech (POS) tagging is the assignment to each word of a single label indicating that word's grammatical category membership.
The utility of POS tagging, especially for languages like English, is that many words are ambiguous in terms of their part-of-speech.
The underlying principle of semantic tagging is the same as POS tagging, namely, the assignment to every word in a text or corpus of labels representing categories within a scheme of analysis.
The difference is that in the case of semantic tagging, the tags represent semantic fields, i.e. categories of meaning -and the tagset as a whole thus represents some ontology, or way of dividing up all possible meanings into various domains or concepts.
Semantic tagging is a harder task for a computer than POS tagging.
Like semantic tagging, automated parsing has a much higher error rate than POS tagging, because the task is inherently more difficult.
Conversely, tagging can disambiguate the words: break is both a noun and a verb, and broken is both a participle and an adjective -POS tags allow a computer to distinguish the different grammatical functions.
Even though researchers can do tagging manually, it is now possible to train computer programs to tag utterances or sentences automatically.
In addition to grammatical tagging and parsing, it is also possible to do semantic, discourse, and problem-oriented tagging.
Depending on how the tagging was done, there may just be simple categories such as verb, noun, adjective, or the categories may be more refined such as past tense verb, present tense verb, etc.
One thing to watch out for with PoS tagging is that it is often automated.
Hand-tagging an entire corpus, even a small one, is a monumental effort.
Many of the classic and larger corpora of English contain PoS tagging, for example, COCA or the Brown family corpora.
For the London-Oslo/Bergen Corpus (LOB) a tagger known as CLAWS was used which stands for 'Constituent Likelihood Automatic Word-tagging System' .
But in addition to PoS tagging syntactic annotations also pick up more aspects of a syntactic structure.
Remember, however, that lexical category information will be contained in the PoS tagging that we have discussed above in its original Brown version, so that, for example, the NP-embedded PP would be fully annotated as in (7.8) (note that the separator in Penn tagging is forward slash / rather than underscore _): These searches will thus give us a count of NPs with and without recursive structures, that is, where an NP occurs embedded in a higher-order NP either as an initial possessor NP or as the complement of a preposition of an NP-embedded PP.
These annotations thus abstract away from the language-specific structures that morphological glossing and most PoS-tagging capture.
This part already contains information on how to tag your data morpho-syntactically, using freely available tagging resources, and how to make use of tagging in your analyses.
Thus, the following sections will try to provide you with a rough overview of what exactly PoS tagging is and how it can be carried out, where its strengths and weaknesses lie, and how you may be able to use it with your own data.
Let's explore this a little further by looking at another of the currently bestknown tagsets, the CLAWS (Constituent Likelihood Automatic Word-tagging System) C7 Tagset, which is already far more detailed at 152 tags, exceeding the 48 tags observed in the Penn tagset by 104 tags.
Unfortunately, most of the best-known taggers that have been developed over the years, such as, for example, the CLAWS system, for which a number of tagsets, such as the C7 discussed above, have been developed, are generally not freely available to the public or may require the purchase of a commercial licence for tagging suitable quantities of text.
Open the copy in your editor and place the cursor at the end of line 66, which should give us enough text to evaluate the tagging.
Just as in the preceding sentence, I often used words that refer to meaning or understanding of linguistic rules in scare quotes, to indicate that, frequently, the notion of linguistic rules and knowledge in tagging may not really represent a proper kind of understanding that is/can be applied to morpho-syntactic annotation.
We'll discuss issues like this further in Chapter 9, but, for the moment, suffice it to say that the PoS tagging of such languages normally requires programs to artificially introduce spaces during the so-called tokenisation process.
The first thing you'll probably notice in terms of tagging errors is that most of the Roman numerals, apart from, strangely, "IX", have been tagged as proper nouns (NNP), apparently because the tagger has mistaken them for initials.
We'll soon discuss why such issues may arise in tagging.
As constructing the BNC was a major exercise involving the digitisation of very large amounts of text, sorting out meta-information as much as possible, and PoS tagging and annotating the data in a number of ways, the care taken in checking and correcting the final result of the tagging has, at least to some extent, been sub-optimal.
After this brief excursion, let's return to investigating how we can make use of PoS tagging in BNCweb.
Next, to confirm your intuitions -and to verify the tagging -, hover over the hits to see which tag CLAWS assigned to them, and whether this is always unambiguous.
From roughly this point onwards, at least some of the following hapax legomena appear to be proper names, so this is perhaps where the tagging errors gradually begin to peter out.
Although the comparison of such wildly different subcorpora in terms of size is, admittedly, not very useful in general, the list of unique items in the written component immediately reveals a number of interesting features of the BNC tagging and composition, or rather, the way BNCweb allows you to work with them.
Looking at the results, we can first notice that there are specific tokenisation and tagging issues that result in inappropriate 'compound' forms. These, we can perhaps safely ignore, bearing in mind that the modals contained in them would normally contribute relatively little to the overall token counts of the genuine types.
These two cases clearly represent issues related to automatic PoS tagging and the theoretical assumptions and rules behind it.
However, if you call up the concordances for these, you'll soon find out that they represent the initial parts of the negative contractions can't, won't, and shan't, which have been separated from the negation 'clitics' in the tagging process and are being treated as individual tokens.
Incidentally, the same thing also applies to BNCweb, due to the use of CLAWS in the tagging of both corpora.
Regarding the latter, you may now, quite rightly, expect to find at least the other two question words what and who in the same concordance list, as they can certainly be followed by the same clitic, but, due to the tagging rules of CLAWS, these are in fact classified in different ways from the other question words.
Unfortunately, the CLAWS tagging simply 'lumps' all these meanings together, using a single general adverb tag for all of them, which again proves the point that taggers like CLAWS are really optimised for written language, but often still have a number of problems when it comes to dealing with spoken language appropriately.
Even the best fully automated annotation still needs to be checked for errors by human editors, although, as we've seen in the many examples of mis-tagging in the BNC and COCA, this is often not done for reasons of time and cost involved, especially the larger the amount of data processed becomes.
In corpus linguistics, we are almost always dealing with nominal data.
In the context of corpus linguistics, the most obvious role for the Fisher exact test is to measure dependencies between collocations, or in the case of Stefanowitsch and Gries, dependencies between words and constructions.
This handout is primarily directed towards corpus linguistics, but as mentioned in section 5 above, we sometimes deal with ordinal data in linguistics, typically in the context of an experimental or sociolinguistic study.
In the context of corpus linguistics, their use is somewhat questionable, and the reader should be aware that it is regrettably not unusual to find these tests employed in ways which do not fit well with their assumptions.
Parts IV-VI aim to offer a user-friendly introduction to the variety of statistical techniques that have been used, or have started to be used, more extensively in corpus linguistics.
The reality of contemporary corpus linguistics is that the corpora we rely on, in most cases, are simply too large for manually adding annotation, and the automated annotation of electronic texts has become the primary focus in the development of annotation methods.
Another is how dispersion measures relate to issues outside of corpus linguistics such as, again, psycholinguisticallyor cognitively-informed approaches.
In this chapter so far, we have described the origins and motivations for the development of the keywords method in corpus linguistics, and shown two representative studies using the technique.
In the future, we recommend investigating the use of statistical power calculations in corpus linguistics.
This chapter discusses the important role of programming in corpus linguistics.
Next, the chapter explains the basic building blocks of all software programs, and then provides a number of criteria that can be used to assess the suitability of a programming language for a particular corpus linguistics project.
The chapter finishes with a critical assessment and discussion of future developments in corpus linguistics programming.
In order to decide which language is most suitable for a particular corpus linguistics programming task, there are at least four important factors to consider at the outset.
It should also be noted that they can be easily extended or adapted to carry out tasks that are quite difficult or impossible to achieve with the most popular tools used in corpus linguistics today.
These features have resulted in Python becoming one of the most popular languages used in corpus linguistics work today.
The tenet that lexicon and grammar form an integrated continuum of form-meaning pairing has gained ground to the point of probably forming the majority position in current corpus linguistics.
In this section, we will discuss a number of heuristic techniques that can be used to interrogate the syntagmatic-paradigmatic organization across the whole lexicogrammatical continuum, and we will illustrate their potential relevance to diachronic corpus linguistics.
Thus, more standardization, also linking to standards existing in other corners of corpus linguistics, seems an indispensable step for exploiting the full range of developmental data that we have.
It will be interesting to see whether some of the major insights generated within monomodal, or text-based, corpus linguistics will be replicated once we are able to truly analyse multimodality at scale.
This article offers a critical view on using frequency data in corpus linguistics in the context of language learning; this critique is, however, applicable to other contexts as well.
In corpus linguistics, bootstrapping has also been proposed as a method for quantifying the degree of homogeneity in a corpus sample, for validation of statistical results, and as a methodological step in random decision forests, an advanced classification method.
We argue in this chapter that bootstrapping is underused in corpus linguistics, and that quantitative corpus linguists would do well to add this tool to their repertoire.
We then present an overview of several applications of bootstrapping that have been used in prior corpus linguistics research.
We conclude with a summary of the chapter and some comments on potential applications of bootstrapping in corpus linguistics that have yet to be attempted.
Explanations and examples of these applications of boostrapping in corpus linguistics are included in Sect. 24.2.2.
This application has been more widely used by corpus linguistics researchers than the previous two applications.
We believe that the applications of bootstrapping in corpus linguistics we have discussed in the previous four sections are just a beginning for corpus linguistics.
Although the use of bootstrapping methods in corpus linguistics is still in its infancy, writing this book chapter has given us reason to be optimistic about the future of bootstrapping in corpus linguistics.
So while these authors have ample anecdotal evidence to suggest that bootstrapping methods need to be better understood, generally, it seems that their application in corpus linguistics research has gotten off to a great start.
Conditional inference trees (CITs) and conditional random forests (CRFs) are gaining popularity in corpus linguistics.
Such situations are very common in corpus linguistics.
At each step we consider and discuss both best practices as well as some of the many challenges unique to meta-analyzing research in corpus linguistics.
A third major challenge to meta-analysis in corpus linguistics and throughout many other fields is that of publication bias.
In this chapter, we have argued that meta-analysis should be more widely applied within corpus linguistics as a method of synthesizing and empirically reviewing previous corpus-based research.
In order to encourage and enable the application of meta-analysis in corpus linguistics, we have also introduced and demonstrated the main procedures involved including how to (1) search for primary studies, (2) develop and implement a coding scheme, (3) aggregate effect sizes using different R packages to obtain estimates of the relationship in question as well as estimates of variance and of different moderator effects.
The underlying theoretical idea of corpus linguistics is quite broad.
Studies in mathematical linguistics, computational linguistics, corpus linguistics, applied linguistics, forensic linguistics, stylometrics, and other domains require statistical and quantitative results from a corpus.
Both from a theoretical and application point of view, corpus linguistics is a powerful method, which is empirical, scientific, realistic, authentic, open-ended, verifiable, and reliable.
Due to many such advantages, criticisms against corpus linguistics, although partly succeeded at the initial stage, have failed to stop corpus linguistics.
The chapter opens with a discussion of how to define a corpus, and then traces the history of corpus linguistics, noting, for instance, that corpus-based research began as early as the fifteenth century, when biblical concordances were created based on passages from the Bible.
Current conceptions of corpus linguistics started with the creation of the Quirk Corpus (which contained print samples of spoken and written English) in 1955 at the Survey of English Usage at University College London.
But as corpus-based research began to expand during this period, its emergence during the dominance of generative grammar in the 1960s created quite a controversy, since to some linguists, corpus linguistics reeked of behaviorism.
But as much research in corpus linguistics has demonstrated, empirical data can both enhance and support the claims made in linguistic analyses.
In short, the goal of the second edition of English Corpus Linguistics is to provide a comprehensive description of all aspects of corpus linguistics, ranging from how to build a corpus to how to analyze a finished corpus.
But as corpus linguistics developed alongside the dominant paradigm of generative grammar, obvious differences and disputes resulted.
But of all the early electronic corpora, the first computerized corpus of written English, the Brown Corpus (described earlier), was really the corpus that ushered in the modern-day era of corpus linguistics.
The early influences on corpus linguistics discussed in this section do not exhaust the many other factors that affected the current state of corpus linguistics.
This is why corpus linguistics has grown as an area of linguistics and why many people now are using linguistic corpora for many different purposes.
Information on gender (which is defined biologically in corpus linguistics) can be requested on a biographical form, and in written texts, one can often tell the gender of an individual by his or her first name.
While word-class annotation is very well established in corpus linguistics, there are other types of annotation as well.
While corpus linguistics has very little to say to those with interests in, for instance, generative grammar, it is of great value to those who are interested in studying pragmatics: how language is used.
Since this era, advances in corpus linguistics have resulted in the creation of many different types of corpora containing authentic texts ranging from spontaneous conversations to technical writing in the social and natural sciences.
Following the works of American linguist Noam Chomsky, it changed from the essentially rationalist discipline it had been since the middle of the 20th Century, and gradually (re)opened up the empirical approaches represented by corpus linguistics and experimental linguistics.
The development of corpus linguistics has led to the creation of new methods for collecting and analyzing linguistic data, which were made possible thanks to the development of computers and the arrival of the Internet.
While it is true that corpus linguistics has imposed itself in an incontestable manner in the English-speaking world and that a significant proportion of French-speaking researchers currently use these methods, the teaching of corpus linguistics still remains marginalized in France.
Working with corpus linguistics therefore implies being in contact with linguistic data in the form of texts, and also in the form of recordings, videos or any other sample containing language.
For most of the questions addressed by corpus linguistics, it would be impossible to search through a paper database, and that is why having computerized corpora becomes essential.
The problem of manual tracking and counting of occurrences is all the more acute since corpus linguistics is often based on large amounts of data which have not been drawn from a single book, in view of observing the multiple occurrences of a certain linguistic phenomenon and thus apprehending its specificities.
For the moment, the important point to bear in mind is that corpus linguistics often resorts to a quantitative methodology (see section 1.5) so as to be able to generalize the conclusions observed on the basis of a linguistic sample to the whole of the language, or belonging to a particular language register.
As we will see in the following chapters, corpus linguistics may be of use in all areas of linguistics, for instance in fundamental (see Chapter 2) or applied (see Chapter 3) linguistics.
But corpus linguistics is also used for conducting research in fundamental areas of linguistics such as the study of syntax, since it makes it possible to identify the types of syntactic structures used in different languages.
Finally, thanks to the existence of a corpus of oral data, corpus linguistics also makes it possible to answer questions related to phonology and sociolinguistics.
To sum up, in this section, we have defined corpus linguistics as an empirical discipline, which observes and analyzes quantitative language samples gathered in a computerized format.
In the following sections, we will discuss in depth the different central points of the definition, indicated in bold, in order to better understand the theoretical and methodological anchoring of corpus linguistics.
According to Chomsky, another problem related to corpus linguistics stems from the fact that corpora are not representative of the language as a whole.
We will return to this in section 1.4, devoted to the connections between computer science and corpus linguistics.
As we will see later in this chapter, this method shares the use of a quantitative methodology with corpus linguistics.
By resorting to many different speakers and including them in reference corpora of speakers coming from different geographical areas, corpus linguistics makes it possible to respond to this problem in a much more satisfactory way.
Even if works related to corpus linguistics have existed for a long time (such as the indexing of the Bible by theologians or the file-based construction of dictionaries by scholars like Antoine Fureti√®re in French or Samuel Johnson in English), this discipline was only able to properly take off after the arrival of computing.
It is for this reason that other computing tools, specifically devoted to corpus linguistics, have been developed.
The arrival of these tools has greatly accelerated research in corpus linguistics.
But corpus linguistics was not only developed thanks to the creation of such tools.
While we acknowledge the use and importance of combining these two approaches, in the rest of the book we will focus on the quantitative approach to corpus linguistics, which poses its own theoretical and methodological challenges.
On the one hand, corpus linguistics focuses on data observation as found in collections of texts, recordings, etc.
On the one hand, corpus linguistics has the advantage of favoring the observation of natural data, that is, those which are not influenced by an experimental context.
In this chapter, we have defined corpus linguistics as an empirical discipline, that is, based on the observation of real data.
We have also seen that corpus linguistics often resorts to a quantitative methodology, studying a large sample of data which is representative of the phenomenon studied, with the aim of generalizing the observations to the whole of the language or to a language's register.
We have shown that the main difference between corpus linguistics and experimental linguistics is the way in which empirical data are collected.
In the case of corpus linguistics, data are collected in a natural context and then observed, whereas in the case of experimental linguistics, one or more causes are manipulated within a controlled context in order to observe their effects.
That being said, good practices in corpus linguistics and experimental linguistics require obtaining information about participants beforehand, which can eventually eliminate this type of bias.
Regardless of the stylistic genre targeted, the use of quantitative methods linked to corpus linguistics has led to many advances in lexicography and has been one of its main application areas.
This is why literary scholars working in stylistics have started to acknowledge the interest of incorporating quantitative methods from corpus linguistics into the analysis of literary texts, so as to have a more objective starting point for identifying interesting themes and relevant excerpts, which should be the subject of a book's qualitative analysis.
As we will see below, in the field of stylistics, corpus linguistics tools do not seek to replace qualitative analysis, but only aim to guide the choice of themes and excerpts to analyze.
In this chapter, we have shown how corpus linguistics can be of use in different areas of applied linguistics.
The situation has changed a great deal since the 1990s, thanks to the arrival of corpus linguistics, which made it possible to empirically compare linguistic systems.
As in the case of contrastive studies, translation studies has benefited from the availability of multilingual corpora, as well as theoretical and methodological advances in corpus linguistics.
In this way, we can see how corpus linguistics can provide translators with tools that may help them adapt their translation choices on the basis of a better identification of the recurrent linguistic properties at work in a text.
There is yet another distinction that needs to be applied to the concept of word, since it is fundamental for corpus linguistics.
The primary goal of research in corpus linguistics is to be able to draw conclusions in relation to a linguistic phenomenon, not only at the level of the observed data sample, but also to generalize it to all data of the same type.
Throughout this work, we have presented the different facets of corpus linguistics, both from the point of view of the theoretical questions to which this discipline provides answers and of its methodological foundations.
In corpus linguistics, the collection and annotation of data commonly involves a relatively balanced combination of computer-aided and manual labour.
The main reason is that I have found, in my many years of teaching corpus linguistics, that most available textbooks are either too general or too specific.
A book that discusses the history and epistemology of corpus linguistics only to the extent necessary to grasp these methodological issues and that presents case studies of a broad range of linguistic phenomena from a coherent methodological perspective.
I then present what I take to be the methodological foundations that distinguish corpus linguistics from other, superficially Preface similar methodological frameworks, and discuss the steps necessary to build concrete research projects on these foundations -formulating the research question, operationalizing the relevant constructs and deriving quantitative predictions, extracting and annotating data, evaluating the results statistically and drawing conclusions.
Let us define a corpus somewhat crudely as a large collection of authentic text (i.e., samples of language produced in genuine communicative situations), and corpus linguistics as any form of linguistic inquiry based on data derived from such a corpus.
I cannot think of any other scientific discipline whose textbook authors would feel compelled to begin their exposition by defending the use of observational data, and yet corpus linguistics textbooks often do exactly that.
Although corpus-based studies of language structure can look back on a tradition of at least a hundred years, there is no general agreement as to what exactly constitutes corpus linguistics.
Thus, I will attempt in this chapter to sketch out a broad, and, I believe, ultimately uncontroversial characterization of corpus linguistics as an instance of the scientific method.
I will develop this proposal by successively considering and dismissing alternative characterizations of corpus linguistics.
The first chapter of this book started with a similar definition, characterizing corpus linguistics as "as any form of linguistic inquiry based on data derived from [...] a corpus", where corpus was defined as "a large collection of authentic text".
In order to distinguish corpus linguistics proper from other observational methods in linguistics, we must first refine this definition of a linguistic corpus; this will be our concern in Section 2.1.
Thus, studies based on such corpora must be regarded as falling somewhere between corpus linguistics and psycholinguistics and they must therefore meet the design criteria of both corpus linguistic and psycholinguistic research designs.
However, if the design had been widely felt to be completely off-target, 2 What is corpus linguistics? researchers would not have used it as a basis for the substantial effort involved in corpus creation.
The impossibility of this task is widely acknowledged in corpus linguistics.
These corpora are certainly impressive in terms of their size, even though they typically contain mere billions rather than trillions of 2 What is corpus linguistics? words.
Given this assumption, the procedure described here clearly falls under our definition of corpus linguistics.
However, the method of collecting citations cannot be regarded as a scientific method except for the purpose of proving the existence of a phenomenon, and hence does not constitute corpus linguistics proper.
On the one hand, this definition subsumes the previous two definitions: If we assume that corpus linguistics is essentially the study of the distribution of linguistic phenomena in a linguistic corpus, we immediately understand the central role of the techniques described above: (i) KWIC concordances are a way of displaying the distribution of an expression across different syntagmatic contexts; (ii) collocation tables summarize the distribution of lexical items with respect to other lexical items in quantitative terms, and (iii) frequency lists summarize the overall quantitative distribution of lexical items in a given corpus.
This definition is close to the understanding of corpus linguistics that this book will advance, but it must still be narrowed down somewhat.
First, it must not be misunderstood to suggest that studying the distribution of linguistic phenomena is an end in itself in corpus linguistics.
However, while the statistical properties of language are a worthwhile and actively researched area, they are not the primary object of research in corpus linguistics.
It is not suitable as a final characterization of corpus linguistics yet, as the phrase "distribution of linguistic phenomena" is still somewhat vague.
In corpus linguistics, the explicandum is typically some aspect of language structure and/or use, while the explicans may be some other aspect of language structure or use (such as the presence or absence of a particular linguistic element, a particular position in a discourse, etc.), or some language external factor (such as the speaker's sex or age, the relationship between speaker and hearer, etc.).
The fourth step consists in collecting data -in the case of corpus linguistics, in retrieving them from a corpus.
Finally, note that our final definition does distinguish corpus linguistics from other kinds of observational methods, such as text linguistics, discourse analysis, variationist sociolinguistics, etc., but it does so in a way that allows us to recognize the overlaps between these methods.
At the end of the previous chapter, we defined corpus linguistics as "the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus" and briefly discussed the individual steps necessary to conduct research on the basis of this discussion.
In corpus linguistics (and the social sciences more generally), hypotheses of this type are the exception rather than the norm -we are more likely to deal with statements about tendencies (think Most swans are white or Most examples of windscreen are British English), where the search for counterexamples is not a viable research strategy.
However, as pointed out above, many (if not most) hypotheses in corpus linguistics do not take the form of universal statements ("All X's are Y", "Z's always do Y", etc.), but in terms of tendencies or preferences ("X's tend to be Y", "Z's prefer Y", etc.).
In these cases, the most common operationalization strategy found in corpus linguistics is reference to a dictionary or lexical database.
We have seen that operational definitions in corpus linguistics may differ substantially in terms of their objectivity.
Next, the predictions must be tested -in the case of corpus linguistics, corpora must be selected and data must be retrieved and annotated, something we will discuss in detail in the next chapter.
In arriving at the definition of corpus linguistics adopted in this book, we stressed the need to investigate linguistic phenomena exhaustively, which we took to mean "taking into account all examples of the phenomenon in question" (cf. Chapter 2).
These measures play a role in situations where a negative outcome of a test is relevant (for example, with medical diagnoses); in corpus linguistics, this is generally not the case.
This is not yet standard in corpus linguistics, but it is a good idea to plan and document your research as though it already were.
Recall, once again, that at the end of Chapter 2, we defined corpus linguistics as the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus.
Potentially ordinal data are actually frequently treated like nominal data in corpus linguistics (cf. Section 5.3.2), and with complex scales combining a range of different dimensions, this is probably a good idea; but ordinal data also have a useful place in quantitative corpus linguistics.
Even our definition of corpus linguistics makes reference to this fact when it states that research questions should be framed such that it enables us to answer them by looking at the distribution of linguistic phenomena across different conditions.
This could be done using the measure of referential distance discussed in Section 3.2 of Chapter 3, which (in slightly different versions) is the most frequently used operationalization in corpus linguistics.
However, as also discussed in Chapter 3, many if not most hypotheses in corpus linguistics have to be formulated in relative terms -like those introduced in Chapter 5.
As mentioned in the preceding chapter, nominal data (or data that are best treated like nominal data) are the type of data most frequently encountered in corpus linguistics.
Unfortunately, studies in corpus linguistics (and in the social sciences in general) often fail to report effect sizes, but we can usually calculate them from the data provided, and one should make a habit of doing so.
In the context of corpus linguistics, there is one fundamental problem with the t-test in any of its variants: it requires data that follow what is called the normal distribution.
Since we are most likely to deal with nominal variables in corpus linguistics, we will discuss in detail an example where both variables are nominal.
There is a range of multivariate statistical methods that are routinely used in corpus linguistics, such as the ANOVA mentioned at the end of the previous chapter for situations where the dependent variable is measured in terms of cardinal numbers, and various versions of logistic regression for situations where the dependent variable is ordinal or nominal.
The (orthographic) word plays a central role in corpus linguistics.
An area in which this is very obvious, and which has therefore become one of the most heavily researched areas in corpus linguistics, is the way in which words combine to form so-called collocations.
At first, this will seem like a somewhat abrupt shift from the topics and phenomena we have discussed so far -it may not even be immediately obvious how they fit into the definition of corpus linguistics as "the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus", which was presented at the end of Chapter 2.
Note that Firth, although writing well before the advent of corpus linguistics, refers explicitly to frequency as a characteristic of collocations.
It would be surprising if corpus linguistics was an exception, and indeed, it is not.
However, under the definition of corpus linguistics adopted in this book, Sinclair's observations would be just the first step towards a full analysis.
Under the definition of corpus linguistics used throughout this book, there should be nothing surprising about the idea of investigating the relationship between words and units of grammatical structure based on association measures: a grammatical structure is just another condition under which we can observe the occurrence of lexical items.
Chapter 3 included a discussion of counterexamples and their place in a scientific framework for corpus linguistics.
Again, corpus linguistics is a uniquely useful tool to investigate this.
Here, too, corpus linguistics provides useful tools, for example to determine whether the choice between affixes is influenced by syntactic, semantic or phonological properties of stems.
In these cases, quantitative corpus linguistics is essentially a variant of sociolinguistics, differing mainly in that the linguistic phenomena it pays most attention to are not necessarily those most central to sociolinguistic research in general.
Two broad alternatives have been proposed in corpus linguistics.
Again, metonymy is a vastly under-researched area in corpus linguistics, so much work remains to be done.
In this book, I have focused on corpus linguistics as a methodology, more precisely, as an application of a general observational scientific procedure to large samples of linguistic usage.
The second reason is that I believe that corpus linguistics has a place in any theoretical linguistic framework, as long as that framework has some commitment to modeling linguistic reality.
While an understanding of the principles, approaches, and advantages of using corpora provides the necessary foundational knowledge of this approach to language analysis, there is a technical side to corpus linguistics that is best acquired through practice and experience with corpora.
When "doing" corpus linguistics, students need to gain experience searching a corpus and interpreting the results of their corpus searches so that they can use this information to explain why their analysis and findings are important or relevant.
In order to allow the students to gain experience in "doing" corpus linguistics, we have intentionally limited the corpora and software used in the corpus projects.
There are many different descriptive and theoretical frameworks that are used in corpus linguistics.
Without knowledge and practice using a specific framework, we have found that students can be quite enamored with the "button-pushing" aspects of corpus linguistics at the expense of interpreting the results of their searches.
The perspective that we will take is, as you can tell from the title of the book, related to an area of language study called corpus linguistics.
One way to understand linguistic analysis and language is through corpus linguistics, which looks at how language is used in certain contexts and how it can vary from context to context.
Now we will consider the defining characteristics of corpus linguistics as they will be used in this book.
Recall that corpus linguistics includes both quantitative and qualitative analysis.
It is written for someone doing corpus linguistics perhaps even for the first time and for someone who has limited or no basic statistical knowledge.
This chapter provides you with an opportunity to use readily available corpora to conduct corpus linguistics projects.
In this sense, all research questions are framed from a particular perspective by reference to a specific methodology (corpus linguistics).
A novice student of linguistics could be excused for believing that corpus linguistics evolved in the past few decades, as a reaction against the dominant practice of intuition-based linguistics in the 1960s and 1970s.
The goals of the CHECL are to complement, but not duplicate, the coverage of existing textbooks and handbooks on corpus linguistics.
There are many excellent textbooks in print, providing thorough introductions to the methods of corpus linguistics, surveys of available corpora, and general reviews of previous research.
Although the focus in this chapter and the handbook is on tools and methods for English corpus linguistics, I highlight issues of support for other languages and corpora and tools that support multiple languages where they are relevant.
In Section 2.4, I will reflect on the question of whether corpus linguistics is now tooldriven, i.e. whether researchers can only ask the research questions that are supported by the existing tools and methods, and whether other important questions are not tackled due to a lack of tool support.
However, in this chapter, the scope needs to be limited carefully to computational methods and tools employed for corpus linguistics research.
In the corpus linguistics field, it is also known as lexical bundles, recurrent combinations or clusters.
Following on from the critical reflection about the limitations of computational methods and tools in corpus linguistics, this section will zoom in on one of the standard methods and illustrate some of the potential problems with it for corpus linguists and some possible solutions.
As described in the previous section, the computational n-grams method appears under various guises in corpus linguistics.
First, tools which provide Computer Assisted Qualitative Data Analysis (CAQDAS), such as ATLAS.ti, NVivo, QDA Miner, and Wordstat, incorporate some very similar methods to those described here but are not widely used in corpus linguistics.
However, here the focus has been on the tools and methods used in the field of (English) corpus linguistics.
I uncovered some limitations of the current crop of computational tools and methods and reflected on whether corpus linguistics could be said to be becoming tool-driven.
Methods and tools for corpus linguistics have developed in tandem with the increasing power of computers and so it is to the computational side that I look in order to take a peek into the future of corpus software.
In this overview, I will discuss statistical tools in corpus linguistics.
Perhaps the most important tool in confirmatory statistics in corpus linguistics is, or should be, the generalized linear model and its extensions, a family of regression models, which serve to model a response/ dependent variable as a function of one or more predictors.
Third, a range of other interesting statistics can help corpus linguistics tackle other statistical challenges.
Other fields have had long and intense discussions about these things -corpus linguistics, unfortunately, has not.
The results can then be displayed in the concordance format familiar to corpus linguistics with the search feature (e.g. rise tone or low key) centered in the concordance.
The term "keyword" has considerable currency outside corpus linguistics, but there it is usually understood in a different sense.
Keyness in corpus linguistics is but the first statistical step in the analysis of texts.
The review in this section demonstrates that corpus linguistics has enabled large-scale collocation analysis and foregrounded collocation in linguistic research while corpus-based collocation studies over the past decades have uncovered a range of interesting collocational behavior and semantic prosody which have been hidden from intuition and can only be revealed by examining a large amount of attested data simultaneously.
Then, in Section 3 we take a step back to consider the state of the art of phraseological research in corpus linguistics, considering some of the core issues still being debated within the field.
For example, one current issue for the study of phraseology in corpus linguistics concerns the best methods to be used for the identification of the most important lexical phrases in a corpus.
These two corpora inaugurated the modern age of corpus linguistics, although both suffered from limitations: the Brown Corpus was restricted to written, printed material; and the SEU corpus remained on paper until the mid 1970s, when most of the spoken part of it was computerized by Jan Svartvik and became the London-Lund Corpus (LLC).
It seems that the simple goal of "describing" some aspect of English grammar, such as seemed sufficiently ambitious in the early days of corpus linguistics, no longer holds the limelight.
One of the ways in which corpus linguistics has changed our view of language, I believe, is in the now widespread recognition that grammar and lexis are not separate components of language, but that they interpenetrate.
Thanks to them, descriptive grammar will continue to have a role in corpus linguistics.
As in all research, there is much in corpus linguistics that is subjective, including the choice of research question and of the procedures and software to employ, not to mention the interpretation of the output data.
This study is significant because it analyzes frequent conversational features that previously had not been systematically researched using corpus linguistics.
As already mentioned, corpus linguistics has been criticized in relation to its suitability for the study of speech acts.
Moreover, if corpus pragmatics is concerned with the interpretation of meaning in context, another disadvantage associated with the relationship between corpus linguistics and pragmatics is that many larger corpora are impoverished both textually and contextually (Ru ¬®hlemann 2010).
Indeed, we speculate that in years to come, much, if not all, pragmatics research will involve corpus linguistics.
Indeed, it is no exaggeration to say that corpus linguistics using large computer-readable language data has established itself as the main methodology in historical pragmatics.
Historical pragmatics has its roots in philology, and it is against this background that we can most clearly see what new corpus linguistics has brought to historical pragmatics and to historical studies in general.
With corpus linguistics the basis has broadened and the focus has shifted to common features and everyday practices.
The following sections highlight key advancements within corpus linguistics on the study of speech, starting with characteristics that differentiate speech from writing (Section 2), and then moving to characteristics of particular spoken registers (Section 3), and specific individual features associated with speech (Section 4).
Together, this research explicitly contradicts the view that corpus linguistics takes an impoverished, decontextualized view of texts and replaces it with a detailed picture of how students and academics write in different genres and disciplines.
Some concern methodology within corpus linguistics.
In Section 2, an initial survey demonstrates the importance of the register factor in historical corpus linguistics and introduces a number of central matters that arise when the concept of register is applied to diachronic material.
In this section, we discuss how two of the central desiderata in corpus linguistics apply to historical registers: representativeness and comparability.
This chapter has highlighted the growing recognition of the crucial role played by the register parameter in historical corpus linguistics.
Future advances in historical corpus linguistics are likely to have theoretical as well as practical effects on how scholars use registers.
The present chapter begins by situating corpus stylistics within the context of corpus linguistics (Section 1) and computational stylistics (Section 2).
While the purpose of the analysis of texts may vary between corpus linguistics and studies interested in style, the methods, however, can still be similar.
In reviews of historical developments in corpus linguistics, reference is often made to the fact that concordances are not an invention of corpus linguistics, but have been used in the study of literature even before computers existed, for instance, to compile concordances of the Bible or of works of Shakespeare.
While in corpus linguistics concordancing has become a mainstream method, in literary criticism it does not seem to play a major role.
This focus is closely related to key concerns in corpus linguistics showing that frequent patterns are not necessarily those that language users are aware of.
English corpus linguistics was kick-started by the compilation of the Brown corpus of written American English (AmE) in 1961.
As opposed to earlier LCR studies that did not include any statistics, most current studies now follow the general trend in corpus linguistics by providing some sort of statistical analysis.
It may be tempting, in corpus linguistics in general and in LCR in particular, to limit the analysis to a quantitative approach.
Yet beginning corpus users might benefit from learning about what may be regarded as tacit knowledge in corpus linguistics, and even the more advanced scholar may encounter issues new to them that have been addressed earlier.
Perspectives on the matter vary between scholarly approaches: the criteria for determining what is "good data" -for example, considerations on the accessibility and processing of the data -can be quite different in Digital Humanities and corpus linguistics.
As a way forward in trying to find solutions to improve the adaptability of databases for linguistic research, Hiltunen calls for more interdisciplinary discussions and collaboration between corpus linguistics, Digital Humanities, and related disciplines, which are still too far from each other because of the disparities in their research agendas.
When Professor Matti Rissanen wrote a short article entitled "Three problems connected with the use of diachronic corpora" for the ICAME Journal in 1989, the landscape of corpus linguistics looked quite different from today.
The Helsinki Corpus, which was to be published in 1991, would revolutionize the historical research of English for decades to come, and its importance for historical corpus linguistics cannot be overstated.
Against this background, it is interesting to note that Rissanen felt compelled to discuss some potential pitfalls related to historical corpus linguistics two years prior to the publication of the Helsinki Corpus.
In this paper, we revisit Rissanen's ideas in order to identify some of the present-day pitfalls in historical corpus linguistics.
Each of the following sections is intended to illustrate one general problem associated with historical corpus linguistics by discussing individual studies that we have carried out previously.
We also suggest that these challenges are ultimately not so different from the pitfalls discussed by Rissanen in his "philologist's dilemma": just like in the early days of historical corpus linguistics, today's challenges pertain to the linguist's knowledge and understanding of their data.
Consequently, we suggest that it would be prudent to think about the principle of "knowing one's data" from a new perspective: as the sheer size of the modern mega-corpora prevents scholars from engaging with either all or the majority of the original source texts in as much detail as in the early days of historical corpus linguistics, they should strive for an intimate understanding of the historical corpora as mediators of the original texts.
As a lot of work has been done on named entity recognition in the field of computational linguistics, a key to address the problems raised in commonly used, large corpora in corpus linguistics would be to increase the collaboration between the areas, and to integrate the systems and practices developed in NLP and computational linguistics into corpora such as the BNC.
LCR as a field only emerged and became visible in the course of the 1990s in the context of the popularization of corpus linguistics at large but has rapidly evolved and grown in scope and sophistication over the past four decades.
I argue that the specific issues encountered with this database are due to fundamentally different ideas of what might constitute "good data" in the fields of Digital Humanities and corpus linguistics, respectively, and this in turn leads to different expectations regarding how the data is made available to researchers (e.g., Mehl 2021).
However, to discuss possible pitfalls associated with research at the interface of Digital Humanities and corpus linguistics, it is necessary to briefly review some basic assumptions associated with these respective areas of study.
It is clear that many of the points made in the previous section apply equally well to corpus linguistics, where research is based on large amounts of language data which have been processed and turned into linguistic corpora.
Even the same metaphors are used: Such similarities are of course not surprising, given that both Digital Humanities and corpus linguistics deal with textual data.
However, there is one major difference that sets corpus linguistics apart from the Digital Humanities, and this relates to the definition of the term corpus.
The argument that I want to make in this chapter is that in order to take advantage of the synergy between corpus linguistics and Digital Humanities, it is often necessary to critically reflect on the digital materials, how they have been collected, processed, and made available.
Section 4 concludes the paper by bringing the two strands of the discussion together by showing how Open Corpus Linguistics can contribute to overcome several widely discussed problems in corpus linguistics.
In the preceding sections, I argued for adopting open research practices in corpus linguistics and examined a number of potential problems that such an endeavor entails.
The latter is even true if we work with commercial corpora but make our concordances and analysis scripts available, as is increasingly common in corpus linguistics.
This is probably quite common -not only in corpus linguistics but also in other domains, for example, when it comes to publishing programming scripts.
Everybody who has ever worked in corpus linguistics knows that no corpus will ever be perfect, and that the quality of a corpus depends less on the competence of the researchers involved than on the resources they were able to put into it.
Most genres traditionally studied in quantitative corpus linguistics tend to comprise of longer texts, compared to the extremely short text lengths of up to only a few dozen words which are overwhelmingly common on, for example, social media.
Platforms which do limit posting length, such as Twitter (now X), usually limit the maximum length, confining all of their content into the range of short texts, which is mathematically difficult to work with in quantitative corpus linguistics.
This chapter highlights genre categorizations as a pitfall at the intersection of corpus linguistics and literature and problematizes the use of the genre category tag from the perspectives afforded by both fields.
However, given the advent of technology and corpus linguistics, it is now possible to study and analyse these patterns of usage.
In this paper, a look will be taken at the area of corpus linguistics.
Firstly, a brief outline of what corpus linguistics is will be given.
One might have expected corpus linguistics to spearhead this development because, following the above logic, corpus linguistics is inherently quantitative.
Returning to corpus linguistics, it is instructive to compare its methodological challenges and trajectories to those of psycholinguistics.
This has an extremely important consequence: both in psycholinguistics and in corpus linguistics, the vast majority of datasets violate the assumptions of the simplest test that an analyst might normally think of first -namely, that the data points are independent of one another.
Thus, the hopefully not too high-flying goal of this paper is to become for corpus linguistics what the abovequoted papers have become for psycholinguistics: a first go-to resource that explains to corpus linguists what (generalised) linear mixed-effects/multilevel modelling ((G)LMM/MLM) has to offer and that provides them with a concrete example and some instructions on how to perform such analyses.
Although this iterative process is often not reported in final publications, it is evident from the many textbook descriptions of corpus linguistics.
Although they are not necessarily viewed as such, some existing techniques in corpus linguistics can be considered as visualisations.
Very few of the existing techniques are tailored for the specific methods in corpus linguistics, and in addition, the existing corpus visualisations do not scale to large bodies of texts, a key requirement to tackle the growing size of corpora.
All these reasons call for new visualisation techniques, or at least the adaptation of existing ones, in order to specifically address the particular needs of corpus linguistics in terms of scalability, and support for iterative exploration.
In the second case study, we propose to use interactive visualisation techniques to improve the interpretation and exploration of the collocation method in corpus linguistics.
We have highlighted tools and techniques that are already used in corpus linguistics that can be considered as visualisation: concordances, concgrams, collocate clouds, and described new methods of collocational networks and exploratory language analysis in social networks.
This book is a practical introduction to statistical procedures in corpus linguistics, a discipline that uses computers to analyse language, organized according to linguistic topics.
The book is intended for anyone interested in corpus linguistics and quantitative analysis of language.
If corpus linguistics wants to retain its scientific status, it should not be content with statements such as 'this feature was found in a large corpus that is, however, not available'.
Mastering these terms will make reading of the rest of the book, and many papers in corpus linguistics, much easier.
In corpus linguistics the most useful ones are: mean, median and 20% trimmed mean.
In corpus linguistics the term 'representative' is often used to describe a corpus.
Let us now think about how the different notions of a 'word' (type, lemma and lexeme) influence the kind of analysis we can carry out in corpus linguistics.
When talking about words in corpus linguistics, we need to specify if we mean tokens (running words), types, lemmas or lexemes.
The practical implications of Zipf's law for corpus linguistics are as follows: we have to learn to critically evaluate the amount of evidence we have for our claims.
It is a well-known fact in corpus linguistics that words occur in combinations that we call collocations.
The degrees of freedom is a complex concept, which is used when dealing with calculations based on a sample (corpus) rather than a population; in corpus linguistics, this is the case most of the time.
Most of the methodological thinking (not only in corpus linguistics but other disciplines as well) has been biased towards looking for differences and ignoring similarities.
The growing demand in corpus linguistics for statistical sophistication and the lack of appropriate resources for beginner and intermediate users of statistics can thus easily lead to frustration.
Every summer, a large number of students from different parts of the world come to Lancaster to learn about corpora and statistics during a week of Lancaster summer schools in corpus linguistics.
In some sense at least, this book is an introduction to corpus linguistics.
Predictably, I think the answer is still "yes" and "yes, even a second edition," and the reason is that this introduction is radically different from every other introduction to corpus linguistics out there.
There is a variety of very good reasons for this, some of them related to corpus linguistics, some more general.
Worse, developers might even discontinue the development of a tool altogether -and let us not even consider how sorry the state of the discipline of corpus linguistics would be if a majority of its practitioners was dependent on not even a handful of ready-made corpus tools and websites that allow you to search a corpus online.
Given especially the predominance of work on English in corpus linguistics, until rather recently many corpora came in the so-called ASCII (American Standard Code for Information Interchange) character encoding, an encoding scheme that encodes 2 7 = 128 characters as numbers and that is largely based on the Western alphabet.
While we will see below that corpus linguistics has a lot to offer to the analyst, it is worth pointing out that, strictly speaking at least, the only thing corpora can provide is information on frequencies.
A little example of something I came across in an undergrad corpus linguistics course was that we generated a concordance of in and found that in one of the SGML-annotated files of the BNC that we looked at, the word in was tagged as VBZ, which means "third person singular form of to be".
In the long run, linguistic theorising without a firm grip on statistical methods will lead corpus linguistics into a dead-end street.
In a certain study titled "Corpus Linguistics and its Applications in Higher Education" by Fuster M√°rquez & Clavel Arroitia (2010), they are set out to depict implied essentials of corpus linguistics and its progress in relevance to theoretical linguistics and its implementations in modern teaching pursuits.
Finally, they resume problems in connection with the application of corpus linguistics in the classroom since knowledge of the restrictions of corpus linguistics is necessary for its coming prosperity.
Answering either of these questions involves answering the other, as the notion of a 'corpus' in contemporary linguistics is inextricably tied up with the ideas and techniques of corpus linguistics.
Describing corpus linguistics as a 'field' or subdiscipline of linguistics is a statement which requires qualification, however.
But for others in the field, the insights into language that arise from the use of the corpus are so radically different from the traditional understanding of how language works that they consider corpus linguistics to constitute an independent field of study or theory of language.
As a result, in the twenty-first century, corpus linguistics has become -in the words of Tony McEnery -a 'killer method' in linguistics, applied to a hugely diverse array of types of linguistic research.
This is, in short, why corpus linguistics matters.
A very large set of questions in corpus linguistics can be answered without going beyond standard, widely available corpora.
An important difference between sampling in corpus linguistics and in other sciences is that a corpus is never a random sample of the population.
The idea that collocation is of high or indeed primary importance to understanding the meaning of a word or other linguistic form originates with J.R. Firth and is very important in the neo-Firthian school of corpus linguistics; but collocation analysis itself is a commonplace of most or all approaches to corpus linguistics.
Unfortunately, there is simply no space here to give a full account of all the areas of linguistics, and of other fields of study, where the methods of corpus linguistics have been productively applied.
This is exactly what the methodology of corpus linguistics offers to researchers in disciplines beyond linguistics.
For several decades now, corpus linguistics has been among the fastest-growing methodological disciplines in linguistics.
Thus, corpus linguistics needs to explore more adequate and conservative ways to extend AMs to n-grams.
However, associations in general and associative learning are certainly not (always) symmetric, which is why, ideally, corpus linguistics would explore the use of directional AMs.
In sum, there are compelling arguments to include type frequencies from theoretical considerations as well as from neighboring disciplines such as psycholinguistics or computational linguistics, and there are promising first results within corpus linguistics proper, but more exploration is definitely required.
Second and more interestingly, the above two models were fitted with user-defined orthogonal contrasts-something else that happens way too rarely in corpus linguistics-to see easily (i) whether the speakers of an unknown sex are different from those where the sex is known, and (ii) whether female and male speakers behave differently.
However, at this point in time, quantitative corpus linguistics is becoming more and more established also in specific linguistic subdisciplines, which raise their own, more specialized problems.
However, this is no time to rest on our laurels-now that corpus linguistics has become mainstream, and that's a good thing, we too must continue to refine our methods just as other fields have to.
Many areas in psycholinguistics and computational linguistics have made interesting discoveries, have developed useful tools, have adopted great methods from neighboring fields, but corpus linguistics is unfortunately not leading the pack and must take care not to lose momentum either in terms of its own evolution or in terms of how it helps to shape linguistics as a whole.
The present paper is an attempt to provide a snapshot of current problems, both in corpus linguistics in general and in selected hot topic areas, as well as to provide ideas and (first) suggestions about how to cope with these problems; I hope it will succeed as a call to (methodological) arms, and thus trigger developments that will help our field advance once more.
They should keep the speech transcriptions and the written data in ASCII (text) format as it is the standard format used in corpus linguistics.
The more specific concern of corpus linguistics is to account for these decisions by systematically investigating related variants and conditions on their choice.
We will turn to the role of corpus linguistics in sociolinguistics in Chapter 9.
We hope this textbook provides a good first foundation for corpus linguistics and generates your excitement about this large and diverse field.
In corpus linguistics, a text is a record of any stretch of language as produced on a specific occasion at some point in time, at some place, and as received or receivable at some place and time.
In corpus linguistics a fundamental unit of reference is the wordform.
In corpus linguistics the procedure identifying and marking the boundaries of word tokens is called tokenisation.
An important notion in corpus linguistics is that of context.
Of particular prominence in corpus linguistics is the contexts of words, and related to this the corpus linguistic concepts of collocations.
The central premise of corpus linguistics is that all of the language-internal andexternal contextual features are relevant to the way that people use language.
A key concern in corpus linguistics is to explain the variable use of wordforms and other structures in dependence on both types of contextual factors.
The basic concepts explained in this chapter are explained in virtually any textbook on corpus linguistics.
In our view, none of the parameters of authenticity, routineness, and spontaneity constitutes a demarcation criterion for corpus text and/or corpus linguistics.
Excluding such data from corpus linguistics will in a sense also deprive us of the possibility to thoroughly evaluate structures vis-√†-vis certain production conditions, for example, the production of elicited texts not based on established routines.
We do not share views here (occasionally uttered in corpus linguistics literature) that findings from any given corpus merely apply to this corpus and nothing beyond.
While this is a fairly trivial reflection of groups of languages with little specific relevance for corpus linguistics, a grouping that is more important for our purposes is that between wellstudied and lesser-studied languages.
Lesser-studied languages typically lack these things, and here corpus linguistics often means to build corpora in the first instance, and these corpora will then often be relatively small, which then comes with various restrictions as to their usability in research.
Understanding the choice between possible variants is one of the main foci of corpus linguistics, especially variationist corpus linguistics.
This is both the exciting and frustrating part of corpus linguistics.
Frequency lies at the heart of corpus linguistics.
The primary aim of corpus linguistics is to understand linguistic patterns and explore how and why they occur.
It is worthwhile to explore various tools whether you are new to corpus linguistics or are a seasoned veteran.
In many early corpus linguistics works, you will find frequency tables as a primary account of data.
In corpus linguistics, we worry about both type frequency and token frequency (cf. 2.2.3) that can tell us different things about our corpora.
An early trailblazer of corpus linguistics is George Kingsley Zipf.
In corpus linguistics, it is also often used to measure the strength of association between phonemes or between a word and a construction it can occur in.
Since mainstream corpus linguistics focuses on English and other Western European languages, the Latinbased script used in these languages is most common, and relevant corpus software and query mechanisms are based on it.
This chapter will focus on a few kinds of statistical analyses that are often used in corpus linguistics and will try to get you in a position to turn a research question into something specific and testable.
This chapter will explain some of the most frequently used quantitative techniques in corpus linguistics today, such as logistic mixed-effects regression, as well as focus on some newer techniques becoming more popular, such as classification trees (and random forests) which are types of recursive partitioning.
There, we concentrate on our strength: variationist corpus linguistics.
In corpus linguistics, sampling is rarely truly random (a random person from a population, or a random text from all existing texts), and is more often a convenience sample (who we can record, what we can find on the web, which newspapers we have access to, etc.) (cf. Chapters 6 and 10).
Interval variables are uncommon in corpus linguistics.
We will also discuss a newer method that is becoming popular in corpus linguistics and linguistics more widely, called recursive partitioning, which includes analyses like classification trees and random forests.
Also common in corpus linguistics is generalized linear regression.
Logistic regression models are easier to fit and easier to interpret than multinomial regression, and are what you will see most commonly in multivariate quantitative corpus linguistics.
Other kinds of corpus linguistics are interested in variation between many kinds of register and genre, and often in the differences between spoken and written modes.
These are generally captured in corpus linguistics as part of the external contextual features.
Language documentation shares with corpus linguistics the basic goal of representativeness.
Given the recent overarching knowledge-building practices and the methodological roles of corpus linguistics, it is necessary to review how a certain body of knowledge has been created according to the common denominator of corpus linguistics.
In the current study, we analyzed co-cited documents published in corpus linguistics during the past twenty years.
As has been observed, corpus linguistics is a fairly new and rapidly growing discipline.
Given the considerable interest in utilizing the corpus linguistic approach, in addition to the dynamic and interdisciplinary nature of current studies involving partnerships among disciplines, a comprehensive and systematic overview of the development of and relationships among individual research in the fields of corpus linguistics is called for.
Thus, the pressing academic quest is to review past achievements as well as future directions of corpus linguistics.
Two sources of important citation data, most-cited journal titles and the individual publications, were analyzed to understand citation patterns in corpus linguistics.
Every journal title has its own aims and scope of publication; therefore, the analysis of the cited journal titles identified trends in how corpus linguistics research has been communicated within the research community.
English for Specific Purposes and TESOL Quarterly, founded in 1980 and 1967, respectively, appeared after the turn of the century and have been consistently cited by the corpus linguistics papers.
Journal of English for Academic Purposes, along with English for Specific Purposes and Cognitive Linguistics, is another example of a specialized journal actively being cited by corpus linguistics papers.
During the earliest period of the current research scope, between 1997 and 2001, the research topics related to new perspectives on grammar were the central issue in corpus linguistics.
Another group of researchers, possibly novices in the discipline, cited general references to corpus linguistics, such as Introduction to Corpus Linguistics, Corpora in Applied Linguistics, and Foundations of Statistical Natural Language.
An alternative explanation for the emergence of the introductory references of corpus linguistics would be because the period was the optimal time for establishing corpus linguistics as a part of linguistics after a "hodgepodge" multi-directional development of corpus linguistics.
Research papers and books have been published to introduce corpus linguistics and define how to use it or build a small corpus for pedagogic purposes.
Considering the relationship among most sited publications and the salient academic research themes, it seems that the corpus linguistics has become a linchpin of certain academic disciplines.
The present study explored the research trends in corpus linguistics over the past 20 years.
Furthermore, journals that first ranked on the list after the early or late 2000s and remained dominant in corpus linguistics until now were English for Specific Purposes, TESOL Quarterly, International Journal of Corpus Linguistics, and Cognitive Linguistics.
Corpus linguists' most-cited publications which served as the foundations of corpus linguistics were constantly referenced.
The pattern of cited publications also suggests that corpus linguistics has been specialized and branched out.
In the years before 2000 and the early 2000s, corpus linguistics tended to be studied by using enormous empirical datasets.
Another recent trend in the corpus linguistics research, according to the current study, was the emergence of large web-based corpus (e.g., COCA).
The book is aimed at a variety of readers, ranging mainly from linguistics students at senior undergraduate, Masters, or even PhD levels who are still unfamiliar with corpus linguistics, to language teachers or textbook developers who want to create or employ more real-life teaching materials. As many of the techniques we'll be dealing with here also allow us to investigate issues of style in both literary and non-literary text, and much of the data we'll initially use actually consists of fictional works because these are easier to obtain and often don't cause any copyright as that of morphology, or even phonology.
Furthermore, as real corpus linguistics is not just about getting some 'impressive' numbers but should in fact allow you to gain real insights into different aspects of language, you should always try to relate your results to what you know from established theories and other methods used in linguistics, or even other related disciplines, such as for example sociology, psychology, etc., as far as they may be relevant to answering your research questions.
This is also why the solutions to, and discussions of, the exercises may often represent those parts of the book that cover some of the more theoretical aspects of corpus linguistics, aspects that you'll hopefully be able to master once you've acquired the more practical tools of the trade.
As this textbook is more practical in nature than other textbooks on corpus linguistics, at the end of almost all chapters, I've also added a section entitled 'Sources and Further Reading'.
However, what you've just seen on the exercise page is not only an issue in corpus linguistics, but actually represents a much more prevalent problem on the internet.
Type in 'corpus linguistics filetype:doc', and hit 'Enter'.
Both of these mistakes basically may cause you to lose valuable time that could be spent on actually analysing your data, thus 'throwing away' one of the most important advantages of using corpus linguistics as a methodology, which is that it allows you to save a significant amount of time finding a large amount of potentially relevant and interesting data quickly.
However, if you persist, I can promise you that soon you'll not only be able to understand the value of regexes for achieving even highly complex tasks in linguistic analysis very efficiently, but will also learn to hone your analysis skills related to morphology and morpho-syntax, something you'll definitely be able to profit from in your work in corpus linguistics, as these two areas often form the basis for further linguistic analyses.
In order to develop this understanding thoroughly, as so often in corpus linguistics we need to look at this task from at least two different angles, a theoretical and a practical one.
The most useful output format in many areas of corpus linguistics for such a list is generally to have a list that is first sorted according to the frequency in descending order, that is, with the most common words occurring first, and then 'alphabetically' if the same number of tokens occurs for more than one type.
At the same time, whenever appropriate, I've also tried to point out other potential applications for such data, for example, in the development of teaching materials/textbooks, grammars, or direct application in the classroom, but of course such a list will always be incomplete as there are too many applications of corpus linguistics to be listed exhaustively.
In the next major section (Chapters 5-10), we then investigated various techniques for analysing language data using established methods of corpus linguistics.
I've therefore had to restrict my discussion of relevant topics in corpus linguistics to what I consider the most essential ones for beginners.
There are, however, also many other applications of corpus linguistics I've been unable to cover, but which may be of interest to you now that you've mastered the basics, and which I'd like to mention here briefly.
Diachronic/historical corpus linguistics is an important area that aims at answering many of the as yet unanswered questions in language history, including differences in style, vocabulary, grammar, and even pragmatics.
The marriage of corpus linguistics and social science seems, initially, straightforward.
We will begin by looking at epistemology, arguing that this is a major driver of corpus linguistics' integration with, or separation from, the social sciences.
In doing so, we will outline our own epistemology and suggest a route that corpus linguistics may take in debates around epistemology in the social sciences.
The route proposed should, in our view, maximise corpus linguistics' engagement with disciplines across the social sciences.
We will see that such variation can militate for, or against, interaction with corpus linguistics.
Throughout our discussion of epistemological concerns, we will note debates within corpus linguistics that echo these debates in the social sciences.
The paper then narrows in focus to look at a group of related areas in the social sciences that might have much to offer corpus linguistics.
In terms of quantitative analyses, we observe how there is much in common between corpus linguistics and the social sciences anyway.
Likewise, we also consider how social science theory is exerting influence on studies in corpus linguistics.
Moreover, it is advisable for corpus linguists to be cognisant of such concepts and the debates surrounding them, as these are likely to colour the perception that social scientists form of corpus linguistics.
Indeed, in the literature we can see objections to corpus linguistics which follow the anti-positivist critique, most notably with reference to context and subjectivity.
It, therefore, taken by corpus linguistics, position and that of the anti-positivists.
The alignment is not complete; howeverwhile the critique in favour of introspection denies the utility of corpus linguistics on similar grounds to those of the anti-positivists, the scientia rationalis approach of the Chomskyans is clearly strongly aligned to a logical form of positivism.
It would be ideal to be able to align corpus linguistics and the social sciences and identify those areas which align and those which do not.
All of this complicates the formation of bridges between corpus linguistics and the social sciences.
The area of the social sciences that arguably comes closest to some of the methodological concerns of corpus linguistics is demographic studies and the related area of social surveys.
An obvious area where a fruitful cross-fertilisation can occur between corpus linguistics and the social sciences relates to data processing and theory.
There have almost been shadow developments occurring between corpus linguistics and the social sciences in this area.
The most obvious area in which the social sciences have shadowed developments in corpus linguistics is corpus markup.
This is a question that is shared by linguistics and other areas of the social sciences, so it is perhaps understandable that annotation schemes are another area where there has been a flow of ideas between corpus linguistics and the social sciences.
Over the years, corpus linguistics has made progress in its home discipline by being results-focused and by spawning new theories that use the data made available by corpus analyses.
The work produced by such researchers often aligns much more strongly with naturalism than corpus linguistics does.
If they are not, it is very easy to bracket corpus linguistics together with approaches to language data which, very often, are free of any serious reflection upon the nature of language in the social world.
Accordingly, corpus linguistics is bound to be, and has been, used as one methodological approach amongst many in studies which use mixed methods and orient to triangulation.
In being clear about what makes corpus linguistics distinct and what it has to offer, the role of corpus linguistics as offering one further methodological tool in the researcher's toolbox in the social sciences Corpus linguistics and social sciences will be easier to make, and the engagement of corpus linguistics with the social sciences will be more easily facilitated.
Throughout the nineteenth and much of the twentieth centuries corpus linguistics was exclusively paper-based.
Digital electronic technology has been a boon the corpus linguistics community, but in linguistics, as in life, it's possible to have too much of a good thing.
The overload in corpus linguistics is symptomatic of a more general trend.
As such, the following review sets a tractable limit on the literature it surveys, and that limit is the literature specific to corpus linguistics.
The Introduction defined corpus linguistics for the purposes of the present discussion as a methodology in the service of the science of language.
The review is in two main parts: the first part outlines work on quantitative methods in corpus linguistics to serve as a context for the second, which deals with the cluster analytic work specifically.
The hypothesis generation methodology described in the foregoing chapters is intended as a contribution to corpus linguistics, whose remit the Introduction described as development of methodologies for creating collections of natural language speech and text, abstracting data from them, and analysing those data with the aim of generating or testing hypotheses about the structure of language and its use in the world.
Over the last 20 or so years, multifactorial modeling has taken much of corpus linguistics by storm.
Compared to, say, the 1980s or even the 1990s, there is now a huge and constantly growing number of studies in corpus linguistics that tackle phenomena of interest with methods that allow researchers to study the effect of multiple independent variables, or predictors, on the dependent variable, or response, of interest.
A 'sample' is a subset of the population that we want to study.
Sometimes Sample the sample is carefully collected based on pre-defined criteria.
However, we sometimes have to work with the sample we happen to have available, like in historical linguistics.
A 'random sample' is a sample where every member of the population has Random sample equal probability of being included in the sample.
A 'null hypothesis', or H 0 , is a term used to denote the default assumption Null hypothesis of most statistical test, namely that all the variation in the sample data is due to random variation.
Phi and Cram√©r V 'factor out' the size of the sample, and give the 'average' contribution of rows and columns (that is, the categories in the table and their respective observations in the rows and columns) to the final result.
Furthermore, the question of sample and population is often quite complex in most real research projects and requires a lot more attention than what was given to it in section 6.
Given that linguistics is descriptive at its core, many linguists study how language is used based on some linguistic sample.
Thus, having chosen to sample such things as popular novels, or technical writing, best-seller lists and library circulation statistics were consulted to select particular examples of them.
With less data-a smaller sample-the claims one is able to make based on one's corpus findings will be more modest.
If we take the two sample studies as an example, they would both have benefitted from multimodal data.
Once trained on a particular sample of sentences, a probabilistic parser could then use that information to determine what the most likely parses would be for any new sentences it encountered, weighing the likelihood of different possible analyses in consideration.
Before making claims, it is important to consider a sample of expanded concordance lines and to maintain vigilance in terms of spotting lines that potentially may be functioning differently to a first glance.
We do not have enough space to discuss the statistical evaluation and detailed results of this study here; suffice it to say that applying a multiple regression approach to the entire data sample of over 9,000 attestations, the authors found that intermediate-advanced German and Spanish learners rely on the same factors as native speakers do when they choose to realize or omit that.
Representations of foreigners were largely concerned with political institutions like the foreign office, although a sample of 21 out of 100 concordance lines taken at random (using an online random number generator) showed negative constructions of foreigners involving stereotyping, implying they were taking up British resources or jobs, or controlling British interests.
The data sample was analyzed with a binary (continued) logistic regression (Chap. 21) in which the dependent variable was the choice of genitive (-s vs. of ) and the 14 variables were the predictor variables.
It is worthwhile mentioning that the concepts of sampling density and coverage used above conflate two aspects of sampling which are of great relevance for theory and practice, viz. sampling intervals (e.g. one month between samples) and durations of recordings (e.g. 2 hours per sample).
This makes a practical difference: recording e.g. a 5-hour sample within a predefined week of the month (the sample can be subdivided in several recordings of different length within this week) will on average be easier to accomplish than recording a 1-hour sample every week, which implies a high demand of discipline both of the recording assistant and the families.
Of course, exploratory methods serve as the basis for the design of predictive modeling, which uses the values found in a sample to predict values for another sample.
Dimension scores are calculated for each text sample by adding up standardized frequencies with salient positive loadings and subtracting salient negative loadings on a dimension.
We have to realise that in corpora we typically sample data at the level of texts/speakers.
This leads to the second step, namely, to choose some Œª, sample from the (normal) distribution of weights for the smooth implied by Œª, and keep tuning Œª until an optimal fit is obtained.
Bootstrapping makes it possible to measure the accuracy and reliability of sample estimates and is often recommended for small samples and samples with unknown or non-normal distributions.
In corpus linguistics, bootstrapping has also been proposed as a method for quantifying the degree of homogeneity in a corpus sample, for validation of statistical results, and as a methodological step in random decision forests, an advanced classification method.
We address several applications of bootstrapping, including the measurement of sample estimate accuracy, the validation of statistical models, the estimation of corpus homogeneity, and random forests.
The overarching goal of statistical analysis is to estimate parameters (e.g. mean, standard deviation) of a population by measuring those parameters in a sample.
However, they can be estimated with a certain degree of confidence from an observed sample drawn from the population.
In most cases, the observed data in a sample provides the best possible insight into the population parameters.
If we can assume that a sample is representative of a given population, it is safe to assume that (a) values observed in the sample are possible in the population, and (b) the distribution of those values in the sample can be used to estimate their distribution in the population.
Although we can get a parameter estimate from this sample, we have no way of measuring the accuracy of our estimate without knowing the sampling distribution.
For example, we can calculate the mean rate of occurrence for nouns in a single corpus sample of size n.
However, if this sample is biased in some way, as most samples are, then our estimated rate of occurrence (i.e. mean) for nouns will be different from the actual rate of nouns in the population.
Assuming that the entire population of texts is inaccessible, our next best option for measuring the degree of bias in our current sample is to collect additional samples of size n from the population and measure the mean rate of nouns in those corpora.
The mean rates for nouns drawn from every possible sample of size n that can be drawn from the population constitute the sampling distribution for this parameter.
Observations about an unrepresentative sample are not generalizable to the target population, and bootstrapping cannot change that.
Each bootstrapped sample has the same number of observations as the original sample.
To summarize these three points: bootstrapping cannot provide better data or more data from the population than what is contained in the sample.
Despite its limitations, bootstrapping can provide a wealth of information about the distribution of a sample.
To summarize, bootstrapping augments the amount and quality of information we can extract from the observed data in a sample.
In addition to bootstrapping the decision trees, random forests sample a random subset of the original variables (e.g. linguistic features) at each stage in the learning process.
The algorithm uses resampling with or without replacement to create a random sample for each tree.
Importantly, only a sample of candidate predictors is randomly drawn for each individual CITs.
The notions of systematicity and objectivity can also be observed in the metaanalytic approach to extracting information across the sample of studies that are obtained.
Broader domains also expose the study to 'apples and oranges'type critiques wherein the heterogeneity of the sample is such that aggregation of findings across studies is deemed inappropriate.
This approach will certainly lead to redundancies; however, redundancy is a small price to pay to achieve a sample that is comprehensive and (nearly) free of bias.
Given these multiple indicators, in this situation in which large differences exist between the effect sizes of the studies included in the meta-analysis, the mean of the effect sizes from all studies obscure patterns in the sample.
In addition, whereas the data used in this sample meta-analysis is purely fictional, in a real meta-analysis, if you were to discover after exhaustively searching for and collecting previous research that the number of "distant" L1 studies was similarly small (e.g., k = 4), the study would also provide evidence of a need for further research in this area.
There are a number of reasons why a sample might be biased.
When present, a publication bias (also referred to as "the file drawer problem") in favor of larger effects may be observed in a given sample of studies, leading to an inflated overall effect.
It also depends on the decision of how many text categories are kept in a corpus, how many text samples are put in each category, and how many words are kept in each sample.
In addition, sample corpus analyses are included to illustrate the many theoretical and practical applications that linguistic corpora have.
Each sample was 2,000 words in length, enabling valid comparisons between the different registers in the corpus.
Prior to the collection of spontaneous dialogues in the BNC, calculations were made to ensure that the speech to be collected was drawn from a sample of speakers balanced by gender, age, social class, and dialect region.
To enable the study of sociolinguistic variables in the spoken part of the BNC, each conversation contains a file header, a statement at the start of the sample providing such information as the age and gender of each speaker in a conversation.
And both programs depend heavily on how accurately and completely sociolinguistic variables have been annotated, and whether the corpora being analyzed provide a representative sample of the variables.
In studying variation by gender in ICE-GB, the analyst will want to review the results with caution, since this corpus does not contain a balanced sample of males and females.
For instance, personal letters are often less than 2,000 words, and a text sample can be comprised of complete letters totaling 2,000 words.
For both the spoken and written parts of the corpus, not all samples are exactly 2,000 words: a sample is not broken off in mid-sentence but at a point (often over or just under the 2,000-word limit) where a natural break occurs.
In addition to studying the distribution of word categories, such as nouns or prepositions, Biber (1993: 250) calculated the frequency with which new words are added to a sample as the number of words in the sample increases.
The mathematical formulas used in probability sampling often produce very large sample sizes, as the example above with books illustrated.
However, this does not necessarily mean that the text must be excluded from the corpus, since there is annotation that can be included in a corpus indicating that certain sections of a sample are "extra-corpus" material; that is, material not considered part of the corpus for purposes of word counts, generating KWIC (key word in context), and so forth.
A further complication is that much writing, particularly scientific writing, is co-written, and if males and females collaborate, it will be difficult to determine precisely whose writing is actually represented in a sample.
This length of conversation increases the probability that a natural and coherent sample of speech can be extracted from this longer sample.
First of all, each text was assigned a number that designated a specific genre in the corpus in which the sample would be potentially included.
After a text was numbered, it was given a short name providing descriptive information about the sample.
For speech samples recorded from television or radio, additional information was written down, such as what station the recording was made from, where the station is located, and who should be contacted to obtain written permission to use the sample.
In addition to keeping records of the texts that are recorded, it is equally important to obtain ethnographic information from individuals contributing either a sample of speech or a written text.
For instance, for the 90 texts labeled S1A-001 to S1A-090, the letter S indicates that each 2,000-word sample represents spoken English; the numerals 001-090 that it was a private conversation (either a spontaneous conversation or a telephone call); and the uppercase A that it was a dialogue.
The first stage involves proofreading each individual sample in a corpus with samples being placed into a "proofread 1" directory.
Of course, some markup is probably better inserted after a text sample is computerized.
After a sample of digitized speech is loaded into the program, short segments of the sample can be automatically replayed until an accurate transcription is achieved.
The first line of the example indicates that this is the second sentence from sample "A01" (the press reportage genre) of the LOB Corpus, sections of which (mainly shorter sentences) are included in the Treebank.
For instance, below is a sample list of examples of Trump's use of the word loser(s).
Working with corpus linguistics therefore implies being in contact with linguistic data in the form of texts, and also in the form of recordings, videos or any other sample containing language.
In order to avoid this problem, it would be necessary to collect a sample of texts, representative of the works of this period.
For the moment, the important point to bear in mind is that corpus linguistics often resorts to a quantitative methodology (see section 1.5) so as to be able to generalize the conclusions observed on the basis of a linguistic sample to the whole of the language, or belonging to a particular language register.
The first distinction we can make among all the existing corpora is the one that classifies them into a sample corpus and a monitor corpus.
Therefore, sample corpora need to be recollected at regular intervals.
It is evidently impossible to collect a sample of the whole language, but in the same way that a general language dictionary aims to describe the common lexicon of a language, the general corpus seeks to offer a global image, including the main textual genres found in language.
We have also seen that corpus linguistics often resorts to a quantitative methodology, studying a large sample of data which is representative of the phenomenon studied, with the aim of generalizing the observations to the whole of the language or to a language's register.
The main response to such criticism is that these areas are based on the use of quantitative methods (namely inferential statistics), which make it possible to draw conclusions from a sample and to extrapolate them to an entire population.
In this section, we will present a sample of studies which illustrate the usefulness of corpora for carrying out contrastive linguistic studies.
Unlike many European languages, French still does not have a reference corpus, a representative sample of the French language in general, similar to the British National Corpus that exists for British English, one of the pioneers in the genre.
It is therefore necessary to prepare a sample of the population of a more modest size, to whom it might be possible to ask such a question.
Later, the results obtained on the basis of this sample can be extrapolated to the entire population.
But in order for this technique to work, it is crucial to carefully choose the sample of respondents, in such a way that it represents the whole population.
For example, if the sample chosen includes 500 students met at the exit of a university building, the sample obtained will most likely not correspond to the actual result of the election, since this sample is not representative.
In order for the sample to be representative, it should also include people with other types of occupations, different age ranges and from different regions.
To continue developing the analogy with polls, if the goal is to know who students will vote for in the presidential elections, it will be enough to interview a sample of students, since such a sample is representative of that population.
The preparation of samples to be included in the corpus poses two important methodological questions: on the one hand, the appropriate size for each sample, and, on the other hand, how to balance the portions of the corpus in such a way that the result is truly representative of the genre.
If 95% of the corpus is made up of Parisian speakers aged between 20 and 30 years, such a corpus will not be a representative sample of the population, even if this means including the other speakers among the remaining 5%.
While it is true that a sample is expected to bring together a rather restricted version of the overall population, it should also reproduce its main features so that the results obtained on this sample can be extended to the entire population.
For example, a corpus created for studying the pronunciation of vowels in Paris and Marseille and which has not been compiled representing a balanced sample of different age groups cannot be used carelessly for studying the evolution in the pronunciation of vowels between generations.
Each of these properties should then be mirrored by the corpus sample proportionally to its prevalence in the population.
For example, in order to create a corpus of French, speakers from different regions should be included in the sample.
For example, if the goal is to reach a sample of 200,000 words per text genre, this number of words can almost be instantly reached by including one or two entire books in the sample.
In order to be able to easily group the files into sub-corpora, it is necessary to represent the features of each sample in an easily accessible manner.
Following this procedure, the sample corresponding to the first 18-year-old male participant from Marseille registered in the corpus would be saved in a file called "221001.txt".
For this information to be made available for future users of the corpus without separating it from the rest of each sample portion, it should be possible to include it in the files, but in such a way that it is not taken into account by a concordancer when counting the words of the corpus.
We also introduced some basic principles regarding sample collection and balancing.
Let us assume that the corpus targets a size of 200,000 words per genre, in order to have a representative sample of each of them.
The first question to ask is whether a sample is representative of the genre it embodies in the corpus.
The second important question concerns the size of the sample that will be included in the corpus.
Depending on the target size for each genre, it is necessary to determine the appropriate size for each sample.
Besides, it is also necessary to determine which metadata will be associated with each corpus sample.
Finally, we should make sure that the sample is acceptable from an ethical point of view, in the sense that it respects the right to anonymity of the person involved and that it does not contain inappropriate content.
The primary goal of research in corpus linguistics is to be able to draw conclusions in relation to a linguistic phenomenon, not only at the level of the observed data sample, but also to generalize it to all data of the same type.
To do this, it is necessary to use inferential statistics, a tool that makes it possible to know whether it is correct to generalize the results found, at the level of a sample, to a population.
It expresses the probability of the sample data being observed if the null hypothesis were true in the population.
We have made an important distinction between descriptive statistics (which relate only to the sample tested) and inferential statistics (which aim to generalize the results to an entire population).
In order to be able to draw quantitative conclusions, it is necessary to carry out inferential statistical tests, which will indicate whether the results obtained on the corpus sample can be generalized to the entire population.
Instead, it seems more likely that the construction is simply too infrequent to occur in a sample of one hundred million words of text.
Note that even if we manage to solve the problem of reliability (as systematic elicitation from a representative sample of speakers does to some extent), the epistemological status of intuitive data remains completely unclear.
Put simply, a representative sample is a subset of a population that is identical to the population as a whole with respect to the distribution of the phenomenon under investigation.
Finally, there are language varieties that are impossible to sample for practical reasons -for example, pillow talk (which speakers will be unwilling to share because they consider it too private), religious confessions or lawyer-client conver-sations (which speakers are prevented from sharing because they are privileged), and the planning of illegal activities (which speakers will want to keep secret in order to avoid lengthy prison terms).
However, in light of the above discussion of representativeness, there is little reason to believe that any of these corpora, or the many others that fall somewhere between BROWN and ICE, even come close to approximating a random sample of (a given variety of) English in terms of the text categories they contain and the proportions with which they are represented.
Since overcoming the fallibility of introspective data is one of the central motivations for using corpora in the first place, the analysis of a given phenomenon must not be based on a haphazard sample of instances that the researcher happened to notice while reading or, even worse, by searching the corpus for specific examples.
The best way to achieve this is to draw a complete sample of the phenomenon in question, i.e. to retrieve all instances of it from the corpus (issues of retrieval are discussed in detail in Chapter 4).
As a consequence, it is usually impossible to extract a complete sample of a given phenomenon manually, and this has lead to a widespread use of computers and corpus linguistic software applications in the field.
Thus, we must formulate one or more queries that will retrieve all (or a representative sample of) cases of the phenomenon under investigation.
Let us further assume that we can assign all other uses of pavement in the sample to the reading 'paved surface', and that two of the four examples of sidewalk in the British English corpus are genuine counterexamples.
While all of these cases have the form of the possessive construction and match the strings above, opinions may differ on whether they should be included in a sample of English possessive constructions.
Thus, an important aspect in constructing a query is to annotate a random sample of our corpus manual for the phenomenon we are interested in, and then to check our query against this manual annotation.
If we categorize data in terms of such a nominal variable, the only way to quantify them is to count the number of observations of each category in a given sample and express the result in absolute frequencies (i.e., raw numbers) or relative frequencies (such as percentages).
Thus, we can easily rank speakers in a sample of university graduates based on the highest degree they have completed.
But we cannot calculate a mean: if five speakers in our sample of ten speakers have a PhD and five have a BA, this does not allow us to claim that all of them have an MA degree on average.
For example, we can categorize a sample of speakers by their age and then calculate the mean age of our sample.
If our sample contains five 50-year-olds and five 30-year-olds, it makes perfect sense to say that the mean age in our sample is 40; we might need additional information to distinguish between this sample and another sample that consists of 5 41-year-olds and 5 39-year-olds, that would also have a mean age of 40 (cf. Chapter 6), but the mean itself is meaningful, because the distance between 30 and 40 is the same as that between 40 and 50 and all measurements involve just a single dimension (age).
It should be obvious that we can calculate the mean length of words or other constituents in a corpus, a particular sample, a particular position in a grammatical construction, etc.
Using these operationalizations for the purposes of the case studies in this chapter, I retrieved and annotated a one-percent sample of each construction (the constructions are so frequent that even one percent leaves us with 222 sand 178 of -possessives (the full data set for the studies presented in this and the following two subsections can be found in the Supplementary Online Material, file HKD3).
For example, 56.18 percent of all possessive constructions in our sample are s-possessives and 43.82 percent are of -possessives; if there were a random relationship between type of construction and givenness of the modifier, we should find the same proportions for the 183 constructions with old modifiers, i.e. 183 √ó 0.5618 = 102.81 s-possessives and 183 √ó 0.4382 = 80.19 of -possessives.
For expository reasons, we are going to look at a ten-percent subsample of the full sample, giving us 22 s-possessives and 17 of -possessives.
This value is known as the median -a value that splits a sample or population into a higher and a lower portion of equal sizes.
The mode is simply the most frequent value in a sample, so the modifiers of the of -possessive have a mode of 5 (or concrete touchable) and those of the s-possessive have a mode of 1 (or human) with respect to animacy (similarly, we could have said that the mode of s-possessive modifiers is discourse-old and the mode of of -possessive modifiers is discourse-new).
There may be more than one mode in a given sample.
To get samples of roughly equal size for expository clarity, let us select every sixth case of the of -possessive, giving us 25 cases (note that in a real study, there would be no good reason to create such roughly equal sample sizes -we would simply use all the data we have).
But with a hypothesis stated in terms of proportions, matters are different: even if the majority or even all of the cases in our data contradict it, this does not preclude the possibility that our hypothesis is true -our data will always just constitute a sample, and there is no telling whether this sample corresponds to the totality of cases from which it was drawn.
Unless we have a very specific prediction as to exactly what proportion of our data should consist of counterexamples, we cannot draw any conclusions from a sample.
But as we will see in the next subsection, we can always specify the exact proportion of counterexamples that we would expect to find if there was a random relationship between our variables, and we can then use a sample whether such a random relationship holds (or rather, how probable it is to hold).
Note that the probability of error depends not just on the proportion of the deviation, but also on the overall size of the sample.
In statistical terminology, this word simply means that the results obtained in a study based on one particular sample are unlikely to be due to chance and can therefore be generalized, with some degree of certainty, to the entire population.
The median animacy of all modifiers in our sample taken together is 2, 5 so the H 0 predicts that the medians of s-possessive and the of -possessive should 6 Significance testing also be 2.
Recall that the observed median animacy in our sample was 1 for the spossessive and 5 for the of -possessive, which deviates from the prediction of the H 0 in the direction of our H 1 .
In a first step, we have to determine the rank order of the data points in our sample.
If every rank value occurred only once in our sample, rank value and rank position would be the same.
As pointed out above, the value for the sample variance does not, in itself, tell us very much.
We can convert it into something called the sample standard deviation, however, by taking its square root.
The standard deviation is an indicator of the amount of variation in a sample (or sub-sample) that is frequently reported; it is good practice to report standard deviations whenever we report means.
A very general precaution against this possibility is to make sure that the corpus (or our sample) is balanced with respect to all potentially confounding variables.
In reality, this is difficult to achieve and may in fact be undesirable, since we might, for example, want our corpus (or sample) to reflect the real-world correlation of speaker variables).
The sparsity of the data in the table is due to the fact that the analyzed sample is very small, and this problem is exacerbated by the fact that the little data available is spread across too many categories.
Second, the expression collocates with verbs of expression (perhaps unsurprising for an expression relating to emotions); this, too, is true for our sample, where such verbs are found in 14 lines: reflect (line 2), show (lines 3, 9, 14, and 19), read (line 5), declare (line 6), disguise (line 7), reveal (line 8), hide (line 10), reveal (line 12), admit (line 13), give vent to (line 15), and tell (line 18).
First, note that Sinclair's approach is quantitative only in a very informal sense -he rarely reports exact frequencies for a given semantic feature in his sample, relying instead on general statements about the frequency or rarity of particular phenomena.
As we saw above, this is easy to remedy by simply determining the exact number of times that the phenomenon in question occurs in a given sample.
In order to determine this, we have to compare our sample of the expression true feelings to related expressions that differ with respect to each property potentially responsible for the semantic prosody.
One possibility is to look at each of the 348 cases in the sample and try to determine the gradualness or suddenness of the beginning they denote.
In cases like this, it may be advantageous to apply both methods and reject the null hypothesis only if both of them give the same result (and of course, our sample should be larger than ten cases).
We can avoid these problems by drawing our sample from the corpus itself.
Let us further limit our sample to cases where both nouns are monosyllabic, as it is known from the existing research literature that length and stress patterns have a strong influence on the order of binomials.
These non-primed comparatives are fairly evenly distributed -194 are analytic comparatives and 187 are synthetic, which is not significantly different from a chance distribution (ùúí 2 = 0.13, df = 1, ùëù = 0.7199), suggesting that our sample of adjectives does indeed consist of cases that are fairly evenly distributed across the two comparative strategies (see Supplementary Online Material for the complete annotated concordance).
The query will capture interrogatives, imperatives, subordinate clauses and other contexts that cannot contain tag questions, so let us draw a sample of 100 hits from both samples and determine how many of the hits are in fact declarative sentences with positive polarity that could (or do) contain a tag question.
Let us assume that we find 67 hits in the female-speaker sample and 71 hits in the male-speaker sample to be such sentences.
Depending on the tokenization of the corpus, this query might miss cases where the word containing the suffix -ness is the first part of a hyphenated compound, such as usefulness-rating or consciousness-altering; we could alter the query to something like ‚ü®[word=".+ness(es)?(--.+=)?"%c]‚ü© if we believe that including these cases in our sample is crucial.
Note that in order to compare type frequencies, we have to correct for the size of the sample: all else being equal, a larger sample will contain more types than a smaller one simply because it offers more opportunities for different types to occur (a point we will return to in more detail in the next subsection).
The TTR is the percentage of types in a sample are different from each other; or, put differently, it is the mean probability that we will encounter a new type if we go through the sample item by item.
An affix may have a high TTR because it was productively used at the time of the sample, or because it was productively used at some earlier period in the history of the language in question.
In other words, while the number of types and the number of hapaxes generally increase as the number of tokens in a sample increases, they do not increase at a steady rate.
However, note that -ify has a token frequency that is less than half of that of -ise/-ize, so the sample is much smaller: as in the example of lexical richness in Pride and Prejudice, this means that the TTR and the HTR of this smaller sample are exaggerated and our comparisons in Tables 9.4 and 9.5 as well as the accompanying statistics are, in fact, completely meaningless.
The simplest way of solving the problem of different sample sizes is to create samples of equal size for the purposes of comparison.
We simply take the size of the smaller of our two samples and draw a random sample of the same size from the larger of the two samples (if our data sets are large enough, it would be even better to draw random samples for both affixes).
There are 373 stem types occurring with -ic in the LOB corpus, with a mean length of 7.32 and a sample variance of 5.72; there are 153 stem types occurring with -ical, with a mean length of 6.60 and a sample variance of 4.57.
There are 314.31 degrees of freedom in our sample (as calculated using the formula in 16), which means that ùëù < 0.001.
This difference is not due to the respective sample sizes: the fiction section in the BNC is much larger than the newspaper section; thus, the difference token frequency would suggest that the suffix is more important in newspaper language than in fiction.
The following discussion is based on the full data from the fiction subcorpus and a subsample of the newspaper corpus that was arrived at by deleting every second, then every third and finally every 192nd example, ensuring that the hits in the sample are spread through the entire newspaper subcorpus.
Overall, there are 96 different types, 48 of which occur in both samples (some examples of types that frequent in both samples are relationship (the most frequent word in the fiction sample), championship (the most frequent word in the news sample), friendship, partnership, lordship, ownership and membership.
In addition, there are 36 types that occur only in the prose sample (for example, churchmanship, dreamership, librarianship and swordsmanship) and 12 that occur only in the newspaper sample (for example, associateship, draughtsmanship, trusteeship and sportsmanship).
This would mean that the hapaxes in any sample would overwhelmingly cease to be hapaxes -the larger our corpus, the fewer hapaxes there will be.
This might be due to the different methods used, or to the fact that I excluded business, which is disproportionally fre-9 Morphology quent in male speech and writing in the BNC and would thus reduce the diversity in the male sample substantially.
As before, we are defining what counts as a hapax legomenon not with reference to the individual subsamples of male and female speech, but with respect to the combined sample.
Many other fields that Schmid investigates make it much more difficult to come up with a plausibly representative sample of words.
Deignan studies this potential difference systematically based on a sample of more than 1500 hits for flame/s in the Bank of English (a proprietary, non-accessible corpus owned by HarperCollins), from which she manually extracts all 153 metaphorical uses.
In this case, since you are more interested in how your keyword is used in different registers, you would want 100 sample concordance lines randomly selected from spoken discourse, and 100 concordance lines randomly selected from academic prose.
To get those concordance lines for your sample, go to the main page of COCA, click on "chart" and type in "sa*_v" in the search box.
Click on the bar for spoken discourse (SPOK) and that will give you 100 sample cases when the verb "say" occurs in spoken discourse.
Type in "say_spoken" under "Create new list" above the sample concordance lines and save the list.
If we jump to the frequency band, we see that the word "the" occurs 22 times in this text sample.
Following the same steps outlined above, choose a sample from another register -for example, academic prose (marked as ACADEMIC on the list).
Follow the same steps to look at how many and what kinds of words you see in this text sample.
Look at your 100-item sample in each of the two registers and determine whether they are always at the beginning of a sentence or utterance.
If only one of these text types is included then the sample might not account for variation in the different types of news texts.
That is, the distribution in the "sample" could be projected to the distribution of the "population".
When the probability level is very low (p <.001), we can feel confident that we are not committing a Type 1 error described above, and that our sample group of students differs from other groups who may have taken the test in the past or who might take it in the future (population).
We test whether the data from that sample "fit" with that of the population.
In a negative directional hypothesis, the sample group will perform worse than the population.
With parametric tests, like One-Way ANOVA, we can generalize to the population that the sample was drawn from.
With parametric tests, like the Two-Way ANOVA, we can generalize to the population that the sample was drawn from.
In this small sample, we not only see differences in nouns ("English" is the third most frequent word in the individual corpus and "students" is the fourth most frequent word in the collaborative corpus) but also differences in function words ("in" is the fourth most frequent word in the individual texts and "of" is the fifth most frequent words in the collaborative texts).
The sample variance S 2 = P(1-P), and for a very small P value, it is roughly equivalent to P, namely x in this case.
VERB lemma, SPEAKER, TEXT, and COUNTY are non-repeatable effects, as a second study relying on randomly chosen verbs, speakers, texts, and counties would result in a different sample.
In the sample sentence, in winter is "thematised," and Hoey shows that this phrase when occurring in Theme position collocates in his corpus with the verb be, and with the names of places, again proportionally more frequently than the alternative phrases.
The chapter then presents a sample study of registers within a specific subject area -civil engineering -to exemplify several characteristics and challenges in more detail.
For example, in text sample 1, the students state that the analysis will provide a comparison, explore the effectiveness, and discuss alternative solutions -but the analysis itself cannot discuss effectiveness or other solutions.
One is simply to sample a larger number of texts, which will make it statistically more likely that the full range of internal variation is present in each period sample.
In total, the WARD corpus contains 26,573,826 words, spread across 159,181 letters, written by 130,659 authors -which is a far larger sample than would be possible if traditional methods for data collection had been applied.
Once generated, the concordance was saved and then a special command -"delete to N" -was used to reduce the concordance lines to a random sample of just 100.
In order to validate this percentage, a second random sample was generated to check consistency.
Six additional tokens in the 1980s sample were produced by an author from Portland, Oregon, and one from Seattle, Washington, which raised strong suspicion of Canadian influence.
Comparing the texts shared by ECCO and ECCO-TCP, we find that more than 90% of the hapax legomena in the ECCO sample (henceforth ECCO-OCR) are not found in ECCO-TCP.
However, by taking a small sample of the instances and going back to the document images, we discovered that about a third of the instances of "economy" were in fact OCR errors for "oeconomy".
This made accurate timing impossible, unless we were to take a more representative sample and check all the document images manually, which would have been a very labour-intensive task.
For example, the most basic sampling method, a simple random sampling, is not sensitive to the increase in available issues and might result in the second half of the 19th century being overrepresented in the sample.
This issue, too, can be illustrated with the help of the same sample corpus from Hiltunen (2021), Corpus A, which comprises complete issues of the four newspapers sampled at 10-year intervals.
This ensures the extracted sample corpus is reasonably clean and can therefore be expected to provide reliable output for standard corpus linguistic tasks.
The flipside is that this may potentially compromise the representativeness and balance of the filtered sample, and this needs to be evaluated on a case-by-case basis due to the uneven and erratic distribution of OCR errors in the database.
From a general standpoint, one could do as I did and simply use fiction in general as the reference, and asking the question of what makes this author stand out amongst a sizeable sample of other authors active during the same period.
In order to limit processing and memory requirements, we retained no more than 1,000 total occurrences per word and used a random sample for more frequent items.
A 15-word sample (91 clusters) was annotated by two annotators in order to test the reliability of the general procedure, obtaining a reasonably high interannotator agreement (Cohen's kappa coefficient of 0.55).
Each sample cluster contains three representative tweets published in Montreal and occurring in a single original cluster output by our analysis.
For each category that we discuss, sample clusters of tweets are provided in order to illustrate the contrast between the contact-related use -the target of our analysis -and the noise that we identified along the way.
A random sample of 2,000 words was taken for each MP, with MPs excluded who had used less than 2,000 words (thereby removing only 3 MPs).
Each MP sample was compared against every other MP using two similarity measures: Jaccard and Log Likelihood.
This process was repeated 10 times with a different 2,000 word random sample each time.
Note that 0.05 or 5% is the conventional cut-off point which can be imagined as the risk we are willing to take when inferring from the sample to the population (see p-value below).
In the example of a sociolinguistic research looking at the use of swearwords by men and women we would get a p-value that would give us the probability of seeing the observed or even more extreme difference between the two groups if the null hypothesis were true, that is, if there really was no difference in the population and the difference observed in the corpus (sample) was merely due to chance as the result of a sampling error.
Representative is a descriptor of a sample when it has similar characteristics to the population it is drawn from.
This allows us to draw conclusions about the population from the sample.
Ideally, this would be achieved by truly random sampling 10 where each text ever produced and each spoken interaction that has ever taken place would have the same chance of appearing in the sample.
To make the sampling manageable, corpus designers often start with a set of categories within which they aim to collect an unbiased sample.
By bias we mean a systematic but often hidden deviation of the sample from the population.
Corpus designers therefore need to reach out to different groups of contributors and use a range of incentives to obtain a representative sample.
If our corpus represents a sample rather than the population (which is typically the case), we should consider the amount of evidence we have in the sample.
Moving beyond the sample, 95% confidence intervals can be calculated.
For the purposes of your study you want to select an unbiased sample of 40 texts that would represent the population.
This form of standard deviation (SD p or œÉ [sigma]) differs slightly from the sample standard deviation (see below).
This is so-called sample standard deviation (SD).
The main options (there are others) include range (R), percentage range (R%), population standard deviation (SD p ), sample standard deviation (SD), coefficient of variation (CV), percentage coefficient of variation (CV%), Juilland's D and DP.
Following the coding scheme described above, two independent raters coded a random sample of 100 concordance lines from a total of 1,053 containing the word religion in the corpus.
Because when studying lexico-grammatical variation, we usually presuppose a certain uniformity among texts of the same type, we don't have to worry about this problem too much apart from making sure that in our sample, linguistic features come from a range of texts.
The standard deviations are calculated according to equation (2.11) from Chapter 2 (standard deviation sample).
In fact, it is the squared version of sample standard deviation (see Section 2.4), hence SD 2 .
The degrees of freedom is a complex concept, which is used when dealing with calculations based on a sample (corpus) rather than a population; in corpus linguistics, this is the case most of the time.
If we have two or more samples from each speaker and are interested in the difference in their language between sample 1 and sample 2 etc. (e.g. linguistic change/development), we are dealing with a so-called repeated measures design, which requires a different version of the statistical test (see below for more information).
If, on the other hand, as was my case, there is a lot of data available (the sample was approximately 200,000 words), the statistical approach is called for.
The second part of the riddle was clear and matched the type of language in the sample.
The women contribute one sample each, while there are 33 speech samples from the male group.
In the graph, it clusters in the proximity of the sample from Dee Dee Myers and far apart from the other samples.
Diachronic corpora sample different stages of language or discourse development across time.
Without a reliable description of the data, generalizing from the sample (corpus) to the population (language as such) using sophisticated statistical tests and p-values lacks grounding.
A function to exemplify this characteristic that is actually also very useful in a variety of respects is sample.
This function outputs random or pseudo-random samples, which is useful when, for example, you have a large number of matches or a large table and want to access only a small, randomly chosen sample or when you have a vector of words and want it re-sorted in a random order.
The functions ? or help, which provide the help file for a function (try ?sample ¬∂ or help(sample) ¬∂), and the functions args and formals, which provide the arguments a function needs, their default settings, and their default order (try args(sample) ¬∂ or formals(sample) ¬∂).
The result is unambiguous: The p-value is much larger than the usual threshold value of 0.05 so we conclude that the frequencies of the two constructions in Gries's sample do not differ from a random distribution.
As is obvious, when the sample is increased by one order of magnitude, so is the chi-squared value.
Since the data violated the assumption of normality, a U-test/Kruskal-Wallis test was computed, which showed that the difference between the two lengths is highly significant (for the U-test: W = 14,453, p < 0.001): In the population of English for which our sample is representative, direct objects are longer than subjects.
In a for-loop, we randomly reorder each play with sample, apply ttr to it, and collect the y-coordinates from its fifth component.
As a broad sample of the English language in general, it is suited to many different research aims.
The key concepts here are the notions of the corpus as a sample and of balance and representativeness.
Any corpus is fundamentally a sample.
Our interest is typically on what we can say about the population on the basis of the sample -and how confidently we can say it.
The very fact that the corpus is a sample means that we need to think carefully about how our sample relates to the population, which means we need a detailed and critical awareness of the process of sampling that has been used in corpus construction.
An important difference between sampling in corpus linguistics and in other sciences is that a corpus is never a random sample of the population.
In research such as opinion polling or medical trials, where the population is a literal population of people, the sample must be as close as possible to a randomly-selected subset of the population, such that every person has an equal likelihood of being selected to be in the sample.
Third, in the process of corpus construction we have to think about the relationship between the population and the sample in a somewhat different way to how this is done in fields where sampling can be done on a random basis.
Corpus linguists deal with a small fraction of all texts that have been produced in a language, so any corpus is always only a small sample drawn from the real-world population of texts (cf. 8.2.1).
We sample Obama's speech and many others that are also representative of the language population we are interested in.
And again, the same sampling considerations of general corpora apply for special corpora: we sample either to achieve proportional equivalence or in order to reflect the population as closely as possible.
Usually a tagger is trained on a smaller hand-annotated sample and then applied (tested) on a larger corpus.
Online you can find websites that help you practice your regular expressions on sample data.
We have a sample, for instance, of 30 Matukar Panau speakers, speaking for a total of 40 hours in a specific setting.
We can describe characteristics of that sample, such as the frequency with which people use serial verb constructions.
We can also infer characteristics about the whole Matukar Panau speaking population in all instances of speech from that sample, that is, how frequently all speakers under all circumstances will use serial verb constructions.
However, we are also aware, when we do this, that we cannot be certain about the population, only about the sample.
In corpus linguistics, sampling is rarely truly random (a random person from a population, or a random text from all existing texts), and is more often a convenience sample (who we can record, what we can find on the web, which newspapers we have access to, etc.) (cf. Chapters 6 and 10).
We sample to estimate parameters.
Parameters are values that characterise an entire population and statistics are estimates of those parameters within a specific sample.
You can use a chi-squared test if you have a random sample from the population of interest, these observations are independent, you have a minimum of five observations, and observations fall into clear categories (these are the assumptions of the chi-squared test that need to be met; the minimum requirement of 5+ observations is a rule of thumb for robustness sake).
Let's take a sample of the factors we looked at and apply them to our data to help understand the statistical methods.
Random effects reflect a random sample of the population we are interested in, for instance, a particular group of speakers from a population of all speakers, or a handful of texts from all texts that could be produced.
The p-value represents the chance that the null hypothesis would be true if we observed this sample of data.
A small sample is more likely to be affected by chance and we may see spurious results.
Corpus linguistics takes real samples that are used to generalise about language, and corpus-based typology helps us to confirm (or confront the idea) that these assumptions are applicable for a more diverse sample.
And even though the language in the latter sample seems fairly natural, we can still easily see that it comes from a scripted text, partly because of the indication of speakers (which I've highlighted in bold-face), and partly due to the stage instructions included in square brackets.
We can already see, therefore, from this relatively short sample of text that a failure to deal with this feature could cause issues in a number of places throughout the text.
When comparing the information in the manuals, you'll hopefully spot that the composition of the different corpora is roughly modelled on that of the Brown Corpus, with only small variations in categories and numbers of sample texts.
For a long time after the publication of the corpus in 1964, many people thought that 1 million words of text represented a general-enough sample to provide sufficient information about the makeup of the English language.
In principle, the former only applies to corpora for general use, though, as it's often assumed that these ought to provide an equal amount of materials from many different genres or areas of relevance that allow us to investigate a representative and realistic sample of the language and draw more or less universally applicable conclusions.
It is therefore best to test whatever capabilities your browser has in this respect and download a sample page, which you can then compare with the original.
As a preliminary awareness-raising exercise, go to the online resource pages and open the page for 'Concordancing', which contains a short sample paragraph, constructed for illustrative purposes.
As you'll hopefully observe while doing the online exercise (and can verify again in the exercise paragraph replicated below), the computer search for our fairly common sample words managed to find this, but not This, in as a word, but also many other occurrences of the character (letter) sequence i+n (where both are lowercase characters) that you would probably not have expected to find.
The only reason why all of them will get highlighted in the sample paragraph is because the script that allows you to display them identifies them globally, that is, each and every one of them individually.
Inspect the results visually by reading through them and trying to identify roughly what kind of a distribution in terms of nouns and verbs you may have in your random sample.
To illustrate the difference, here are two short samples that only contain the first hit and all associated meta-information: This first sample contains only the reference numbers, while the next one contains the full textual descriptions, where 'n/a' (not applicable) essentially indicates that those fields don't apply to written, but only spoken, texts.
For reasons of space, we won't look at further examples of other types of annotation here now, but will later work more extensively on a sample that combines some of the different levels.
The following is a brief sample paragraph that illustrates what a paragraph annotated in XML might look like in terms of elements and attributes.
Go through the above sample paragraph and make a list of how many elements there are and which type they belong to.
Because these processors generally parse the XML and often produce tree structures based on the hierarchy of the elements, they'd thus have rendered our sample in the same way, even without the additional line breaks I suggested you insert for readability.
To answer the question, a representative sample of Tyneside speech is collected and relevant data is abstracted from it.
The sample used here is the Diachronic Electronic Corpus of Tyneside English (henceforth DECTE), a collection of interviews with Tyneside English speakers that will be fully described in due course.
One approach would seem to be to try and tweak hyperparameters of fitting trees, such as (1) the minimum decreases in deviance that define when trees stop splitting, (2) the minimum sample sizes per node, or (3) the tree depth.
In the detailed sampling process, it is decided exactly what texts or text chunks to include.
There is a range of possible considerations to take in deciding about sampling procedures for a corpus, one of which concerns to what extent to organize the overall design around text production or text reception.
While they often make it possible to search A. √Ñdel the archive, they may not make the text files downloadable other than one by one by clicking a hyperlink.
For data in the public domain, such as published fiction or online newspaper text, it is not necessary to secure consent.
A plain text format (such as .txt) is often used for corpus files.
The corpus builders have thus augmented the existing data by adding various types of information-about the speakers (sex, age, social class, region of origin, educational background), about the text (type of narrative; whether a stand-alone story or part of a 'narrative chain') and about the utterance (the roles of the participants vis√†-vis the narration; type of quotative verb used to signal who said what in a narrative; to what degree the discourse is represented as being verbatim or more or less indirect).
While the raw text of an unannotated corpus has its own unique value, (wisely) annotated corpora offer great advantages over the raw text when it comes to the investigation of linguistic phenomena.
Although current English-language lemmatization tools make this process much easier to carry out on large bodies of text, it is often worth bearing in mind that even the most sophisticated lemmatization software will inevitably run into cases that are not entirely clear-cut (e.g., should the lemma of the noun axes be AXE or AXIS?) and where the resulting lemmas are not necessarily what one might expect.
While syntactic structures can be assigned to sentences manually (and may be necessary when developing a corpus for a language for which few syntacticallyannotated corpus resources have already been developed; see Sect. 2.3), it is more common for syntactic annotations to be added automatically by a syntactic parser, a program that provides information about different kinds of syntactic relationships that exist between words in a given text (parses).
By tracing these relationships, it is possible to extract features of corpus sentences-subjects of active and passive verbs, objects of prepositions, etc.-that would be difficult to retrieve from searches of unannotated text alone.
This involves manually annotating samples of corpus text as examples of how the new annotation categories should be assigned.
Creating a new POS tagging model is often an iterative process: small samples of text are manually annotated and used as training data to create a provisional POS tagger, which then automatically annotates more texts.
As presented in Chaps. 1 and 2, annotations and metadata are sometimes relatively simple: markup can be added to set them apart from the text and control their inventories.
This is often only possible if tools ensuring consistency across layers are developed (e.g. the underlying text, and perhaps also tokenization must be kept consistent across tools and formats).
The graph-based data model allows for completely independent annotation layers, united only by joint reference to the same primary text.
On one level, researchers, language instructors and other users would like to be able to search through learner data directly: the base text is, trivially, whatever a learner may have written.
This can be especially tricky, as there is often some variation in how these items appear across a corpus (or even within a text); for example, although the hyphenated form, well-being, is clearly the most common option found in the Corpus of Contemporary American English (COCA) (>8000 occurrences), the forms well being and wellbeing each occur several hundred times.
If we then want to use these lists to profile lexis in other texts or corpora, the same procedure is necessary for each new text.
This text provides a comprehensive, practical, and very accessible discussion of important issues in frequency list design and evaluation.
They allow us to see what words appear (and do not appear) in a text, and give an indication of their prominence if we sort the list in frequency order.
Beyond the world of the corpus linguistic researcher, frequency lists can be used directly or indirectly to support language learners by providing a way to focus on the more frequent words in a text, or suggesting priorities for language teachers when preparing lesson materials.
Again, the example of bogus refugees is cited in this text in order to be critical of it, arguing that such representations do not help to support genuine refugees, although the speaker still makes a distinction between genuine and bogus refugees.
To date, most concordancing research has been carried out on corpora of plain text.
Secondly, we only list monolingual concordancers, i.e. tools that let the user examine text from one corpus representing one language.
However, some languages are more suited to the tasks commonly carried out by corpus linguists, i.e., reading large text files into memory, processing text strings, counting tokens, calculating statistical relationships, formatting data outputs, and storing results for use with other tools or for later use.
Incorporating text dispersion into keyword analyses.
To achieve this goal, they compare various keyness measures that are primarily based on the frequency of words in a corpus together with their own "text dispersion keyness" measure, which is based on the number of texts in which a word appears.
Using Python scripts as part of their analysis, the authors are able to quickly and accurately compare a range of possibilities for calculating keyness, including their own "text dispersion keyness" measure, which is not yet available in ready-built tools.
This library is used to parse HTML files and extract plain text.
For this case study, imagine that a toy corpus that comprises just three UTF-8encoded, plain-text files needs to be processed.
Letters, for instance, have several advantages as a source of historical text material.
Moreover, this is one area where, despite their merits, current big data projects may (for now) be exacerbating the problem by their tendency to go where the data is and harvest text materials indiscriminately.
For example, given a rich historical record and a long tradition of high-quality text editions, it is perfectly possible to create a sizeable corpus of Old French, as shown by the FRANTEXT database.
Besides awareness of these difficulties, a more bottom-up and data-driven approach to describing diatypic text properties may, in the long term, provide the more satisfactory solutions.
Added to this is a layer of inadvertent 'noise' created along the way as a corpus text travels from historical manuscript or print to digital edition.
It contains some of the oldest French texts from 950 up to the present-day and, with currently about 300 million words of text, it decidedly qualifies as a large corpus.
With its 24.4 million words it is a sizeable corpus, but what really sets it apart is the kind of text material it contains, with published transcripts of the spoken interactions in court.
More bottom-up text-based approaches to text classification, for instance, can reduce the need to rely on more aprioristic classifications.
Complex questions, involving large data sets and requiring specialized knowledge of diverse domains such as language history, text traditions, and computational techniques, can be handled if information passes between specialists efficiently.
Options to avoid the time-consuming manual orthographic transcription are beginning to materialise: for example, it is possible to train speech-to-text software such as the one included in GoogleDocs on one's voice and then record oneself repeating the content of the raw data file.
It is important to point out from the outset that the term parallel corpus is to some extent ambiguous, because it is sometimes used to refer to comparable original texts in two or more languages, especially texts that 258 M.-A. Lefer belong to comparable genres or text types and deal with similar topics (e.g. Italian and German newspaper articles about migration or English and Portuguese medical research articles).
In cross-linguistic research, it is of paramount importance to know, for instance, whether a given text was translated from Spanish into German or vice versa, because corpus studies have shown that translation direction influences translation choices, and hence the linguistic make-up of translated text (e.g. Dupont and Zufferey 2017).
In practical terms, this means that a text originally written in, say, Slovenian or Dutch is first translated into English.
This imbalance can take several forms: either there are simply fewer texts translated in one direction than in the other (especially when the language pair involves a less "central", or more "peripheral", language), or certain text types are only (or more frequently) translated in one of the two directions.
It is also important to stress here that the concepts of source language and source text are becoming increasingly blurred.
In multilingual translation projects, there are also cases where there is no single "source" text, as translators translate a given text while accessing some of its already available translations (e.g. when confronted with an ambiguous passage).
As is often the case in corpus-based translation studies, parallel data are used with a view to identifying the source-text items/structures that have triggered the use of a given item/structure in the translations (in this case, the presence of a trigger term in the English source texts, such as unit, job, or team).
Second, whenever possible, we should go beyond the inclusion of translated novels, news, and international organizations' legal and administrative texts, and strive for the inclusion of more genres and text types, especially those that are dominant in today's translation market, to which corpus compilers have had limited access to date, for obvious reasons of confidentiality and/or copyright clearance.
Including different versions of the same translation would also prove to be rewarding (e.g. draft, unedited, and edited versions of the translated text).
For example, most popular part-of-speech taggers are trained on newspaper text, a register which tends to follow fairly strict style guides and contain few errors.
Blogs offer similar advantages when it comes to boilerplate removal and text dating (see Representative Corpus 2 for more information on the Birmingham Blog Corpus) and are one variety of so-called 'Web 2.0' content, which is being used increasingly in linguistic research.
Monomodal text-only corpora in English are sometimes annotated automatically with the use of 'taggers' and 'parsers'.
It will be interesting to see whether some of the major insights generated within monomodal, or text-based, corpus linguistics will be replicated once we are able to truly analyse multimodality at scale.
Dimension scores are calculated for each text sample by adding up standardized frequencies with salient positive loadings and subtracting salient negative loadings on a dimension.
The next fifteen columns correspond to the text categories.
These 15 columns correspond to the 15 text categories.
For example, we know that the use of nouns and adjectives in text is strongly correlated.
A multiple linear regression analysis can determine whether there are factors that allow us to predict whether a given text will have a relatively high TTR value or a relatively low one.
This means that there are two independent variables that can be used to make predictions about the TTR values of any given text: One independent variable is time, which is a continuous variable expressed in a continuous sequence of numbers that represent successive years.
For the genitive alternation, the text ID and the possessor head lemma are used as crossed random effects.
For the dative alternation, the model includes the text ID, the register (which nests the text ID) as well as the lemma of the theme argument and the verb.
At the moment of writing, the only working method for dealing with dependent observations seems to be including the grouping factor (e.g. the speaker or text IDs) on a par with all other predictors.
It should be noted here that there is a budding line of meta-analytic methods largely outside of linguistics that attempts to use text mining to carry out certain aspects of the synthetic coding process.
Meta-analyses provide strong evidence of validity generalization (e.g., across text types, languages, linguistic features, registers).
By training computer algorithms on extensive corpora, researchers have been able to develop language models that possess the ability to understand and generate text that resembles human language.
For instance, consider the task of part-of-speech (POS) annotation (i.e., determining the correct POS of each word in a given sentence of a text).
For example, for a question-answering system, some of the typical features (or functions) involve the recognition of a linguistic expression as a question, analysis of the type of the question, understanding the syntactic structure of a question, capturing meaning embedded into it, recognizing the sentiment hidden in the question, searching out an appropriate document that may carry an answer, extraction of the answer from a document, evaluation of the answer before responding, and finally generating the answer either in text or speech mode.
All text samples should be collected from genuine use of speech and writing.
The text should be free from any annotation that carries linguistic and extralinguistic information.
It is known to us that corpora can be of many types with regard to texts, languages, modes of data sampling, methods of corpus creation, manners of text annotation, and goals of text utilization, etc.
It also implies that the kind of text included as well as a combination of different text types is crucial in classifying corpora.
Each corpus developer is, however, free to adopt unique methods in language representation, text collection, and text application.
It also depends on the decision of how many text categories are kept in a corpus, how many text samples are put in each category, and how many words are kept in each sample.
Although size does matter on the scale of the 'reliability' of a corpus, in reality, a corpus, however big it may be, is a small representation of the vast amount of speech and text varieties that are produced in a language.
A small corpus with wider text variations is a much better representative than a large corpus with fewer text variations.
In any corpus, the greater the number of unique text samples, the higher the reliability of the analysis of linguistic variables.
The Brown, LOB corpus, and Survey of English Usage (SEU), for instance, contain a wide variety of text types, and therefore, they are considered much better representatives of English.
However, the BNC and the ANC, which are not only very large but also far more diversified in structure and text types, empirically settle the issues relating to size and representation.
In the case of a special corpus, we like to include text samples produced by non-native users as the aim of this corpus is to highlight linguistic peculiarities that are typical to non-native users.
It is necessary to decide if a corpus should contain all types of written text samples or specific sets of text samples.
A general corpus usually includes standard text samples as it aims to identify what are central and common features of a language.
Therefore, we can furnish a general corpus with standard text samples of contemporary writings and a calculated and proportional representation of texts can suffice our requirement.
On the other hand, in case of a monitor corpus, which tries to represent all possible text varieties, we may include all non-standard and ordinary text samples along with standard and established text samples.
A heterogeneous monitor corpus includes text samples from all subject areas and the individuality of the source of a text and its writer are well attested.
The task of sampling of texts has to be done with collected text materials based on the character of a corpus.
Alternatively, we may apply selective sampling methods that are used in the Brown Corpus and the LOB corpus or consider more suitable methods of text representation keeping in mind the goal and purpose of a corpus.
There are many linguistic and technical issues relating to corpus sanitation (e.g., text normalization, orthographic error correction, spelling error correction, real word-error correction, grammatical error correction, punctuation error removal, and tokenization).
In qualitative statistical analysis, on the other hand, we look at multidimensional analysis of observed phenomena in a corpus and try to find factors behind the occurrence of a feature or an element in a text.
Since a corpus contains text samples either from a written or spoken discourse, annotation is done by adopting a value-encoding process, which attaches interpretative information at different levels of a text.
Interpretative information is classified into two types: (i) intralinguistic information, which is linked with linguistic features and properties of a text, and (ii) extralinguistic information, which is related to information not linked with linguistic properties and features of a text.
While intralinguistic information helps to understand internal issues and aspects of a text, extralinguistic information provides external information based on which we have a better insight and interpretation of a text.
Both types of information are annotated to a text-either attaching them to the elements of a text or interspersing them within a text.
The primary purpose of the annotation is to add additional information to a text so that future analysis and interpretation of a text becomes easy, accurate, and useful.
Compared to raw text, annotated text is complex in structure and difficult to access, but easier in analysis, better in interpretation, and more user-friendly in an application.
In extralinguistic annotation, we annotate a text with that kind of information, which is not physically available inside a text.
For instance, when we annotate a text at an anaphora level, there is no apparent information available in a text-based on which we can identify how words are linked with others in relation to co-reference.
The primary goal of semantic annotation is to identify the sense a word denotes when it is used in a text.
We do this to understand discourse and pragmatic strategies deployed in a text.
An etymologically annotated text addresses all the questions and challenges related to the etymology of words.
We identify and mark various rhetorical devices used in a text.
The use of rhetorics is a common practice in text generation.
Each piece of text (either spoken or written) contains a few rhetorics-either explicitly visible or implicitly embedded.
Analysis of rhetorics sheds new insights into the theme and structure of a text.
For instance, a piece of text made with rhetorical devices is different from a text without rhetorics.
A rhetorically developed text differs in choice of words, order of words, sentence structure, grammatical form, sense implication, and reference.
For instance, we have not yet investigated how a poetic text should be annotated; and what kinds of textual, linguistic, stylistic, prosodic, and figurative annotation techniques we should design and apply to poetic texts.
In a purely lexical corpus (i.e. a corpus containing just the text), only individual words (or sequences of words) can be searched for.
Other sections of the chapter focus on additional issues relevant to planning the construction of a corpus, such as how to determine the appropriate size of a corpus and the length of particular texts that the corpus will contain (complete texts versus shorter samples from each text, e.g. 2,000 words); how to select the particular genres to be included in a corpus (e.g. press reportage, technical writing, spontaneous conversations, scripted speech); and how to ensure that the writers or speakers analyzed are balanced for such issues as gender, ethnicity, and age.
The chapter concludes with a detailed description of various kinds of textual markup and linguistic annotation that can be inserted into a text.
Topics discussed include how to create a "header" for a particular text.
Headers contain various kinds of information about the text.
For instance, for written texts, the header would include such information as the title of the text; the author(s); if published, where it was published; and so forth.
Other textual markup is internal to the text, and in a spoken text would include such information as speaker IDs, and the beginnings and ends of overlapping speech.
In addition, there is an Appendix that contains a listing of all corpora discussed in the text with brief descriptions of their content as well as links to where further information on the corpora can be obtained.
The MPC satisfies this criterion too: each sentence in the corpus was selected from some larger text of which it is a part.
In the British component of the International Corpus of English (ICE-GB), ethnographic information on speakers and writers is stored in a database, and a text analysis program designed to analyze the corpus, ICECUP (the International Corpus of English Corpus Utility Program) can draw upon information in this database to search by, for instance, age or gender.
Because so much text is now available in computer-readable form, many stages of dictionary creation can be automated.
And as Furthermore, if the lexicographer desires a copy of the sentence in which a word occurs, it can be automatically extracted from the text and stored in a file, making obsolete the handwritten citation slip stored in a filing cabinet.
This decision is important because it will determine exactly what the corpus will look like: what types of texts will be included in it (e.g. spoken, written, both), how long the individual texts will be (e.g. full length, text excerpts), how detailed the annotation will need to be (e.g. word-class tagging or a purely lexical corpus with minimal annotation), and so forth.
For instance, if the corpus is to be used primarily for grammatical analysis (e.g. the analysis of relative clauses or the structure of noun phrases), the corpus can consist simply of text excerpts rather than complete texts, and will minimally need part-of-speech tags.
However, some of the texts had to be typed in manually because they did not appear in edited editions, a very "laborious method of keying in the text" (www.helsinki.fi/varieng/CoRD/corpora/ CEEC/generalintro.html).
The ultimate length of a corpus might therefore be better determined not by focusing too intently on the overall length of the corpus but by focusing more on the internal structure of the corpus: the range of genres one wishes to include in the corpus, the length and number of individual text samples required to validly represent the genres that will make up the corpus, and the demographic characteristics of the individuals whose speech and writing will be chosen for inclusion in the corpus.
In the Helsinki Corpus, text samples range from 2,000 to 10,000 words in length.
While many corpora contain only text samples, others contain entire texts.
Ideally, it would be desirable to include complete texts in corpora, since even if one is studying grammatical constructions, it is most natural to study these constructions within the context of a complete text rather than only part of that text.
Of course, just because only text samples are included in a corpus does not mean that sections of texts ought to be randomly selected for inclusion in a corpus.
Likewise, for written texts, one can include the first 2,000 words of an article, which contains the introduction and part of the body of the article, or one can take the middle of an article, which contains a significant amount of text developing the main point made in the article, or even its end.
For instance, personal letters are often less than 2,000 words, and a text sample can be comprised of complete letters totaling 2,000 words.
But even though it is possible to include coherent text samples in a corpus, creators and users of corpora simply have to acknowledge that corpora are not always suitable for many types of discourse studies, and that those wishing to carry out such studies will simply have to assemble their own corpora for their own personal use.
And as more text is considered, there is a greater chance (particularly in humanities texts) that new words will be encountered.
For corpus linguists, this involves determining how many text samples need to be included in a corpus to ensure that valid generalizations can be made about the genre, and what range of individuals need to be selected so that the text samples included in a corpus provide a valid representation of the population supplying the texts.
The BNC is defined as "a monolingual British English corpus: it comprises text samples which are substantially the product of speakers of British English" (www.natcorp.ox.ac.uk/docs/URG/BNCdes.html#spodes).
If the residence history on the biographical form is unclear, it is also possible to interview the individual afterwards, provided that he or she can be located; if the individual does not fit the criteria for inclusion, his or her text can be discarded.
However, this does not necessarily mean that the text must be excluded from the corpus, since there is annotation that can be included in a corpus indicating that certain sections of a sample are "extra-corpus" material; that is, material not considered part of the corpus for purposes of word counts, generating KWIC (key word in context), and so forth.
If a text is collected, and saved for later computerization and annotation, the individual who collected the text may not be around to answer questions, and information about the text may consequently be lost.
Of course, logistical constraints may necessitate collecting texts at one stage and computerizing and annotating them at a later stage, in which case it is crucial that as much information about the text be obtained initially so that, at a later stage, those working with the text will be able to easily recover this information.
After the project began, it was discovered that not all the regional groups involved in the project would be able to collect all of the text categories originally planned for inclusion in the corpus.
If attempts are made to secure permission to use copyrighted material in a corpus, it is best to over collect the number of texts to include in each part of the corpus: A written text may be gathered for possible inclusion in a corpus, and its author (or publisher) may not give permission for its use, or (as was a common experience gathering written texts for inclusion in the American component of ICE) contact is made requesting permission but no reply is ever received.
For ICE-USA, research assistants filled out a written checklist supplying specific information for each spoken and written text that was collected.
First of all, each text was assigned a number that designated a specific genre in the corpus in which the sample would be potentially included.
After a text was numbered, it was given a short name providing descriptive information about the sample.
For each spoken text, a record was kept of when the text was recorded, where the recording took place, who was recorded, who did the recording, and how long the recording was.
For written texts, a complete bibliographical citation for the text was recorded along with the address of the publisher or editorial office from which permission to use the written text could be obtained.
In addition to keeping records of the texts that are recorded, it is equally important to obtain ethnographic information from individuals contributing either a sample of speech or a written text.
This database can contain not just information taken from the forms described above but other information as well, such as whether the text has been computerized yet, whether it has been proofread, and so forth.
It is therefore crucial to the success of any corpus undertaking that accurate information be kept about each text to be considered for inclusion in the corpus.
First, because texts to be included in a corpus will be edited with some kind of text editing program, it may be tempting to save computerized texts in a file format used by a word processing program (such as files with the extension .doc in Microsoft Word).
In earlier corpora, the standard was the ASCII (or text) file format, a format that had both advantages and disadvantages.
The main advantage at the time was that ASCII was a universally recognized text format that could be used with any word processing program and the numerous software programs designed to work on corpora, such as taggers and parsers and concordances.
Once a draft version of a text contains "metadata" and "textual markup", it can then be placed into a "lexical (pending proofreading)" directory to indicate that the text will be ready for use as a lexical version of the corpus once it has been proofread.
While a text is being worked on at a particular stage of analysis, it receives an additional file extension to indicate that work on the text is in progress.
For instance, while a draft version of a text in the category of business transactions is being created, the text is saved as "S1B-071di", the "i" indicating that work on the text is incomplete.
As a particular text is being worked on, a log is maintained that notes what work was done on the text and what work needs to be done.
At each stage of analysis, to avoid duplication of work, it is most efficient to have a single person work on a text; at the proofreading stage, it is best to have the text proofread by someone not involved with any prior version of the text, since he or she will bring a "fresh" perspective to the text.
Finally, although inserting "structural" markup into a text is separate from the process of actually computerizing the text, there are many instances where markup can be inserted while texts are being computerized.
If annotating overlapping segments of speech is done separately from the actual transcription of the text, the individual doing the annotation will have to go through the tape repeatedly to reconstruct the overlapsa process that could be done more efficiently by the person doing the transcription.
With written texts, if two line breaks are inserted between paragraphs while a text is being computerized, then paragraph tags can be inserted automatically at a later stage.
Of course, some markup is probably better inserted after a text sample is computerized.
But because computerizing and annotating a text is such an integrated process, it is best to combine the processes whenever this is possible.
Consequently, before any transcription is undertaken, it is important to decide just how much that exists in a spoken text one wishes to include in a transcription of it.
The Santa Barbara Corpus of Spoken American English, for instance, contains not only an exact transcription of the text of a spoken conversation (including hesitations, repetitions, partially uttered words, and so forth) but also annotation marking various features of intonation in the text, such as tone unit boundaries, pauses, and pitch contours.
The main drawback of this kind of detailed transcription is the lengthy amount of time it takes to annotate a text with information about intonation.
The advantage is that very detailed information about a spoken text is provided to the user, thus ensuring that a broad range of studies can be conducted on the corpus without any doubt about the authenticity of the data.
As Cook (1995: 37) notes, a spoken text is made meaningful by more than the words one finds in a transcription: How a conversation is interpreted depends crucially upon such contextual features as paralanguage (e.g. gestures and facial expressions), the knowledge the participants have about the cultural context in which the conversation takes place, their attitudes towards one another, and so forth.
Likewise, because written corpora were encoded in text files, there was no way to indicate certain features of orthography, such as italicization or boldface fonts.
In contrast, a written text will contain features of orthography, such as paragraph boundaries or font changes.
To prevent this, the ICE Project has special markup that encloses the entire sequence of repetitions (<}_><}/_>) and then places special markup (<=_>the police boat<=/>) around the last instance of the repetition, the only instance counted in analyses done by ICECUP, the text analysis program used in the ICE Project.
Because writing is linear, it is not difficult to preserve the "look" of a printed text that is converted into an electronic document and transferred from computer to computer in text format: although font changes are lost, markup can be inserted to mark these changes; double-spaces can be inserted to separate paragraphs; and standard punctuation (e.g. periods and commas) can be preserved.
Because written texts are primarily linear in structure, they can easily be encoded in text format: Most features of standard orthography, such as punctuation, can be maintained, and those features requiring some kind of description can be annotated with a TEI-conformant tag.
Consequently, much of what could potentially be annotated in a written text is likely to be of marginal interest to future users of the corpus.
But other text types can prove more difficult to parse, resulting in lower accuracy rates.
While voice recognition software has made tremendous progress recently, it still works best on monologic speech: an individual speaking slowly into a microphone that is then transferred into written text in a document.
Written texts, in contrast, are now widely available in digital formats and can easily be incorporated in a corpus after permission has been received to use a given text.
It is also the case that voice recognition software will continue to improve so that in the future it could very well be possible to automate the process of converting speech to text, and thus expedite the inclusion of, for instance, spontaneous dialogues in corpora.
Also included is a short context containing a span of text that precedes and comes after the search terms.
The size of the search window can be widened or narrowed depending upon how much text is desired both before and after the search term.
For instance, the opening of the section in ECL1 on "Structural Markup" makes reference to ASCII (American Standard Code for Information Interchange) text formats and standard generalized markup language (SGML).
If we have a paper version of that book, finding these passages will be a long and tedious task, which will require going through the entire text.
Similarly, in the first half of the 20th Century in the United States, the so-called distributionist approach to syntax focused on the study of sentence formation in syntactic structures as they appeared in text corpora, and from there, tried to infer language's general functioning.
In all these fields, the use of text corpora has been obvious for a long time and corpora use was never interrupted as a result of Chomsky's work.
First of all, in order to verify that all occurrences found when looking for the verb to love correspond to expressions of love as a feeling rather than to modal uses as in the phrase "I would love it that you kept quiet", it is necessary to examine each occurrence and thus browse the entire text.
For example, if we want to know whether learners of French as a foreign language at an advanced level are able to use collocations as native speakers do (collocations such as "prendre une d√©cision" -to make a decision -or "pleuvoir √† verse" -to pour with rain), we can search for occurrences of these expressions in text corpora produced by learners and compare the number of times these expressions appear -and their frequency -in a corpus of similar textual productions made by native speakers.
Likewise, a text produced by a learner is also natural, insofar as it is produced in its usual conditions, without there having been any particular manipulation.
The advantage of these corpora is that they have been designed to contain a set of texts representative of the language, or a part of the language to be studied, with a balanced representation of the different text genres, for example.
More rarely, certain corpora contain a syntactic analysis of all of their sentences, as well as other types of information, such as an annotation of the discourse relations (cause, condition, etc.) which interconnect the sentences within the text corpora.
The results indicated that text messages differ from other text genres in that 73% of them do not contain an opening and/or closing expression.
Inexperienced writers most frequently produce text messages with opening and closing expressions.
However, the choice of excerpts is often complex, since the identification of interesting characteristics to be studied in a text is not always visible at a first reading.
Another advantage of the quantitative analysis of literary texts is that it provides clues for identifying the author of a text when the latter is unknown or controversial.
In this case, a series of linguistic indicators are used for measuring the formal similarities between this text and other reference works by different possible authors.
The problem then is to find out the best indicators for identifying the similarities between an author's text and its differences with the texts of other authors and to later apply these measures to the controversial text in order to determine which author this text shares more similarities with.
This is particularly the case of requests concerning the attribution of a text to one or more alleged authors.
We will discuss the particularities of translations as a text genre and show that, due to these particularities, they cannot be used as if they were original language texts.
In addition, usage conventions may vary considerably between languages even when the same text genre exists in both, which makes them difficult to compare.
Finally, journalistic text translations are generally carried out by journalists, who are language professionals but not translation professionals.
In this way, we can see how corpus linguistics can provide translators with tools that may help them adapt their translation choices on the basis of a better identification of the recurrent linguistic properties at work in a text.
For example, lexical simplicity implies that the number of different words should be smaller than in an original text.
For example, the AntConc concordancer that we discuss can only process text format files (or files with XML or HTML tags which can also be treated as text files).
In the field of new media, the CoMeRe database includes communication corpora mediated by networks, such as SMS/text messages, tweets, blogs, etc.
This is the case of the EMA √©crits scolaires corpus (Bor√© and Elalouf 2017), a longitudinal corpus bringing together the text productions of primary and middle school children.
The corpus includes images of handwritten texts, raw transcriptions in text format and an annotated transcription, also in text format.
All of these can be downloaded in text format.
For a start, AntConc can only read text format files.
So, to begin with, it is necessary to convert the files included in the corpus to text format.
Depending on their origin, this can be rather easy or more difficult: usual word processors generally have a text format saving functionality, but extracting text from a PDF file can be difficult.
AntConc can also read XML files, since these contain text which is accompanied by tags.
However, this encoding does not correspond to text files containing French characters, because of accented characters.
When the text of a file is not displayed correctly in AntConc, the encoding used can be changed to make it match the file's encoding, for example, the ISO 8859-1 format (Latin.1).
It is actually not always easy to find texts available in a digitized format for all text genres, and even when such texts exist, they might not all be usable due to copyright issues.
We can be sure about one thing: these criteria should not be related to the linguistic content of the samples, but rather to a classification made on the basis of external criteria, such as text genres and language registers.
Even when working within a text genre, we should aim to diversify its sources as much as possible.
These differences inevitably induce a certain bias towards specific text categories.
For example, if the goal is to reach a sample of 200,000 words per text genre, this number of words can almost be instantly reached by including one or two entire books in the sample.
For other types of text such as letters, text messages, etc., the units are so small that it makes no sense to not fully include them.
In all cases, it is important to systematically avoid including the same portions of text, for example, always the beginnings or the endings.
However, retrieving text from digital formats may have varying degrees of difficulty depending on the original format; for example, it can be very difficult to isolate the texts of different articles in a journal page formatted as a PDF document, or even impossible when the PDF file includes text images.
So, even when we are working with data in a digital format, the task of converting the original format into a usable format, based on the text, is still necessary, as we will see below.
As we saw in Chapter 5, most concordancers (like AntConc) only read files in text format, which can include texts tagged in XML format.
Therefore, all newly created files for a corpus should be directly saved into text format.
Files saved in these formats can be easily transformed into text files thanks to specific options such as the "save as" command found in word processors.
These later become visible when a file is opened with an editor in pure text format.
In order to avoid this problem, software should be used for retrieving text from these files and eliminating unnecessary markings (HTML tags).
Word processing software often include this feature, simply by choosing the "text format" option from the "save as" command.
This software can be downloaded for free and used for converting any number of XML, HTML or sometimes even PDF files into text format.
In this way, it is later easier to combine them in different ways for creating sub-corpora, rather than having to retrieve text portions from a larger file.
We have pointed out that corpus files should contain plain text, in order to facilitate data analysis.
The header section contains metadata, a description of the file, the encoding, the text profile, mainly the language, the context or participants, and even a history of its revisions.
The body of the text mainly contains tags, which are destined to delimit text units such as paragraphs or even sentences.
As we will see in Chapter 7, TEI tagging can also be used for making more detailed annotations than dividing the text into sentences.
A final and very important point is to ensure that copyright is respected, either because the text is copyright-free or because the author has provided written consent granting permission to include their text in the corpus.
Though it is acceptable to quote portions of a text while indicating its source, these quotations should not exceed a clearly defined size limit.
Furthermore, the annotation itself is accompanied by a hermeneutic dimension, which reflects the annotators' point of view about the text.
Being able to annotate relations is also essential for associating anaphoric relations in a text.
In this language, the elements of a text are marked up using named tags including one or more attributes.
Concordancers generally make it possible to export the data retrieved in text format.
What is interesting here is that even if the value of 120 in text no. 10 was replaced by 1,200, the median would be the same as before.
Indeed, we can observe that text no. 10 contains almost three times more passive sentences than text no. 1.
If text no. 10 were roughly three times longer than text no. 1, the two texts would be equivalent in terms of the proportion of passive sentences.
In the text of an article, the two frequency elements can be reported as follows: "There were 94 occurrences of parce que in the corpus, corresponding to a relative frequency of 3.61 occurrences every 10,000 words".
The second part of the book presents a range of case studies from the domains of lexicology, grammar, text linguistics and metaphor, including variationist and diachronic perspectives.
Let us define a corpus somewhat crudely as a large collection of authentic text (i.e., samples of language produced in genuine communicative situations), and corpus linguistics as any form of linguistic inquiry based on data derived from such a corpus.
Instead, it seems more likely that the construction is simply too infrequent to occur in a sample of one hundred million words of text.
The first chapter of this book started with a similar definition, characterizing corpus linguistics as "as any form of linguistic inquiry based on data derived from [...] a corpus", where corpus was defined as "a large collection of authentic text".
First, any text, spoken or written, will lose not only its communicative context (the discourse of which it was originally a part), but also some of its linguistic and paralinguistic properties when it becomes part of a corpus.
It is also true for written texts, where, for example, visual information about the font, its color and size, the position of the text on the page, and the tactile properties of the paper are removed or replaced by descriptions (see further Section 2.1.4 below).
These are sometimes good reasons for eliciting a special-purpose corpus rather than collecting naturally occurring text.
In other words, I use it as a superordinate term for text-linguistic terms like genre, register, style, and medium as well as sociolinguistic terms like dialect, sociolect, etc.
It also includes a much broader range of written text categories than previous corpora, including not just edited writing but also student writing and letters.
However, in light of the above discussion of representativeness, there is little reason to believe that any of these corpora, or the many others that fall somewhere between BROWN and ICE, even come close to approximating a random sample of (a given variety of) English in terms of the text categories they contain and the proportions with which they are represented.
There are several projects gathering very large corpora on a broader range of web-accessible text.
Finally, note that our final definition does distinguish corpus linguistics from other kinds of observational methods, such as text linguistics, discourse analysis, variationist sociolinguistics, etc., but it does so in a way that allows us to recognize the overlaps between these methods.
Finally, some language varieties cannot be attributed to a single speaker at all -political speeches are often written by a team of speech writers that may or may not include the person delivering the speech, newspaper articles may include text from a number of journalists and press agencies, published texts in general are typically proof-read by people other than the author, and so forth.
For example, we defined American English as "the language occurring in the BROWN and FROWN corpora", but we saw that the FROWN corpus contains at least one misclassified text by a British author, and we also saw that it is questionable to assume that all and only speakers of American English produce the language we would want to call "American English" (recall the uses of sidewalk by British speakers).
In terms of reliability, this is a good thing: If we apply the same tagger to the same text several times, it will give us the same result every time.
The basic distinction in (28) looks simple, so that any competent speaker of a language should be able to categorize the referents of nouns in a text accordingly.
There is a range of more or less specialized commercial and non-commercial concordancing programs designed specifically for corpus linguistic research, and there are many other software packages that may be repurposed to the task of searching text corpora even though they are not specifically designed for corpuslinguistic research.
Finally, there are scripting languages like Perl, Python and R, with a learning curve that is not forbiddingly steep, that can be used to write programs capable of searching text corpora (ranging from very simple two-liners to very complex solutions).
In the simplest case (which we 4.1 Retrieval assumed to hold in the examples discussed in the previous chapter), a corpus will contain plain text in a standard orthography and the software will be able to find passages matching a specific string of characters.
All words without a subscript are new types and hapax legomena at the point at which they appear in the text; if a word has a subscript, it means that it is a repetition of a previously mentioned word, the subscript is its token frequency at this point in the text.
As we move through the text word by word, initially all words are new types and hapaxes, so the type-and hapax-counts rise at the same rate as the token counts.
We will encounter the same problem when we compare the TTR or HTR of particular affixes or other linguistic phenomena, rather than that of a text.
In written corpora, there is one level other than the lexical that is (or can be) directly represented: the text.
In other words, the corpus-linguistic identification of keywords is analogous to the identification of differential collocates, except that it analyses the association of a word W to a particular text (or collection of texts) T in comparison to the language as a whole (as represented by the reference corpus, which is typically a large, balanced corpus).
The keywords now convey a very specific idea of what the text is about: there are two proper names of rivers (the Neosho already seen on the frequency list and the Marais des Cygnes, represented by its constituents Cygnes, Marais and des), and there are a number of words for specific species of fish as well as the words river and channel.
The text is clearly about fish in the two rivers.
The text in question is indeed a scientific report on fish populations: Fish Populations, Following a Drought, In the Neosho and Marais des Cygnes Rivers of Kansas (available via Project Gutenberg and in the Supplementary Online Material, file TXQP).
The tagged version of the BROWN corpus does not contain quotation marks because they have intentionally been stripped from the text.
Again, there are other clear cases of metaphor, such as [NP EXP burst with NP EMOT ] (line 30), [NP EMOT flood NP EXP 's heart] (line 31), and [NP EXP be filled with NP EMOT ] (line 32) (again, they seem to be from the same text).
The keyness of the former is due to the fact that one of the files in the BNC Baby commercial subcorpus is from the Northern Echo, a regional newspaper covering County Durham and Teesside -Middlesbrough is the largest town in this region and is thus mentioned frequently, but it is not generally an important town, so it is hardly mentioned outside of this text.
From a written perspective, one may be interested in contexts such as news writing, text messaging or academic writing.
While there are many different corpora available (see a selection at the end of the book), as an example, we will use the Corpus of Contemporary American English (COCA), which is a publicly available collection of over 1 billion words of text of American English (freely available at www.english-corpora.org/coca/), to investigate the use of two words: equal and identical.
The written text produced in two different conditions can be analyzed for variation in linguistic features and interpreted functionally.
This is not to say that texts that vary in communicative purpose or time allotted to write a text are of different registers, but these contextual differences are likely to result in different linguistic features and that can be functionally interpreted.
This approach provides a comprehensive linguistic picture of the text under consideration and allows for a functional interpretation of the relevant linguistic features.
We may also want to use mode and medium when referring to different types of written language found on the internet or cellular phones (such as social media posts, tweets, and text messages), which vary in grades of permanence depending on the topic and potential effect they have.
We likely can think of examples when a writer has either deleted or regretted a particular tweet, social media post, or text message.
In the written mode, an academic research article has gone through an extensive planning, drafting, and revising process; in a social media post or text message, generally this is not the case.
To understand how to apply the situational variables to different text types, we provide three examples of situational analyses below.
Primarily, this difference is attributed to the ability of instantaneous revisions of the text.
In the classroom, there is plenty of room to negotiate and revise the text on the spot since questions can be asked at any time and clarifications can be requested.
At the symposium presentation, there is a lack of spontaneous interaction, or immediate requests for clarification; there is no room for immediate negotiation of the text.
Since these text segments are uneven in length (one is 157 words long and the other is 241 words long), we had to scale the raw frequency counts as if both texts were 100 words long.
In register studies, as we saw in the previous chapters, contextual differences refer to differences in particular aspects of the situational characteristics in the construction and production of a given text.
As mentioned above, concordance lines highlight the word you pick and provide additional text around it.
This word clearly refers to Tony Blair, a former Prime Minister of Great Britain, and will be more prominent in British than in American press reporting (64 times in nine texts versus one time in one text, respectively).
If you need to convert webtext, i.e., from html to text, follow the steps on this website: www.computerhope.com/issues/ch001877.htm#text.
If you need to convert pdf files to a text format, the best tool to use is another free program by Laurence Anthony, and it is called AntFileConverter: www.laurenceanthony.net/software/antfileconverter/.
Download the files and make sure they are saved in text format (see the website referenced above on how to do that).
A special computer program is often designed to process the text and to look at sequences of words in a window of text as they emerge in a corpus.
This way we can investigate patterns in larger units such as a text.
You can see the actual words from the text in these three bands on the right-hand side.
Their frequency in that text is also shown.
For example, the word "thief" is a low-frequency word (in band three marked with yellow) and it occurs three times in this text.
If we jump to the frequency band, we see that the word "the" occurs 22 times in this text sample.
Follow the same steps to look at how many and what kinds of words you see in this text sample.
If you want to check any text of your choice, all you have to do is copy and paste the text in the textbox and the words will be marked up for your text in the same way as it is done in the examples.
This is a very powerful and useful tool to determine the vocabulary characteristics of a text.
For example, if you were to look at "text" as a verb, you would see that this verb has increased from 1990 to 2020 while "fax" has moved in the opposite direction.
Concordance lines and short language samples (e.g., fewer than 25 words) are preferable over larger stretches of text.
Another issue related to corpus balance in your corpus relates to text types.
If only one of these text types is included then the sample might not account for variation in the different types of news texts.
A balanced news writing corpus would either include texts of sports, lifestyle, and general news texts or would select only one of these text types for analysis.
A text that is read on a web browser such as Google Chrome or Safari looks like this: You are not very good at parking.
In order to ensure that the text numbers relate to the characteristics of each text, each text will also have the relevant header information described above.
For example, "news_15_election_00" would mean a news text from 2015 about an election in 2000.
Depending on your research goal and the design of your corpus, you may also want to include specific information on the situational characteristics of each text in individual text files in your corpus.
Specific information such as the length of each text (by number of words), the topic, or the type of each text (if you included different types within a specific genre) can also be included in individual text files.
This type of information is not a part of the text analysis but it can provide important interpretive information used in the functional analysis of your results.
Because you do not want this information counted as part of the linguistic characteristics of your text, you can put this or any other relevant information that you do not want to be a part of the linguistic analysis into angled brackets (< >).
This type of information is included in the "headers" of the text but will not be read by the concordance software.
Among those analyses are: KWIC (keyword in context), n-grams, collocates in a particular text, and word lists.
Numeric values are relevant if you have frequencies of a linguistic feature (e.g., the number of personal pronouns in a text).
For register studies, an observation is typically each text that you enter into your database.
That means that each observation is a text in which we look for the particular variable that we hope to see variation in (i.e., the dependent variable).
Each text then will have other, "extra-textual" features as well.
Another extra-textual feature can be the time the text was produced: whether the text comes from the year 1920 or 2015.
Yet, it is the text produced by them, and that will be the basis for comparison.
Here's the text again: You are interested in finding out whether undergraduate students majoring in natural sciences use fewer "hedges" than students in the humanities or in the social sciences.
Because SPSS is not good at processing "string" data, i.e., text, for its variables, we need to give a nominal numeric value to each discipline.
Each observation (i.e., each text with each normed count) will be in a different row.
If you wish to show that one text is very similar to another, the higher the overlap the better.
The essays were then typed and saved as text files with headers and codes to show the authors and topics of the essays.
The difference might be attributed to other situational variables such as the difference between an exam and an in-class activity or the relationship between participants and the text construction.
This is a possible explanation, but it seems more likely that the differences have to do with the production circumstances related to constructing texts individually as opposed to constructing texts as writers are interacting with another person to produce a single text collaboratively.
Should you choose to do a slide show presentation, try not to put too much text on your slides.
Phenomena that can be researched with three text archives / Web 1.4.
Frequency of very infrequent words in BNC, COCA, and three text archives / Web 1.5.
Rather than attempting to create a complete and exhaustive list, I focus on a handful of corpora (and related resources, such as text archives and the "Web as Corpus") that are representative of general classes of corpora.
Large text archives, such as Lexis-Nexis 5.
Extremely large text archives, such as Google Books 3 6.
Finally, we will consider very large "hybrid" corpora, which take data from text archives or the Web, but which then deliver this data through powerful architectures and interfaces.
In this section, we will consider three examples of text archives -Lexis-Nexis (representing a wide range of similar text archives, such as ProQuest or EBSCO archives, other newspaper archives, or archives like Literature Online or Project Gutenberg), the Web (via Google), and Google Books.
There is no way to create frequency listing from text archives, at least via the standard interfaces for these resources.
Another challenge with text archives is working with the often limited interface.
However, even this would probably not be possible, since most text archives severely limit the number of "snippets" for a given search (e.g. 1,000 in Lexis-Nexis, Google (Web), and Google Books).
But as we have seen in Section 4, size is not everything -most text archives have such a simplistic interface that they also are very limited in the range of queries that they offer.
As we will see in this section, the best solution may be to take the texts from a text archive or the Web (containing billions of words of data), and then combine this with a robust corpus architecture.
All of the corpora in Sketch Engine that are publicly accessible and that are more than a billion words in size are based on web pages, and there are currently three corpora of English that contain more than a billion words of text.
Before this, even simple methods for studying language such as extracting a list of all the different words in a text and their immediate contexts was incredibly time consuming and costly in terms of human effort.
Other text-rich disciplines can trace their origins back to the same computing revolution.
Many similar computational methods and tools would be seen if areas such as content analysis, Computer Assisted Qualitative Data Analysis (CAQDAS), digital humanities, and text mining had been considered.
Unfortunately, speech recognition software is not yet accurate enough to automatically create text from sound recordings unless they are of broadcast quality.
If the source material is available in hardcopy form, e.g. a printed book or magazine, then a scanner is required in order to turn the printed version into a digital image and then OCR software creates a machine-readable version of the text contained in the image.
Such processes are vital in order to ensure that the machine-readable text is as accurate as possible.
Most corpus tools require plain text versions with optional XML encoding, so where material is sourced in another form, some format conversions will be in order.
These vary in quality and it is obviously important for later linguistic analysis to check that the original text flow has been preserved, especially where the source has multiple columns or tabular formatting.
If the text is annotated or corrected by hand then this could form the basis of a training corpus for an automatic tagging system which can then learn from the human annotators in order to attempt to replicate their coding later on larger amounts of data.
Concgrams are repeated sequences of words that may be discontinuous and in any order, and this allows the user to find possibly interesting phraseological patterns in text which contain optional intervening items.
The n-gram procedure was applied to the full text of Alice's Adventures in Wonderland (one of the most frequently downloaded texts from the Internet Archive and Project Gutenburg) 13 using Ted Pedersen's N-gram Statistics Package (NSP).
The text is only 26,400 words long but it produces 1,810 2-grams, 737 3-grams, 192 4-grams and 51 5-grams that occur three times or more.
This illustrates the first problem with the n-gram method, since even with a small text such as this, a large number of results is generated.
Their application tends to be in areas other than linguistics research but where language, texts, or documents are key sources, e.g. for political text analysis or other social science research questions.
Third, another similar set of tools is employed in the field of digital humanities for text mining of language properties in order to answer traditional humanities research questions and the formation of new research questions that are more difficult to answer with small-scale manual text analysis.
Software tools such as Voyant and MONK are designed to allow large quantities of text to be searched, analyzed, and visualized alongside other tools such as Geographical Information Systems (GIS) and Social Network Analysis (SNA).
However, using a High Performance Cluster (multiple connected computers running small batches of text) at Lancaster, we were able to complete the task in three days.
To measure the style of a passage, the frequencies of its linguistic items of different levels must be compared with the corresponding features in another text or corpus which is regarded as a norm and which has a definite relationship with this passage.
For text analysis and similar contexts, the use of LL scores leads to considerably improved statistical results.
Using the LL test, textual analysis can be done effectively with much smaller amounts of text than is necessary for statistical measures which assume normal distributions.
A reference corpus cannot in any sense represent the language, unless it is subdivided into text categories or subcorpora representing a broad range of registers, as in the BNC or the Bank of English.
In this chapter, we use a fairly liberal definition of "grammatical variation," including both genuinely variationist research -where grammatical variants are modeled as competing against each other -and text-linguistic research that explores variable text frequencies of particular grammatical constructions in corpora.
Not all of these forms increase in frequency, and those that do, notably need to and want to, are relatively low in text frequency and hence do not match the declining numbers of core modals such as will or would.
Today, searches in large corpora and the even larger masses of text stored in digital archives will do the same job much more effectively, and numerous ante-datings are in fact reported regularly.
The first is that the phenomenon of the unequal distribution of lexis accounts for much more about naturally occurring text than might be expected from reading any of the papers discussed so far.
It has been argued that an explanation for cooccurrence of lexeme and structure may sometimes be found in the more extensive co-text.
Inevitably, studies of co-text and phraseology are "messier" than those of lexeme and structure alone.
We then passed from the concordance to close reading of the co-text around occurrences of Mubarak, a very common process in corpus-assisted discourse studies.
At this point, we concordanced month-by-month the items violence and side(s) and the co-text of the resulting occurrences were read.
The KWIC (keyword in context) concordances show the narrow linguistic co-text and provide perhaps the easiest and most useful way of getting acquainted with the material.
Genre context already moves on a more abstract level as genres are generalizations made on the basis of individual texts, and placing a text in its genre context reveals some of its meaning.
The author has a clear role in controlling the text and refers to the parties of the dispute known to all with general nouns: THO I have been much solicited, to shew my Opinion, about the Debate betwixt the two Physicians, concerning . . . and violently oppos'd by a certain Club of Physicians; . . .
Rates of occurrence were computed for the stance features in each text of the corpus.
Bringing an empirical dimension to the study of academic writing allows us not only to support intuitions, strengthen interpretations, and generally to talk about academic genres with greater confidence, but it contrasts markedly with impressionistic methods of text analysis which tend to produce partial and prescriptive findings, and with observation methods such as keystroke recording, which seek to document what writers do when they write.
Typically, researchers have relied on changes in discourse function, or what particular stretches of a text are contributing to the overall purpose of the discourse.
Another criticism of academic corpus studies is that, until fairly recently, these have been largely text-focused so that features seem rather abstract and disembodied from real users.
Studies are needed that do not just analyze text corpora but which involve the authors or the readers of the texts in the analysis by also collecting interview data.
Both devices project the author as a participant in the text, indicating that the writer is prepared to debate issues and contribute half of a dialogue with readers.
A drawback of this approach is that the 1988 model is dated; text messages, e-mails, and blogs are undoubtedly common registers for today's students, but they are not included in the model.
For example, in text sample 1, the students state that the analysis will provide a comparison, explore the effectiveness, and discuss alternative solutions -but the analysis itself cannot discuss effectiveness or other solutions.
A related issue concerns the form in which texts from various registers are available for inclusion in corpora: as manuscripts or in the form of text editions.
If the data contain examples that occur just once, or patterns that occur repeatedly only because they are all from the same text, these cases will usually be discarded in the search for general patterns.
This tension is determined by the text(s) under analysis.
From both perspectives the underlying assumption is that repeated occurrences of sequences of words reflect their functional relevance in a specific text or a register more generally.
In the reading process patterns in the text determine which area of background knowledge or previous experience are relevant to the creation of meaning.
City subcorpora were then created for the 206 cities whose residents contributed at least 30,000 words of text.
The data presented above have shown that the envelope of variation that is studied will result in a different picture of the relation among ENL and ESL varieties: it makes a difference, for instance, whether the overall text frequency of PPs is compared or whether the variable is defined more narrowly, e.g. as an alternation between PP and SP in perfect contexts.
First, corpus tools can be applied to individual texts, in helping decide whether a text is appropriate and what elements to focus on.
Free software such as VocabProfile online (www.lexutor.ca/vp) or AntWordProfiler offline (www.antlab.sci.waseda.ac.jp/) allows a teacher to input a text which is then returned with the lexis color-coded according to the frequency of each word in the BNC or COCA corpus.
Weekly and pre-post tests recorded word knowledge on both definitional and novel-text gap-fill measures.
The results showed that both experimental and control groups made significant and substantial pre-post gains on the definitional measures (4 to 8 percent), but only concordancers made significant gains on the novel-text/gap-fill measure.
These date from 1989 to 2012, and include journal papers and book chapters, but also PhDs and conference proceedings (published as text and not just slides or oral presentations).
First, in terms of research focus, we would hope the future would bring more discourse-level studies with a focus on text and associated features of genre, stance, etc., to complement the current dominance of studies on lexis and specific grammar points.
When it comes to textual coherence (understood here as exercises which do not contain isolated and unrelated sentences), we note marked differences between the books, with EGT featuring only one-third of the exercises with textual coherence and G&B offering 100 percent of exercises displaying textual coherence (even if in some exercise sentences are numbered individually, they form a text or relate to one coherent topic).
Here again, whilst we agree that newspapers usually contain more passive forms than some other text types and that many learners (who often underuse the passive) need practice in using passive sentences, we feel that the exercise presented here might be counterproductive.
The empirical study of translation has traditionally been text-based, typically focusing on the comparison of single originals and their translations.
The application of corpus methodologies to translation research can be traced back to Mona Baker's seminal paper in which she argues that: the most important task that awaits the application of corpus techniques in translation studies . . . is the elucidation of the nature of translated text as a mediated communicative event.
In order to do this, it will be necessary to develop tools that will enable us to identify universal features of translation, that is features which typically occur in translated text rather than original utterances and which are not the result of interference from specific linguistic systems.
She concludes that, in both translation directions, explicitating shifts (i.e. cases in which co-textually recoverable ST material is made explicit in the TT, e.g. when ellipsis in the ST is replaced by a noun in the TT) are more common than implicitating ones (in which an explicit ST item is rendered by one that relies more on the co-text for reader interpretation, e.g. when a lexical word in the ST is translated as a proform in the TT).
The translational component of the comparable corpus of narrative texts contains a higher proportion of high-frequency words and its list head covers a greater percentage of text with fewer lemmas than the nontranslational component.
Six of these are text types (instructive, administrative, etc.) and three are translation-related varieties (non-translated, translated from English, translated from French).
This result may be taken to suggest that the method is not ideal for very small text collections, or that different parameters and thresholds should be used in these cases.
They stress the significance of "knowing one's data" for researchers dealing with vast corpora, emphasizing collaboration with historians, and systematic text examination for qualitative analyses and data interpretation.
While perfect solutions may not yet exist, Liimatta offers useful insights to the study of linguistic questions within the context of varying text lengths.
As an example of a pitfall related to text sampling, we discuss a particular usage of the complex adverb as well, which caught our attention some time ago.
On the other hand, they are part of the text, they cannot simply be removed, as this may reduce the context or even render the text illegible.
Section 2 of this chapter provides a brief outline of different approaches to digital text analysis.
To some extent, these shortcomings are addressed by Gale's Digital Scholar Lab, 5 a text analytics platform through which the British Library Newspapers database can also be accessed.
The usefulness of the available text mining tools is also seriously limited by their lack of customisability.
The database contains potentially relevant information about text categories under two headings: Document type contains information about "the format, genre, or other characteristics of the document", and Publication section allows users to limit searches to a specific section of the publication.
The register categories (like newspaper titles in the previous section) are not evenly distributed across the database, and different choices will inevitably result in different representations of the text-external reality, and as before, this information is only accessible to users systematically looking into this.
It is evident that the accuracy of the text depends crucially on the quality of the image used as the basis of the digitised text.
The OCR confidence value corresponding to each text can be conveniently accessed with Octavo, like any other metadata.
Compared to carefully constructed traditional corpora with often hand-picked text samples, there is obviously much less control over individual choices, and the fact that the exact 8.
Make sure that your analysis scripts are extensively commented (formats like R Markdown or Jupyter Notebooks invite extensive comments, but plain text scripts can also be used, of course).
Since a longer text by definition contains more words, there are also more opportunities for any given linguistic item or feature to appear.
In other words, a longer text will, on average, contain more instances of any item or feature simply because it is longer.
In other words, the fewer words there are in a text, the larger the normalized value is.
If we calculate the normalized frequency of first-person pronouns in this text, we get as the result 200 first-person pronouns per 1,000 words.
But the same normalized rate of occurrence value also applies to a text of 1,000 words which contains 200 first person pronouns.
Second, extremely short texts cannot be easily compared with other texts using many typical quantitative corpus-linguistic methods, such as normalization, because the normalization results become meaningless as the length of the text becomes increasingly short.
Most genres traditionally studied in quantitative corpus linguistics tend to comprise of longer texts, compared to the extremely short text lengths of up to only a few dozen words which are overwhelmingly common on, for example, social media.
Of course, it is not obvious where the line should be drawn when considering whether a text of a certain length should be considered an outlier, but from the point of view of the data, the best practice would be to have the length limit as low as possible.
The main upside of the combining method, when compared to the exclusion method, is that no data is completely ignored: all text available for the analysis is included in the analysis.
For example, if one text is highly edited in style, and another one is highly casual, combining them together results in a loss of a lot of this information, and makes the combined text look somewhat average on both counts.
For instance, instead of studying individual classified advertisements, we might consider the entire classified advertisements section a single text for the purposes of our analysis.
A straightforward approach is to simply split a text into chunks of a certain number of words.
Another solution, which keeps the structural and discourse units of a text together even more, is to divide the text into its paragraphs, or multi-paragraph chunks.
In order to make feature frequencies more comparable across text lengths, Liimatta (2020) proposes a family of methods called lengthwise scaling.
In lengthwise scaling methods, feature counts in each text are first compared against texts of the exact same length ("intra-length comparison") using some suitable method of comparison.
Based on the results of this comparison, each text receives a new, scaled value, which is a representation of how typical the text is in terms of the range of variation seen in all texts of the exact same length.
These scaled values can then be compared between text lengths like normalized frequencies would be, such as by visual exploration of graphs or by using some further statistical or computational analysis.
This is because the longer a text becomes, the more likely it is to include any given feature.
As the texts get longer, more and more of the features of interest start appearing in every text.
Like its name suggests, the type-token ratio is the ratio of the number of different words in a text (types) to the number of all words in the text (tokens).
This ratio is notoriously sensitive to variation in the length of the text it is calculated for.
However, since all texts in a normal-sized corpus are rarely close enough to each other in length, as a typical workaround, the ratio is calculated for a set number of words (such as 400 words) taken from the beginning of each text.
While this workaround has been used for a long time to good effect, it is also not optimal, since in many cases it excludes a large majority of the text from the calculation.
Since resampling methods are very powerful for estimating the distribution based on smaller datasets, they appear as a potentially useful avenue for the development of new methods for the analysis of texts across text lengths.
Nevertheless, there is still a lot of room left for the development of new ways to analyze datasets with a wide range of text lengths, and particularly datasets which contain extremely short texts, which are more common today than ever.
So far, the categorization is fairly intuitive and creates no overlapping tags, that is, a text is never both a short story and a novel.
From a qualitative perspective and using a close-reading method, as is often the case in literature research, the use of overlapping genre categorizations can provide a way of connecting the content and style of the text being read to different movements or periods from which the author might have drawn their inspiration or within which they might have been an active participant.
The genre features found in a piece of fiction can also be used to extend an argument regarding the intertextual properties of a text in such a way that the intellectual context of the writing can be connected to the written text.
An example of this could be the use of genre tropes to create connection through allusions to other texts or even direct references to previous works that add to the narrative being conveyed, for instance a character coming across a different text within the story and having it influence the narrative.
In general, their positions on genre could be described to as defined by their relationship to the text as an object.
The components highlighted as interesting from a linguistic perspective are features such as style, intended readership, intended function and the context in which the text is intended to fulfill a communicative purpose.
The different purposes of the text are also of interest, especially when seen in relation to the other features.
The position could be oversimplified as being one that is interested in the purpose and form of a text, and often focused on the intersection between the two.
It does not influence the close-reading method in a meaningful way and does not indicate information that could tie the text(s) to a broader intellectual context.
For example, if the genre "Reports" is defined by text-internal features, those features would be very frequent within the resulting category.
The users identified in this way were narrowed down to those whose free-text profile location strictly corresponded to one of the three target cities.
We fed each tweet in its raw text form as a single sequence into BERT, which then produced context-informed vectors for each token in the tweet.
They are generally used in a span of French text within a codeswitched tweet where most tokens are in English; this explains why the tweets were tagged as written in English and retained in the corpus.
Next, there will be a discussion of patterning, usage and phraseology in text.
It involves the collection of data; spoken, written, or both, and collating it into one or more text files.
These text files are then searchable and the resulting data can be further studied for the purpose of linguistic research.
First and foremost, the concordance view with one word of interest aligned vertically in the middle of the text and the left and right context justified in the middle, is a way of visualising the patterns of the context of a particular word, and is the main way that corpus linguists engage with corpora.
Each file was converted from PDF by saving as text from Adobe Reader.
For instance, an explanatory variable can be the genre/register or date of publication of a text as well as speaker's age, gender and language proficiency, to name only a few.
Ideally, this would be achieved by truly random sampling 10 where each text ever produced and each spoken interaction that has ever taken place would have the same chance of appearing in the sample.
The Brown family consists of 15 genre-based categories according to which the total of 500 2,000-word text samples are selected.
Yet in practice, the selection is guided by text selection principles (see below) to avoid bias in the selection process.
If a corpus samples only certain sections, e.g. by taking the first or the last 2,000 words of each text, these sections will be overrepresented.
In corpora that do not include whole texts but only text samples (like the Brown family) it is therefore important to achieve a balance in terms of the different sections of texts represented.
These texts usually contain a number of specific topicrelated vocabulary items that are repeated multiple times in each text.
Topic bias is a problem especially in small corpora where each individual text forms a relatively large proportion of the corpus.
When selecting the sampling frame we are often predisposed to see as salient the text types that have traditionally been included in language corpora such as those in the Brown family sampling frame (see ‚Ä¢ Legal considerations bias: corpus designers often face the problem of copy- right; this is especially the case when corpus creators want to share their corpora with other researchers.
In this study, each file representing a text written by a single author was considered as a separate observation.
When we ask about word types we are asking about how many different word forms there are in the text/corpus.
Both word tokens and word types are identified based on the form of a word (external appearance, if you like). To identify lemmas and lexemes, we first need to perform linguistic analysis of the text; lemmas are based on grammatical (morphological) analysis, while lexemes are based on both grammatical and semantic analysis.
The first word (w 1 ) occurs only in one text, at corpus positions 1 -5 -10 -15 and 20, while the second word (w 2 ) is evenly distributed at positions 1 -200,000 -400,000 -600,000 and 800,000.
The idea is that a larger number of different word forms (types) relative to the number of all words in text (tokens) points to a lexically more varied text.
This shows that text B (academic text) is more lexically diverse than text A (informal speech).
However, we have to remember that the type/token ratio is very sensitive to the length of the text; it decreases as the text becomes longer and more words get used again (recycled).
The TTR values are as follows: 0.36 (text 1), 0.33 (text 2) and 0.39 (text 3).
For the purposes of illustration, all occurrences of the word love with their immediate contextone word to the left and one word to the rightare highlighted and the poem is displayed without line breaks to create one paragraph of run-on text.
Imagine also that there were no associations between words in the poem and words appeared randomly in the text.
Finally, the position of the collocate in text (whether it occurs predominantly before or after the node) is shown by the position of the collocate in the graph (left, middle or right).
We can start enumerating the differences between the texts by pointing out that text A is about American politics, while text B is a part of a story about a person called Rincewind.
Text A comes from a newspaper, text B from a novel.
Text A uses American spelling (defense), whereas text B sticks with the British spelling conventions (colour).
Some readers might also notice the unusual onomatopoeic word gloink in text B and words related to the war in Iraq (Bush, Iraq, terrorism, war etc.) in text A.
By virtue of being in a text together, many linguistic variables are related in some way.
For instance, you might have noticed that the spoken dialogue (Excerpt A) uses shorter syntactic structures (so called utterances) than the academic text (Excerpt B), which employs relatively long and complex sentences.
Finally, the mean dimension score for each register is calculated by taking all text dimension scores belonging to the same register and calculating the average value.
Group the individual text types into larger categories based on their functional similarity.
Robustness is typically operationalized in terms of text coverage -the extent to which words on the list account for the total running words in a corpus.
For example, if you want to generate a frequency list of a corpus or a concordance of a word in a corpus with R, you must write a small script or a little bit of code in a programming language, which is the technical way of saying you write lines of text that are instructions to R.
The main one is probably that, in addition text-processing capabilities, R offers a large number of ready-made functions for the statistical evaluation and graphical representation of data, which allows you to perform just about all corpus-linguistic tasks within only one programming environment.
These are small assignments which you should try to complete before you read on; answers to them follow immediately in the text.
This does not mean, however, that corpus linguists only deal with raw text files -quite the contrary: some corpora are shipped with sophisticated retrieval software that makes it possible to look for precisely defined lexical, syntactic, or other patterns.
It does mean, however, that you would have a hard time finding corpora on paper, in the form of punch cards or digitally in HTML or Microsoft Word document formats; the probably most widely used format consists of text files with a Unicode UTF-8 encoding and XML annotation.
Many people would prefer to consider newspaper data not corpora, but text archives.
As the above discussion already indicated, however, the distinction between corpora and text archives is often blurred.
As you might already guess from the names, the former have been compiled to provide information about one particular language/variety, whereas the latter ideally provide the same text in several different languages.
In the following section, we will be concerned with how to load files containing text.
After that, ideally you choose "Unicode (UTF-8)" as the character set, "{Tab}" as the field delimiter and, usually, delete the text delimiter.
While you can always save data frames as tab-delimited text files with write.table, which are easy to use with other software, sometimes your files may be too large to be opened with other software (in particular spreadsheet software such as Microsoft Excel or LibreOffice Calc).
The option of using raw text files may also not be easily available with lists.
The first of these is the one you have already seen, a text form in natural human language.
The other form is its 'translation' into a statistical form in mathematical language, which brings me to the issue of operationalization, the process of deciding how the variables in your text hypotheses shall be investigated.
This is necessary because, often, the formulation of the hypotheses in text form leaves open what exactly will be counted, measured, etc. and how.
We then need nchar to determine how many three-grams we will have to create/retrieve and then store, and a for-loop that applies substr to the cleaned text vector to extract the threegrams.
We then need length to determine how many three-grams we will have to create/retrieve and then store, and a for-loop that applies paste (with collapse) to the text vector to create the three-grams.
Finally, we use abline and text to add some annotation into the plot; we use lines(lowess(...)) to add a trendline, and I introduce mtext to add colored axis labels.
Most aspects of this script should be straightforward, given that everything happening in the loop is relatively simple text and data processing.
Then, we use plot, text, abline, and lines(lowess(...)) for the scatterplot and cor.test for the correlation.
We summarize those in a matrix, to which we apply prop.table to get the percentages of split infinitives, and then plot with text and abline to visualize the adverbs' preference for the split construction.
Like the phase 2 work outlined above, there is an attempt to rely on information that emerges from the text ('trust the text', as Sinclair says), rather than on information that is presupposed.
What we did instead was to assign each article -each text -a value on each of the identified dimensions.
The term corpus (plural corpora) is simply the normal word in Latin for 'body', as in a body of text.
Since the 1960s, a yet more specific meaning for the term has emerged: corpora in this sense are very large, machine-readable collections of spoken and written text that are analysed using computational methods.
However, a text that does use the word elephant once is then rather likely to go on and use it several more times (for example, if it is a text about elephants).
This means that the probability of a given word occurring at some point in a text is influenced by the words that have occurred before and thus the words in a corpus are not random.
Second, when we analyse a corpus we must always be aware of dispersionlooking out for the difference, for instance, between a word that occurs 100 times in our corpus where all 100 instances are bunched close together in one text, and a word with the same 100 occurrences that are spread out across the whole corpus.
These will typically be collected in the form of plain text files -that is, computer files containing just the actual characters of the text, without any kind of formatting, as found in word-processor files, for instance.
The plain-text words of the corpus are sometimes called the raw data of the corpus.
Annotation, by contrast, encodes into the corpus the results of some linguistic analysis which we have undertaken on the corpus text.
The third thing which can be added to the raw text of a corpus is metadata.
Metadata is data about data, that is, information about the texts that have been included in the corpussuch as whether they are written or spoken, information about genre or domain, bibliographic information on original publication of a written text, or demographic information about the speakers in a spoken text.
There are several ways to store metadata for a corpus, but one popular way is to insert a chunk of descriptive information at the beginning of each corpus text in a fixed, specified format: this metadata chunk is called the header of the text.
If we are examining a series of examples extracted by searching a corpus, for instance, it is often very useful to be able to see, via the metadata, precisely what text each example comes from: knowing, for example, whether a particular instance comes from a novel or from a newspaper report can be a valuable aid to interpretation.
To annotate a corpus is to insert codes into the running text to represent a linguistic analysis.
The underlying principle of semantic tagging is the same as POS tagging, namely, the assignment to every word in a text or corpus of labels representing categories within a scheme of analysis.
As such, parsing represents an implementation within a computerised text of the traditional tree-diagram style of syntactic analysis; for that reason parsed corpora are often called treebanks.
This is done by introducing codes into the text to represent the beginning and end points of all the phrases.
Part of the reason for this is that, as a practical matter, both can be inserted into the corpus using the same encoding system of tags inserted into the running text.
In an XML document, anything inside <angled brackets> is part of a tag; anything outside angled brackets is part of the text itself.
The special meaning of the angled brackets < and > means that actual less-than or greater-than signs in the text itself must be represented by the special codes &lt; and &gt;.
A key items analysis is performed as follows: first, frequency lists are created for two texts or corpora -either two corpora which we wish to contrast, or a single text of interest and a generic reference corpus.
A more common approach is to begin with a single word (or lemma) of interest, called the node, for which a concordance search has been performed; the surrounding text is then scrutinised to establish what elements occur frequently in the vicinity of the node.
Many such disciplines are focused on one form or another of close analysis of text, for instance history and literary criticism.
To use corpora for contrastive analysis and translation theory, researchers need to use a parallel corpus, meaning that the same text has been translated into various languages.
The proportion of text types has to remain constant so that each year is comparable with every other.
However, the researcher should also be ready for changes during the course of data collection since the process of text compilation may not be as smooth as expected.
If a written text is planned to be used in a corpus, researchers should take the required permissions from the author of the text, which will prevent copyright issues that might be faced in the future.
Even though the authors allow researchers to use the text in their corpus, some of the owners of the writing texts may not allow researchers to share the corpus with other researchers worldwide.
They should keep the speech transcriptions and the written data in ASCII (text) format as it is the standard format used in corpus linguistics.
The first step of annotation is called the structural markup, which gives descriptive information about the text; thus, structural markup is used to create the text's metadata.
Similar to taggers, once the parsers are trained, they automatically annotate the text for you.
You can see from these examples that 'text' in this sense is broader than a document or what you produce when typing in a text editor.
In corpus linguistics, a text is a record of any stretch of language as produced on a specific occasion at some point in time, at some place, and as received or receivable at some place and time.
It is therefore important that we always ensure we understand how the text of a corpus is internally structured, and to critically relate these to our linguistic analyses and categories.
Type frequencies are then lower than token frequencies, and the type/token ratio can reveal interesting insights into properties of texts, essentially telling us how lexically variable a text or corpus is.
While wordforms constitute a text on the surface, they also form abstract units, namely phrases which in turn form more complex phrases and clauses and sentences.
Let's recall Obama's speech cited above: this is an example of a text produced in (American) English.
Many written ones (emails, text messages, etc.) are not preserved either.
Metadata is what distinguishes a corpus from a random collection of texts by giving it explicit structure: minimally any user of a corpus will know what amount of text data is contained in it, what characteristics texts have, for example, whether they are written for a newspaper or spoken during a conversation with friends or colleagues, who produced the text, and so on.
Wordforms and in turn the larger structures they form are embedded in text-internal contexts, and the text as a whole relates to situational context.
The first question we can ask about the content of a corpus is how much text it contains: its size.
This is obviously not immediately comparable to corpus sizes given in word tokens since time length does not relate directly to text size: texts may differ in the amount and length of pausing and the speech rate of speakers, so that texts of similar duration in time may exhibit very different numbers of wordform tokens.
One means to represent different situationally defined text types evenly in a given corpus is to aim for balance.
The major idea here is to avoid massive skewing in results by over-representing just a single or very few text types.
In longer written text production, like a book, producers of texts may go through various rounds of pre-planning and rewriting a text before it is actually delivered.
But this is not true for all written text, for example, text messages may be formulated fairly quickly and without much planning.
In our view, none of the parameters of authenticity, routineness, and spontaneity constitutes a demarcation criterion for corpus text and/or corpus linguistics.
We have seen above that the creators of the Brown corpus (and other subsequent corpora following the same design principles) sampled written English texts according to their intuitions about the proportions of different text types.
In potentially all societies there are text varieties that are received by a large number of people but are produced by only a fraction of the societies' members.
One can test this by taking a corpus, establishing a catalogue of all structures from certain domains (sounds, words, sentence structures, etc.) attested, and then seeing to what extent the addition of any further text will expand the inventory of these structures.
We also need to consider matters of composition, since some lexemes or constructions come up only in specific text types.
As we have explained in Chapter 2, texts have different properties on two different dimensions, text-internal/linguistic and text-external/situational.
The much higher rate of complex NPs in written registers may also be due to considerations of mode and reception alone: a reader will have more time to process complex structures, and knowledge of this fact may carry over to considerations of text production.
The next dimension of text varieties is genre.
These are genre markers: linguistic features of genres are not pervasive, rather they occur often only once, in specific positions in a text.
Most relevant for general linguistic concerns is the register perspective on text varieties since this offers important insights into the functionality of linguistic structures in use.
In addition to the corpus text data, modern corpora are closely interlinked with two further types of data, namely raw data and metadata.
The term raw data refers to the recording of a speech event or a written text, including its paralinguistic properties (what else is happening or done by participants about the communicative event).
In your first language, think of text varieties that you encounter in the form of digital writing (published or shared on websites, in social media channels, etc., or as part of private communication) and that may qualify as genres.
For instance, to evaluate whether a language has been changing over time requires metadata not only on the time of text production but also on the demographic features of people like their age.
For instance, a factor drastically lowering the representativeness of written corpora is the fact that we often do not know anything about specific circumstances of text production and editing before the written texts were published.
Several special corpora contain only specific types of text, which could also be included in a general corpus.
Even for very restricted corpora in terms of text types, like ATC corpora, variability in situational features is relevant, and so these will have to contain text specimens produced by female and male pilots of different age groups, different linguistic backgrounds, and so forth.
This figure represents the first 2,000 words in any given text, plus the number of words to complete a sentence started so that the text snippets included contain all complete sentences.
A conversation includes many situational clues to interpreting language such as eye gaze, gesture, facial expressions, tone of voice for spoken language, as well as means to establish common ground between conversational partners, all of which written text lacks, which should affect language choices.
This is usually made up of a text grid that divides the speech signal into characters that represent phones, which can be viewed or computer-processed with accompanying audio files.
For some languages, the written words are similar enough to their phonemic representations that text corpora can be used to examine questions relating to phonology.
Most text editors and word processing programs have a search function that will allow a user to find segments, words, or phrases.
Within a text, some words may be restricted to particular sections, which is also useful to know.
For our example text, these regexes return the same information.
As discussed in 3.1.6, a good first step in this process is to elicit names for text varieties or speech events from native speakers.
Once we have identified all text varieties that should be included, we need to decide how to collect relevant specimens and which ones of these to select for inclusion.
Moreover, although authenticity is not a demarcation criterion for corpora, spoken texts in particular should come from common text varieties as well and not be restricted to scripted or semi-scripted TV, radio, or otherwise broadcasted texts or texts elicited in experimental setups.
In order to be able to determine the size of our text collection and relevant proportion in subsections, we need to evaluate it accordingly.
After evaluation, you may determine that additional texts or text types are necessary to achieve better representativeness.
In particular they need to be annotated, at least transcribed, and it is the transcription that will eventually resemble our corpus text.
This means that the transcription of spoken/signed texts needs to contain fairly meticulous special annotations for characteristics of text production.
Furthermore, the corpus text should be linked to the recording media of the primary spoken/signed text in a time-aligned format.
For languages with other scripts, for example, Cyrillic scripts in many languages of Eastern Europe and Central Asia or various scripts of East Asian languages (Mandarin, Japanese, etc.) corpus builders will either need to use encoding such as Unicode (cf. 5.11) or add a layer of transliteration to the corpus text.
Consider again the major difference between published and private texts: where a text is published it is fixed to some medium in some basic format, whether digital or analogue.
Texts that exist only in printed or even handwritten form on paper require work on digitisation so that the text is machine-readable.
And where the corpus text is in a language that is not known to the scientific community it needs to be translated to a more widely known language.
This written form is what we treat as the corpus text since this is what can be searched and further annotated.
The major goal is to provide an accurate rendition of the spoken text in written form, including certain paralinguistic aspects.
The primary purpose of the transcription is to make the spoken text searchable.
As we saw in (6.1), individual passages in the text refer to passages in the recording through time code information that allows users to navigate across the two files.
This requires that texts be similar in two regards: text data format and pre-processing and annotation.
The main point that you need to be aware of is the fact that any digital text is encoded in some form.
This means you have to make sure that when working with different software or on different machines the text and letters that look the same really are the same in terms of their encoding.
If encoding does not match, you will potentially not find relevant text.
If you are dealing with multiple file formats, exporting or converting everything to plain text format is probably the simplest way to compile it all together.
Alongside corpus text data we have to integrate metadata.
Each text that is part of the corpus will also have metadata such as the date and place it was recorded, who spoke or signed, who else was present, and what media files are associated with the text.
Metadata will also provide information about other situational characteristics, for instance, what register and genre properties a text has, what its global topic is, etc.
In fact, if metadata is included at the top or bottom of the file, it may be read as text rather than meta-information about the text.
In this excerpt from a written fictional text in COCA, any human reader will understand that both instances of her refer back to 'the woman' .
First of all, they should be linked to the corpus text data and their metadata so that corpus users can evaluate how annotators arrived at their annotation decisions; at the same time the annotations are ideally extractable in order to be analysed outside the corpus infrastructure itself, for example, by programmes for statistical analysis.
In principle this can be done by using the symbols of the IPA to render the corpus text.
It is applied to a corpus text which has already been annotated and translated, as required.
Annotation of semantic categorisation is useful, for example, for various investigations of text content.
Language use does not consist merely in the production of individual utterances; instead, utterances are interrelated with each other in what is called text or discourse (cf. 2.2.1).
The various interrelations between utterances in a discourse lead to (text/discourse) coherence so that interlocutors (or writers/readers) share the meaning built up during production and reception.
On the 'grammatical word' tier the text is divided into grammatical words which is essentially a step of tokenisation.
Find a short text and trial your system.
In corpus linguistics, sampling is rarely truly random (a random person from a population, or a random text from all existing texts), and is more often a convenience sample (who we can record, what we can find on the web, which newspapers we have access to, etc.) (cf. Chapters 6 and 10).
The assumption of the Poisson process is that we know the average time between events, but not their exact timing (i.e., we have an expectation of how many verbs should occur in a text, but not whether they are evenly distributed or more at the end, or beginning, or occur close together in chunks, etc.).
When describing decision trees in text, you can refer to the node numbers (found in our figure at the top of each section/bin of the data, followed by the number of tokens in that group) and the split numbers (the numbers at the top of the circles).
However, much of sociolinguistics has focused on phonetic variables, especially vowels, and on non-standard varieties of languages, neither are well represented in mainstream corpora which often include mainly written text in the standard variety of a language.
Try to access an annotated text and find the respective word for 'woman' .
In this way the corpus text is searchable and users can grasp the meaning of specific passages.
This data is often considered the best kind of language data available to understand how people really use language because it is less considered and belaboured than written text.
In doing so, he determines per text (and thus per speaker) what he calls its referential density (RD).
The RD of a text is determined by counting all possible argument positions (S, A, P, obliques; cf. 11.2.1 and 11.2.2 above) and among these all positions are filled by an overt form (pronoun, lexical NP) rather than being left zero (where a referent is clearly identifiable from preceding context although no overt form appears); then the number of overt argument positions is divided by the number of all argument positions to yield the RD.
For other research questions, control of content is less of an issue: for instance, the relatively low lexicality of A-arguments should be observable in any text (but see below).
Original text corpora are thus an option where the phenomenon of interest can be considered ubiquitous enough to be represented sufficiently in the corpus (cf. Seifart In Press).
COCA is not a published article; rather, it is a web-based corpus complied of a 520 million-word database from newspapers, magazines, fiction, and other academic text documents.
The book is aimed at a variety of readers, ranging mainly from linguistics students at senior undergraduate, Masters, or even PhD levels who are still unfamiliar with corpus linguistics, to language teachers or textbook developers who want to create or employ more real-life teaching materials. As many of the techniques we'll be dealing with here also allow us to investigate issues of style in both literary and non-literary text, and much of the data we'll initially use actually consists of fictional works because these are easier to obtain and often don't cause any copyright as that of morphology, or even phonology.
And even though the language in the latter sample seems fairly natural, we can still easily see that it comes from a scripted text, partly because of the indication of speakers (which I've highlighted in bold-face), and partly due to the stage instructions included in square brackets.
As we haven't discussed any of the issues in processing such text samples yet, it may not be immediately obvious to you that these different types of register may potentially require different analysis approaches, depending on what our exact aims in analysing them are.
What is in fact problematic to some extent for processing the text is that these dashes actually look like double hyphens, i.e. they're not surrounded by spaces on either side, as would be the normal convention.
Now, many computer programs designed to count words will split the input text on spaces and punctuation.
Something similar, albeit not to signal a parenthetical but instead some kind of pseudo-punctuation, happens again for "Yes-or" a little further down in the text.
We can already see, therefore, from this relatively short sample of text that a failure to deal with this feature could cause issues in a number of places throughout the text.
Again, as 'human consumers' of the text, this will not cause any processing problems, but for the computer, the, The, and THE are in fact three different 'words', or at least word forms.
We'll talk more about this type of issue in Section 4.4.1, where we'll explore ways of dealing with such features of the text in order to retain relatively 'clean' data.
Single quotes ('‚Ä¶') signal that an expression is being used in an unusual or unconventional way, that we're referring to the meaning of a word or construction on the semantic level, or to refer to menu items or sometimes button text in programs used.
The latter may also be represented by a stylised button text, e.g. .
Monospaced font indicates instructions/text to be typed into the computer, such as a search string or regular expression.
On the other hand, though, this makes it possible for the basic text to be linked to various types of data-enriching annotations, as well as to perform more complex search operations, or to store intermediate or final results of such searches for different users and for quicker access or export later.
When creating a spoken corpus, one also needs to think about whether an orthographic representation of the text will be sufficient, whether the corpus should be represented in phonetic transcription, or whether it should support annotation on various different levels (see Chapter 11).
As we already have a basic understanding concerning the composition of the earlier, much smaller, corpora, we can now try and develop a basic understanding of how large-scale mega corpora may differ in that respect, and what the advantages in this may be, apart from simply having more text from different genres.
As we've seen before, the very first electronic corpus, the Brown Corpus of written American English, contained 500 (written) text samples of 2,000+ words, and its size and composition set a standard for the compilation of new corpora for many years.
For a long time after the publication of the corpus in 1964, many people thought that 1 million words of text represented a general-enough sample to provide sufficient information about the makeup of the English language.
In order to do so, in the past corpus compilers have frequently resorted to using samples of approximately 2,000 words from different texts and genres to achieve balance, but both that number and 'cutting out' part of a text appear fairly arbitrary, and may in fact make such samples somewhat unrepresentative of the texts they're supposed to represent as a whole.
For instance, while the beginning (Introduction) of a research article may be very similar to its end (Conclusion) in that both discuss the background and aims of the article, the 'middle parts' that describe the actual research are normally very different, and therefore arbitrarily picking either text from the beginning/end or the middle will potentially provide a very skewed picture of the language of research articles.
Representativeness, albeit also difficult to measure, may be more easily achievable, especially for domain-specific corpora or limited fields of investigation, because often there are relatively clearly definable criteria for what represents a certain genre of text or domain.
Some countries also recognise the concept of fair use, which allows relatively insubstantial parts of copyrighted materials to be used for purposes such as research, education, review, etc. (Wikipedia: Fair Use), although, in practice, this will probably not allow you to include sufficient amounts of text or other materials in your corpus.
Having covered the basic theoretical and legal aspects of creating corpora, we'll now turn to the structural nature of texts and other associated properties that may exist in the form of meta-data (e.g. additional information about the text, etc.).
When we read ordinary printed documents, such as books, we mainly concentrate on their text content, and often -though by no means always -tend to ignore that they may in fact contain a number of additional pieces of information apart from the main text.
The content of these sections represents meta-data, i.e. additional data about the text, but does not form part of the original text.
Although all this is interesting information which is conventionally represented inside the document, as well as affecting its pagination/layout, it generally does not form part of the meaning potential of the text itself, which is what we're usually most interested in when we analyse texts from a linguistic point of view.
Headings fulfil multiple functions in a text.
The first of these is that they act as a means to reflect the hierarchical structure and logic of the text.
In the simplest case, such as in a novel, this may just involve breaking the text into chapters and labelling them Chapter1, Chapter 2, Chapter 3, etc., with a slightly 'more advanced' situation being that the individual chapters may also be given a title.
On the other hand, this very redundancy may help us to classify -or even identify the exact genre ofa text better.
This is because, due to their occurrence both in headings and the text, some of the key words that we find in the headings may occur with a higher overall frequency in the text, and thus help us to 'summarise' the content.
Incidentally, the level of redundancy would increase even further if we analysed the whole text, including the front matter, because there the headings might show up once more inside the table of contents (TOC) of the document, where their meaning is 'purely' to serve as a navigational aid by listing them side by side with their respective page numbers.
Now that you already have a pretty good idea of what types of features you might encounter in a text and how this relates to aspects of its organisation, as well as issues that may affect its analysis, let's move on to finding and processing some data in electronic form as a first step in laying the foundation for creating your own corpus and analysing it.
Although we all encounter a number of different file formats containing text on a daily basis while using the computer, many people generally tend not to be aware of the fact that the text contained in these files may not always be easy to process.
Why we might need such special programs for displaying the text is because we often want to be able to render it with special types of formatting, such as italics, boldface, etc., use a particular layout, or that we want to be able to generate a table of contents automatically in a word-processing application, where this is based on the headings inside the document.
Any document that purely contains text (plus potentially some form of markup, see Chapter 11), is generally referred to as a plain-text document and should be readable (and writable) with any basic text editor (see Section 4.2.1 for a more in-depth discussion of these).
Use the links in the online materials on the page 'Understanding File Formats & Their Properties' to go through each of the examples of different types of 'text' documents above and see how easy/difficult it is to identify where the actual text is.
If you do have the relevant programs installed on your computer, you can also try to create more complex versions, containing more text and formatting, of the different document types yourself and then investigate them.
Exercise 6 has hopefully already alerted you to the fact that it isn't easily possible to just use any document that we can read in some way on the computer equally well as a source for our corpus-linguistic analysis, simply because it contains text.
Instead, what we've seen is that it's often also necessary to understand the nature of how that text has been produced to some extent, or which particular features the program that was used to produce the text may offer.
In addition to this, if we want to be able to exchange documents efficiently, we also need to develop a basic understanding of how exactly text may be represented on the computer, which is what we'll do next.
The reason for this was that the formatting and layout options contained in documents of these types are usually not stored as plain text, but instead are often binary coded, and therefore we have no (easy) way of dealing with these.
The particular flavour we'll want to use for this course is a variable byte encoding called UTF-8, which stores characters in up to six bytes, but generally has the advantage that the basic Latin characters appear at exactly the same positions as in the simple encoding discussed above, so that opening an English text in UTF-8 would essentially be the same as opening it in ASCII or Latin1.
Plain-text files in general tend to be much smaller than other files because representing characters, even if some of them may take up six bytes in UTF-8 in some cases, does not require much space.
As pointed out in Section 3.3.1, for most characters appearing in an English text, one single byte will be enough.
Shorter plain-text files are therefore generally very small, sometimes even less than a kilobyte (kB; 1kB = 1024 bytes).
In order to collect this much -or rather, little -text, we'd probably need the equivalent of about 2.4 to 3 pages of scanned text, as the number of words in texts roughly varies between 250 per page for double-spaced texts of average font size (i.e. 12 pt) in printed books such as novels, and maximally about 600, which I found in a monospaced article from a scientific journal where the print size was around 10 pt.
This text size doesn't increase dramatically, even if we add other types of enriching information -generally referred to as annotations (see Chapter 11) -to our files, provided that these annotations are also stored in plain-text form, and aren't excessive.
We'll explore plain-text-based file formats, such as HTML and XML, that may contain such markup further in later sections, as well as discussing their use(fulness) for linguistic annotation/analysis.
Now that you have a basic idea regarding the formats and encodings a text might come in, and the issues involved in being able to work with them, we can move on to finding out how we can obtain our own texts for analysis purposes.
Furthermore, most HTML page sources do contain a fair bit of information before the actual start of the text, which is signalled through the <body> tag, so most of what precedes it should be considered 'non-text'.
You may well notice that the header (<head> in HTML) generally doesn't really contain any part of the text itself, but simply stores meta-information, i.e. information about the text or relevant to how the browser is supposed to handle the page, for example, in which way to display it.
Header elements may for instance be the page title (contained in the <title>‚Ä¶</title> tag, where the forward slash indicates the end of the tag in the closing part), which then appears in the title bar of your browser, or possibly meta-information (<meta>‚Ä¶</meta>), such as the text (content) type or encoding/character set (charset), as well as style (sheet) information/references (<style>‚Ä¶</style> or <link rel="stylesheet" href="./corpus.css" type="text/css" />, in our case) responsible for some of the page formatting.
Many -if not even most -of the texts available through such archives are free of copyright or available under academic licences, but, as pointed out in Section 3.1.4, before you publish anything based on such a text, it's usually advisable to inform yourself about any potential issues, such as how to acknowledge the source of your materials or whether there are any restrictions concerning re-distribution, etc.
For some initial practice, let's start by downloading a text from the Project Gutenberg website and taking a look at it.
We'll use this later on to see how we can extract text from a PDF file.
Open either one of the text files and scroll through it to see whether you may be able to recognise anything special about the formatting, layout, etc.
Obviously there are some problems with this approach due to the format of the data that will be retrieved, which often includes other types of information apart from the main text, but we'll deal with these issues only partially now, and explore the remaining options a little later.
The following is a brief summary of desiderata for suitable plain-text editors.
Perhaps the easiest option to obtain texts from web pages is to use your browser's text export function, if one is available.
Some browsers will remove the HTML and only leave plain-text content, while others may retain some bits of HTML, such as, for example, email addresses contained in mailto links (i.e. those that allow you to fire up an email client when you click on them), etc.
This should copy your text to the clipboard.
Open your favourite text editor and paste in the contents by pressing 'Ctrl + v' on Windows/Linux, ' + v' on the Mac.
Open the file in your text editor and examine its format.
The download and processing progress will be reported in the text window on the right, and a number of sub-folders will be created once the pages have been downloaded and processed successfully.
If you want to see frequency statistics for a given directory, you can also use the 'show stats' button and a new text window will open, presenting you with detailed descriptive statistics.
Try this with at least one of the downloaded HTML files and its corresponding text version.
Please note that the extensions for the files containing the word tokens (.tok) and frequency lists (.frq) are not extensions that are generally recognised by any programs, such as editors, but you'll be able to view them with any text editor if you use the usual file-opening mechanisms.
We'll soon investigate ways of extracting the text parts from these documents.
Later on, we'll move on to learning about ways of extracting text data from files that contain formatted text, where of course the same, or at least similar, clean-up operations might be necessary after the main data extraction has been performed.
Sometimes you may also have to correct artificial line breaks, for example, when you've extracted text from graphical formats -such as .ps or .pdf -, or those inserted by some browsers, as we've seen in Section 4.2.2.
We need to do this because these line breaks, as indeed anything that doesn't really form part of our text, may in fact interfere with the processing of the text later and even create a number of problems that could affect the meaningfulness of at least part of your data for linguistic analyses.
Call up the search-and-replace function, either from the 'Edit' menu or by pressing 'Ctrl + h', which is the shortcut available in most general text editor s these days, even in the non-Windows world.
Searching and replacing text along the lines of what we just practised above is a very useful semi-automatic means of preparing your data efficiently, but you nevertheless constantly need to be aware of potential errors that might be introduced by replacing the wrong things or replacing them in the wrong order.
The most important thing to remember/look for in such a program is a menu item that either provides us with a 'Save as‚Ä¶' or 'Export' option from the 'File' menu to save the text as plain text, as we've seen for web pages earlier, or some item on a different menu that will probably contain the word 'extract', such as in older versions of Adobe Acrobat or GSview.
To make it a little easier to understand what the shortcuts do, just try to remember that the basic keyboard combinations without pressing the 'Shift' key -the one that switches between small and capital letters -will only help you to navigate through the document more efficiently, while combining them with 'Shift' will also select the text spans covered by the shortcuts.
ICEweb already caters for some of that information by keeping a little text database that you can open with Excel or OpenOffice Calc.
The first is that you can employ the right mouse button (two-finger tap on touchpads on the Mac) to activate the context menu inside the browser in order to be able to save the material, as otherwise clicking on the link will simply get the text displayed inside a browser window, rather than saved to your computer.
Hyperlinks are preserved and rendered in angle brackets (<‚Ä¶>), italicised text surrounded by forward slashes (/‚Ä¶/), and underlined text surrounded by underscores (_‚Ä¶_).
As I generally use (more or less) the same text in the <title> tag of my pages, this effectively duplicates the text of the heading inside the saved text version.
Just like Firefox, it also breaks longer paragraphs into shorter lines, thus adding extra line breaks that do not form part of the original text.
The copy-and-paste versions retrieved from Chrome (ver. 31 and above) and Safari (ver. 5 on Windows, ver. 8 on the Mac) seem to be almost identical in that they generally strip all the formatting and hyperlinks from the document and simply leave the text with a minimal degree of formatting.
If you have a number of different browsers installed, you can also use them to download the same page in text format and compare the versions they produce.
What all instances of these have in common is that they contain some text, and that this text had </p> at the end, to close the tag.
A by-product of this exercise is, of course, that you should now also have a number of Word and PDF documents that you can extract some text from later.
Again, the basic operations for extracting the text should be relatively straightforward, especially for Word documents, provided that you either have Word itself or a reasonably good word-processor that can handle Word documents, such as OpenOffice Writer, installed.
Of course, more complex constructs, such as tables or lists, will never quite look the same in plain-text format, but the essential thing is that we can somehow get a handle on the text content.
PDFs, on the other hand, may present more of a problem as, due to their graphical nature, it's unavoidable that we'll end up with unwanted line breaks within paragraphs which probably need to be replaced by spaces manually as and when appropriate to avoid complications when trying to create frequency lists or otherwise processing the text documents later.
The word context here refers to something different from what we discussed above, that is, not the situational usage in a particular place and time, but instead the immediately surrounding text, something we can also refer to as co-text in case of ambiguity.
This is a phenomenon we'll frequently encounter in our analyses from now on, and it requires us to always have an open mind and the willingness to let the data 'drive' our interpretation, rather than trying to force the data to fit the theory, which is something I've frequently observed, for example, when colleagues who were doing literary analysis were not working closely enough to the actual text, but always somehow tried very hard to make the text fit the theory they were using.
Most concordancers are also stream-based, which means that they 'suppress' line breaks by replacing them with spaces, so that the text is essentially read as a continuous stream of words.
Thus, words that actually occur on different lines in the text may still be presented as part of the context.
In order to get around this slight limitation of AntConc, you can of course always use your favourite plain-text editor and do a search-and-replace operation where you replace the search term by something like >>> [search term] <<< (leaving out the square brackets).
This type of basic initial coding needn't be very complex, but can consist of a few easily identifiable markers that you insert somewhere in a text.
The key to this is in the two words "easily identifiable" because whatever codes you may insert in your file, those shouldn't easily be confused/confusable with any regular text.
Therefore, maybe simply inserting your codes in round brackets may look too much like regular text to be really useful, and even replacing the round ones by other types of brackets may be problematic, for instance, if you happen to be analysing a linguistics text where different types of brackets are used to indicate different levels of analysis, such as, for example, curly brackets to enclose morphemes, or square ones to mark phonetic representation.
Thus, you need to find something that distinguishes you code from any ordinary text, ideally by using characters that do not form a part of any normal text.
The only 'word' that was apparently not an issue in this exercise is very, but I said "apparently" above because of course the same issue as for this also applies to very, only that you may not have noticed it because no form of very with an initial capital letter occurs in the text.
As you can see, in this basic form, concordancing is very similar to a Google search, only that it shows you the results in one or more pre-selected pieces of text, rather than trying to find them online.
Furthermore, if you've paid close attention to the configuration options, you may already have noticed that the program not only allows you to work with single, or a number of different, files at the same time, but that you also have some degree of control over the particular (plain text) input format and its encodings, something you should by now be able to understand better through the exercises we did in previous sections.
The text on the online page is editable, so you can also delete the u and see whether the quantification using the question mark still works, or even try some of your own words, if you know any other differences in spelling that relate to one character only.
This is because what it would in fact generally match is the very first occurrence of the character sequence, followed by the rest of the text, that is, a combination of word characters, whitespaces, punctuation marks, etc., until we reach the end of the text itself, which is -perhaps not so obviously -also a kind of word boundary.
Examples of type a) allow us to work with different dialectal variants, or sometimes also to cover spelling variants in historical texts, where the spelling may not yet have been standardised, so that even one and the same text may contain alternate forms that represent exactly the same word meaning.
If you cannot immediately recognise what this means, then I suggest you spend a few minutes thinking about this and maybe even test it on a longer text in AntConc, as described in Section 6.5.
Adding an option for this will then provide us with an improved solution for the regex, \b(((assum|believ|guess|suppos|think)(e?[ds]? |ing)?)|thought)\b, increasing the number of hits to 414, and indicating the importance of the past tense form of think in the text.
A similar problem to the one above could also exist for the singular and plural forms of the noun guess because they look identical to the base form of the verb and the 3rd person singular present, but luckily an investigation of the data reveals that those don't actually occur in our text.
Unfortunately, most of the best-known taggers that have been developed over the years, such as, for example, the CLAWS system, for which a number of tagsets, such as the C7 discussed above, have been developed, are generally not freely available to the public or may require the purchase of a commercial licence for tagging suitable quantities of text.
Open the copy in your editor and place the cursor at the end of line 66, which should give us enough text to evaluate the tagging.
Another way to get a sizable amount of text tagged is to use the CLAWS trial service.
However, even for written language, the accuracy of a tagger across different text types/genres may vary strongly.
Moving slightly further down the text, we can see that, again, the determiner "A" in the first section/chapter title has, again, erroneously been tagged as NNP, but what's more surprising is that the word "Scandal", which was previously tagged as NN, is now tagged as NNP, too.
In the main text, things seem to be going better.
As constructing the BNC was a major exercise involving the digitisation of very large amounts of text, sorting out meta-information as much as possible, and PoS tagging and annotating the data in a number of ways, the care taken in checking and correcting the final result of the tagging has, at least to some extent, been sub-optimal.
The text displayed here just looks like any plain text from a book, article, etc., with only the hit highlighted and formatted as a hyperlink.
If you're only viewing the result in a good text editor, this won't really make any difference, but if you may be planning to use other programs to further process your results automatically, it may make a difference if these programs expect operatingsystem specific line endings, and thus potentially lead to errors.
Later on, depending on how much information you might need about a text, you can also output as much meta-information as required (or is actually available), in case you need to determine and/or distinguish which particular type of texts the results come from.
And even for the negated version of the word smoker, which may at first glance seem to be quite uncontroversial, we find the three variants non smoker (3x in 1 text), un-hyphenated nonsmoker (7x in 4 texts) and -most frequentlynon-smoker (55 in 36 texts).
Now that you know about some of the issues that may affect our counting of words in a text/corpus, we can start looking at frequency lists properly.
Both ways of course require the text to be suitably cleaned, normalised, and tokenised into words in the first place, which is at least part of the reason why I placed such a lot of emphasis on cleaning up our data in earlier sections of this book.
However, we may also sometimes want to look at the least frequent words first, assuming that they can possibly tell us something very specific about the terminology used, for instance in a highly technical text that contains a number of specialised words.
Yet, in order to ascertain the 'spokenness' of a text/corpus until such time, we may often want to handle contractions as single units.
Almost all texts, apart from maybe certain text types including telegrams and recipes, tend to have a rather high occurrence of high frequency function words, something we've just seen during our first explorations of frequency lists. Since these words don't actually tell us much about the lexical richness or the content of a text/corpus, anyway, they're often regarded as redundant and thus lists that exclude them, at least in theory, ought to help us develop better insights into the nature of any text or corpus under investigation.
Just as with auxiliaries and pronouns, we also ought to be very careful when eliminating prepositions and conjunctions from our frequency lists because they may equally tell us something about the domain or genre of a particular text. Imagine, for example, a text from the domain of finance about developments on the stock market, where certain values rise above/to or fall below/down to certain thresholds, etc., where the verbs on their own may not give us enough grounds to distinguish the type of domain, just like the verbs that form part of phrasal/prepositional verb combinations are often semantically relatively empty.
If you still want to retain the original list without pruning, though, you can use a little trick and simply add a # symbol in front of the number indicating the rank, and when you later save the list as text, all lines marked thus will be excluded from the analysis when you use it in AntConc.
For instance, the ratio for the above example, which we obtain by dividing the relative frequency of text 1 by that of text 2, would be 1.6, which clearly indicates that modals are more than 1¬Ω times as frequent in text 1.
Conversely, if the ratio had been below 1, we could easily have observed that they were more frequent in text 2.
The latter, strangely, are still part of the text representation of the XML version of BNCweb, despite the fact that they can easily be encoded in UTF-8, which is, after all, the assumed default encoding for XML.
If you don't use the 'Paste Special‚Ä¶' option, which in Calc even triggers the text import wizard, you'll end up with frequency values that the spreadsheet cannot interpret as numbers because they still contain some (hidden) HTML formatting.
Pasting the token counts from the 'Make/edit subcorpora' page also presents no problem because, this time, we only have one single piece of 'text' without HTML codes, so there's nothing to misinterpret for the spreadsheet program.
From these questions, it may be clear that it's by no means obvious -or at least should not lightly be considered obvious -what the real unit in text could be.
A & B represent different speakers, where each speaker indication in the text is followed by a dot and the turn number.
What therefore seems to be even more important than recognising the c-unit as the correct unit is the fact that one should always be aware that there are certain boundaries in text that form natural borders between the units of text we may want to see as belonging together, an issue that gains in importance once we move away from looking at single words only.
As the previous exercise has shown, finding individual larger units of/in text is quite difficult to achieve.
One might assume that punctuation, something we use all the time in order to delimit 'sentences' from one another, is fairly unambiguous, but, as you may also have noticed, once we start exploring all the varied functions the different punctuation marks may perform in a text, we may be quite surprised by their capacity for creating ambiguity.
We'll discuss options of how to actually mark (up) and distinguish larger units of text in Chapter 11, when we talk about annotation.
The 'n' here theoretically means 'any number of', but usually n-grams are restricted to relatively short sequences, such as bi-or tri-grams, because finding n-grams can be very expensive computationally as long lists have to be kept in memory, and their size grows with increasing length of a text/corpus, and each increment of n.
We can create two basic forms of n-grams/clusters in AntConc, one where we produce lists of sequences of all words in the text indiscriminately, which takes the longest, and another where we create clusters around a given search term.
Also move the full copy of the Sherlock Holmes text here.
The best way to do this is to save both sets of result to text files as we did earlier for our concordancing results.
Open the text files in your editor Next, either simply separate the lines that contain scores above the cutoff points by spaces or some other marking from the rest of the results, or even delete all results below the cut-off points.
While the term markup is sometimes used to indicate the physical act of marking specific parts of a text using specific symbols, and, in contrast, annotation may often refer to the interpretative information added, the two may also be used synonymously.
As such, they can refer to either a particular format for enriching a text, so as to r categorise parts thereof, or r make these parts more salient, or to the actual process of applying this form of annotation.
We'll see some examples and different ways for applying markup to text later on, but for now, we're more interested in the reasons that exist for applying markup to our data in the first place, and roughly which different types of linguistic annotation currently exist.
As pointed out in Section 11.1, all the formats we'll be discussing here are essentially plain text-based, and thus constitute 'human-readable' formats where the text itself contains different types of additional information, sometimes related to its structure, and sometimes to its linguistic content.
Therefore, a paragraph with the number 5 may be represented as <p n="5">‚Ä¶</p>, where the ellipsis (‚Ä¶) stands for the text contained inside it, or as <para n="5">‚Ä¶</para> or even <paragraph n="5">‚Ä¶</paragraph>, if you want to be even more explicit about it being a paragraph.
Furthermore, concordancers, such as AntConc, generally also allow us to search for XML tags because they simply represent plain searchable text, but may also sometimes provide options to hide them when we don't want to display them.
By wrapping our text in the container element, we've effectively created the first (top) level of a hierarchy or categorisation, as well as a file that is now, at least technically, valid XML, despite the fact that it hasn't been sub-divided into meaningful parts yet.
If you look at the text again, you'll notice that the next level below the dialogue in our text is the speaker turn, so it would be quite logical to use this label for a tag surrounding each turn.
For simplicity, and to improve readability, always separate each c-unit text from the tags by line breaks again, so that both container tags and text end up on separate lines.
Knowing this, we now only need to learn how to manipulate/refer to the text we want to pre-pend to the turn.
All we have to do to achieve this is first to declare them as inline elements and then write the appropriate content definitions, where we use the same text as in the actual elements, but get their relevant attribute values using the attr() syntax we used previously for retrieving the turn numbers.
However, I still refer to this as 'Simple XML' because it remains easily readable due to the relatively low number of container elements and the separation of text from nonempty tags.
And, to be able to automatically keep the turn tags separated from the text, we can add new lines in the appropriate places.
Regarding overlap tags, you'd actually be quite right in assuming that, theoretically, these should be container elements because they mark up specific spans of text.
As backchannels constitute text where another speaker simply provides feedback to the current speaker who holds the turn, but doesn't really interrupt to take over, it also makes sense to incorporate this via an empty element.
By contrast, in many ways, software tools in the social sciences that facilitate textual analysis are oriented towards qualitative researchers and focus squarely on providing support for manual text annotation.
For example, it is a good environment for adding annotations to a text, it has excellent multimedia capabilities, and it is XML-compatible.
In those concerned with the structure of the language faculty, and in generative linguistics more specifically, the role of text is less obvious but no less fundamental: native speaker grammaticality judgment has historically been used as the basis for hypothesis generation and confirmation instead of evidence derived from corpora, but such judgment is based on example sentences generated by the researcher's language faculty, not on direct access to the language faculty within the human cognitive system itself, and therefore on text in the above sense.
On the one hand, the possibility of representing language electronically rather than as visual marks on paper, together with the development of electronic media, infrastructure, and computational tools for creation, emendation, storage, and transmission of electronic text have led to a rapid increase in the number and size of corpora available to the linguist, and these are now at or beyond the limit of what an individual researcher can efficiently use in the traditional way.
One response to digital electronic text and data overload is to use only corpora of tractable size or, equivalently, subsets of large corpora, but simply ignoring available information is not scientifically respectable.
In response, research disciplines devoted to information abstraction from very large collections of electronic text have come into being.
They overlap to greater or lesser degrees but share an essentially identical remit: to make interpretation of large collections of digital text tractable.
This book is one of a series entitled Quantitative Linguistics, whose remit is to cover 'all aspects of quantitative methods and models in linguistics, text analysis, and related research fields', and more specifically 'the whole spectrum of theoretical and empirical research, ultimately striving for an exact mathematical formulation and empirical testing of hypotheses: observation and description of linguistic data, application of methods and models, discussion of methodological and epistemological issues, modelling of language and text phenomena'.
The DECTE data are generated by counting the frequency of phonetic segments in interviews, so completeness and accuracy should not be issues if the survey is carefully done using a reliable procedure; manual counting of features in physical text by direct observation is in general far less accurate than the software equivalent for electronic text.
The hypothesis generation methodology described in the foregoing chapters is intended as a contribution to corpus linguistics, whose remit the Introduction described as development of methodologies for creating collections of natural language speech and text, abstracting data from them, and analysing those data with the aim of generating or testing hypotheses about the structure of language and its use in the world.
This work is based on the intuition that there are restrictions on which words can co-occur within some degree of contiguity in natural language strings, and that the distribution of words in text can therefore be used to infer grammatical and semantic categories and category relatedness for them.
A concept that is intrinsically related to representativeness is balance, which has to do with the proportions of the different samples included in the corpus.
But most importantly, as discussed above, the issue of representativeness needs to be addressed before a corpus, regardless of size, can be considered appropriate for a given study.
Some researchers have gone further still, dismissing the notion that representativeness is achievable or important in web corpora.
The exact composition of the searchable web is something we know surprisingly little about as a research community, making it difficult to assess the representativeness of our web-derived corpora.
Perceived shortcomings relating to the size, scope (and representativeness and reusability) and the level of availability of current multimodal corpora have been mentioned, along with some of the challenges regarding the representation and analysis of multimodal corpora.
Bootstrapping could be used as a method for evaluating the linguistic representativeness of a corpus (cf. Chap. 1).
It is for this reason that it is difficult to maintain a perfect balance between the different parts of these corpora, whose representativeness cannot be fully guaranteed.
We will return to the problem of representativeness in Chapter 6.
Then, we will address various important methodological concerns for creating a corpus, in particular questions related to the size and representativeness of samples, and will explain simple methods for data sampling and coding.
The problem we have just mentioned involves a key methodological point for corpus studies, which is the representativeness of data.
The question of representativeness is therefore essential so that a corpus can be used for answering a research question.
The main issue is to determine the criteria according to which it is advisable to classify the elements included in the corpus to ensure its representativeness.
Very often, the creators of new corpora solve the problem of representativeness by following the criteria used in existing reference corpora, such as the British National Corpus, a pioneer of the genre.
In cases where researchers need to compile specialized corpora, the question of representativeness is posed a little differently.
In the same way, the question of the representativeness of specialized corpora is clearly simplified, because this can be achieved by choosing texts or recordings belonging to a specific genre.
In the case of monitor corpora, while the new data added at regular intervals may improve their representativeness, they nonetheless pose balancing problems between the different portions of the corpus, an aspect we will discuss in the following section.
The representativeness of a corpus also depends on its balance and the choice of samples it contains.
More recent corpora at first glance appear to take a more principled approach to representativeness or balance.
However, in light of the above discussion of representativeness, there is little reason to believe that any of these corpora, or the many others that fall somewhere between BROWN and ICE, even come close to approximating a random sample of (a given variety of) English in terms of the text categories they contain and the proportions with which they are represented.
It seems to me that, in fact, corpus creators are not striving for representativeness at all.
It is important to acknowledge that this does not mean that diversity and representativeness are the same thing, but given that representative corpora are practically (and perhaps theoretically) impossible to create, diversity is a workable and justifiable proxy.
On the other hand, assuming (as we did above) that language structure and use are not infinitely variable, size will correlate with the representativeness of a corpus at least to some extent with respect to particular linguistic phenomena (especially frequent phenomena, such as general vocabulary, and/or highly productive processes such as derivational morphology and major grammatical structures).
A well-designed corpus can provide representative samples of registers (see Clancy 2010 on representativeness).
These matters include attempting to achieve comparability and representativeness, two important but sometimes mutually exclusive goals.
In this section, we discuss how two of the central desiderata in corpus linguistics apply to historical registers: representativeness and comparability.
However, both Biber's and Leech's approaches to representativeness may lead the linguist into difficulties when s/he is confronted with historical material.
Moreover, potentially conflicting desiderata such as comparability and representativeness make it necessary for scholars to consider carefully the make-up of their corpora and the extent to which results based on a selection of registers can be generalized to the language as a whole.
The empirical basis on which researchers can now rely, especially for writing, is more solid than in previous data collections which, in the eyes of SLA specialists themselves, suffered from a lack of representativeness.
Over the years, scholars have noted and discussed the occasionally elusive nature of the challenges and suggested solutions to the problems, tackling broader and abstract notions such as representativeness and balance of corpora, but also more concrete questions like sampling, various levels of annotation, precision and recall of corpus queries, dispersion of search hits across a corpus, statistical evaluation, among other points.
Of course, the issue of genre balance also affects smaller diachronic corpora like the Helsinki Corpus, because some genres may disappear and new ones appear over time, and compromises must be made in the compilation process in terms of representativeness vs. diachronic comparability.
The flipside is that this may potentially compromise the representativeness and balance of the filtered sample, and this needs to be evaluated on a case-by-case basis due to the uneven and erratic distribution of OCR errors in the database.
Yet the by far most serious issue is the presence of errors in the source material, which introduces errors to analyses, and, in the worst case, may compromise the representativeness of corpora.
However, the use of adjectives is a stable enough linguistic variable, so there is no need to worry about the representativeness too much.
In addition, when considering diachronic representativeness, we need to deal with certain limitations inherent in historical data.
If there is not enough evidence, or the evidence is biased towards, for example, a certain genre or certain types of speakers/writers (see 'diachronic representativeness' above), the patterns we observe are ambiguous.
This assumption requires reflection on what is meant by corpus representativeness.
Notable differences between lists would suggest limitations to their generalizability, and, ultimately, to the representativeness of the corpus upon which they were based.
This, in turn, also provides evidence of increasing linguistic (i.e., lexical) representativeness of the corpora upon which the lists are based.
The key concepts here are the notions of the corpus as a sample and of balance and representativeness.
The concepts that we normally apply for this purpose are representativeness and balance.
As with representativeness, the issues around balance are fraught with difficultly, but the critical thing for the user of any corpus is to be actively aware of how that corpus is balanced, to think carefully about what this means for the generalisability of results based on that corpus.
We, therefore, take a melioristic approach where we outline ways in which corpora can fare better or worse in regards to representativeness, focusing on the previously mentioned corpus parameters.
Generally, when it comes to size it is obvious from a representativeness point of view that bigger is better.
If the degree of expansion is high, then saturation is low (as is representativeness).
This also means that -like representativeness -full saturation is not attainable but only approachable.
For instance, a factor drastically lowering the representativeness of written corpora is the fact that we often do not know anything about specific circumstances of text production and editing before the written texts were published.
Such a focus in representativeness is motivated by a concomitant focused research agenda; for example, the communication during air traffic control (ATC) between pilots and air traffic controllers.
Since they attempt to achieve maximal representativeness for a language, they add new texts being produced with the flow of time, such as COCA mentioned above.
After evaluation, you may determine that additional texts or text types are necessary to achieve better representativeness.
According to our design criteria and our generalised corpus-building scheme, we select and collect texts carefully following considerations of representativeness.
Language documentation shares with corpus linguistics the basic goal of representativeness.
We discussed in Chapter 3.1 how the size and composition of a corpus reflect in various degrees its representativeness and saturation.
The integrity and representativeness of complete artefacts is far more important than the difficulty of reconciling texts of different dimensions.
A concept that is intrinsically related to representativeness is balance, which has to do with the proportions of the different samples included in the corpus.
However, even despite efforts to preserve genre balance, any diachronic comparison across this timespan has to take into account that the language used in such a corpus changes from a relatively standardized written language with more or less direct roots in a contemporary vernacular to a functionally impoverished and much more writing-dependent language used mainly in religious contexts and as a lingua franca among European scholars and scientists.
In other words, we need to find a good balance between undersmoothing (resulting in too much wiggliness) and oversmoothing (resulting in missing out on significant wiggliness).
The solution offered by GAMs is to balance two constraints, one constraint demanding that we want to stay faithful to the data (by minimising the summed of squared errors), and one constraint that demands we keep our model as simple as possible.
But while gender balance is more easily achieved in modern corpora such as the BNC or COCA, in CEEC it was much more difficult, largely because literacy rates were much higher among men during this period than women.
Achieving gender balance in a corpus involves more than simply ensuring that half the speakers and writers in a corpus are female and half male.
In certain written genres, such as scientific writing, it is often difficult to achieve gender balance because writers in these genres are predominantly malean unfortunate reality of modern society.
To summarize, there is no one way to deal with all the variables affecting the gender balance of a corpus.
When we work, for instance, our conversations take place in a specific and very common social contextthe workplaceand among speakers of varying types: equals (e.g. co-workers), between whom a balance of power exists, and disparates (e.g. an employer and an employee), between whom an imbalance of power exists.
It is for this reason that it is difficult to maintain a perfect balance between the different parts of these corpora, whose representativeness cannot be fully guaranteed.
The preparation of samples to be included in the corpus poses two important methodological questions: on the one hand, the appropriate size for each sample, and, on the other hand, how to balance the portions of the corpus in such a way that the result is truly representative of the genre.
The representativeness of a corpus also depends on its balance and the choice of samples it contains.
One of the problems is the need to find a good balance between tags which are specific enough so as to avoid unwanted amalgams, and categories which are sufficiently generic so that they can be reliably identified by annotators (see section 7.5).
In other words, the proportion of language varieties that speakers encounter varies, requiring a notion of balance based on the incidence of language varieties in the linguistic experience of a typical speaker.
More recent corpora at first glance appear to take a more principled approach to representativeness or balance.
Instead, they seem to interpret balance in terms of the related but distinct property diversity.
However, there is a potential problem that we need to take into account: the values of the variables Sex and 6.6 Complex research designs Age and their intersections are not necessarily distributed evenly in the subpart of the BNC used here; although the makers of the corpus were careful to include a broad range of speakers of all ages, sexes (and class memberships, ignored in our study), they did not attempt to balance all these demographic variables, let alone their intersections.
All other cases are clearly figurative: a taste for excitement in (12a) means 'an experience', instantiating the metaphor experience is taste; transport sb. across the world in (12b) means 'make sb think of a distant location', instantiating the metaphor imaginary distance is physical distance, bulwark of society in (12c) means 'defender of society', instantiating the metaphor defense is a wall, give one's right arm to do sth in (12d) means 'want sth very much', instantiating the metonymy body part for personal value; be within arm's length in (12e) means 'be in close proximity', instantiating the metonymy arm's length for short distance; make sth serve one's aims in (12f) means 'put sth to use in achieving sth', instantiating the metaphor to be used is to serve; cash in in (12g) means 'be successful', instantiating the metaphor life is commercial transaction; hold gun to sb's head in (12h) means 'coerce sb to act', instantiating the metaphor power is physical force; (12j) is from a speech by the Soviet head of state Nikita Khrushchev in which he uses artillery to refer metonymically about nuclear missiles; you told me in (12k) means 'your co-employee told me', instantiating the metonymy employee for company; the superego is soluble in alcohol in (12l) means 'self-control disappears when drunk ', instantiating the metaphor character is a physical substance; balance the books in (12m) means 'make sure debits and credits match', instantiating the metaphor abstract entities are physical entities.
Note that the balance should consider not only the number of texts in each sub-corpus but should also consider the word count of the sub-corpora.
Another issue related to corpus balance in your corpus relates to text types.
In a corpus of general song lyrics, you would want to include lyrics from different types of music (rock, rap, country, popular music, etc.) in order to achieve balance in your corpus.
Second, in Section 6 we will consider the issue of variation within English, by looking primarily at genre coverage and balance in the corpora.
This choice reflects speakers' effort to find a balance between explicitness and economy.
Studies in lexical grammar are currently pulling in two directions, and any research project has to find a balance between the two.
However, this balance needs to be redressed and projects such as the Role Play Learner Corpus and the Louvain International Database of Spoken English Interlanguage are particularly welcome.
Over the years, scholars have noted and discussed the occasionally elusive nature of the challenges and suggested solutions to the problems, tackling broader and abstract notions such as representativeness and balance of corpora, but also more concrete questions like sampling, various levels of annotation, precision and recall of corpus queries, dispersion of search hits across a corpus, statistical evaluation, among other points.
Here, we discuss issues related to the reliability of the database with respect to OCR (Optical Character Recognition) accuracy as well as problems pertaining to the balance of the database and the metadata associated with it.
Of course, the issue of genre balance also affects smaller diachronic corpora like the Helsinki Corpus, because some genres may disappear and new ones appear over time, and compromises must be made in the compilation process in terms of representativeness vs. diachronic comparability.
The flipside is that this may potentially compromise the representativeness and balance of the filtered sample, and this needs to be evaluated on a case-by-case basis due to the uneven and erratic distribution of OCR errors in the database.
In corpora that do not include whole texts but only text samples (like the Brown family) it is therefore important to achieve a balance in terms of the different sections of texts represented.
However, especially at the beginning, it is probably wise to try to strike a balance between minimizing typing on the one hand and maximizing transparency of the code on the other.
This is not simply a matter of finding a balance, but constantly exploring new ways of approaching language so that possibilities of finding new knowledge constantly come into view.
The key concepts here are the notions of the corpus as a sample and of balance and representativeness.
The concepts that we normally apply for this purpose are representativeness and balance.
As with representativeness, the issues around balance are fraught with difficultly, but the critical thing for the user of any corpus is to be actively aware of how that corpus is balanced, to think carefully about what this means for the generalisability of results based on that corpus.
In this textbook, we attempt to counter-balance the traditional focus on written texts and refer to corpora of non-written language texts as much as possible.
One means to represent different situationally defined text types evenly in a given corpus is to aim for balance.
A researcher should then balance the efforts and rewards of making changes throughout an entire corpus; keeping some glosses underspecified, as described above, is a relevant technique here.
Within the written section of the BNC, there's a 75:25 balance between 'informative' and 'imaginative' prose, where the latter also includes a certain amount of 'written-to-be-spoken' materials, i.e. speeches, plays, etc.
In order to do so, in the past corpus compilers have frequently resorted to using samples of approximately 2,000 words from different texts and genres to achieve balance, but both that number and 'cutting out' part of a text appear fairly arbitrary, and may in fact make such samples somewhat unrepresentative of the texts they're supposed to represent as a whole.
As an example, see Chap. 8 and the subsection on quantitative analysis of concordance lines.
MS Word formats are avoided, as these add various types of information to the file and do not work with corpus tools such as concordance programs.
If, say for various reasons related to copyright, it is not possible to make the complete set of corpus files available to others, the corpus could still be made searchable online and concordance lines from the corpus be shown.
The corpora were compared qualitatively as well, by identifying patterns in the concordance lines and analysing the context ("collocational profiles") of the references to hosts, specifically of people and locals, which occurred in both corpora.
Now, however, this is considered one of the standard five methods of the field, alongside frequency, concordance, n-gram and collocation.
In a section devoted to qualitative analysis, we detail how a discourse-analytical approach, either on the basis of unannotated concordance lines or on the basis of output generated by a prior quantitative examination of the data, can help describe and, crucially, explain the observable patterns, for instance by recourse to concepts such as semantic prosody.
In a section devoted to quantitative analysis, we discuss how concordance lines can be scrutinized for various properties of the search term and annotated accordingly.
Annotated concordance data enable the researcher to perform statistical analyses over hundreds or thousands of data points, identifying distributional patterns that might otherwise escape the researcher's attention.
Depending on what your end goal is, you may want to not only sort (and sort in different ways to obtain different perspectives on your data), but also prune a concordance.
By pruning, we here mean one or more of the following: deleting certain concordance lines and keeping others; narrowing down the context window; or blanking out the search term and/or collocates.
Most typically, we delete concordance lines and/or clip the context window in the interest of saving space.
Spatial restrictions apply to a handbook article like this one (hence the 15line snippets as opposed to displaying the concordance in its entirety) as much as to the use of concordances for classroom use (not many students would want to inspect thousands of concordance lines).
In a research context, in contrast, especially when researchers want to make a claim for exhaustive data retrieval and/or when the sequencing of attestations in the corpus matters for the research question at hand, one would only delete concordance lines that contain false hits.
In any case, concordance lines serve to provide the lexical and/or grammatical context of a search term and thus can be needed at all stages of an analysis, from data coding to interpretation.
In order to show the benefit of a qualitative analysis of concordance lines, we stick with the subject of constructions of refugees in the BNC.
It is often wise to look at expanded concordance lines before making a strong claim, in order to consider more context.
Before making claims, it is important to consider a sample of expanded concordance lines and to maintain vigilance in terms of spotting lines that potentially may be functioning differently to a first glance.
Many quantitative corpus analyses are based on concordance data (though not necessarily all: one could think of, for example, a study that is based on frequency or collocation lists instead, see Chaps.
The conversion of the concordance lines into a spreadsheet like in Fig.
Representations of foreigners were largely concerned with political institutions like the foreign office, although a sample of 21 out of 100 concordance lines taken at random (using an online random number generator) showed negative constructions of foreigners involving stereotyping, implying they were taking up British resources or jobs, or controlling British interests.
They annotated 2,986 attestations captured as concordance lines for 14 variables that were previously shown to impact native speakers' choices, including the semantic relation encoded by the noun phrases, the morphological number marking on the noun phrases, their animacy, specificity, complexity, and, crucially, the L1 background of the learners, among others.
The inspection of dozens of alphabetically sorted concordance lines enables patterns to emerge from a corpus that an analyst would be less likely to find from simply reading whole texts or scanning word lists.
Corpus, concordance, collocation.
There are various ways of processing (e.g., data analytics, concordance, collocation, keyword marking, local-word-grouping, lexical clustering, lemmatization, morphological processing, sentence processing, and named entity marking), which are applied to a corpus with appropriate technological supports.
Then, they analyzed the occurrences of these words one by one using the concordance file (see Chapter 5, section 5.7).
These databases are useful for quickly finding concordance examples but cannot be seriously considered as representative corpora (see Chapter 6).
A concordance provides a quick overview of the typical usage of a particular (set of) word forms or more complex linguistic expressions.
In other words, the researcher will go through the concordance and assign every instance of the orthographic string in question to one word-sense category posited in the corresponding lexical entry.
These non-primed comparatives are fairly evenly distributed -194 are analytic comparatives and 187 are synthetic, which is not significantly different from a chance distribution (ùúí 2 = 0.13, df = 1, ùëù = 0.7199), suggesting that our sample of adjectives does indeed consist of cases that are fairly evenly distributed across the two comparative strategies (see Supplementary Online Material for the complete annotated concordance).
This mental concordance is accessible and can be processed in much the same way that a computer concordance is, so that all kinds of patterns, including collocational patterns, are available for use.
As we described in the previous chapter, after logging into the latest version of COCA (currently 2023), you have different choices, namely, choose "list", which will provide you with concordance lines, "chart" which will provide a chart of the frequency of your selected keyword in different registers, or "word" which will give you a plethora of information about the word you are searching (e.g., collocations, synonyms, clusters, related words, and much more).
The most common form to display a keyword in context (KWIC) is through concordance lines.
As mentioned above, concordance lines highlight the word you pick and provide additional text around it.
In this case, since you are more interested in how your keyword is used in different registers, you would want 100 sample concordance lines randomly selected from spoken discourse, and 100 concordance lines randomly selected from academic prose.
To get those concordance lines for your sample, go to the main page of COCA, click on "chart" and type in "sa*_v" in the search box.
Type in "say_spoken" under "Create new list" above the sample concordance lines and save the list.
This type of information is included in the "headers" of the text but will not be read by the concordance software.
The function of Anthony's concordance program is similar to what we saw at the main interface of COCA.
Using your own corpus, you should be able to do KWIC searches through the concordance lines, and other types of lexical as much as grammatical analyses in your own texts.
In addition, this program is able to show you how the word (or collocate or lexical bundle or any n-gram) is distributed within each of your texts (a concordance plot) as well as how many texts include examples of your search term.
Google Books (BYU) cannot generate these concordance lines, because it is based just on n-grams.
Digital humanities scholars cite the work of Roberto Busa working with IBM in 1949, who produced his Index Thomisticus, a computer-generated concordance to the writings of Thomas Aquinas.
I highlight some limitations of the existing tools and methods, which include for example limited support of manual categorization of concordance lines and categorization of key words.
All concordance tools provide for searching by a simple word and some tools permit searching for suffixes, multiple word phrases, regular expressions, part-of-speech tags, other annotation embedded within the corpus, or more complex contextual patterns.
The idea of such a concordance arrangement predates the computer by quite a significant margin and scholars have in the past created concordances by hand for significant texts such as the Qur'an and the Bible.
For example, Cowden-Clarke (1881) took sixteen years to manually produce a complete concordance of all words (apart from a small set of words considered insignificant and occurring frequently such as be, do, and have) in Shakespeare's writings.
For instance, a concordance can be produced for a certain part-of-speech tag, a frequency list of lemmas, key semantic tags, and calculate collocation statistics for which semantic tags relate to a given word.
This usually involves the inspection of concordance lines of an element and their annotation for various linguistic and/or contextual features: if one wants to determine when speakers will use the ditransitive (V NP Recipient NP Patient ) and when the prepositional dative with to (N NP Patient PP to-Recipient ), one needs to inspect the whole sentence involving these two patterns and their larger contexts to determine, for instance, the lengths of the patient and the recipient, whether the clause denotes transfer or not, etc.
The results can then be displayed in the concordance format familiar to corpus linguistics with the search feature (e.g. rise tone or low key) centered in the concordance.
In pursuit of such a measure, Hilpert searched the OED quotations for all ment-types in the database, retrieving a concordance of 91,908 lines and approximately 655,000 words in total.
Since each quotation in the OED is tagged with its historical date, the concordance could be binned into fifty-year increments, which form the basis for subsequent assessments of productivity.
On further scrutiny of the concordance lines, it was found that mention that is largely used to report a negative: someone did not mention or failed to mention a fact.
For instance, in a study of how the Arab revolts were debated in the briefings, the first step was to concordance the names of some of the countries involved, namely, Libya/Libyan(s), Syria/Syrian(s), and Egypt/ Egyptian(s), along with the names of the countries' leaders, Qaddafi, Assad, and Mubarak.
We then passed from the concordance to close reading of the co-text around occurrences of Mubarak, a very common process in corpus-assisted discourse studies.
In fact a concordance of restraint and another of violen* in an 8-word span of on either/both/all sides yielded altogether 18 results, all of them contained in the Podium's turns.
The concordance of all/both sides allowed us to identify the countries where both government and opposition were urged to show restraint and those where only the government was being blamed for the violence.
It also offers a starting point for corpus-driven investigations of academic corpora by generating list of items which can be further explored in more detail using concordance analyses.
So, by checking the frequency of definite, indefinite and "zero" articles in a corpus of bios and then looking at concordance lines for each, we find that professors are far more likely to use naming terms that collocate with definiteness (she is professor of, he is the author of) which serve to uniquely identify them.
The more specific corpus tools and methods that are employed comprise relatively basic techniques: the retrieval of clusters, key comparisons, concordance searches, and the identification of (significant) collocates.
At a glance, it may seem clear that at first is an adverbial expression ("initially"), but with each potential phrasal expression identified an additional concordance of that item was run, and then it would become clear that at first also has non-phrasal expression manifestations, as in love at first sight.
What the random sampling entailed was simply generating a concordance of the potential phrasal expression in question using the entire BNC corpus.
Once generated, the concordance was saved and then a special command -"delete to N" -was used to reduce the concordance lines to a random sample of just 100.
In the case of at first, out of 100 randomly selected concordance lines, 84 exemplars of at first in its phrasal adverbial sense remained -or 84 percent of the original total.
Experimental subjects used concordances to work with their new words exclusively, inferring meanings from multiple concordance lines and only using a dictionary to confirm their inferences, while controls used the same software but with a bilingual dictionary as the information source.
Perhaps most strikingly in need of study are the longer-term or secondary effects of regular concordance work on language awareness and sensitivity, autonomy, motivation, noticing, and other cognitive and metacognitive skills, and so on; their virtual absence in the studies covered here is no doubt due in large measure to the difficulty of assessing such features over time.
Not only have new corpora become available that are massive in size compared to those compiled in the 1980s and 1990s, but search engines or online interfaces have also become technically more and more sophisticated, giving users the opportunity to observe statistically organised search results that go far beyond the presentation of mere concordance lines.
Nevertheless, based on these observations, it would be highly advisable to inspect concordance lines of the highest-ranking collocations to check for possible skewing effects by names.
On the one hand, from the point of view of quantitative L2 analysis, these could thus be considered unwanted items or 'false positives' as their inclusion in word counts and concordance analyses will affect findings on what structures learners may have acquired and are able to produce.
While such an "individual-document approach" (McEnery & Hardie 2011: 232) might indeed be appropriate for some purposes, corpus linguists would usually be interested in looking at a concordance to identify patterns in the use of the search term or calculate and compare its normalised frequency across different sections of the database.
Typically, the research question itself (step 1) is refined in the light of categorisation and analysis of concordance results and comparison operations between corpora, and then the stepwise process begins again.
First and foremost, the concordance view with one word of interest aligned vertically in the middle of the text and the left and right context justified in the middle, is a way of visualising the patterns of the context of a particular word, and is the main way that corpus linguists engage with corpora.
Finally, it should be noted that GraphColl has a concordance feature built-in so that users can use the interface to more closely examine specific collocations in context.
Topics discussed herecollocations, keywords and manual coding of concordance linesplay a key role both in the study of semantics ('dictionary' meanings of words) and in discourse analysis.
This is a variable that involves categorization or evaluation of cases (e.g. concordance lines) by the analyst that might bring an element of subjectivity into the study.
Another example is the categorization from the 'Think about' task where you were asked to decide which concordance lines show the use of the word religion in a positive context and which in a negative context.
Let's assume that we have asked two raters, a religious person and an atheist, to code the concordance lines from the 'Think about' task.
Following the coding scheme described above, two independent raters coded a random sample of 100 concordance lines from a total of 1,053 containing the word religion in the corpus.
For example, if you want to generate a frequency list of a corpus or a concordance of a word in a corpus with R, you must write a small script or a little bit of code in a programming language, which is the technical way of saying you write lines of text that are instructions to R.
This way, the actual effort of generating a frequency list, a collocate display, a dispersion plot, etc. often reduces to about the time you need with a concordance program.
In fact, R may even be faster than competing applications: For example, some concordance programs read in the corpus files once before they are processed and then again for performing the actual task -R requires only one pass and may, therefore, outperform some competitors in terms of processing time.
Second, by learning to do your analyses with a programming language, you usually have more control over what you are actually doing: Different concordance programs have different settings or different ways of handling searches that are not always obvious to the (inexperienced) user.
For instance, ready-made concordance tools often have slightly different settings that specify what 'a word' is, which means you can get different results if you have different programs perform the same search on the same corpus.
For example, if you call up a concordance of the word difference, then you will most certainly find that the most frequent L1 collocate is the while the most frequent R1 collocate is between.
This method, probably the most widespread corpus-linguistic tool, is the concordance.
You generate a concordance if you want to know in which (larger and more precise) contexts a particular word is used.
Thus, a concordance of a word w is a display of every occurrence of w together with a user-specified context; it is often referred to as KWIC (Key Word In Context) display.
Strictly speaking, a concordance does not have to list every occurrence of a word in a corpus.
A little example of something I came across in an undergrad corpus linguistics course was that we generated a concordance of in and found that in one of the SGML-annotated files of the BNC that we looked at, the word in was tagged as VBZ, which means "third person singular form of to be".
There is no alternative to knowing your corpora, this cannot be done more easily, and any concordance programs that come with more refined search options also require you to thoroughly consider the format of the corpus files even if their interface 'hides' such decisions behind clickable buttons with smiling corpus linguists on them, in settings, or in .ini files.
This approach to Corpus Linguistics is based on the observation of pattern in concordance lines, where a word or short phrase is the node of the line and the few words occurring before and after the phrase are shown for each occurrence.
In addition, a concept known as 'semantic sequences' was developed -this suggested that concordance lines can be read to identify 'what is often said', thus moving from lexis to grammar to discourse.
When a corpus has been tagged, the fundamental techniques of corpus analysisfrequency counts and concordance searches, which will be introduced at length in the next section -can be undertaken at levels of analysis higher than the orthographic word-form.
The two most basic forms of data which we can extract from a corpus are the concordance and the frequency list.
Concordancers vary greatly in terms of what analyses, other than basic concordance searches, they allow.
As we saw above, a concordance is the result of a corpus search: all the examples in a corpus matching some specified search pattern, together with some preceding and following context.
At one level, analysis can be more or less impressionistic -based on scanning the eye up and down the concordance lines in an attempt to observe features of note that recur in the concordance, or to identify different functions of the word or phrase that was originally searched for.
It is especially important not to look at just the beginning of a concordance, as these early concordance lines may present examples from the early part of the corpus, which is not necessarily representative of the corpus as a whole.
In the same way that key items analysis is based on, and abstracts away from, two frequency lists, so a collocation analysis is based on, and abstracts away from, a concordance search.
A more common approach is to begin with a single word (or lemma) of interest, called the node, for which a concordance search has been performed; the surrounding text is then scrutinised to establish what elements occur frequently in the vicinity of the node.
One way is manual and largely qualitative: to undertake a hand-and-eye analysis of the concordance, as explained above, where the focus of analysis is to identify co-occurring elements, the collocates of the node, in the concordance lines.
Also, researchers can use ASCII files in parsers, concordance programs, and taggers.
For example, if the purpose is to study the use of specific collo-cations, lemmas, or n-grams, the researcher can use concordance programs such as Wordsmith or AntConc.
Then, the researcher should transfer the results obtained through the concordance programs to statistical software programs such as Excel or SPSS.
Often a concordance display gives information about the word by putting that word in the middle of a line with a certain amount of words preceding and following it.
Below is an example concordance from COCA of adjective + woman, showing the first five lines.
Essentially, a concordance is a listing of individual word forms in a given specific context, where the exact nature of the context depends on the requirements of the analysis and which particular program one may be using.
However, as useful as an ordinary KWIC concordance may be, AntConc also offers us the functionality to create much better views of our search results by providing options for sorting the results based on their immediate left or right contexts.
Once you're finished, you can just load the resulting file in AntConc instead of the original corpus, and filter out or sort the relevant entries in another concordance.
This exercise was designed to allow you to explore (very roughly) the different types of functionality a concordance program, such as AntConc may provide in addition to pure concordancing.
The only thing that could happen is that you accidentally either delete an entry you hadn't intended to delete, in which case you'll need to re-run the concordance and delete more carefully, or that you may accidentally select too many hits before pressing Delete.
In the latter case, you won't need to re-run the concordance, but can simply click anywhere in the hits to remove all selections, although you'll still need to select the ones you want again.
Now, even if you are aware of all the relevant forms you may need to identify, and search for each of these forms separately in a row in a concordance program, you can only save the results, maybe even print them out, and then compare them afterwards.
Most concordance packages support at least some basic forms of regexes, although they're not necessarily as advanced as the options offered by command-line search utilities, such as (e)grep (global regular expression printer), or programming languages, such as Perl, Python, or Java.
Essentially, being a concordance facility, too, some of its basic features are rather similar to the ones we've already discussed for AntConc.
As you'll have observed, the concordance output itself consists of the number of the hit, the name of the file it was found in -as a hyperlink -and the concordance result.
You may now be tempted to investigate further why you can find these American spelling variants in the corpus, and of course the easiest option for this is to click on the link for 'colour' in the frequency breakdown to get to the concordance for the results, and then checking the meta-information for each result.
First of all, when you take a close look at the listing of words + tags in the table, you'll notice that they're in fact hyperlinks that, once clicked, provide you with a concordance of exactly the combination specified, so that you can already narrow down your search in this way.
If you're unsure what a particular 'word' entry in the list means, click on it and this will take you to a concordance of that entry.
Unlike the sorting options we had for concordance lines, where we were able to sort according to n number of words to the left or the right quite freely, in this case, we have a more limited set of options, based on the options for combinations of output for types and frequencies, as already mentioned above.
To check on 'strange items' in the list, you can use a right mouse click on the frequency to display a concordance of the item in a new tab.
You can verify this for sil by clicking on it in the 'Word' window, which will take you to a concordance view of the item, where you can also see that this annotation normally appears in angle brackets in the source texts.
Again, clicking on the item and investigating the concordance lines will soon tell us that s isn't only used to mark a particular speaker, but of course also represents the clitic (contraction) forms of is (as in that's) and us (as in let's), although there are no possessive markers in the corpus.
And, just in case you're curious to find a single letter A in the data (as you should be), you can investigate this through the concordance by clicking on it.
This is because the default option for the concordance module is to ignore case.
Of course, you could fix this manually, but, since we've now separated the data from our ability to concordance on it, we'd have to go back into BNCweb to look at the original frequency list.
Even if (sorted) concordance lines can already represent an extremely valuable asset in a teaching context because learners -as well as teachers -can investigate words as they're really used in authentic materials, such an analysis may be rather time-consuming.
In addition to straightforward concordancing, though, also explore other ways of investigating the results, such as those that the sorting options/restrictions for the concordance lines offer.
This obviously isn't as useful because there's no way to directly investigate the context by clicking a link whenever you find an interesting example, but of course better than nothing -or looking through hundreds of concordance lines generated for single prepositions in the hope of finding suitable combinations, which you could still do by using the KWIC display option and sorting according to first word on the right of the node.
Unfortunately, though, as useful as AntConc is for most purposes, since it's a stream-based concordancer it'll ignore all line breaks and match more than we want to see in our concordance, so using it here is not an option.
Regarding the latter, you may now, quite rightly, expect to find at least the other two question words what and who in the same concordance list, as they can certainly be followed by the same clitic, but, due to the tagging rules of CLAWS, these are in fact classified in different ways from the other question words.
As you may have noticed, this editor is really not optimised for handling large files, so to just view the frequency list without the ability to concordance on it, a dedicated editor, such as Notepad++, would be far better.
On the other hand, using such an editor would only allow you to search through, but not concordance, on the file.
Unfortunately, when working with most corpora, and in most concordance programs so far, the option for handling data involving a measure of the syntactic units they occur in is still absent, something we just saw in the Exercise 80.
Nowadays, many concordance packages, such as SketchEngine and LancsBox, have annotation packages built in, enabling automated annotation of features such as parts-of-speech, parsing and even semantic annotation.
Corpus enrichement is an important way forward, and this applies not only to metadata but also to linguistic annotation.
The reality of contemporary corpus linguistics is that the corpora we rely on, in most cases, are simply too large for manually adding annotation, and the automated annotation of electronic texts has become the primary focus in the development of annotation methods.
In order to simplify the discussion that follows, we illustrate our points about the fundamentals of annotation using primarily English data.
Even if an automatically annotated corpus is unlikely to meet all the expectations of a researcher in terms of its categories of annotation, it can still be an invaluable resource.
Another kind of higher-level annotation is syntactic parsing, which aims to provide information about the grammatical structure of sentences in a corpus.
An example of an annotation model like this is the UCREL Semantic Analysis System (USAS).
For one thing, the more fine-grained an annotation system, the more difficult it will be to achieve high accuracy.
All these decisions and the availability of existing conventions and annotation tools can make a significant difference to the overall process of annotation that follows.
As mentioned in Sect. 2.2.1, this is a common task in corpus development, and one on which other forms of linguistic annotation (e.g., lemmatization, syntactic annotation) often rely.
In the most straightforward case, it may be possible to use existing annotation software to automatically apply a given set of POS tagging conventions to corpus source materials.
In situations like these, it is possible to train some kinds of annotation software to apply new tagging conventions to your corpus sources.
This involves manually annotating samples of corpus text as examples of how the new annotation categories should be assigned.
While annotation facilitates linguistic research and enables more immediate access to certain kinds of patterns in a corpus, one should acknowledge the potential for valuable linguistic research to be carried out even on unannotated corpora (cf. Chap. 8).
Additionally, some data models add grouping mechanisms to annotation graphs, often referred to as 'annotation layers', which can be used to lump together annotations that are somehow related.
Basic annotation graphs, such as syntactically annotated treebanks, can be described in simple inline formats.
This also extends into the choice of annotation tools, as one can use separate tools, for example to annotate syntax trees, typographical properties of source documents, or discourse annotations.
The complexity of the corpus architecture results from its annotations: as the data is collected, student annotators iteratively apply a large number of annotation schemes to their data using different formats and tools, including document structure in TEI XML, POS tagging, syntactic parsing, entity and coreference annotations and discourse parses in Rhetorical Structure Theory.
For example, the constituent S representing the clause has a similar meaning to the sentence annotation in the 'sentence types' layer, but these have been modeled as separate.
As a result, it is at least technically possible for this corpus to have conflicting constituent trees and sentence span borders for sentence type annotation.
If these layers are generated by separate automatic or manual annotation tools, then such conflicts are in fact likely to occur over the course of the corpus.
The graph-based data model allows for completely independent annotation layers, united only by joint reference to the same primary text.
To remedy this, MERLIN includes a wide range of error annotations, with a major error-annotation category layer (EA_category, e.g. G_Morphol_Wrong for morphological errors), and more fine grained layers, such as G_Morphol_Wrong_type.
In practice, a large part of the choice of corpus architecture is often dictated by the annotation tools that researchers wish to use, and the properties of their representation formats.
For example, the Universal Dependencies project is managed entirely on GitHub, including publicly available data in multiple languages and the use of GitHub pages and issue trackers for annotation guidelines and discussion.
Having an annotation tool which supports a complex data model may be of little use if the annotated data cannot be accessed and used in sensible ways later on.
By far the best -in the sense of most versatile and powerful -approach to exploring issues of dispersion is with programming languages such as R or Python (see Chap. 9), because then the user is not dependent on measures and settings enshrined in ready-made software but can customize an analysis in exactly the way that is needed, develop their own methods, and/or run such analysis on data/annotation formats that none of the above tools can handle.
A number of other studies make use of further computational methods to undertake semantic annotation and categorisation.
The incorporation of a computational annotation system allows for systematic, rigorous annotation followed by statistical analysis.
Conversely, even the most sophisticated quantitative analysis typically entails item-by-item annotation of each data point, which equates to a qualitative analysis of sorts; likewise, the results of a quantitative analysis demand interpretation, which typically requires a qualitative approach.
Apart from that, diachronic corpora differ widely in size, composition, scope, annotation, and the nature of their textual material.
Turning from the level of the texts that make up a corpus to the internal properties of those texts, perhaps the most fundamental question compilers and users of diachronic corpora must ask is to what extent they can rely on methods devised for the annotation and analysis of contemporary data in handling data from older periods. Older texts are in principle somewhat alien.
For this reason, rich annotation is best seen only as a means to facilitate querying a corpus.
This is chiefly due to the greater costs and challenges in terms of technology and time that are connected with the compilation and annotation of spoken corpora.
Annotations of existing spoken corpora differ in two ways: annotation layering and time-alignment.
The term 'annotation layer' refers to the issue of whether different types of annotation are integrated together in one linear transcription or whether they are represented individually on separate layers.
This collection of papers focuses on questions of standards for the construction, annotation, searching, archiving and sharing of spoken corpora used in conversation analysis, sociolinguistics, discourse analysis and pragmatics.
As a matter of fact, issues related to multilingual annotation (e.g. whether it should be languagespecific or language-neutral, or, more generally, how cross-linguistic comparability can be achieved) have received relatively little attention in contrastive linguistics and translation studies (one notable exception is Neumann 2013).
Longitudinal corpora imply regular recordings over at least several months, transcriptions of each utterance (ideally not only of the target child, but of all interlocutors), and a multitude of different annotation levels, depending on the questions of the respective project.
This point is crucial considering how additional target children multiply the amount of time and money required to compile a corpus with minimum annotation standards.
These annotation steps are often referred to as "tagging" or "coding" in the literature.
First, it does not follow the principle of separating distinct semantic layers (such as segments vs. morpheme functions) in the annotation syntax, thus making this format harder to read, process, and convert than others.
Some common examples include the annotation of semantic roles or other properties linked to reference, syntactic dependencies, speech acts, errors on various levels, or actions accompanying speech (gaze and pointing).
CHILDES also provides a number of tools for corpus development including transcription and annotation programs.
These taggers and parsers and their associated annotation systems tend to be unable to support the analysis of language in use beyond spoken discourse.
As a 'pilot' study, the paper with a corpus only comprising six recordings of circa 7 min each, problematizes the collection, annotation and analysis of multimodal corpora for research.
All of the tools listed require manual transcription, annotation and analysis of data: these processes are not automated.
Two major types of pre-treatment can be distinguished, i.e. automatic annotation and sampling.
For instance, consider the task of part-of-speech (POS) annotation (i.e., determining the correct POS of each word in a given sentence of a text).
In the present context, we use this term to refer to a large digital collection of natural language texts that are assumed to be representative of a given language, dialect, or a subset of a language, to be used for linguistic annotation, processing, and analysis.
The text should be free from any annotation that carries linguistic and extralinguistic information.
It is known to us that corpora can be of many types with regard to texts, languages, modes of data sampling, methods of corpus creation, manners of text annotation, and goals of text utilization, etc.
Development of a speech corpus involves issues and factors like purpose of use, selection of speakers, choice of settings, size of a corpus, use of recording instruments, manner of data sampling, manner of data elicitation, nature of transcription, types of data encoding, management of audio files, editing of input data, processing of spoken texts, annotation of speech, analysis of speech corpus, etc.
Since a corpus contains text samples either from a written or spoken discourse, annotation is done by adopting a value-encoding process, which attaches interpretative information at different levels of a text.
The primary purpose of the annotation is to add additional information to a text so that future analysis and interpretation of a text becomes easy, accurate, and useful.
That is why it is also known as part-of-speech (POS) annotation.
In extralinguistic annotation, we annotate a text with that kind of information, which is not physically available inside a text.
Some common types of extralinguistic annotation include semantic annotation, anaphoric annotation, discourse annotation, etymological annotation, rhetoric annotation, and ethnographic annotation.
The primary goal of semantic annotation is to identify the sense a word denotes when it is used in a text.
For instance, we have not yet investigated how a poetic text should be annotated; and what kinds of textual, linguistic, stylistic, prosodic, and figurative annotation techniques we should design and apply to poetic texts.
Problems and challenges are also there in the annotation of medical texts, texts of court proceedings, conversational texts, multimodal texts (where audio-visual elements are included), mythological texts, and others.
Further challenges are involved in the annotation of heritage and classical texts, which require separate tagsets and methods for annotation of mythological characters, events, anecdotes, ecology, and sociocultural properties.
Considerable progress has also been made in the annotation of corpora.
This involves not only collecting data (speech and writing) but encoding it: transcribing recorded speech, for instance, as well as adding annotation to it, such as markup indicating in a conversation when one person's speech overlaps another speaker's, and in writing where such features as paragraph boundaries occur in written texts.
The chapter concludes with a detailed description of various kinds of textual markup and linguistic annotation that can be inserted into a text.
This decision is important because it will determine exactly what the corpus will look like: what types of texts will be included in it (e.g. spoken, written, both), how long the individual texts will be (e.g. full length, text excerpts), how detailed the annotation will need to be (e.g. word-class tagging or a purely lexical corpus with minimal annotation), and so forth.
However, this does not necessarily mean that the text must be excluded from the corpus, since there is annotation that can be included in a corpus indicating that certain sections of a sample are "extra-corpus" material; that is, material not considered part of the corpus for purposes of word counts, generating KWIC (key word in context), and so forth.
Linguistic annotation varies from corpus to corpus as well.
If a text is collected, and saved for later computerization and annotation, the individual who collected the text may not be around to answer questions, and information about the text may consequently be lost.
This problem can be handled with annotation during the actual transcription of the recording that marks certain sections as inaudible.
If annotating overlapping segments of speech is done separately from the actual transcription of the text, the individual doing the annotation will have to go through the tape repeatedly to reconstruct the overlapsa process that could be done more efficiently by the person doing the transcription.
The Santa Barbara Corpus of Spoken American English, for instance, contains not only an exact transcription of the text of a spoken conversation (including hesitations, repetitions, partially uttered words, and so forth) but also annotation marking various features of intonation in the text, such as tone unit boundaries, pauses, and pitch contours.
More recently, however, the Text Encoding Initiative (TEI) has developed a standardized system of annotation not just for linguistic corpora (www.tei-c.org/release/doc/ tei-p5-doc/en/html/CC.html) but for electronic texts in general (www .tei-c.org/).
The next section will describe this annotation as it applies to spoken language; a later section will focus on the annotation used in written texts.
Because of the complexity of the TEI system of annotation for speech, the discussion will be more illustrative than comprehensive.
As long as transcriptions contain clearly identified speakers and speaker turns, and appropriate annotation to mark the various features of speech, a transcription will be perfectly usable.
While word-class annotation is very well established in corpus linguistics, there are other types of annotation as well.
Corpus annotation has changed considerably since ECL1.
More rarely, certain corpora contain a syntactic analysis of all of their sentences, as well as other types of information, such as an annotation of the discourse relations (cause, condition, etc.) which interconnect the sentences within the text corpora.
For these phenomena, it is necessary to use data annotation and this annotation must be done manually on the basis of an observation of the entire corpus.
Thanks to the annotation performed on data themselves, it also shows that certain frequency changes can be associated with the emergence of new functions.
For each occurrence, the pronunciation should be noted, either by a phonetic analysis of the production of the vowel or by a manual annotation carried out by two native speakers.
However, syntactic annotation tools are still imperfect and sometimes too complex to implement or use.
In addition, research on the metaphorical uses of a word must be subject to manual annotation on the basis of their context, since metaphors have no lexical markers that make it possible to differentiate them from the literal uses of the same words.
This annotation therefore involves an interpretation on the part of the researcher, which is, in part, necessarily subjective.
In addition, these corpora often contain an annotation of errors, which favors an explicit learning process and enables learners to become conscious of typical errors and to avoid them.
In this case, it is essential to properly outline the research question that will be studied on the basis of new data, since the latter will have a crucial influence on the whole process, both during the data collection and the annotation phases.
However, they are unsuitable for research questions that require data processing, for example, some kind of annotation or those that have to take into account a large context, in order to identify certain linguistic phenomena such as speech acts or discursive phenomena.
Within the transcripts, it might be useful to include an annotation of the non-standard words used, for example, in verlan, with their equivalent standard so that they can be found in a concordancer.
We will also discuss best practices to follow when making a manual annotation and present the different ways to assess the reliability of such annotations.
Finally, we will present the principles to be respected in order to make annotation sharing easier.
This annotation makes it possible to exclusively look for the noun occurrences of ferme, for example.
Furthermore, the annotation itself is accompanied by a hermeneutic dimension, which reflects the annotators' point of view about the text.
Lemmatizing a corpus is an essential process for studying lexicon, since annotation makes it possible to look for occurrences without having to detail all the morphological variants it may adopt.
The most common annotation is syntactic parsing.
In this case, it is rare to be able to rely on automatic annotation tools.
This annotation is useful since a pronoun like he is not an autonomous referential expression, meaning that it does not by itself make it possible to identify the referent in question if we ignore the context.
Finally, corpora including children's language or productions of foreign language learners may contain an annotation of errors.
When starting an annotation project, it is advisable to gather information about the existence of such standards and to consult previous studies to decide on the best way to annotate data.
Whenever possible, it is preferable to reuse existing annotation schemes and to adapt them to the needs of the study rather than creating new schemes from scratch, for which comparisons with literature could be difficult to draw.
One of the main initiatives, within the formal framework of the International Organization for Standardization (ISO), more specifically in technical committee no. 37 dedicated to language resource management (see the URL at the end of this book), has enabled around 20 standard drafts for linguistic annotation, be it lexical, syntactic, semantic or discursive in nature.
For example, the annotation of speech acts requires setting up a list of the acts to be annotated, for example, requesting, promising, asserting, threatening, etc.
However, a semantic annotation of verb types could differentiate their aspect (state or event verbs).
Finally, an annotation of the grammatical categories associated with each word requires establishing a list of these categories.
There is no single answer to these questions, as the right level of granularity of a (more or less precise or generic) annotation depends, to a large extent, on the objectives of the annotation and its feasibility.
For example, the category question could be defined as follows for an annotation of speech acts: "Any utterance used in context as a request for information, whether a direct or indirect one".
The results of the annotation will be easily understood and it will be possible to reuse it in future work.
As far as possible, the chosen tag sets should be based on annotation standards which have been broadly accepted in the literature.
Should these standards be lacking, it would be a good idea to consider the annotation schemes used in previous studies.
For some phenomena such as the annotation of morphosyntactic categories or speech acts, every word or sentence in the corpus will be involved.
However, for other phenomena, the annotation will only refer to very precise elements in the corpus.
For example, an annotation of discourse markers such as well, you know and I mean will only relate to these elements.
In the rest of this section, we will focus on a detailed annotation of specific elements in the corpus.
An annotation of cleft structures in French can start by looking up structures containing the verb form c'est.
Whatever the annotation considered and the tag set chosen, the annotation of the first occurrences is generally difficult and many problems and borderline cases arise.
On the other hand, there are tools that facilitate the process of manual annotation of data by providing an interface to perform the annotation, as well as a format for representing such annotations.
If necessary, this annotation can be corrected manually.
All of these are part of the event (invite) and can be linked to it by an annotation describing the named entities involved and the semantic links between them.
Another example is the EXMARaLDA tool, which has been specifically developed to assist in the transcription and annotation of spoken corpora, and later be able to manage them.
This tool makes it possible to perform a time-aligned annotation with audio or video files, to insert metadata and to have access to the results of the annotation in different formats.
Tools have been developed in order to make data annotation in XML format easier, such as the Nite XML tool, for example.
In this case, the quality of the automatic annotation is measured in comparison with a reference annotation produced by human annotators.
This definition of the accuracy of an annotation is often subdivided into two separate criteria.
For example, we can report the number of noun occurrences in the reference annotation that the system correctly identified as such, weighing the total number of nouns in the reference.
While calibrating the performance of automatic tools is ultimately based on a reference human annotation, the question arises whether the annotations made by a human are necessarily 100% accurate.
As we have seen, annotation always requires a certain type of interpretation and this may vary slightly from one annotator to another.
In addition, humans can also make mistakes due to fatigue, lack of attention or a poor understanding of the annotation instructions.
In order to improve the reliability of annotations made by humans and to help define clear categories, a commonly used method is to have the same annotation made by two different annotators.
The more convergent their annotation, the more it can be considered reliable and the divergent case can be resolved through a discussion a posteriori.
As a matter of fact, if a certain annotation turns out to be impossible to achieve in a convergent manner for human annotators, this indicates that the phenomenon may be poorly defined or that the categories have been poorly delimited.
The annotation manual should also list the rules that have been applied to certain borderline or ambiguous cases in the corpus in order to deal with them systematically.
Many examples of annotation manuals can be found online and are provided with all the corpora made available to the public.
However, an annotation can be tested from the point of view of its reliability.
An annotation is reliable if two annotators produce convergent annotations or if the same annotator produces convergent annotations at two different times.
Ask a second person to make this annotation and calculate the percentage of agreement and the kappa coefficient.
This advantage is all the more evident inasmuch the same annotation can often be used for answering different research questions.
Finally, the annotation of a corpus represents a transparent process when it is well documented, which enables an objective evaluation of the data by the scientific community, contrary to other methodological choices which are often less transparent.
Since most of the words in the non-specialized lexicon are polysemic, this annotation is essential.
For the annotation above, this table looks as follows: The two annotators converge on 13 of the 20 sentences, that is, there is 65% of agreement.
This value is very low and shows that the annotation is not reliable and should be revised.
This manual should also indicate how the annotation was carried out (by one or two people, throughout how many sessions) and whether successive versions were produced.
A first pilot annotation phase should be used for testing these categories, as well as for identifying problematic cases and leading to the preparation of a more or less detailed annotation manual.
The detailed presentation of the study begins by describing the corpus used and explaining how data was retrieved from it, as well as their annotation.
In corpus linguistics, the collection and annotation of data commonly involves a relatively balanced combination of computer-aided and manual labour.
Given how labour-intensive manual data annotation is, it is difficult to meet the growing need to annotate larger samples for robust statistical research.
Finally, we show that cross-validated results also allow us to employ a powerful model comparison method that helps us determine which methods are worth deploying in future automatic annotation settings.
For our purposes, we will test whether the data annotation for such a study could be done semi-automatically.
In this section, we summarize the evidence gathered across the case studies, and show which classification method is expected to give strongest results in similar semi-automatic annotation setups.
Thus, given its robustness, high reliability, flexibility, and potential for reusability and replicability, it is at least worth considering whether this (semi-)automated data annotation procedure could be what is next for corpus linguistic methodology.
In this section, we will illustrate these types of annotation and discuss their practical implications as well as their relation to the criterion of authenticity, beginning with paralinguistic features, whose omission was already hinted at as a problem for authenticity in Section 2.1.1 above.
This problem is even more obvious in the case of linguistic annotation.
Retrieval and annotation are discussed in detail in Chapter 4.
At the very least, it is crucial to understand that tagging and other kinds of annotation are the result of applying operational definitions by other researchers and if we use tags or other forms of annotation, we must familiarize ourselves with these definitions by reading the fine manuals that typically accompany the corpus.
Second, such cases demonstrate vividly why the two operational definitions of parts of speechby tagging guide line and by tagger -are fundamentally different: no human annotator, even one with a very sketchy tagging guideline, would produce the annotation in (23).
We will deal with data retrieval and annotation in the next chapter and return to the issue of methodological transparency at the end of it.
We already saw that the issue of data annotation is extremely complex even in the case of individual lexical items, and the preceding chapter discussed some more complicated examples.
The power of software-aided searches depends on two things: first, on the annotation contained in the corpus itself and second, on the pattern-matching capacities of the software used to access them.
This syntax is very powerful, allowing us to query the corpus for tokens or sequences of tokens at any level of annotation.
We can also combine two or more attribute-value pairs inside a pair of square brackets to search for tokens satisfying particular conditions at different levels of annotation.
We can also address different levels of annotation at different positions in a query.
Thus, an important aspect in constructing a query is to annotate a random sample of our corpus manual for the phenomenon we are interested in, and then to check our query against this manual annotation.
In the case of many other phenomena, however, automatic annotation is simply not possible, or yields a quality so low that it simply does not make sense to base queries on it.
In the absence of clear instructions they may not know, among other things, 4 Data retrieval and annotation whether to treat ligatures as one or two letters, whether apostrophes or wordinternal hyphens are supposed to count as letters, or how to deal with spelling variants (for example, in the BNC the noun programme also occurs in the variant program that is shorter by two letters).
In order to keep different research projects in a particular area comparable, it is desirable to create annotation and coding schemes independently of a particular research project.
Often, however, such a search will come up empty, or existing annotation schemes will not be suitable for the specific data we plan to use or they may be incompatible with our theoretical assumptions.
In these cases, we have to create our own annotation schemes.
Data retrieval and annotation for this task.
When we are satisfied that the scheme can be reliably applied to the data, the final step is the annotation itself.
Where an annotator is at fault, they could correct their annotation decision.
We will conclude this chapter with a discussion of a point that may, at first, appear merely practical but that is crucial in carrying out corpus-linguistic research (and that has some methodological repercussions, too): the question of how to store our data and annotation decisions.
The first option is routinely chosen in the case of automatically annotated variables like Part of Speech, as in the passage from the BROWN corpus cited in Here, the annotation (i.e., the part-of-speech tags) are attached to the data they refer to (i.e., words) by an underscore (recall that alternatively, a vertical format with words in the first and tags in the second column is frequently used, as are various kinds of XML notation).
We also need annotation schemes detailing how to distinguish proper names from other uses and how to identify of -constructions that encode relations that could also be encoded by the s-possessive.
The 5 Quantifying research questions annotation for whether or not an of -construction encodes a relation that could also be encoded by an s-possessive can be done as discussed in Section 4.2.3 of Chapter 4.
For expository reasons, let us distinguish between the rank value and the rank position of a data point: the rank value is the ordinal value it received during annotation (in our case, its value on the Animacy scale), its rank position is the position it occupies in an ordered list of all data points.
In contrast, the inductive approach can be applied to a large data set because it requires no a priori annotation.
It also does not require any choices concerning annotation categories; however, there may be a danger to project patterns into the data post hoc.
We have already discussed some of the challenges inherent in the creation of large corpora, in terms of accurate metadata and word-level annotation.
Adding annotation allows the researcher to encode linguistic information present in the corpus for later retrieval or extraction using tools described in the next section.
Focusing on the first kind for a moment, it would be possible of course to manually annotate texts using any standard word processor, but here it is useful to have software tools that check annotation as it is added, e.g. to ensure that typos in tags or category labels do not occur, and to allow standard mark-up formats (such as XML) to be employed consistently and correctly in the resulting corpus, e.g. as in the Dexter software.
All concordance tools provide for searching by a simple word and some tools permit searching for suffixes, multiple word phrases, regular expressions, part-of-speech tags, other annotation embedded within the corpus, or more complex contextual patterns.
They can equally well apply to tags within a corpus, if any levels of annotation have been applied.
As mentioned, Wmatrix performs both automatic annotation and retrieval.
This usually involves the inspection of concordance lines of an element and their annotation for various linguistic and/or contextual features: if one wants to determine when speakers will use the ditransitive (V NP Recipient NP Patient ) and when the prepositional dative with to (N NP Patient PP to-Recipient ), one needs to inspect the whole sentence involving these two patterns and their larger contexts to determine, for instance, the lengths of the patient and the recipient, whether the clause denotes transfer or not, etc.
Section 2 is devoted to the "first group," i.e. statistics directly involving corpus-linguistic tools; Section 3 then turns to the "second group," i.e. statistics that are usually applied to the annotation of concordances.
For example, the British National Corpus (BNC) has annotation that shows the corpus compilers considered of course, for example, for instance, according to, irrespective of, etc. to be one lexical item each, which means one would count of course, not of and course separately.
Beyond POS-tagging, the most important level of annotation for grammar is obviously the syntactic level, which allows the investigator the means to extract tokens of particular constituent structure configurations.
The independent variables in our annotation layer are detailed in the following.
LGSWE is more explicit about its methodology, which is based on the annotation of a corpus with the categories used in the book.
While a raw corpus is a highly useful resource, annotation provides an extra layer of information, which can be counted, sorted, and compared.
CEA, on the other hand, provided the opportunity to ponder on the notion of error and introduce a higher degree of standardization at each level of the error analysis process: from error identification to error interpretation through error annotation and counting methods.
Over the years, scholars have noted and discussed the occasionally elusive nature of the challenges and suggested solutions to the problems, tackling broader and abstract notions such as representativeness and balance of corpora, but also more concrete questions like sampling, various levels of annotation, precision and recall of corpus queries, dispersion of search hits across a corpus, statistical evaluation, among other points.
The authors examine a set of case studies, highlighting issues like part-of-speech annotation, miscategorization challenges, and problems related to metadata and digitized texts in historical databases, such as the 10.5-billion-word Eighteenth Century Collections Online, a database which was originally not compiled for linguistic purposes.
The chapter advocates for more detailed annotation of named entities in large linguistic corpora that are already available.
Learner corpora, particularly those containing academic texts, often include instances of multilingual practices, code-switching, and borrowed content, presenting difficulties for annotation and analysis.
Callies concludes his chapter by addressing potential bias in annotation methods within Learner Corpus Research (LCR) and related disciplines.
While the original formulation of the philologist's dilemma is no less relevant for scholars engaged in, say, historical sociolinguistics, we argue that in more grammatically oriented studies that make use of big data, increasing attention should be paid to the corpus itself: to its sampling principles and layers of annotation.
In Section 2, the focus is on part-of-speech (POS) annotation, which can be regarded as one of the most useful kinds of linguistic annotation in contemporary research, but which, as we shall see, does not come without problems.
However, it severely complicates the study of certain types of research questions that rely on POS annotation, as will become apparent below.
Our first examples concerned problems related to part-of-speech annotation, and these challenges are hard to overcome entirely.
The chapter calls for more detailed annotation of named entities in already available large linguistic corpora and reminds of the importance of close inspection of the search hits.
Much work has gone into developing efficient annotation strategies for differentiating items belonging to named entities from those representing regular uses of the words.
A similar practice has been followed in the annotation of the Spanish Learner Language Oral Corpora (SPLLOC), a set of corpora of L2 Spanish that were transcribed using the CHAT system developed by the CHILDES project.
In fact, as ICE components have been compiled over three decades, annotation practices seem to vary from component to component.
Finally, annotation and analysis should be separated from normative, oftentimes (over)prescriptive interpretation.
The problem, after all, is not so much that each data point becomes less reliable if we add more annotation categories, but rather that we often do not have enough data points to obtain a truly informative picture when addressing research questions that require us to take many categories into account simultaneously.
But there is another dimension to extensibility: The reliability of a particular annotation can also "vanish" because it turns out to be misguided, for whatever reason.
For example, it could turn out that an annotation set used for a corpus is based on false assumptions.
While this means that it cannot provide reliable results in an unsupervised manner, we show that a coarse manual annotation -conducted on the automatically identified clusters of tweets rather than individual occurrences -provides an efficient way of eliminating false positives.
On average, 8 clusters per word (min = 3, max = 10) were retained for annotation.
Our analysis also allows for a degree of uncertainty, as the annotation targets the predominant use in a cluster.
Once again, it is important to note that the manual annotation was conducted on the level of clusters, rather than individual tweets, meaning that some non-contact-related occurrences may have been included in the counts.
The exploration could proceed as described for the structural use case above but would now be extended to cover other levels of linguistic annotation assuming that they were represented in the corpus.
In this study, we have presented UMUCor-pusClassifier, a NLP tool that assists in the compilation and annotation of linguistic corpus.
You can do your data processing, data retrieval, annotation, statistical evaluation, graphical representation . . . everything within just one environment, whereas if you wanted to do all these things in Perl or Python, you would require a huge amount of separate programming.
It does mean, however, that you would have a hard time finding corpora on paper, in the form of punch cards or digitally in HTML or Microsoft Word document formats; the probably most widely used format consists of text files with a Unicode UTF-8 encoding and XML annotation.
The answer is that there were instances of innit as a reduction of isn't it in the files, and innit is tagged in the BNC in SGML annotation as "<w‚Ä¢VBZ>in<w‚Ä¢XX0>n<w‚Ä¢PNP>it"thus, you really must know what's in your data.
This can be extremely useful, for instance, when you know where in a line of annotation, say, the three-character identifier of a speaker is located (as may be the case in CHAT files from the CHILDES database) or when you want to access the line numbers in some version of the Brown corpus, which are always the first eight characters of each line, etc. (see, for example, Sections 5.4.5 and 5.4.9).
Imagine you have a sentence with an SGML word-class annotation as in the BNC.
XML is a hierarchical format (that lends itself well to representation as a tree that does not allow cross-nesting/overlapping) in which you add to data markup and annotation in the form of either start and end tags (which may contain attribute-value pairs), or just start tags (with attribute-value pairs), and in fact you have seen examples above that are similar to that annotation already.
Above, we mostly looked at retrieving the data values of our XML data, but of course we want to also use the often detailed annotation that is within the tags.
For instance, we have seen that words have much more annotation and provide three attribute-value pairs: two with differently detailed POS tags and one with the word's lemma.
By using the function xmlGetAttr with a specification of which attribute we want (which I hope is reminiscent of the function attr discussed above): Finally, let us do some more advanced searches, searches that tap into different kinds and levels of annotation at the same time.
Along the same lines, the next example does something similar but does not retrieve data values, but values of an attribute, namely lemma annotation.
We need to define a vector words that contains the words for which we want to compute DP values and then also a vector search.expressions that changes words into regular expressions we can search for in the corpus files (using the annotation of the corpus).
We will need the functions switch and menu (which you do not know yet so you may want to briefly look at their help pages -they are not difficult and the script will show you how they are used anyway) to prompt the user to choose the annotation format that will be processed, and we need a conditional with if to then define regular expressions for either choice.
After the loop, we will again use exact.matches.2 to retrieve the tags from the matches, we will use gsub to retrieve the forms only (i.e., discard all annotation), and we will use substr to define the lemma of each match (by retrieving the first letter, which gives us enough information to distinguish run and walk) and using ifelse to define a lemma column).
Finally, we use abline and text to add some annotation into the plot; we use lines(lowess(...)) to add a trendline, and I introduce mtext to add colored axis labels.
The next script does something seemingly elementary -we will create a frequency list of word-tag combinations (so as to be able to distinguish run as a noun from run as a verb) -but we will do it on a relatively large data set, the complete BNC World Edition with XML annotation, and we will use the annotation well by utilizing information provided by the BNC's multi-word tags, i.e., tags that mark multi-word units such as because of, in spite of, on behalf of, etc.
Then we use if and any (see the very simple definition at ?any ¬∂) to let R check whether the file has XML or SGML annotation; depending on that, search.expression.alph and search.expression.ord will be defined in the required way.
In a second for-loop for each line, we use substr to extract speaker/annotation names etc. and then use an if-conditional to distinguish utterances from annotation.
If we have an utterance, we just fill relevant slots of our collector list, but if we have annotation, we use a second if-conditional with length to check whether that annotation has already been used -if not, we store the annotation; if it has, we paste together a newly numbered annotation name and then store the annotation.
Once annotation has been carried out, however, quantification can be carried out.
There are three types of additional information that can be added to a corpus, namely metadata, markup and annotation.
There are many other kinds of annotation which a researcher might apply for the specific purposes of their own research questions, such as different kinds of discourse-pragmatic or stylistic annotation.
As I mentioned above, there exists recurring confusion regarding the distinction between annotation and markup.
Different XML tags are used for markup, metadata and annotation.
So for instance, if we see on a frequency list for an English corpus that the is the most frequent word, occurring 60,000 times, then we could say that the single type the is represented in this corpus by 60,000 tokens. Frequency lists can also be compiled for any annotation that is present in the corpus, and we can talk about types and tokens of annotation tags in the same way.
The first step of annotation is called the structural markup, which gives descriptive information about the text; thus, structural markup is used to create the text's metadata.
Linguistic transcriptions are more or less exact renditions of spoken texts and constitute one form of linguistic annotations which will be discussed in Chapter 7 on annotation.
Identifying tokens correctly and consistently in a corpus is another aspect of pre-processing and annotation of corpora.
This annotation has sentence structures represented in a tree-like structure showing hierarchical dependencies, which is useful for testing assumptions of some theories of grammar.
In practice, more specialised annotation is restricted to special corpora created for specific research purposes, but some larger corpora fall into these particular categories; for instance, COCA is a tagged corpus.
However, many kinds of questions require special kinds of corpora that have additional information or annotation, an issue we discuss further in Chapter 7.
Even if a corpus is basic, simply a string of wordforms with no other annotation, it is easy to search for individual words since usually words are a kind of string of characters, separated by white space.
Speech corpora are based on spoken language but necessitate detailed annotation including not only written transcription but transcription in phonetic alphabets and careful connections with the time course of speaking.
Searching for such phenomena requires some string information and some annotation.
Because of the many interactions and the many proxy effects, the study also provides the basis for future research looking at some categories in more detail, which may require additional annotation.
In that case, additional annotation for specific categories is probably needed.
A more sophisticated way of establishing a direct link between the recorded speech signal and its annotation is offered by specialised software that is designed to build up time-aligned annotation of media files.
Time-alignment means that the annotation -of whatever kind -is directly linked to the rendition of the speech signal.
Underneath this waveform are two annotation tiers.
Both are cut up into chunks, and these segments are where annotation values are placed.
Second, subsequent corpus users can much more easily re-evaluate the accuracy of annotations and potentially modify them or add further annotation detail, as required for any research agenda.
Hence, it can be strategic to separate these two transcription tasks and this can be done best if the media recording is directly linked with its annotation.
A further layer of annotation vital for many corpora is a free translation into a major world language (Schultze-Berndt 2006).
This requires that texts be similar in two regards: text data format and pre-processing and annotation.
First of all, they should be linked to the corpus text data and their metadata so that corpus users can evaluate how annotators arrived at their annotation decisions; at the same time the annotations are ideally extractable in order to be analysed outside the corpus infrastructure itself, for example, by programmes for statistical analysis.
Second, the annotation practices need to be documented.
In Section 7.2 we present a selection of conventions for annotation that target different linguistic levels.
We will repeatedly refer back to Chapters 4 and 5 where different types of annotation were relevant.
This section is tightly connected with Chapter 11 where we will explain various types of research that builds on these or similar types of annotation systems.
Corpus annotation involves enormous amounts of work.
This means that when we decide to use an annotation system or devise one ourselves, we need to make sure that the work is worthwhile, that is, the information we require cannot be extracted in some other way using data that is readily available, such as smarter ways of querying.
The specific design of annotation systems is codetermined by the specific research interests involved and therefore, we will need to refer to various research projects time and again.
Corpora containing syntactic annotation for constituent or dependency structure are called treebanks since syntactic structure is commonly visualised in the form of trees in models of syntax.
Various annotation systems have been developed for these different domains.
Given that semantic annotation systems can be exceedingly complex by comparison to grammatical annotation, given the huge range of distinctions, we will not discuss these here in greater detail.
Moreover, some semantic aspects are part of the typologically oriented annotation systems, and we will outline these in 7.3.
The annotation system has two basic facets: (1) it captures the identity of referents mentioned by different referring expressions, (2) it captures various aspects of the anaphoric relation.
In this section we outline annotation procedures that have been developed with a comparative perspective in mind.
The multiple annotation schemes of SCOPIC are organised along functional categories.
The grammatical word tier forms the basis for all further annotation, that is, morphological glossing, GRAID and RefIND which are all successively symbolically associated.
Semantic feature annotation is separated from the form slot by a <.>.
The absence of an annotation in the second slot is read as 'non-human' .
GRAID differs from other syntactic annotation system like the Treebank II annotations discussed in 7.2.4 in that zeroes are confined to instances where they contrast with a possible overt form, so that, for example, notional subjects of infinitive clause constructions in English do not receive a regular zero annotation.
Note that while the annotation as such seems simplistic, the operationalisation of underlying categories is particularly complex in this area.
Now let's have a look at a set of examples that illustrate the annotation practice and the rationale behind the system.
Combining the levels of GRAID and RefIND annotations will open up further possibilities: for instance, we can search for all instances where an annotation appears on the RefIND tier combined with a search for a syntactic function on the GRAID tier to get all functions in which a discourse referent is introduced.
As we saw above, a myriad of annotation schemes exist and they can range from very general applicability to appropriate for ultra-specific research questions.
Practical considerations are also relevant when it comes to the processing and annotation of data collected in a documentation project.
At the same time, however, given that documentations target languages that are not known to a wider scientific community, a greater minimum of annotation is key for documentation corpora, as will be discussed further in 10.3 (cf. Chapter 7 on annotations for requirements on annotations and their added value).
In Chapters 6 and 7 we have discussed various layers of annotation and how they add value to a corpus.
Additional important layers of annotation are translation, morphological glosses where software such as ELAN can be useful (cf. Chapters 6 and 7).
Constructions, however, can be difficult to search for unless they have a particular feature or string that can be searched for, or unless additional coding or annotation has already gone into the corpus, for example, in the form of GRAID annotations mentioned in 10.3.1 that enable searches for different types of clause constructions including the distinction between intransitive, (mono-) transitive, and ditransitive clause constructions.
Universal Dependencies (UD) (cf. 7.3.1) provide a system of consistent, crosslinguistic annotation of grammar (parts of speech, morphological features, and syntactic dependencies).
Allowance for an additional layer of comparisonbetween different layers of annotation -allows for an integrated and informative perspective on social cognition aspects of language.
Alternatively, they're also used in certain types of linguistic annotation.
When creating a spoken corpus, one also needs to think about whether an orthographic representation of the text will be sufficient, whether the corpus should be represented in phonetic transcription, or whether it should support annotation on various different levels (see Chapter 11).
To verify this in a very crude manner, I ran another test by comparing the raw and annotated files for my home page (in HTML), one dialogue annotated on a number of linguistic levels by one of my own programs, and one dialogue from the BNC, which contains a rather large amount of meta-information in its header and extensive word-level annotation.
We'll explore plain-text-based file formats, such as HTML and XML, that may contain such markup further in later sections, as well as discussing their use(fulness) for linguistic annotation/analysis.
For example, if you're investigating grammatically polysemous words like the ones we searched for above, you can add an underscore and a simple word category identifier -such as N for 'noun', V for 'verb', etc. (we'll soon learn more about this when we discuss morpho-syntactic annotation in Section 7.1) -to each occurrence of the word (form).
Just as in the preceding sentence, I often used words that refer to meaning or understanding of linguistic rules in scare quotes, to indicate that, frequently, the notion of linguistic rules and knowledge in tagging may not really represent a proper kind of understanding that is/can be applied to morpho-syntactic annotation.
The reason for this error seems to be that CLAWS was unable to identify the remainder of the sentence as being the rest of a complex NP, presumably because it doesn't 'understand' comma-separated lists that well, and thus 'mis-took' the comma as a phrase boundary, in which case the annotation would have made perfect sense.
This really only makes sense if you're planning to put the result into a relational database for complex analysis and annotation, and where you'll automatically be able to look up what the numbers mean from a lookup table.
However, despite the fact that interrupted words are very common in spoken language, even that of highly fluent speakers, the CLAWS tagset provides no tag for this, something that is probably due to the CLAWS tagsets originally having been created for the morpho-syntactic annotation of written language, and later adjusted for spoken language to some extent.
You can verify this for sil by clicking on it in the 'Word' window, which will take you to a concordance view of the item, where you can also see that this annotation normally appears in angle brackets in the source texts.
We'll discuss options of how to actually mark (up) and distinguish larger units of text in Chapter 11, when we talk about annotation.
As in the heading of this chapter, we frequently encounter two different terms when talking about the process of storing and enriching linguistic data, markup and annotation.
While the term markup is sometimes used to indicate the physical act of marking specific parts of a text using specific symbols, and, in contrast, annotation may often refer to the interpretative information added, the two may also be used synonymously.
As such, they can refer to either a particular format for enriching a text, so as to r categorise parts thereof, or r make these parts more salient, or to the actual process of applying this form of annotation.
We'll see some examples and different ways for applying markup to text later on, but for now, we're more interested in the reasons that exist for applying markup to our data in the first place, and roughly which different types of linguistic annotation currently exist.
For reasons of space, we won't look at further examples of other types of annotation here now, but will later work more extensively on a sample that combines some of the different levels.
Some of these formats are rather constrained in the types of information that can be added to a document, while others are more flexible, but sometimes even the less flexible ones can be 'coerced' into allowing us to add suitable types of annotation.
In this section, we want to explore how to use a form of XML that I refer to as 'Simple XML' in order to do annotation on multiple levels.
The form of annotation I introduced you to above already contains many different bits of information, so that it may appear fairly complex to you.
However, this argument ignores the fact that, before any annotation is finished, it repeatedly, and often for very long periods of time, needs to be read and edited by humans, so that readability does indeed represent an issue in annotation.
Even the best fully automated annotation still needs to be checked for errors by human editors, although, as we've seen in the many examples of mis-tagging in the BNC and COCA, this is often not done for reasons of time and cost involved, especially the larger the amount of data processed becomes.
In addition to this, using such software also ties the average user unnecessarily into using often complex annotation tools that themselves represent a relatively steep learning curve, apart from further potential issues regarding platform availability and setup.
And one always ought to remember that, even though one might never know how a corpus may potentially be used by others, not all types of annotation need to be included in all corpora, simply because this may be possible or relatively easily achievable.
As an alternative to some of the steps we modelled as regexes above, and for slightly more convenient manual annotation of the remaining XML structure, you could also use an annotation tool, such as my Simple Corpus Tool, which actually includes an editor that'll allow you to add these tags and attributes through the click of a button.
For a fully automatic large-scale annotation of the syntax, speech acts, etc., you can also try my Dialogue Annotation and Research Tool (DART), which not only allows you to annotate hundreds of dialogues in this way within minutes, but also to post-edit/correct the annotations, as well as to carry out similar analysis operations to those we learnt how to perform in AntConc, including concordancing, n-gram analysis, etc.
One of the major issues we've repeatedly encountered, especially concerning the mega corpora we've worked with, is that the creation of large-scale resources may frequently lead to the compilers taking shortcuts when it comes to ensuring the quality of the data in terms of tokenisation and annotation.
Furthermore, the design of the annotation and interfaces available may sometimes exhibit flaws from a linguistic perspective, as we've, for instance, seen for the CQP architecture behind BNCweb, which treats punctuation tokens in exactly the same way as genuine words, thereby potentially skewing all the statistics produced by the tool.
Following from that, we consider how data processing procedures in corpus linguisticsin terms of corpus mark-up, annotation and exploitationappear to be converging, to an extent, with those in (especially qualitative) social science.
Nowadays, many concordance packages, such as SketchEngine and LancsBox, have annotation packages built in, enabling automated annotation of features such as parts-of-speech, parsing and even semantic annotation.
The emphasis of corpus software packages tends to be on quantitative exploration and automated annotation.
By contrast, in many ways, software tools in the social sciences that facilitate textual analysis are oriented towards qualitative researchers and focus squarely on providing support for manual text annotation.
This is a question that is shared by linguistics and other areas of the social sciences, so it is perhaps understandable that annotation schemes are another area where there has been a flow of ideas between corpus linguistics and the social sciences.
In lemmatization, each orthographic word encountered in a corpus is assigned a lemma, or 'base form', which provides a level of abstraction from any inflection that might appear in the original orthographic word.
The choice of lemmatization software often depends on the kinds of language found in the corpus materials.
Although current English-language lemmatization tools make this process much easier to carry out on large bodies of text, it is often worth bearing in mind that even the most sophisticated lemmatization software will inevitably run into cases that are not entirely clear-cut (e.g., should the lemma of the noun axes be AXE or AXIS?) and where the resulting lemmas are not necessarily what one might expect.
Different analyses can thus lead to different lemma assignments; accordingly, many lemmatization tools are careful to document the language-specific lemmatization guidelines that they follow.
As mentioned in Sect. 2.2.1, this is a common task in corpus development, and one on which other forms of linguistic annotation (e.g., lemmatization, syntactic annotation) often rely.
Studies with a focus on the development of grammar rely on grammatical annotations, especially lemmatization, parts of speech, and interlinear glossing (cf. below).
There are various ways of processing (e.g., data analytics, concordance, collocation, keyword marking, local-word-grouping, lexical clustering, lemmatization, morphological processing, sentence processing, and named entity marking), which are applied to a corpus with appropriate technological supports.
Finally, we should bear in mind that the usefulness of lemmatization may change significantly from language to language.
Once the index was complete, the list was further analyzed and restructured in a process of lemmatization.
In lemmatization, each orthographic word encountered in a corpus is assigned a lemma, or 'base form', which provides a level of abstraction from any inflection that might appear in the original orthographic word.
Although current English-language lemmatization tools make this process much easier to carry out on large bodies of text, it is often worth bearing in mind that even the most sophisticated lemmatization software will inevitably run into cases that are not entirely clear-cut (e.g., should the lemma of the noun axes be AXE or AXIS?) and where the resulting lemmas are not necessarily what one might expect.
Different analyses can thus lead to different lemma assignments; accordingly, many lemmatization tools are careful to document the language-specific lemmatization guidelines that they follow.
The study investigated a range of lexico-grammatical variables including the lemma FUCK, the adverb lovely, the discourse markers you know, sort of, and really, negative concord (e.g. he didn't say nothing), and the determiners the and some.
However, random slopes (for situations where fixed effects vary per group) and multilevel models (for situations where group-wise tendencies can be predicted from other variables, for example when lemma frequency is useful to predict lemmaspecific tendencies) are also introduced.
The verb lemma also influences the probability of either variant being used.
The sub-term (1|LEMMA) encodes that an intercept will be added to the linear term depending on which lemma is observed.
However, an additional value will be predicted for each lemma, and this value has to be added to the fixed coefficient.
For the genitive alternation, the text ID and the possessor head lemma are used as crossed random effects.
For the dative alternation, the model includes the text ID, the register (which nests the text ID) as well as the lemma of the theme argument and the verb.
Additionally, verb and particle lemma grouping factors are annotated.
For Kindlemma, there are: Kindfreq (numeric, z-transformed), which encodes the lemma frequency; Kindgender (binary), which encodes the grammatical gender of the kind noun; Kindattraction (numeric, z-transformed), which encodes the influence of neighbouring constructions.
The following is the standard glm() call to estimate a model with the measure lemma (150 levels) as a fixed effect.
The measure lemma and the kind noun lemma were specified as varying-intercept random effects.
The following code extracts the Kindlemma random intercept for item 99, which is -0.159 for the lemma Wasser 'water'.
In addition, some concordancing programs can detect prefixes and suffixes and irregular forms and sort words by "lemmas"; that is, words such as runs, running, and ran will not be counted as separate entries but rather as variable forms of the lemma run.
This canonical form of the word is called its lemma.
In the case of adjectives, their lemma is by convention the singular masculine form.
Similarly, the words mouse and mice are the singular and plural variants of the same lemma (mouse), whereas eat, eaten and eating are the conjugated forms of the lemma eat.
The notion of the lemma should be differentiated from the concept of the lexeme, which is used for defining a word from a semantic point of view.
So, for example, bat is a lemma which can correspond to two different lexemes, as this word is polysemic and may either refer to a flying mamal or an object used to hit a ball.
For example, Sketch Engine provides the option to search by lemma or by grammatical category.
This research specifies that all forms of the lemma acteur will be retrieved from the corpus when a word tagged as an adjective appears to its right.
To find the adjectives used to the left of this word, the element order of the query must be reversed [tag = "ADJ"] [lemma = "acteur"].
This research focuses on the word rather than on the lemma in order to exclude the masculine occurrences, acteur.
For the word film, the search can focus on the lemma rather than on the word, in order to identify singulars and plurals.
The syntax is therefore: [lemma = "film"] [tag = "ADJ"].
The search for adjectives to the left, with the syntax [tag = "ADJ"] [lemma = "film"] results in the adjectives meilleur, prochain, nouveau and seul.
One exception is the lemma ardent, where KNN performs slightly better than SVM.
Starting with micro f1 and its accompanying standard deviation, we find relatively high scores for all of the classification tasks for each lemma individually as well as for the grouped set.
This appears to be particularly true for fine-tuning, which noticeably struggles with low-frequency categories (e.g. MET for the lemma mass; f1 = 46.4).
A study of the word (in either or both of its senses) would obviously require that we look at the lemma PAVEMENT, comprising at least the word forms pavement (singular), pavements (plural) and, depending on how the corpus is prepared, pavement's (possessive).
In Chapter 3, we implicitly treated the second issue as a problem of retrieval, noting in passing that we queried our corpus in such a way as to capture all variants of the lemma PAVEMENT.
It is also important to note that unlike English, in which different forms of a lemma may have different collocates and semantic prosodies (e.g. consequence vs. consequences), Chinese does not have a rich morphology which can affect collocation and semantic prosody in this way.
VERB lemma, SPEAKER, TEXT, and COUNTY are non-repeatable effects, as a second study relying on randomly chosen verbs, speakers, texts, and counties would result in a different sample.
The paper presents its findings as a number of case studies, moving from a study based on a single lemma, cause, to ones based on grammatical categories such as the imperative and the past tense.
The lemma MENTION has a very different distribution pattern from DECIDE, with mention that being proportionately more frequent than decide that.
Taking into account important data related to each lemma's range (its frequency across academic disciplines) and dispersion, the researchers arrived at a new Academic Vocabulary List (AVL) of just over 3,000 words (the full list can be explored at www.wordandphrase.info/academic).
We believe, however, that a note of caution may be in order before concluding that the lemma should be what all lists should consist of.
As an example, the lemma result is presented with result as a noun (72,083), as a verb (20,138), and derived adjectival forms (resulting/resultant).
In other words, while the newer lists should be lauded for their more careful compilation and choice of lemma over word family for its superior discrimination of different senses of meaning, perhaps a way forward is to explore the lexicon beyond single orthographic words, to aim instead for the lexeme over the lemma.
If you cannot make the full texts freely available for copyright reasons, try to make the corpus as accessible as possible, for example, by allowing queries via flexible search engines such as NoSketchEngine or CQPweb and by publishing word and lemma lists (and ideally, n-gram lists), or by publishing it in password-protected form via a repository that allows for closed-access corpora (e.g., CLARIN).
Let us now think about how the different notions of a 'word' (type, lemma and lexeme) influence the kind of analysis we can carry out in corpus linguistics.
Our options are type, lemma or lexeme.
Looking now more closely at the list of lemmas, we note that among the 3,196 lemmas meeting our inclusion criterion there are 13 lemmas related to weather (this analysis had to be done manually by going through the lemma list): cloud, cold, flood, heat, hot, ice, rain, storm, sun, temperature, warm, weather and wind.
Nevertheless, there is one exceptionthe lemma flood.
Although this lemma occurs in a majority of corpus parts (Range 10) it has a very low Juilland's D (0.2).
There are different concepts of a 'word' -token, type, lemma and lexeme.
For instance, we have seen that words have much more annotation and provide three attribute-value pairs: two with differently detailed POS tags and one with the word's lemma.
Along the same lines, the next example does something similar but does not retrieve data values, but values of an attribute, namely lemma annotation.
After the loop, we will again use exact.matches.2 to retrieve the tags from the matches, we will use gsub to retrieve the forms only (i.e., discard all annotation), and we will use substr to define the lemma of each match (by retrieving the first letter, which gives us enough information to distinguish run and walk) and using ifelse to define a lemma column).
Then we group the frequencies by the lemmas with tapply and sum up all frequencies of each lemma (we don't do the insertion into a long vector with counter here because, since we're only looking at BNC folder A, the data set is small enough for this to work even on a computer that's not state-of-the-art).
That logical vector is used to subset all.lemma.freqs, the result of which is then sorted.
A lemma is a group of word-forms that are judged to constitute a single entry in the lexicon of the language -different inflectional forms of a single dictionary headword.
Lemmatisation is conceptually the most straightforward of the word-level annotations; every word is, simply, annotated with the lemma to which it belongs (where each lemma is represented by its morphological base form, such as go for go, goes and went).
However, even this relatively simple operation can raise issues regarding what is, and what is not, considered to be part of the same lemma.
On the other hand, if it is tagged as an adjective, broken is lemmatised as broken -since the change of category to adjective is deemed to create a new lemma.
A more common approach is to begin with a single word (or lemma) of interest, called the node, for which a concordance search has been performed; the surrounding text is then scrutinised to establish what elements occur frequently in the vicinity of the node.
However, since it's somewhat of a nuisance having to do this all the time, apart from being error-prone, it's quite useful to be able to specify a search for all different forms of a headword or lemma.
Here, the distinction between the two is essentially that the headword encompasses all the occurrences of a base form, regardless of PoS, while the lemma always represents a combination of base form + PoS tag (forms).
To limit our searches to lemmas by specifying a PoS, BNCweb provides us with two separate options, both based on the following simplified (representation of a) tagset: To specify the lemma form, BNCweb allows two different variants.
Run a search for the lemma of film, this time constraining the results to nouns, as, of course, film can also be a verb and we're not interested in this.
Rather than seeing the relative frequency in terms of percentages as in BNCweb, though, the bars on the right-hand side provide us with a visual indication of the extent to which each sub-form of the lemma contributes to the total.
This is referred to as lemmatisation (c.f. also Section 8.1.8, where we looked at lemma queries in BNCweb), and many programs that produce frequency lists offer this kind of facility.
The latter, however, is only shown if the option for this is activated in the 'Tool Preferences' for word lists, and a suitable lemma list loaded.
As presented in Chaps. 1 and 2, annotations and metadata are sometimes relatively simple: markup can be added to set them apart from the text and control their inventories.
Often these nodes and edges will be annotated with labels, which usually have a category name and a value (e.g. POS=NOUN); in some complex architectures, annotations can potentially include more complex data types, such as hierarchical feature structures, in which annotations can contain not only simple values, but also further nested annotations (see ISO 24612 for a standardized representation for such structures in corpus markup).
CHILDES and the CHAT format as well as widespread markup languages such as XML have made a great contribution to unification but still leave room for enormous variation, even in areas that are of great interest to many researchers such as word meaning, morphology, or syntax.
The boilerplate removal process usually goes hand in hand with the stripping of HTML markup code from web documents as HTML can offer clues about what is boilerplate and what is the main textual content.
This involves not only collecting data (speech and writing) but encoding it: transcribing recorded speech, for instance, as well as adding annotation to it, such as markup indicating in a conversation when one person's speech overlaps another speaker's, and in writing where such features as paragraph boundaries occur in written texts.
The chapter concludes with a detailed description of various kinds of textual markup and linguistic annotation that can be inserted into a text.
Other textual markup is internal to the text, and in a spoken text would include such information as speaker IDs, and the beginnings and ends of overlapping speech.
The process of annotating a corpus involves running software that can (1) tag a corpus (add part-of-speech tags to all of the words in the corpus, such as nouns, prepositions, and verbs), or (2) parse a corpus (add markup that identifies larger structures, such as verb phrases, prepositional phrases, and adverbials).
Grammatical markup is inserted when a corpus is tagged or parsed.
Textual markup is important too, though corpora will vary in terms of how much of such markup they contain.
Once a draft version of a text contains "metadata" and "textual markup", it can then be placed into a "lexical (pending proofreading)" directory to indicate that the text will be ready for use as a lexical version of the corpus once it has been proofread.
Finally, although inserting "structural" markup into a text is separate from the process of actually computerizing the text, there are many instances where markup can be inserted while texts are being computerized.
Of course, some markup is probably better inserted after a text sample is computerized.
Consequently, different corpora contained different kinds of markup to describe the same linguistic phenomena.
Metadata and textual markup are important because they can narrow the range of possible choices when a corpus is searched.
In the ICE Project, such incomplete utterances are given an orthographic spelling that best reflects the pronunciation of the incompletely uttered word, and then the incomplete utterance is enclosed in markup, <.> i </.>, that labels the expression as an instance of an incomplete word.
To prevent this, the ICE Project has special markup that encloses the entire sequence of repetitions (<}_><}/_>) and then places special markup (<=_>the police boat<=/>) around the last instance of the repetition, the only instance counted in analyses done by ICECUP, the text analysis program used in the ICE Project.
Because writing is linear, it is not difficult to preserve the "look" of a printed text that is converted into an electronic document and transferred from computer to computer in text format: although font changes are lost, markup can be inserted to mark these changes; double-spaces can be inserted to separate paragraphs; and standard punctuation (e.g. periods and commas) can be preserved.
However, instead of representing these overlaps with markup (as a TEI-conformant system would do (cf. www.tei-c.org/release/doc/tei-p5-doc/en/html/TS.html#TSSAPA), this system attempts to indicate them iconically by vertically aligning the parts of the conversation that overlap to give the reader of the conversation a sense of how the flow of the conversation took place.
But while many features of writing can be annotated in a written corpus with TEI-conformant markup, annotating all of these features may prove to be unnecessary.
For instance, semantic tagging involves annotating a corpus with markup that specifies various features of meaning.
For instance, the opening of the section in ECL1 on "Structural Markup" makes reference to ASCII (American Standard Code for Information Interchange) text formats and standard generalized markup language (SGML).
XML is a hierarchical format (that lends itself well to representation as a tree that does not allow cross-nesting/overlapping) in which you add to data markup and annotation in the form of either start and end tags (which may contain attribute-value pairs), or just start tags (with attribute-value pairs), and in fact you have seen examples above that are similar to that annotation already.
There are three types of additional information that can be added to a corpus, namely metadata, markup and annotation.
For instance, in a spoken corpus, who said what is often critical to an analysis: only the markup of utterance boundaries and speaker identities allows us to know this.
As I mentioned above, there exists recurring confusion regarding the distinction between annotation and markup.
Different XML tags are used for markup, metadata and annotation.
The first step of annotation is called the structural markup, which gives descriptive information about the text; thus, structural markup is used to create the text's metadata.
Most corpora use Standard Generalized Markup Language (SGML), Text Encoding Initiative (TEI), or Extensible Markup Language (XML) to have a unified structural markup system.
Any document that purely contains text (plus potentially some form of markup, see Chapter 11), is generally referred to as a plain-text document and should be readable (and writable) with any basic text editor (see Section 4.2.1 for a more in-depth discussion of these).
We'll explore plain-text-based file formats, such as HTML and XML, that may contain such markup further in later sections, as well as discussing their use(fulness) for linguistic annotation/analysis.
And once you've reached a more advanced level in your research, you might also want to consider using a 'proper' markup language, such as HTML or XML, for annotating and categorising your data.
The initially perhaps odd-seeming type sil is further proof of this, as it represents an abbreviation for 'silence', that is, a pause of undefined length, while utt represents markup for an utterance.
As in the heading of this chapter, we frequently encounter two different terms when talking about the process of storing and enriching linguistic data, markup and annotation.
While the term markup is sometimes used to indicate the physical act of marking specific parts of a text using specific symbols, and, in contrast, annotation may often refer to the interpretative information added, the two may also be used synonymously.
We'll see some examples and different ways for applying markup to text later on, but for now, we're more interested in the reasons that exist for applying markup to our data in the first place, and roughly which different types of linguistic annotation currently exist.
In the 1960s, first attempts at standardising markup for information exchange began to be made.
This first official type of markup language, however, as we'll see further below, was relatively complex and also had a number of serious drawbacks.
After Tim Berners-Lee had invented the World Wide Web (WWW) in 1989, it was necessary to develop a new, and simpler, markup language in order to take full advantage of the new hypertext medium.
As can be seen from its name, XML allows users to create their own extended markup frameworks, and thereby makes it possible to increase the separation of form from content even further than was already possible by using HTML and CSS.
Well, for one thing, markup helps to structure information, for example, in separating documents into appropriate sections/divisions with headings, sub-headings, paragraphs, etc.
Elements, in their most basic form, are generally represented in markup languages by pairs of opening and closing angle brackets, i.e. < & >, with the name of the element appearing in between the two.
Style sheet languages come in different flavours for different markup languages, namely DSSSL for SGML, CSS 1, 2 & 3 for HTML, and both CSS & XSL for XML.
The most obvious area in which the social sciences have shadowed developments in corpus linguistics is corpus markup.
Corpus linguistics has long championed, and pioneered, markup schemes to permit the systematic encoding of metadata and interpretative analyses within corpora.
The complexity of the corpus architecture results from its annotations: as the data is collected, student annotators iteratively apply a large number of annotation schemes to their data using different formats and tools, including document structure in TEI XML, POS tagging, syntactic parsing, entity and coreference annotations and discourse parses in Rhetorical Structure Theory.
TigerXML is a more expressive XML format, capable of representing multiple node annotations, crossing edges, edge labels and two distinct types of edges.
Some tools (e.g. the online XML and spreadsheet editor GitDox, Zhang and Zeldes 2017) are opting for online storage on GitHub and similar platforms as their exclusive file repository.
CHILDES and the CHAT format as well as widespread markup languages such as XML have made a great contribution to unification but still leave room for enormous variation, even in areas that are of great interest to many researchers such as word meaning, morphology, or syntax.
All the occurrences of A as NP were extracted from the BNC-XML, amounting to 1,819 tokens.
In 2011, a TEI-XML version of the corpus was released.
For example, the AntConc concordancer that we discuss can only process text format files (or files with XML or HTML tags which can also be treated as text files).
AntConc can also read XML files, since these contain text which is accompanied by tags.
As we saw in Chapter 5, most concordancers (like AntConc) only read files in text format, which can include texts tagged in XML format.
This software can be downloaded for free and used for converting any number of XML, HTML or sometimes even PDF files into text format.
In some cases, the abundance of metadata requires the use of a precise syntax for tags, based on the conventions of computer languages for XML or SGML coding.
Following XML conventions, TEI tags always begin with chevrons < > and close with </ >.
Many tools, among which we can mention EXMARaLDA, use the XML format, which is a coding language (eXtensible Markup Language) for defining, and then storing and transmitting structured data.
However, in order to be interpreted, a document marked up in XML coding must follow certain syntax, in the form of a document type definition (DTD), which specifies the list of tags with their potential attributes, as well as the possible order of appearance for tags.
XML can also be used for encoding the metadata of a corpus (see Chapter 6, section 6.4).
Tools have been developed in order to make data annotation in XML format easier, such as the Nite XML tool, for example.
It is also important to be able to export annotations in a standardized format, based on the XML language, for example.
We have also seen that these tools often work in standardized formats like XML, which makes it easy to share annotations.
The first option is routinely chosen in the case of automatically annotated variables like Part of Speech, as in the passage from the BROWN corpus cited in Here, the annotation (i.e., the part-of-speech tags) are attached to the data they refer to (i.e., words) by an underscore (recall that alternatively, a vertical format with words in the first and tags in the second column is frequently used, as are various kinds of XML notation).
Ideally the transcription that is produced by these different methods would be aligned with the audio and video streams using software such as EXMARaLDA and the NITE XML Toolkit.
Most corpus tools require plain text versions with optional XML encoding, so where material is sourced in another form, some format conversions will be in order.
In this case, XML editing software may be required to simplify the process and check for consistency of the results.
Focusing on the first kind for a moment, it would be possible of course to manually annotate texts using any standard word processor, but here it is useful to have software tools that check annotation as it is added, e.g. to ensure that typos in tags or category labels do not occur, and to allow standard mark-up formats (such as XML) to be employed consistently and correctly in the resulting corpus, e.g. as in the Dexter software.
By providing a manually checked, dual POS tag which encodes both form and function (VOICE Project 2013), the XML corpus can be searched for all cases of an "innovative" form, in Cogo and Dewey's terms.
Minor editing was required to format headers, footers and page numbers in XML tags, and converted n-dashes, pound signs, begin and end quotes to XML entities.
It does mean, however, that you would have a hard time finding corpora on paper, in the form of punch cards or digitally in HTML or Microsoft Word document formats; the probably most widely used format consists of text files with a Unicode UTF-8 encoding and XML annotation.
XML is a hierarchical format (that lends itself well to representation as a tree that does not allow cross-nesting/overlapping) in which you add to data markup and annotation in the form of either start and end tags (which may contain attribute-value pairs), or just start tags (with attribute-value pairs), and in fact you have seen examples above that are similar to that annotation already.
Often this is fast and unproblematic and I must admit that the vast majority of my work with XML files is really just that.
Instead of an absolute path, where you move down the XML tree via the single slash-separated parts, you can also use relative paths indicated by two slashes to skip multiple intervening levels.
Above, we mostly looked at retrieving the data values of our XML data, but of course we want to also use the often detailed annotation that is within the tags.
I will focus in particular on three things: (1) how speaker-specific information can be retrieved; (2) how you can search and retrieve information from different levels of the XML tree; and (3) some XPath syntax aspects that allow for simple character processing.
First, you can use so-called XPath axes which define the relationships between different nodes in an XML tree using kinship and ordering terms such as ancestor, child, sibling, or parent on the one hand and preceding and following on the other.
Let me finally very briefly mention the package xml2, which is useful to avoid the XML package's memory leak on Windows.
Thus, if the user chose "XML" then menu made that a 1 and switch then assigns TRUE to xml.version.
Remember that the most powerful package to handle XML data in R is XML, but also remember that it has a bad memory leak when used on Windows systems.
The next script does something seemingly elementary -we will create a frequency list of word-tag combinations (so as to be able to distinguish run as a noun from run as a verb) -but we will do it on a relatively large data set, the complete BNC World Edition with XML annotation, and we will use the annotation well by utilizing information provided by the BNC's multi-word tags, i.e., tags that mark multi-word units such as because of, in spite of, on behalf of, etc.
Then we use if and any (see the very simple definition at ?any ¬∂) to let R check whether the file has XML or SGML annotation; depending on that, search.expression.alph and search.expression.ord will be defined in the required way.
We load all required packages (rChoiceDialogs, XML, xml2), source exact. matches.2, and define whitespace.
The current standard method for adding tags to a corpus is by means of XML (the eXtensible Markup Language).
In an XML document, anything inside <angled brackets> is part of a tag; anything outside angled brackets is part of the text itself.
Different XML tags are used for markup, metadata and annotation.
Using an R script (R Core Team 2014), I extracted all instances of I and you (when tagged as PNP) from all 21 files of the British National Corpus World Edition (XML) whose names begin with 'KR'.
Most corpora use Standard Generalized Markup Language (SGML), Text Encoding Initiative (TEI), or Extensible Markup Language (XML) to have a unified structural markup system.
Among these systems, XML systems are used frequently since they include both SGML and TEI.
Like the SBC, a corpus created with ELAN will have two components, namely, a set of media files with the raw audio and/or audio-visual data and a corresponding set of ELAN document files (a special kind of XML file).
Given this structure, the transcriptions entered in ELAN exhibit explicitly marked up structure in the XML output that can then be imported into other platforms and also is further analysed based on a more common vertical format, as will be shown shortly below.
We'll explore plain-text-based file formats, such as HTML and XML, that may contain such markup further in later sections, as well as discussing their use(fulness) for linguistic annotation/analysis.
And once you've reached a more advanced level in your research, you might also want to consider using a 'proper' markup language, such as HTML or XML, for annotating and categorising your data.
The latter, strangely, are still part of the text representation of the XML version of BNCweb, despite the fact that they can easily be encoded in UTF-8, which is, after all, the assumed default encoding for XML.
The context options we have are indicated via XML tags (explained in more detail in Chapter 11), e.g. <p> for paragraph, <s> for s-units, and <u> for turns, etc., where 'u' apparently means 'utterance', although it may, strictly speaking, possibly consists of a number of these.
Because the formatting and linking capabilities offered by HTML were not always sufficient for all needs in document handling, and SGML proved too unwieldy and error-prone, a new hypertext format, XML (eXtensible Markup Language) Version 1.0, was eventually created and released by the W3C (World Wide Web Consortium) in 1998.
As can be seen from its name, XML allows users to create their own extended markup frameworks, and thereby makes it possible to increase the separation of form from content even further than was already possible by using HTML and CSS.
Furthermore, XML can not only be used in conjunction with CSS, but also has its own stylesheet language XSL, which far exceeds the capabilities of CSS in that it also provides mechanisms for transforming documents from XML into other forms of XML, as well as various other types of formats, for display and processing.
The best examples we've seen for this were the meta-textual choices BNCweb allowed us to make for selecting specific parts of the BNC for different analysis purposes, which were all made possible by the fact that the BNC (in its most recent version) is marked up in XML, albeit with somewhat over-elaborate header information, as we'll soon discuss.
The following is a brief sample paragraph that illustrates what a paragraph annotated in XML might look like in terms of elements and attributes.
SGML also has two other major disadvantages compared to HTML or XML, the first being that it absolutely requires a DTD (Document Type Definition) specifying its structure, in order to allow any type of serious processing, and -last but not least -that it's not supported by any 'standard' (browser) software.
One big advantage, at least in comparison to HTML, is that a large set of tag definitions/DTDs for linguistic purposes, such as for the TEI (Text Encoding Initiative), were originally designed for SGML, although more and more of these have been or are being 'ported' to XML these days, too.
Although HTML is a direct descendant of SGML, it only provides a limited set of tags, which, on the one hand, makes it less flexible than XML, but, on the other, also much easier to learn.
XML, however, is much more versatile than HTML because it was designed to be extensible by the user in providing the ability to completely define one's own tags.
All XML documents minimally have to be well-formed, that is, no overlapping tags (as in HTML, e.g. <b>‚Ä¶<i>‚Ä¶</b>‚Ä¶</i>) are allowed.
Unlike in older forms of HTML, where case did not matter, XML is case sensitive, so that linguistic tags like <turn>, <Turn> & <TURN> for representing individual speaker turns in dialogues are all treated as being different from one another.
If no style sheet is explicitly provided, most browsers will try to render the XML content using their own default style sheets, though.
If an XML document conforms with one of these two types of specification, we talk of a valid document.
We won't go into issues of designing DTDs or schemas here because they're fairly complex, but will at least have a look at some of the rendering options for XML documents using style sheets.
Style sheet languages come in different flavours for different markup languages, namely DSSSL for SGML, CSS 1, 2 & 3 for HTML, and both CSS & XSL for XML.
This is best done inside the style sheet definition, but may also happen inside the XML file itself.
Layout design for other (printed) media is also supposed to be enhanced through XML Formatting Objects (XSL-FO).
Of course, XML doesn't need to be viewed inside a browser or transformed in any way, but can either be viewed, or even -at least to some extent -meaningfully searched, in one of our basic editors, or manipulated in other ways through programming languages, too.
Furthermore, concordancers, such as AntConc, generally also allow us to search for XML tags because they simply represent plain searchable text, but may also sometimes provide options to hide them when we don't want to display them.
In this section, we want to explore how to use a form of XML that I refer to as 'Simple XML' in order to do annotation on multiple levels.
The following exercise is designed to provide you with an introduction to this which is broken down into a series of steps that all successively allow you to deal with a particular XML-related or linguistic issue in creating a well-formed XML document.
The line we just inserted identifies the document as being in XML for any application that's capable of displaying it, at the same time specifying its encoding as UTF-8.
The question marks at the beginning and end identify the tag on this line as a processing instruction, in our case specifically the one referred to as the XML declaration.
Please note that, so far, what we've done has not turned the file into valid XML yet as there are still some parts missing, so we're really just 'pretending' that the file is XML.
This container element is also known as the root element, and every well-formed XML document needs to have one.
By wrapping our text in the container element, we've effectively created the first (top) level of a hierarchy or categorisation, as well as a file that is now, at least technically, valid XML, despite the fact that it hasn't been sub-divided into meaningful parts yet.
This is the case because the browser now assumes that, since you've 'told' it that you want to style the XML yourself, it should no longer apply its own built-in style sheet, but instead leave all the styling up to yours.
To be able to access information about the speaker attribute, we need to use an attribute-value pair with an equals sign, similar to the way we define attributes in XML, but without the quotation marks around the value.
However, I still refer to this as 'Simple XML' because it remains easily readable due to the relatively low number of container elements and the separation of text from nonempty tags.
This separation, incidentally, is possible because most XML processors simply ignore any whitespace they 'perceive' as redundant.
Because these processors generally parse the XML and often produce tree structures based on the hierarchy of the elements, they'd thus have rendered our sample in the same way, even without the additional line breaks I suggested you insert for readability.
Some proponents of 'pure' XML technology would even frown upon what we've done here and argue that XML was meant to be processed by the computer, anyway, and could then be rendered in whichever way it would be best viewed.
If your browser indicates an error, go back to the XML file and first verify whether you've set all start and end tags correctly, as this is generally the most common error.
One of the most common error messages you'll probably encounter will be something like "XML Parsing Error: mismatched tag.
The browser will usually also try to provide a line and column (i.e. character position) number for where the error was found, but this will often not be very helpful because XML parsers are unfortunately not very good at locating such errors, so that, frequently, the error location pointed out is only towards the end of the file where the browser (finally) 'notices' a mismatch between start and end tags.
However, as XML forbids the use of overlapping tags, and overlap passages span across different speaker turns, using <overlap>‚Ä¶</overlap> elements would break the document's well-formedness, so it's best to use the empty tags <overlap type="start" /> and <overlap type="end" />, possibly in combination with a number ('n') attribute.
As an alternative to some of the steps we modelled as regexes above, and for slightly more convenient manual annotation of the remaining XML structure, you could also use an annotation tool, such as my Simple Corpus Tool, which actually includes an editor that'll allow you to add these tags and attributes through the click of a button.
As with XML, in CSS slight mistakes in the syntax may cause errors, so if the display you get doesn't seem to reflect our properties, you need to check your style sheet for errors.
In the final brief section (Chapter 11), I've tried to provide you with a short, but nevertheless highly practical, glimpse at what current technology in the form of XML has to offer to linguists who want to enrich their data and/or visualise important facts inherent in it.
For example, it is a good environment for adding annotations to a text, it has excellent multimedia capabilities, and it is XML-compatible.
The single node in the layer 'sentence types' above the tokens is annotated as s_type = decl (declarative sentence), and is attached to both tokens, but the edges attaching it are unannotated (no labels).
Finally, some layers, such as 'constituent syntax', contain a complex subgraph: an NP node is attached to the token I, and a VP node is attached to know, and together they attach to the S node denoting the clause.
TigerXML is a more expressive XML format, capable of representing multiple node annotations, crossing edges, edge labels and two distinct types of edges.
Two spans were used: four words either side of the node and nine words either side of the node; ‚Ä¢ whether counts were based on lemmatized or non-lemmatized counts.
For example, fitting a tree with a lower minimal criterion may be useful in a pilot study performed for exploratory purposes; ‚Ä¢ minimum 20 observations in a node for a split to be considered, specified by minsplit = 20; ‚Ä¢ according to the default settings, there should be at least seven observations in one node after a split, minbucket = 7.
These contexts are not split further and constitute together a terminal node (Node 2) with 45 observations.
Sometimes, the collocates of a node word (or larger expressions) fall into a more or less clearly recognizable semantic class that is difficult to characterize in terms of denotational properties of the node word.
Altering the span of the window around the node word where possible collocate words are considered can also significantly affect the results.
Further typical options include whether to consider punctuation as a boundary to collocation window spans or impose minimum frequencies on collocates or node words.
The z-score test is a measure which adjusts for the general frequencies of the words involved in a potential collocation and shows how much more frequent the collocation of a word with the node word is than one would expect from their general frequencies.
A higher z-score indicates a greater degree of collocability of an item with the node word.
All of these are negative, but no significant collocate was found for the two node words.
Both CONE and GraphColl permit partial exploration of graphs, accentuating this issue: a user chooses which nodes to expand (and thus compute collocates for), and this means it is possible to deliberately or unintentionally miss significant links to second-order collocates (or symmetric links back from a collocate to a node word).
We will call the word love, our word of interest, a 'node'.
A node is a word that we want to search for and analyse.
The words around the node are candidate words for collocates.
Collocates are words that co-occur with the node in a specifically defined span around the node, which we call the collocation window.
Metaphorically, we can imagine this as a 'magnetic field' around the node that attracts particular collocates like a magnet attracts metal objects.
In the example given above, within a 1L, 1R collocation window around each occurrence of the node, we can observe the words that co-occur with love, namely, in the order of appearance, my (three times), is (twice), in (once), am (once), will (twice), thee (twice), only (once), and (once) and thou (once).
The disadvantage of this approach is that the top collocates will typically be function words co-occurring with the node merely by the virtue of their frequency anywhere in the corpus.
Frequency-based collocates are therefore fairly generic (we can expect a similar set of collocates for almost any node) and have only a limited usefulness.
Some collocation measures such as MI highlight rare exclusivity of the collocational relationship, favouring collocates which occur almost exclusively in the company of the node, even though this may be only once or twice in the entire corpus.
Frequency refers to the number of instances in which a node and collocate occur together in a corpus.
A collocation graph is a visual representation of the collocational relationship between a node and its collocates.
Instead of a list of collocates displayed in a table (the usual format), the graph shows the relationship between the node and the collocates by displaying collocates closer to or further apart from the node.
Finally, the position of the collocate in text (whether it occurs predominantly before or after the node) is shown by the position of the collocate in the graph (left, middle or right).
For instance, fall, falling and fell occur always before the node love, while you occurs both before and after love with approximately the same frequency.
You are interested to see content words around the node rather than frequent grammatical words.
For example, over the period 1940-2009, 171 collocates of the node war were identified.
Often, one uses the letters L and R (for 'left' and 'right') together with a number indicating the position of one collocate with respect to the main/node word to talk about collocations.
Collocational statistics quantify the strength of association or repulsion between a node word and its collocates.
Node words tend to attract some words -such that these words occur close to the node word with greater than chance probability -while they repel others -such that these words occur close to the node word with less than chance probability -while they occur at chance-level frequency with yet others.
This approach to Corpus Linguistics is based on the observation of pattern in concordance lines, where a word or short phrase is the node of the line and the few words occurring before and after the phrase are shown for each occurrence.
It has been widely accepted with Corpus Linguistics that collocation needs to be measured within a fairly short span: + / -5 words from the node is common.
There is no node word and no directional influence, and the purpose is not to find out more about an individual word.
A more common approach is to begin with a single word (or lemma) of interest, called the node, for which a concordance search has been performed; the surrounding text is then scrutinised to establish what elements occur frequently in the vicinity of the node.
One way is manual and largely qualitative: to undertake a hand-and-eye analysis of the concordance, as explained above, where the focus of analysis is to identify co-occurring elements, the collocates of the node, in the concordance lines.
However, given how labour-intensive this approach is, it is very common to automate the process of extracting collocates from the local context around the node.
It should be noted that this is fundamentally rather similar to the procedure of a key items analysis, except that we are comparing the vicinity of the node to the rest of the corpus, rather than comparing two different corpora, and some of the same considerations apply.
Second, as with key items, a rigorous collocation analysis will always go beyond the raw list of statistical collocates to look at actual instances of those collocates in usage alongside the node.
For example, if we observe that a node word almost always occurs after an adjective -even if no single adjective co-occurs frequently enough with the node to be considered a collocate -we could say that the category of adjective is a colligate of that node.
A colligate can also be a syntactic structure that the node tends to occurs within or alongside -such as a sentence position, or a verb complementation pattern.
Finally, semantic prosody or discourse prosody refers to a broad function or meaning which tends to co-occur with the node but which may be variously realised.
You can also determine the relative length and complexity of phrases (understood as dependency structures) by considering the number of head indices in the head column cross-referencing the head word of the phrase in question, or all phrases dependent on the verb node.
For each node (or sub-group), the data is then split into two more sections based on which data points are most different from each other using the remaining independent variables and remaining levels of independent variables.
Because each set of data under a node is looked at anew, the same variable can be used again in a lower level of the tree, with whatever levels are still relevant for that node.
This splitting (division of the data) continues until a stopping criterion is reached, at which point the data in each node should be relatively homogenous.
When describing decision trees in text, you can refer to the node numbers (found in our figure at the top of each section/bin of the data, followed by the number of tokens in that group) and the split numbers (the numbers at the top of the circles).
Speech act participants (the first and second person) are significantly more likely to be expressed by a pronoun, and the first person even more so, as seen in node 10.
This obviously isn't as useful because there's no way to directly investigate the context by clicking a link whenever you find an interesting example, but of course better than nothing -or looking through hundreds of concordance lines generated for single prepositions in the hope of finding suitable combinations, which you could still do by using the KWIC display option and sorting according to first word on the right of the node.
Another approach, which usually looks at a larger span of potentially discontinuous words, is that of calculating the degree of relatedness of words that occur near a particular node word.
The following illustration shows how this relationship may be represented by referring to the negative (left-hand) or positive (right-hand) positions relative to the node.
The t-score, on the other hand, mainly takes into account the frequency of cooccurrence of node and collocate.
As you'll hopefully have noticed, apart from simply giving us the option to look at/sort by the statistic or frequency of the collocates, AntConc also allows us to see or sort by whether the results occur on left-or right-hand side of the node, and with which frequency.
All collocational analyses have to be conducted by running a query on the node word first.
If you enter a specific word in the search box, the interface will allow you to calculate an MI score for the node and that particular collocate, while selecting from the 'POS LIST' options will allow you to look for colligations directly.
Starting our investigation with particles occurring one position to the right of the node verb form, we can choose the position '1 Right' and set the restriction to 'any preposition'.
While investigating position '2 Right' with the same restriction, you'll have to ignore many examples where the node verb form may in fact be an auxiliary, followed by a finite verb and then the preposition, as these examples are really only similar to what we just investigated.
First of all, keeping the 'Search Term Position' set to 'On Left' will help to mainly identify noun or NP collocates, while changing this to 'On Right' reveals other types of combinatorial options, such as combinations of determiners, possessive pronouns, intensifying adverbs, etc. with the node.
Experimenting with the options should allow you to find that BNCweb also lets you list the PoS categories that collocate with the node by selecting 'collocations on POS-tags' next to 'Information'.
One approach would seem to be to try and tweak hyperparameters of fitting trees, such as (1) the minimum decreases in deviance that define when trees stop splitting, (2) the minimum sample sizes per node, or (3) the tree depth.
Finally, some layers, such as 'constituent syntax', contain a complex subgraph: an NP node is attached to the token I, and a VP node is attached to know, and together they attach to the S node denoting the clause.
One consequence of using a TH layer for the architecture of the corpus is that the data may now in effect have two conflicting tokenizations: on the 'learner' layer, the first '?' and the second 'Da' stand at adjacent token positions; on the TH1 layer, they do not.
Below, we will use it to analyze a corpus based measure, namely the type-token ratio (TTR) of different texts that have the same overall length.
For each token, the analysis will produce a predicted value of the dependent variable.
We add a numeric second-level fixed effect which specifies the token frequency for each level of LEMMA in the following R code.
This can be measured thanks to the type/token ratio (see Chapter 8).
The command also helps you to calculate the type/token ratio (see Chapter 8), which represents a measure of lexical diversity.
At this age, her type/token ratio was 0.37.
At 3 years and 5 months old, the most frequent word was √ßa with 54 occurrences, and her type/token ratio was 0.21.
At this age, his type/token ratio was 0.38.
The type/token ratio was 0.24.
The type/token ratio cannot therefore be considered as a reliable measure of linguistic development.
Then, we will present two types of descriptive statistics that, respectively, make it possible to measure lexical diversity (the type/token ratio), and to calculate lexical dispersion in a corpus.
For example, in the spoken corpus of French from French-speaking Switzerland OFROM (see Chapter 5), the type/token ratio is 0.02, whereas in the French section of the children's literature corpus ByCoGliJe, it is equal to 0.04.
The problem with the type/token ratio is that it is very sensitive to the size of a corpus (for this reason, the type/token value 9/12 = 0.25 obtained for sentence (1) is in no way representative).
For example, the type/token ratio in the Sciences Humaines written corpus that contains around 260,000 word occurrences is 0.06, but it drops to 0.007 for the 2012 installment of the Le Monde corpus, which has over 26 million word occurrences.
We also showed how to measure the lexical richness of a corpus using the type/token ratio, and presented the limitations of such a measurement.
The type/token ratio can only be used for comparing texts of similar length.
The type/token ratio can also be used for comparing texts and their translations, only in cases of similar length.
Ditto tags are a way of tokenizing the corpus at orthographic word boundaries while allowing words to span more than one token.
When looking at occurrences of a linguistic item or structure in this way, they are referred to as tokens, so 1 651 908 is the token frequency of the possessive.
Let us look at one more example of the type/token distinction before we move on.
When studying words, we will normally be interested in how often they are used under a particular condition, so it is their token frequency that is relevant to us; but we could imagine designs where we are mainly interested in whether a word occurs at all, in which case all that is relevant is whether its type frequency is one or zero.
Again, it is the token frequency that is relevant to us.
There are five word tokens that contain this suffix (directly, briefly, wisely, truly, and wisely), so its token frequency is five; however, there are only four types, since wisely occurs twice, so its type frequency in this passage is four.
Once we have cleaned up our concordances (available in the Supplementary Online Material, file LMY7), we will find that -icle has a token frequency of 20 772 -more than ten times that of mini-, which occurs only 1702 times.
This means that their token frequency can reflect situations that are both quantitatively and qualitatively very different.
Specifically, a high token frequency of an affix may be due to the fact that it is used in a small number of very frequent words, or in a large number of very infrequent words (or something in between).
In other words, the high token frequency of -icle tells us nothing (or at least very little) about the importance of the affix; if anything, it tells us something about the importance of some of the words containing it.
This is true regardless of whether we look at its token frequency in the corpus as a whole or under specific conditions; if its token frequency turned out to be higher under one condition than under the other, this would point to the association between that condition and one or more of the words containing the affix, rather than between the condition and the affix itself.
For example, the token frequency of the suffix -icle is higher in the BROWN corpus (269 tokens) than in the LOB corpus (225 tokens).
For mini-, the type-token ratio is much higher: it occurs in 382 different words, so its TTR is 382 /1702 = 0.2244.
The differences in their TTRs suggests that mini-, in its own right, is much more central in the English lexicon than -icle, even though the latter has a much higher token frequency.
So a word may be a hapax legomenon because it is a productive coinage, or because it is infrequently needed (in larger corpora, the category of hapaxes typically also contains misspelled or incorrectly tokenized words which will have to be cleaned up manualy -for example, the token manualy is a hapax legomenon in this book because I just misspelled it intentionally, but the word manually occurs dozens of times in this book).
We will refer to this measure as the hapax-token ratio (or HTR) by analogy with the term type-token ratio.
All words without a subscript are new types and hapax legomena at the point at which they appear in the text; if a word has a subscript, it means that it is a repetition of a previously mentioned word, the subscript is its token frequency at this point in the text.
As we move through the text word by word, initially all words are new types and hapaxes, so the type-and hapax-counts rise at the same rate as the token counts.
Six words later, there is another occurrence of a, so type and hapax counts remain, respectively, at 12 and 11 as the token count rises to 14, and so on.
However, note that -ify has a token frequency that is less than half of that of -ise/-ize, so the sample is much smaller: as in the example of lexical richness in Pride and Prejudice, this means that the TTR and the HTR of this smaller sample are exaggerated and our comparisons in Tables 9.4 and 9.5 as well as the accompanying statistics are, in fact, completely meaningless.
The point of this case study was not to provide such an explanation but to show how an empirical basis can be provided using token frequencies derived from linguistic corpora.
The suffix has a relatively high token frequency: there are 2862 tokens in the fiction section of the BNC, and 7189 tokens in the newspaper section (including all sub-genres of newspaper language, such as reportage, editorial, etc.) (the data are provided in the Supplementary Online Material, file LAF3).
This difference is not due to the respective sample sizes: the fiction section in the BNC is much larger than the newspaper section; thus, the difference token frequency would suggest that the suffix is more important in newspaper language than in fiction.
However, as extensively discussed in Section 9.1.1, token frequency cannot be used to base such statements on.
Instead, we need to look at the type-token ratio and the hapax-token ratio.
In order to compare the two genres in terms of the type-token and hapaxtoken ratios, they need to have the same size.
In terms of how the information is conveyed, we see differences in the type-token ratio.
If a word is repeated, it counts as a new token but not as a new type.
One is concerned with the fact that words can theoretically have identical type and token frequencies, but may still be very differently distributed.
This measure takes into consideration how many different word types make up a token frequency.
For example, a frame occurring 200 times with 25 distinct fillers would have a type-token ratio of .
Like its name suggests, the type-token ratio is the ratio of the number of different words in a text (types) to the number of all words in the text (tokens).
We fed each tweet in its raw text form as a single sequence into BERT, which then produced context-informed vectors for each token in the tweet.
The model outputs multiple vector representations per token, each corresponding to a different hidden layer in the neural network architecture.
These techniques include frequency profiling: listing all of the words (types) in the corpus and how frequently they occur, and concordancing: listing each occurrence of a word (token) in a corpus along with the surrounding context.
For this reason, we have made the decision to replace URLs with a fixed token, which makes it easier to identify certain tweets.
Some tools (e.g. CQPweb, Sketch Engine) include punctuation in token counts.
Finally, since word counts (mostly token and type counts) are a part of almost every statistical equation that is discussed in this book, it is important that you have a good grasp of these definitions.
When describing corpora, we should always include the information about the exact token count.
Because token counts differ from tool to tool we should also provide details about the tool and/or how the token count was arrived at.
In our study, we used the 100-million-word British National Corpus (exact token count: 98,313,429; BNCweb; punctuation excluded from token count) .
The simplest lexical diversity statistic is the type/token ratio (see Section 2.2 for the definition of types and tokens).
Type/token ratio (TTR) expresses the proportion of types (different word forms) relative to the proportion of tokens (running words).
For the two texts from the 'Think about' task the type/token ratio is 0.8 (28/35) and 0.93 (28/30) respectively.
However, we have to remember that the type/token ratio is very sensitive to the length of the text; it decreases as the text becomes longer and more words get used again (recycled).
Simple type/token ratio (TTR) was used to compare the texts because they were of the same length (2,000 tokens).
There are different concepts of a 'word' -token, type, lemma and lexeme.
Since the notion of word is a little ambiguous here, it is useful to introduce a common distinction between (word) type and (word) token.
In this parlance, a frequency list lists the types in one column and their token frequencies in the other; often you will find the expression type frequency referring to the number of different types attested in a corpus (or in a 'slot' such as a syntactically defined slot in a grammatical construction).
As you saw above, the function table takes as an argument one or more vectors or factors and provides the token frequency of each type or each combination of types.
As a result, we have one vector with only -ic tokens (both.adjectives), but another one that says which suffix each token was attested with originally (their.suffixes), which we can then tabulate for both raw frequencies and percentages.
Let us begin by discussing how we compute type-token ratios and vocabulary-growth curves using a small vector tokens as a 'corpus', something that I always recommend to get started on a new project: Create a data set realistic enough in its make-up but small enough to be seen on one screen, and start developing your code with that.
It is a line plot that shows positions of all tokens of a vector on the x-axis (as in Section 5.1.2) and all type frequencies on the y-axis; in other words, each point's y-coordinate divided by each point's x-coordinate provides the point's type token ratio.
The x-coordinates of each point are obvious: It's just the numbers from 1 to 10, one for every element of tokens, which means that, given what we said above, the y-coordinates are then the type-token ratios of each token slot multiplied by the number of tokens of each slot.
For instance, note how the rightmost value is 7, i.e., the type-token ratio of the whole vector (0.7) times the length of the vector (10).
After that we store the results we obtain by sapplying functions to the list to count lengths and compute type-token ratios (with an anonymous/inline function); we compute a mean type-token ratio using all types and tokens, and a mean of all lengths of utterances with mean, but to avoid the effect that outliers might have we use the argument trim=0.05 to discard both the smallest and the largest 5 percent of the data.
We use lines to add the type-token ratio plot and add smoothers with lines(lowess(...)).
For the comparison of the first and last recording, we just generate regular scatterplots of type-token ratios against mlus, but we jitter the points to avoid overplotting and add the usual grid.
A token is a single instance of any word at a particular place in the corpus; a type is an individual word-form, which can occur many times in the corpus: thus one type is usually instantiated by multiple tokens.
In particular, all of the above approaches only deal with the minimal amount of information one should include-the more comprehensive information regarding token and type frequency distributions and entropies still awaits first exploration.
While much of that work has, of course, been successful because, for example, high token frequencies in b and c are positively correlated with high type frequencies, and high token frequencies in a are negatively correlated with clumpy distributions, it is unclear how potentially skewed the results are for cases where those correlations do not hold.
Type frequencies are then lower than token frequencies, and the type/token ratio can reveal interesting insights into properties of texts, essentially telling us how lexically variable a text or corpus is.
Although Matukar Panau is an agglutinating language, we see that by token frequency, most words are monomorphemic or bimorphemic in an 8,961-word annotated corpus.
The median token frequency is 1 morpheme, the median type frequency is 2 morphemes.
In corpus linguistics, we worry about both type frequency and token frequency (cf. 2.2.3) that can tell us different things about our corpora.
If you are writing your own computer scripts to build your corpus, you can integrate (or merge in) your metadata, bringing in all the metadata for each participant and all the metadata for each file for every token in the corpus.
The LLC tagset is much larger than the Brown tagset, comprising 204 tags, reflecting finer-grained and additional grammatical distinctions, for example, the case forms of pronouns, and including cliticised forms whereby clitic and host are treated as one 'contracted' token word (cf 2.2.2).
Zero is meaningful for ratio variables (zero token counts of parsimonious in a corpus signal an absence of that word in the corpus).
Discrete variables have measurements that cannot be divided, like token counts or word lengths in characters.
For now, the table below shows our token numbers for a 2-way DV.
So, if our token is she, in the sentence the cat jumped on the couch and then she went to sleep, and has the same referent as the cat, the antecedent of she is the cat.
A handful of examples of a particular token is not enough to give a confident sense of the full range of its behaviour, even if they can give a general sense of meaning.
However, if we scroll further down the list until we find ranks 112-116, which all have the same token frequency of 20, we find that guess is in fact listed above OJ because it starts with letter g.
AntConc uses a different concept of 'word' from BNCweb, based on the token definitions you've chosen, and may thus throw an error when it encounters something that doesn't fit this definition, aborting the keyword list comparison.
Next, get the token count from the 'Make/edit subcorpora' page and paste it into the spreadsheet, ideally at the top and to the right of the second frequency list, as we may need to shift some items in the list down later to align the data.
Interestingly enough, though, AntConc does appear to have a secondary sort order based on the frequency because otherwise uppercase A would have to appear before lowercase a, and the latter is only ranked higher because it has a frequency of 161 as opposed to a single token of the former.
And, if you're worried about not being able to get an exact token count of all the data after making modifications, the spreadsheet will also help you there because all you need to do in order to obtain this is to place the cursor in the field immediately below the count for the final token in the list and use the AutoSum function (symbolised by ) to automatically count the total for you.
Other things you'll need to delete are single letters followed by dots, as well as abbreviations, such as i.e. or e.g., and any genuine words that are followed by dots, as the dots don't form part of the general token definition we're using, and most of these forms (apart from the abbreviations) are probably the results of tokenisation errors in the BNC, anyway.
You should definitely also delete all numbers, unless you want to change the token definition to include those, but, as I pointed out before, numbers may take many different forms and their meaning may be difficult to identify.
Pasting the token counts from the 'Make/edit subcorpora' page also presents no problem because, this time, we only have one single piece of 'text' without HTML codes, so there's nothing to misinterpret for the spreadsheet program.
Looking at the results, we can first notice that there are specific tokenisation and tagging issues that result in inappropriate 'compound' forms. These, we can perhaps safely ignore, bearing in mind that the modals contained in them would normally contribute relatively little to the overall token counts of the genuine types.
The overall number of types (194,570, at least based on my token definition) is also fairly high, reflecting the variability of expressions.
By the same token, where it exceeds the abilities of some corpus tools, it is not necessarily the case that those corpus tools are lacking; they were simply developed for a different set of users.
Of particular note in their study is an additional step that they employ for addressing reliability: comparing frequency list items across different corpora.
Two critical obstacles that need to be addressed in frequency list research is the handling of homoforms (e.g., river bank vs. investment bank vs. bank as a verb) and multi-word units (e.g., I didn't care for the movie at all vs.
Such disambiguation remains both a theoretical and practical challenge for corpus-based frequency list research.
In the midst of these important developments, one issue that deserves more attention is frequency list generalizability.
At present, evidence of frequency list generalizability tends to come in one or two forms, both of which are indirect.
Manuals on lexical frequency list research, however, do recommend such comparisons.
This text provides a comprehensive, practical, and very accessible discussion of important issues in frequency list design and evaluation.
Imagine, finally, the frequency range he is currently interested in is between 35 and 40 words per million words and, as he browses the frequency list for good words to use, he comes across an adjective and a verbenormous and staining -that he thinks he can use because they both occur 37 times in the Brown corpus (and are even equally long) so he notes them down for later use and goes on.
The keywords now convey a very specific idea of what the text is about: there are two proper names of rivers (the Neosho already seen on the frequency list and the Marais des Cygnes, represented by its constituents Cygnes, Marais and des), and there are a number of words for specific species of fish as well as the words river and channel.
In addition, a Brown-based frequency list (for all words in the corpus) would be quite sparse.
For instance, a concordance can be produced for a certain part-of-speech tag, a frequency list of lemmas, key semantic tags, and calculate collocation statistics for which semantic tags relate to a given word.
However, one cannot use a simple frequency list of an English engineering corpus, because its most frequent words would still be the, of, in, . . . -these are frequent everywhere.
Ru ¬®hlemann uses frequency counts to demonstrate the importance of laughter to conversation, for example, if "between-speech laughter" is considered as a linguistic item in and of itself, it would be placed in 29th position on the BNC conversational subcorpus frequency list.
With massive amounts of computerized texts -now more easily obtained than ever before -at a keystroke one can generate a frequency list in a fraction of the time it would have taken to achieve the same task by hand.
The words were collected from all tweets and a frequency list created for each MP.
It can be used to rank-order words in a frequency list to highlight the most frequent and evenly dispersed items.
The frequency list based on lemmas confirms our initial assumption (based on Zipf's law) about the number of words with the frequency of 30 and over: there are 3,196 such lemmas in the corpus.
For example, if you want to generate a frequency list of a corpus or a concordance of a word in a corpus with R, you must write a small script or a little bit of code in a programming language, which is the technical way of saying you write lines of text that are instructions to R.
This way, the actual effort of generating a frequency list, a collocate display, a dispersion plot, etc. often reduces to about the time you need with a concordance program.
While this is no problem with a one-dimensional frequency list, this is much harder with multidimensional frequency tables: Perl's arrays of arrays or hashes of arrays etc. are not for the faint-hearted, whereas R's table is easy to handle, and additional functions (table, xtabs, ftable, etc.) allow you to handle such tables very easily.
The most basic corpus-linguistic tool is the frequency list.
You generate a frequency list when you want to know how often something -usually words -occur in a corpus.
Thus, a frequency list of a corpus is usually a two-column table with all words occurring in the corpus in one column and the frequency with which they occur in the corpus in the other column.
In this parlance, a frequency list lists the types in one column and their token frequencies in the other; often you will find the expression type frequency referring to the number of different types attested in a corpus (or in a 'slot' such as a syntactically defined slot in a grammatical construction).
First, a frequency list may provide the frequencies of all words together with the words with their letters reversed.
These observations are a subset of Zipf's laws, the most famous of which states that the frequency of any type is approximately proportional to its rank in a frequency list, and such a distribution is often referred to as a Zipfian distribution.
Then, in a second step, you do a second loop in which you load and amalgamate all 4,049 frequency list files.
Think how this seemingly small decision has potentially big implications: If you generate a frequency list of the BNC using the first approach, then you lose the counts of all multi-word units but every because of, in spite of, out of, etc. increases the frequencies of of as well as because, in, spite, and out.
A frequency list generated with the second approach doesn't have that problem, which is why we will go with the second approach here.
That means we want our frequency list to first give the frequency of the letter "c", then that of "d", then "f", then "j", then "i" (because "c" is already covered), etc.
Finally, we will use sort and table to create our frequency list of the three-grams, quantile to explore their distribution, and plot with type="h" again to visualize the results.
Finally, we will use sort and table to create our frequency list of the three-grams, quantile to explore their distribution, and plot with type="h" again to visualize the results.
The next script does something seemingly elementary -we will create a frequency list of word-tag combinations (so as to be able to distinguish run as a noun from run as a verb) -but we will do it on a relatively large data set, the complete BNC World Edition with XML annotation, and we will use the annotation well by utilizing information provided by the BNC's multi-word tags, i.e., tags that mark multi-word units such as because of, in spite of, on behalf of, etc.
We use the usual for-loop with scan and grep to choose and process the files, and then we use exact.matches.2 to retrieve all the infinitives as well as all lemmas for the overall frequency list.
The last step consists of merging the frequency list files: We generate an empty table first, and then use another for-loop to load each frequency list file and merge them into one long table (with c).
This is because this exercise is really only one (complicated) regular expression: The goal is to match all kinds of formats of numbers, which is something that can easily come up when you generate frequency lists of (large) corpora and want to avoid having potentially tens of thousands of frequency list entries that are really just different numbers -in such a situation, you would probably want just one entry "_NUM_" or something similar; thus, this is a realistic situation.
We use exact.matches.2 to find hyphenated forms and sub to clean away the tags and spaces; then we apply table to create a frequency list and then save it.
In the second part, we use character and numeric to create empty collector vectors to merge the hyphenated forms and their frequencies from all over the corpus, a for-loop to load each frequency list file, and, if there are hyphenated forms in the file, we merge them with subsetting, incrementing the vector counter on each iteration (as in Section 5.2.8).
As we turn to the Wikipedia entries, we scan them, strsplit them into words, and create a sorted frequency list.
The two most basic forms of data which we can extract from a corpus are the concordance and the frequency list.
So for instance, if we see on a frequency list for an English corpus that the is the most frequent word, occurring 60,000 times, then we could say that the single type the is represented in this corpus by 60,000 tokens. Frequency lists can also be compiled for any annotation that is present in the corpus, and we can talk about types and tokens of annotation tags in the same way.
We could compare the frequency of the to that of other words on the frequency list, and observe whether it is more or less frequently used than those other words.
A frequency list is inherently quantitative in nature.
The frequency list may be of word types, lemmas or any kind of tag -thus, we often talk about keywords and key tags.
Unfortunately, though, this would leave us with some very strange 'words' (that superficially look like hyphenated compounds ), them-their and honey-moon-over, in any resulting word-frequency list.
In this case, if we purely look at individual (untagged) words and don't actually analyse the context, we won't necessarily be able to group the second part (i.e. the clitic) with its appropriate full counterpart in a frequency list (something we'll discuss in detail in Section 9.2).
Thus it's generally advisable to first check any output of a frequency list produced by some program to see whether it may exhibit any unusual features that could influence the analysis negatively.
As will hopefully have become clear from the discussion of Exercise 53, the default frequency list in AntConc treats clitics, such as 's (but without the apostrophe) as separate words, which is often what we want because they're in fact abbreviated words that have been fused with a preceding word.
Now, simply click again to re-create the frequency list including the two extra characters, and observe the changes in the list by scrolling through it.
Return to the original frequency list sorted by frequency.
Select the frequency list you just saved and click .
We now have our frequency list stored in a very convenient format, as spreadsheets not only allow us to re-sort our data easily (and repeatedly, if necessary), but also because this makes it possible to investigate and enrich the data in various ways.
Create a frequency list based on the new subcorpus, import it into a spreadsheet, and sort it as we just did in the previous exercise.
As Exercise 63 will have shown you, the keyword list, at least in our case and for positive keywords, may not necessarily provide you with more information than a frequency list that has been filtered well through the use of stopwords.
In addition, the ability to highlight negative keywords in AntConc may also allow you to investigate under-use of specific vocabulary relatively easily, for instance when comparing learner data with that produced by native speakers, etc., an option that, obviously, a pure frequency list of only the source corpus is unable to provide.
Next, get the token count from the 'Make/edit subcorpora' page and paste it into the spreadsheet, ideally at the top and to the right of the second frequency list, as we may need to shift some items in the list down later to align the data.
This goes to show that, by observing items in a frequency list, we may often be able to see things we might have overlooked or ignored while concordancing, simply because the results would have been easier to understand.
Provided that you don't forget to change the option for creating a new corpus, there should be no issues in completing this exercise and creating a suitable frequency list.
Of course, you could fix this manually, but, since we've now separated the data from our ability to concordance on it, we'd have to go back into BNCweb to look at the original frequency list.
Essentially, the option we just explored has relatively little to do with keywords as calculated through the options from the top part of the same page, as all it really does is eliminate all word types both corpora share, and then display whatever remains as a frequency list.
As you may have noticed, this editor is really not optimised for handling large files, so to just view the frequency list without the ability to concordance on it, a dedicated editor, such as Notepad++, would be far better.
If we then want to use these lists to profile lexis in other texts or corpora, the same procedure is necessary for each new text.
This case study demonstrated a complex design involving grammar, lexis and semantic categories.
However, once we have extracted and -if necessary -manually cleaned up our data set, we are faced with a problem that does not present itself when studying lexis or grammar: the very fact that affixes do not occur independently but always as parts of words, some of which (like wordform-centeredness in the first sentence of this chapter) have been created productively on the fly for a specific purpose, while others (like ingenuity in the same sentence) are conventionalized lexical items that are listed in dictionaries, even though they are theoretically the result of attaching an affix to a known stem (like ingen-, also found in ingenious and, confusingly, its almost-antonym ingenuous).
From a theoretical perspective, these results may seem to be of secondary interest, at least in the domain of lexis, since lexical differences between the major varieties of English are well documented.
Similarity of lexis in web-based GloWbE and genres in COCA and BNC 2.1.
Part II is organized as a progression of the linguistic levels, beginning with corpus-based analyses of prosodic characteristics, moving on to chapters dealing with lexical characteristics (keywords, collocations, and phraseology), followed by chapters on grammatical features (descriptive grammar, grammatical variation, grammatical change, and the intersection of grammar and lexis), and finally concluding with chapters on the corpus-based study of discourse functions and pragmatics.
And this one example from the domain of syntax can be multiplied endlessly for other variations in syntax, or in lexis, morphology, phraseology, or meaning.
One of the ways in which corpus linguistics has changed our view of language, I believe, is in the now widespread recognition that grammar and lexis are not separate components of language, but that they interpenetrate.
There is no consistent terminology to describe research of this kind, but the phrase "Lexical Grammar" directs us to the combination of lexis and grammar embodied in it.
With the exception of Hoey, the lexis-to-grammar tradition sketched above is agnostic with regard to psycholinguistic theories.
This particular quantitative information about lexis and grammar suggests a complex interaction of grammar, lexis, register, and phraseology in relation to frequency (ibid.: 459).
In some other sections of LGSWE the information about lexis is more extensive.
In this section we look at the other side of the coin and discuss studies that begin with lexis and investigate grammatical aspects of their context.
As noted above, LGSWE and the PG books have complementary approaches: LGSWE views lexis through the lens of grammar; the PG books arrive at grammar through a study of lexis.
Thus the PG books are more comprehensive in terms of lexis but LGSWE covers more topics in terms of grammar.
For example, the ditransitive construction appears as the pattern "verb phrase + noun phrase + noun phrase," but the interrogative construction has no pattern equivalent because there are no restrictions on the lexis with which it occurs.
The first is that the phenomenon of the unequal distribution of lexis accounts for much more about naturally occurring text than might be expected from reading any of the papers discussed so far.
The study also confirms the variation between written and spoken texts, with textbooks containing twice as many different words as classroom teaching, despite their broadly similar instructional purposes, largely due to their use of specialized lexis.
As against SLA studies which have traditionally prioritized morphology and grammar, LCR is characterized by a strong focus on lexis, lexico-grammar, and a range of discourse phenomena.
The shift of focus from morphosyntax to lexis and discourse has proved to be particularly fruitful for the analysis of advanced interlanguage.
That meticulous counting method resulted in what was probably a more accurate representation of the nature of the lexis in the corpus from which the 1953 GSL was derived, with counts that reflected separate lexemes, including multi-word expressions.
Free software such as VocabProfile online (www.lexutor.ca/vp) or AntWordProfiler offline (www.antlab.sci.waseda.ac.jp/) allows a teacher to input a text which is then returned with the lexis color-coded according to the frequency of each word in the BNC or COCA corpus.
First, in terms of research focus, we would hope the future would bring more discourse-level studies with a focus on text and associated features of genre, stance, etc., to complement the current dominance of studies on lexis and specific grammar points.
These all highlight the relationship between lexis and grammar and are useful to a language learner.
As will be noted below, those contributions may relate to the mechanisms by which language changes over time, or to the nature of the difference between registers, or to the relationship between lexis and grammar.
An area of constant exploration is the role that a technical expertise in language structure -lexis, grammar, discourse -plays in relation to expertise in quantification.
In addition, a concept known as 'semantic sequences' was developed -this suggested that concordance lines can be read to identify 'what is often said', thus moving from lexis to grammar to discourse.
Such disambiguation remains both a theoretical and practical challenge for corpus-based frequency list research.
The area of research/application where dispersion has gained most ground is that of corpus-based dictionaries and vocabulary lists.
For three of the verbsencourage, support, and fear -use in one of the searched constructions is rare (less than 1% of occurrences of the verb) (continued) 7 Analyzing Co-occurrence Data 151 and not listed as a possible form in the corpus-based Collins Cobuild English Language Dictionary.
There is a growing strand of research that explores the efficacy of so-called datadriven learning (DDL) approaches, which give students access to large amounts of authentic language data (Frankenberg-Garcia et al. 2011), and corpus-based materials naturally lend themselves to use in such an approach.
This is an example of an innovative study using the corpus-based method to gain new insights into the linguistic subdiscipline of pragmatics.
Changing legislation in these areas might pose further difficulties for corpus-based research in the future.
As is often the case in corpus-based translation studies, parallel data are used with a view to identifying the source-text items/structures that have triggered the use of a given item/structure in the translations (in this case, the presence of a trigger term in the English source texts, such as unit, job, or team).
Their study convincingly illustrates the need to adopt multifactorial research designs in corpus-based translation studies, as these make it possible to go beyond the monofactorial designs where, typically, only the "translation status" variable is considered (translated vs. non-translated).
Until relatively recently, corpus-based studies have tended to focus almost exclusively on the English language, and on British and American English in particular.
This dependency is due to the fact that our ability to analyse and examine complex and extensive multimodal corpora in a systematic and accurate way (particularly when integrating traditional corpus-based methods into the approach), is reliant on the availability of appropriate technology.
For example, if a corpus-based analysis contains the length of a phrase as an independent variable, and we introduce the pronominality of that phrase as a second independent variable, these two variables might correlate significantly, since full lexical phrases are by necessity longer than single pronouns.
Let us assume that we conduct a corpus-based analysis of how speakers address each other in conversation.
It is also argued that the observed tendencies -such as short-before-long and animate referents firstare in line with synchronic corpus-based and experimental findings about general cognitive principles underlying the framework of probabilistic grammar.
While it is not found in a large number of studies, its use seems to be on the rise to answer questions in corpus-based variationist linguistics.
During the process of reviewing the corpus-based research that uses bootstrapping, we have seen no evidence of bootstrapping results being misinterpreted or misrepresented.
In this section we demonstrate how to bootstrap corpus-based data using R.
For more general info about how to report the results of a quantitative corpus-based study, see also Chap. 26.
Imagine further that a number of corpus-based studies had examined this issue using different corpora, with each study yielding a result expressed in occurrences per million words.
In this chapter, we have argued that meta-analysis should be more widely applied within corpus linguistics as a method of synthesizing and empirically reviewing previous corpus-based research.
Both types of analysis have something to contribute to corpus-based language study.
This brief overview of the current state of corpus-based research is explored in detail in this second edition of English Corpus Linguistics.
The chapter opens with a discussion of how to define a corpus, and then traces the history of corpus linguistics, noting, for instance, that corpus-based research began as early as the fifteenth century, when biblical concordances were created based on passages from the Bible.
But as corpus-based research began to expand during this period, its emergence during the dominance of generative grammar in the 1960s created quite a controversy, since to some linguists, corpus linguistics reeked of behaviorism.
The chapter concludes with a description of the many different areas of linguistics (e.g. lexicography and sociolinguistics) that have benefited from the use of linguistic corpora, followed by a linguistic analysis illustrating that corpus-based methodology as well as the theory of construction grammar can provide evidence that appositives in English are a type of construction.
For instance, many of the corpus-based reference grammars of English, such as Quirk et al.'s A Comprehensive Grammar of the English Language, are more qualitative, as they draw upon linguistic corpora for authentic examples to illustrate the many points of English grammar discussed throughout the grammar.
In the book, it is noted that these two approaches are not necessarily mutually exclusive: the generative linguist, for instance, could very well work with examples that are corpus-based rather than examples that are made up.
It is for this reason that the approach that we will adopt in this book corresponds to a corpus-based approach, considering these as available tools for linguists to be able to test their hypotheses.
On the other hand, this type of research is corpus-based, because it starts from a hypothesis (e.g. "passive sentences tend to be used more frequently with state verbs"), and seeks to verify it in the corpus, which, in that way, only works as an analysis tool.
The data provided by these contrastive corpus-based studies are not only useful in theoretical linguistics for understanding how languages work, but may be helpful for other applications, notably for the development of tools such as bilingual dictionaries (see section 4.6).
Indeed, corpus-based studies aim to test empirical hypotheses that are often formulated from the results of other studies, either more theoretical or empirical in nature, which have already been published in the scientific literature.
On the other hand, there are textbooks that focus one or more specific corpus-based techniques, discussing very specific phenomena (often the research interests of the textbook authors themselves) using a narrow range of techniques (often involving specific software solutions).
Although corpus-based studies of language structure can look back on a tradition of at least a hundred years, there is no general agreement as to what exactly constitutes corpus linguistics.
As we saw in the preceding chapter, corpora fell out of favor just as linguistics grew into an academic discipline in its own right and as a result, corpus-based studies of language were relegated to the margins of the field.
They may, however, inform corpus-based syntactic argumentation (cf.
Due to the practical and theoretical difficulties of defining and measuring complexity, the vast majority of corpus-based studies operationalize Weight in terms of some measure of Word Length even if they theoretically conceptualize it in terms of complexity.
It also demonstrated that such corpus-based studies may result in very specific hypotheses about the function of lexicogrammatical structures that may become the basis for claims about mental representation.
The case study was also meant to provide an example of a corpus-based design where it is more useful to operationalize one of the constructs (Frozenness) as an ordinal, rather than a nominal variable.
It does not pose particular problems for corpus-based morphology, which studies structures smaller than the word.
Among other things, the corpus-based study of (small set of) source domain words may provide insights into the systematicity of metaphor (cf. esp. Deignan 1999b).
In one sense, the distinction between corpus-driven and corpus-based research methods can be misleading.
One might argue that this second step can be seen as corpus-based because it identifies features in the corpus that merit further attention.
On the other hand, it might also be argued that the feature was not identified by previous corpus-based research so it is, by definition, not corpus-based.
As we have discussed many times throughout the book, one can follow a) a corpus-driven or b) a corpus-based (often called corpus-informed) approach to linguistic analysis.
We showed how we can do a corpus-based study with already identified language features, whether doing a lexical study or searching for the use of particular grammatical patterns.
If you don't have such skills (yet), perhaps the best solution is to continue doing corpus-based studies.
You can continue to look for lexico-grammatical patterns that you find interesting, or you can carry out corpus-based studies that rely on the results of previous, corpus-driven studies.
In fact, it can be argued that intuitionbased linguistics developed as a reaction to corpus-based linguistics.
Although each chapter includes a broad summary of previous research, the primary focus is on a more detailed description of the most important corpus-based studies in this area, with discussion of what those studies found and why they are especially important.
Part II is organized as a progression of the linguistic levels, beginning with corpus-based analyses of prosodic characteristics, moving on to chapters dealing with lexical characteristics (keywords, collocations, and phraseology), followed by chapters on grammatical features (descriptive grammar, grammatical variation, grammatical change, and the intersection of grammar and lexis), and finally concluding with chapters on the corpus-based study of discourse functions and pragmatics.
This part begins with chapters on the corpus-based description of spoken English, written academic English, and patterns of variation (synchronic and diachronic) among a wider range of spoken and written registers.
Finally, the CHECL concludes with a major section on applications of corpus-based research.
Part IV of the handbook surveys these major areas of application, including classroom applications, the development of corpus-based pedagogical materials, vocabulary studies, and corpus applications in lexicography and translation.
In this section, examples of recent corpus-based studies of prosody are described to provide a sense of the types of current work that are underway.
Second, corpus-based collocation studies have so far focused on, or indeed have largely been confined to, the English language.
As intuition is usually an unreliable guide to patterns of collocation and semantic prosody, this study takes a corpus-based approach to addressing these research questions.
This chapter has sought to provide a critical account of the current debates in corpus-based collocation research.
The review in this section demonstrates that corpus linguistics has enabled large-scale collocation analysis and foregrounded collocation in linguistic research while corpus-based collocation studies over the past decades have uncovered a range of interesting collocational behavior and semantic prosody which have been hidden from intuition and can only be revealed by examining a large amount of attested data simultaneously.
Sections 2.1 and 2.2 contrast studies based on their major methodological approaches, distinguishing between corpus-based studies of pre-selected lexical expressions versus corpus-driven studies to identify the full set of important multi-word sequences in a corpus.
Although the focus of this handbook is on corpus-based investigations of English language texts, a few of the studies reviewed here have addressed the issue of phraseology in other languages.
In our case study, we illustrated how this distinction is not absolute: some previous studies of discontinuous lexical frames (like Biber 2009 and Ro ¬®mer 2010) are intermediate along this continuum, beginning with a corpus-driven approach to identify a set of continuous lexical sequences, but carrying out the corpus-based investigation of only those sequences, to analyze the extent to which they occur as discontinuous frames.
To start out, we clarify briefly what we mean by the terms grammar, grammatical change, and corpus-based analyses.
In a loose sense, much classical philological work on grammatical change was corpus-based.
As the results of corpus-based research on grammatical change accumulate and as the methods of analysis become more sophisticated, the corpus ceases to be merely a tool and becomes an active ingredient in the further development of usage-based theoretical models.
First, it vividly illustrates, through the application of a corpus-based, bottom-up methodology, the close interrelationship between pragmatic function and context.
It needs to be explained that pragmatic research which truly focuses on language in use has to be corpus-based because there is no convenient way of making speakers say what you want them to say where language is unpredictable, messy, and extended over many turns.
In general, the repertoire of corpus-linguistic studies is fairly broad from corpus-based but mainly qualitative studies to advanced statistical methods and computer programs specially designed for the research question under investigation.
The case study provided in Section 7 illustrated a common focus of corpus-based studies of writing and increasingly speech, namely stance devices; it also provided an example of a new direction in spoken discourse analysis in its exploration of variation within spoken interactions.
Perhaps most corpus studies of academic writing have followed what Tognini-Bonelli (2001) calls a corpus-based approach, where the researcher begins with a pre-selected list of potentially productive items and uses the corpus to examine their frequencies and the ways they behave in different contexts.
Although always important, this issue is especially important for corpus-based studies, which generally seek to produce more generalizable results than many other approaches and to facilitate comparisons between different corpora.
However, corpus-based register studies could have far greater impact than they currently do.
Walker's study is a good example of a diachronic corpus-based investigation where the author needs to consider the special nature of his or her data and carefully assess the reliability of the data sources used.
Although far less common, a corpus-based approach to data collection also has several advantages, including allowing for dialectologists to collect large amounts of data from a large number of informants, observe dialect variation across a range of communicative situations, and analyze quantitative linguistic variation in large samples of natural language.
In addition to national variation, one of the more successful applications of the corpus-based approach to dialectology is in the field of historical sociolinguistics.
First, corpus-based studies have shown that dialect variation exists across a wide range of different varieties of language, including forms of written and standard language, where dialect patterns had never been sought or even believed to exist.
Second, corpus-based studies have shown that dialect variation can involve a much wider range of linguistic variables than is generally analyzed in dialectology and sociolinguistics.
In addition to reviewing previous corpus-based dialect studies, this chapter presents an analysis of dialect variation and change in not contraction in a corpus of American letters to the editor.
The corpus-based approach to dialectology contrasts with the standard approach, which is based on analyzing language elicited through interviews and questionnaires.
As demonstrated by the studies reviewed in this chapter, the corpus-based approach to dialectology is a growing field of inquiry that allows for new types of research questions to be pursued and new types of dialect variation to be identified.
In particular, a corpus-based approach is especially conducive to the analysis of quantitative grammatical variation, because it allows for large samples of natural language to be collected for analysis, and for dialect variation to be observed and compared across a variety of communicative situations.
Because of these advantages, corpus-based dialect studies have greatly expanded our knowledge of dialect variation, showing that social and regional linguistic variation are far more complex and pervasive than has previously been assumed.
Nevertheless, there are disadvantages to the corpus-based approach as well.
To a large extent, these are reasons why the corpus-based approach is not more popular in dialectology and sociolinguistics today.
When this level of technology is reached, corpus-based dialect studies will become the norm.
Finally, a recent trend in corpus-based research of World Englishes is the detailed statistical modeling of variation, often including ENL, ESL, and EFL varieties.
The article is especially interesting for its discourse-oriented perspective, starting from a rhetorical function rather than from one or two isolated items, as well as for its focus on scientific prose and its plea for more corpus-based English for Academic/Specific Purposes textbooks.
CIA studies raised the issue of the norm (native vs. non-native; novice vs. expert) and pointed to the benefit of relying on an explicit corpus-based norm rather than the implicit and intuitive norm that underlies many SLA studies.
The Oxford Collocations Dictionary for Students of English is the only one of the three collocation dictionaries which is corpus-based.
Conrad argued that three changes prompted by corpus-based studies of grammar had "the potential to revolutionize the teaching of grammar" (2000: 549): first, monolithic descriptions of English grammar would be replaced by register-specific descriptions; second, the teaching of grammar would become more integrated with the teaching of vocabulary; and third, the emphasis would shift from structural accuracy to the appropriate conditions of use for alternative grammatical constructions.
We carry out a more indepth analysis of two of the four research questions listed above, namely how corpus-based research applications are presented, and how (and how much) corpus-based research has been incorporated into materials.
What the checklist reveals is that some corpus-based information is rather coherent across the various books (get-passives are rare and are more commonly found in speech; agentless passives are more frequent than passives with by-agent, etc.).
Our aim in carrying out the case study was not to come up with the conclusion that all grammar textbooks should include corpus-based information on all grammar points.
Whilst corpus-based information on article usage might not add much to the treatment of articles in grammar books, we believe that some areas would really benefit from the inclusion of the results of corpus studies.
The first part of this chapter surveys the development of corpus-based translation studies (CBTS), from the programmatic proposals in the early 1990s to recent developments and trends.
Furthermore, there are some new trends in corpus-based research that Rissanen could not have anticipated in his article, and these introduce challenges of their own.
Very similar discourse is often found in corpus linguistic literature in support of a corpus-based approach.
The replication crisis in linguistics is highly relevant to corpus-based research: Many corpus studies are not directly replicable as the data on which they are based are not readily available.
That said, our analysis identified clear trends regarding the use of semantic shifts based on a large amount of data, further confirming the potential that corpus-based analyses have in understanding the patterns behind complex linguistic behaviors.
Dataset is a series of corpus-based findings that can be statistically analysed.
The assumption underlying basically all corpus-based analyses, however, is that formal differences reflect functional differences: Different frequencies of (co-)occurrences of formal elements are supposed to reflect functional regularities, where functional is understood here in a very broad sense as anything -be it semantic, discourse-pragmatic, etc. -that is intended to perform a particular communicative function.
It involves the notion of lexical frequency profiles, which is a corpus-based approach in applied linguistics concerned with the amount and the kind of vocabulary second/foreign-language learners use in their speaking and writing (see Laufer & Nation 1995 for an introduction; Meara 2005 for a critique; and Laufer 2005 for a response).
The essence of this review is on the modern corpus and corpus-based research conducted by researchers affiliated with different common-section institutions all over the world.
When a researcher taking part in corpus-based research is recognized, his/her names are employed as key terms to deal for research offspring in basic scholastic data.
The variety of topics, the copra volume employed and languages studied to provide us a powerful tone that corpus and corpus-based research is a lifelike domain of research.
For instance, many corpus-based analyses of media discourse are based on examining media texts that address some particular topic.
Both approaches have their place in different kinds of corpus-based study.
One sub-type is a reference corpus: a general corpus taken to be a standard reference for corpus-based research on a given language.
Section 7.3 is devoted to comparative corpus annotations in the context of corpus-based typology.
Video recordings also allow us to document sign languages; see Chapter 4.3 for some details on their corpus-based investigation.
A related question is the variable realisation of objects as either pronoun or zero, and in 8.4 we have presented such a corpus-based study for Vera' a in some detail.
Hence, corpus-based typologists need to carefully evaluate the specific properties of the corpora they use vis-√†-vis the phenomena they are researching. .
Corpus linguistics takes real samples that are used to generalise about language, and corpus-based typology helps us to confirm (or confront the idea) that these assumptions are applicable for a more diverse sample.
Between 2002 and 2006, although researchers still cited corpus-based grammar references for their studies (e.g., Cambridge Grammar of the English Language), one group of researchers made use of newly developed datasets, both large or small, such as The CHILDES Corpus, Wordnet, and A New Academic Word List.
During that time, corpora enabled researchers to conduct studies on grammar using corpus-based parsers, such as the Penn Treebank and its parts of speech tags.
For the latter, you could opt for only investigating the speech of men or women, or speakers from the same or different social classes, or create subcorpora for both, and then compare the language in some way, which makes it possible to carry out corpus-based sociolinguistic analyses on the data, as do the options for restricting speakers to particular education levels, etc.
Although it's of course somewhat easier to confine our corpus-based investigation to single lexical items, single words and their frequencies aren't the only interesting things we may want to analyse with the help of corpora.
The first one basically gave you a means of designing your own ways of classifying your data in a sensible manner, which then enables you to use the corpus-based analysis methods we've discussed throughout the course in order to extract and count relevant information.
In corpus-based linguistics the research domain is some collection of natural language utterances.
As a first step, two frequency sorted word lists are prepared, one from the corpus being studied (the 'target') and one from a reference corpus.
The resulting sorted list contains two kinds of keyword: positive (those which are unusually frequent in the target corpus relative to the reference corpus) and negative (words which are unusually infrequent in the target corpus).
Often, stylistic, grammatical, or syntactical features of the target corpus are highlighted through keyness comparison with a general reference corpus.
Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specific research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identification of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results.
Selection of a reference corpus will impact the results, and some care should be taken to select an appropriate 'benchmark' to highlight differences aligned with a given research question.
Those wishing to answer more general questions (e.g. 'aboutness') may choose to make use of a general reference corpus.
Finally, the dispersion of a collocation across a reference corpus had only a weak relationship with knowledge (more widely spread collocations were better recognized), and this relationship was significant only in the BNC.
While one corpus can be compared to another reference corpus, these tools also make it possible to extract a list of keywords that are specific to the corpus studied.
Unlike many European languages, French still does not have a reference corpus, a representative sample of the French language in general, similar to the British National Corpus that exists for British English, one of the pioneers in the genre.
Finally, some concordancers can be used to extract a list of keywords in a corpus by comparing them with a reference corpus (see Chapter 6).
More specifically, the concordancer determines which of the words are used significantly more (or even less used in the case of negative keywords) in the search corpus compared to the reference corpus.
In a nutshell, the keyword list of a corpus is very useful to identify its main topics, provided that the comparison with the reference corpus is appropriate.
Finally, AntConc makes it possible to create a list of keywords from the corpus based on the comparison with a reference corpus.
We have observed that, despite the absence of a reference corpus, numerous more specific corpora are available, which can be combined to carry out research in many areas of linguistics, as we will see in the subsequent exercises offered.
When it comes to creating a reference corpus, the data collection phase is so time-consuming that it can only be tackled by a group of experts.
In order to be a representative, a reference corpus should contain a balanced set of samples covering the main stylistic genres, both in the spoken and written modes.
In other words, the corpus-linguistic identification of keywords is analogous to the identification of differential collocates, except that it analyses the association of a word W to a particular text (or collection of texts) T in comparison to the language as a whole (as represented by the reference corpus, which is typically a large, balanced corpus).
In this case, you compare words in one corpus, called target corpus, with words in another, called reference corpus.
The British press reporting is your target corpus and the American press reporting is your reference corpus.
The higher the keyness value for the words, the more likely that they appear in the target versus the reference corpus.
In addition, the keyness method and the n-gram method can be combined in order to highlight key clusters, i.e. repeated sequences whose frequency differs significantly in one corpus compared to a reference corpus.
The significance test calculates the significance of the difference in frequency between the word in the target data and in the reference corpus.
It is recognized that the notion of exactly what qualifies as key in any study is influenced by the settings and parameters of the program used, and by the comparator texts (in the reference corpus).
A reference corpus cannot in any sense represent the language, unless it is subdivided into text categories or subcorpora representing a broad range of registers, as in the BNC or the Bank of English.
One of the novel software applications, the Keyword analysis, uses significance tests to distinguish words that are significantly more frequent or significantly less frequent than in a reference corpus; calculations are carried out automatically by the program and it is possible to gain valuable insights into the material that cannot be achieved with qualitative study.
It is easy for the end user to apply the method to their data, but the researcher's own input shows in the selection of the target corpus and an optimal reference corpus and the interpretation of the machine-produced key word lists, which is not simple at all (see below).
Comparing the features of target writers' texts with a much larger reference corpus of work in the same discipline can help to determine what is general in the norms of a community and what represents more personal choices.
These corpora were individually compared with a larger reference corpus representing a spectrum of current published work in applied linguistics and in the same genres as the target texts.
Self-reference, in fact, occurs 9.1 times per 1,000 words in the Swales corpus compared with 5.2 in the reference corpus, imparting a clear authorial presence of a thoughtful reflective colleague thinking through issues.
This functional analysis also takes account of distributions across Dickens's texts and the nineteenth-century reference corpus and hones in on detailed textual examples discussed mainly from an intrinsically explanatory point of view.
This includes errors (which were the focus of pre-corpus interlanguage studies), but also cases of under-or overuse, i.e. the use of significantly fewer or more instances of a particular item as compared to the reference corpus.
There are many different types of English corpora (see Chapter 1 this volume) but the most widely used corpus in lexicography is the large monolingual reference corpus.
In a nutshell, bigram types are first obtained from the corpora of translated and non-translated texts used for the study; they are then matched with frequency and Mutual Information data obtained from a reference corpus of English, and ranked according to these data.
The intervening function words are used to constrain searches but are not retained in the subsequent phases: only the association between lexical words is tested against the reference corpus.
While the reference corpus did offer sub-categorization of the fiction component of the corpus, aligning those sub-categories with Hemingway's oeuvre is not a straightforward task.
Typically, a reference corpus is larger than or similar in size to the corpus of interest so as to provide a large enough amount of evidence about word frequencies (see question 2 below).
Generally speaking, the larger and the more similar the reference corpus is to the corpus of interest the more reliable and focused the comparison is.
We should also consider which words would get highlighted as keywords had we chosen a different reference corpus.
Let's also assume that AmE06 is our corpus of interest (C) and BE06 is the reference corpus (R).
However, as we have seen, the keyword list is a result of multiple decisions in the process starting with the selection of the reference corpus and finishing with the choice of the particular statistic.
The outcomes of the keyword procedure are influenced by three crucial parameters: (i) the selection of the reference corpus, (ii) implementation of minimum frequency cut-off points and (iii) the choice of the statistical measure.
When extracting keywords for one of the newspapers, the comments of the readers from the other newspaper acted as a reference corpus in order to highlight words specific to the Guardian or Daily Mail readership.
In this section, we will discuss a script that computes different measures of keyness for words occurring in a target corpus by comparing their frequencies in that target corpus to that of a reference corpus and quantifying the relative attraction of each word to the target corpus by means of a keyness statistic.
A key items analysis is performed as follows: first, frequency lists are created for two texts or corpora -either two corpora which we wish to contrast, or a single text of interest and a generic reference corpus.
Since it can be used to produce reference materials, it is sometimes called a reference corpus.
One sub-type is a reference corpus: a general corpus taken to be a standard reference for corpus-based research on a given language.
The resulting sorted list contains two kinds of keyword: positive (those which are unusually frequent in the target corpus relative to the reference corpus) and negative (words which are unusually infrequent in the target corpus).
Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specific research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identification of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results.
Incorporating text dispersion into keyword analyses.
However, this approach was not scalable to the increasingly vast web and such directories have now largely been replaced by keyword-based search engines like Google.
There are various ways of processing (e.g., data analytics, concordance, collocation, keyword marking, local-word-grouping, lexical clustering, lemmatization, morphological processing, sentence processing, and named entity marking), which are applied to a corpus with appropriate technological supports.
The presence of proper nouns in the keyword lists is very frequent because these words often specifically refer to a particular person, which is not used equally in different corpora.
In a nutshell, the keyword list of a corpus is very useful to identify its main topics, provided that the comparison with the reference corpus is appropriate.
If we compare the reports of university students with a year of the Le Monde newspaper rather than the rest of their academic work, the keyword list includes √©l√®ves, activit√©, formation, r√©flexivit√©, √©criture, √©valuation, r√©sum√©, pens√©e, enseignants, savoir, etc., because the topics covered vary more widely between the two corpora than between the different types of university work.
More interesting is the fact that would is a keyword for the Liberal Democrats; this is because their manifesto 10.2 Case studies 10 Text mentions hypothetical events more frequently, which Rayson takes to mean that they did not expect to win the election.
As we described in the previous chapter, after logging into the latest version of COCA (currently 2023), you have different choices, namely, choose "list", which will provide you with concordance lines, "chart" which will provide a chart of the frequency of your selected keyword in different registers, or "word" which will give you a plethora of information about the word you are searching (e.g., collocations, synonyms, clusters, related words, and much more).
The keyword that we are searching for here and now is "say".
The most common form to display a keyword in context (KWIC) is through concordance lines.
Each time you run the search, even with the same keyword (by choosing the button "KWIC" in the top left corner of COCA), it will display a different set of randomly selected examples.
In this case, since you are more interested in how your keyword is used in different registers, you would want 100 sample concordance lines randomly selected from spoken discourse, and 100 concordance lines randomly selected from academic prose.
Words used by different characters in classic literary works have been a very popular topic for keyword analyses.
Before we start, please watch the tutorial on how to use the keyword feature in AntConc: www.youtube.com/watch?v=SludW4FHatI&list=PL iRIDpYmiC0SjJeT2FuysOkLa45HG_tIu&index=10.
As we have discussed earlier in this chapter, in the KWIC section, you could then see your keyword in a textual context.
The purpose of your search for keywords could be that you analyze a) what kinds of words surround your keyword, b) how they are positioned in the sentence or in the phrase, or c) what the frequencies are of your keywords.
Report on the overall frequency of the keyword "way", its normed frequency, and its distributional patterns across all seven registers (as well as their use through time) noted in COCA -namely, spoken discourse, fiction, magazine, TV shows and movies, newspaper, and academic prose.
Among those analyses are: KWIC (keyword in context), n-grams, collocates in a particular text, and word lists.
Without access to tagged corpora, advanced programming and statistical knowledge, your corpusdriven research will be limited to focusing on word lists, keyword analyses, and n-grams.
The term "keyword" has considerable currency outside corpus linguistics, but there it is usually understood in a different sense.
The problem is that in a list of keyword results, mixing frequent items with very infrequent items often means mixing generalized phenomena with phenomena that are extremely localized, making an account of the keyword list problematic (see the following subsection for a statistical technique designed to reduce this problem).
In addition to charting the history and development of keyword research (in Section 1), our discussions have been designed to emphasize that key lexical items should be used as a guide for what to analyze qualitatively, and not considered the end product in themselves.
We have emphasized (particularly in Section 1.5) that the usefulness of key items, and the quality of analyses and conclusions based upon them, relies on careful and explicit manipulation of the keyword tools settings as well as interpretation.
Several corpus techniques, for example, keyword and key-cluster tools, have the specific aim of facilitating comparison.
In a keyword list comparing WH-Obama with the one-million-word spoken section of the BNC Sampler (a collection of diverse discourse types) the following items all appeared among the top 200 keywords: continue (as in continue our efforts, continue to work on . . .), forward (move the economy forward, as we go forward to create an America that . . .), action, progress, effort/efforts, measures, steps, commitment, decision/decisions.
In the first part of this study outlining forced priming keyword and key-cluster analyses were conducted contrasting WH-Obama with both the BNC and WH-Bush.
The KWIC (keyword in context) concordances show the narrow linguistic co-text and provide perhaps the easiest and most useful way of getting acquainted with the material.
One example from Cameron's writing is the significantly above-average use of is; which was the fifth most frequent keyword in her corpus.
The differences seen in the rates of samurai in named entities potentially also reflect some cultural differences and the interests of the writers represented in the corpus: the named entities including samurai in the GB and US sections included more references to drama films (The Seven Samurai, The Last Samurai), the Asian sections featured references to food-related items, computer software (Market Samurai, a keyword research tool), action toy figures (Samurai Predator AC-01).
This is because every two corpora are different in a number of aspects which get highlighted to different degrees in the keyword procedure.
In practice, every keyword (or lockword) identified should be related back to what we know about the composition of the two corpora (C and R) and the multiple sources of difference between them.
In fact, we can select multiple reference corpora and carry out the keyword procedure with each of them and compare the results.
The question is how to deal with these words in the keyword procedure.
In practice, some corpus tools let us set the minimum cut-off limits for the frequencies of words in C and R before considering them in the keyword procedure.
However, as we have seen, the keyword list is a result of multiple decisions in the process starting with the selection of the reference corpus and finishing with the choice of the particular statistic.
If you have chosen your favourite keyword list for American English, you might be interested in knowing which procedure was used to identify these keywords.
The outcomes of the keyword procedure are influenced by three crucial parameters: (i) the selection of the reference corpus, (ii) implementation of minimum frequency cut-off points and (iii) the choice of the statistical measure.
In Chapter 3 (see Sections 3.2 and 3.4), we have seen contingency tables that were constructed for collocation and keyword analyses to show all possibilities (contingencies) of word (co)occurrence.
This kind of display is called keyword in context or KWIC.
AntConc uses a different concept of 'word' from BNCweb, based on the token definitions you've chosen, and may thus throw an error when it encounters something that doesn't fit this definition, aborting the keyword list comparison.
As Exercise 63 will have shown you, the keyword list, at least in our case and for positive keywords, may not necessarily provide you with more information than a frequency list that has been filtered well through the use of stopwords.
Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specific research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identification of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results.
This might help us answer the perennial question, 'How big should my corpus be?' and help researchers determine comparability and the relative sizes of sub-corpora defined by metadata such as socio-linguistic variables.
Not only is it difficult to approximate the full range of synchronic variability for a language at a given point in time, there is a further difficulty in doing so without compromising diachronic comparability.
A completely different response to the issue of comparability is to create singlegenre diachronic corpora that cover relatively short time spans and draw their data from a single historical source.
The single-genre approach has the advantage of improving diachronic comparability, but comes at the expense of coverage, both with respect to synchronic variability and time-depth.
Moreover, although diachronic comparability improves, it may still not be optimal, because genres themselves tend to be moving targets.
As a matter of fact, issues related to multilingual annotation (e.g. whether it should be languagespecific or language-neutral, or, more generally, how cross-linguistic comparability can be achieved) have received relatively little attention in contrastive linguistics and translation studies (one notable exception is Neumann 2013).
Buysse (2020) compared the oral productions of French-speaking and Dutch-speaking learners of English in the LINDSEI corpus with the productions of native English speakers in the LOCNEC corpus, a corpus which was compiled to be comparable with the LINSDEI (see Chapter 4 for a definition of the concept of comparability).
In summary, in addition to being based on corpora with high comparability, contrastive studies should use neutral points of comparison that make it possible to establish comparisons between linguistic phenomena across languages, which are as relevant and adequate as possible.
Such semi-structured elicitation techniques may also be used where a phenomenon is frequent enough in a typical corpus, but where the researcher wants to vary certain aspects systematically, or where the researcher wants to achieve comparability across speakers or even across languages.
These matters include attempting to achieve comparability and representativeness, two important but sometimes mutually exclusive goals.
In this section, we discuss how two of the central desiderata in corpus linguistics apply to historical registers: representativeness and comparability.
This, however, means that another desideratum of corpus composition must be considered, viz. comparability.
If the period coverage of a study is considerable and a great deal of societal or politico-cultural change has affected language users during that time, it is likely that registers will have gained new features and conventions, developed into other registers, been replaced by new registers, or fallen into oblivion; such shifts affect the comparability of period samples.
Moreover, potentially conflicting desiderata such as comparability and representativeness make it necessary for scholars to consider carefully the make-up of their corpora and the extent to which results based on a selection of registers can be generalized to the language as a whole.
Thus, corpus comparability needs to be addressed in a critical evaluation of ICE for the study of World Englishes.
Section 3 focuses on some pitfalls related to big corpora, and our examples in this section concern both the reliability of the semi-automated sampling of such resources and the comparability of the research results when new genres are introduced to the corpus.
Of course, the issue of genre balance also affects smaller diachronic corpora like the Helsinki Corpus, because some genres may disappear and new ones appear over time, and compromises must be made in the compilation process in terms of representativeness vs. diachronic comparability.
Thus, the importance resides in the comparability of the design of the two corpora from which the lists were culled rather than the "quality " or impact of the texts in the corpora themselves.
This brings us to the crucial feature of UDs, namely their cross-linguistic comparability.
This can be done by calculating frequency, variance, vmr, and t fid f values for each column of the data matrix, sorting each set of values in descending order of magnitude, z-standardizing for comparability, and then co-plotting.
Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specific research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identification of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results.
So the lemmatisation does not always group elements together; in some cases it can draw distinctions between elements that are superficially identical.
For instance, lemmatisation allows frequency lists to be compiled at the level of the dictionary headword, which may subsume many distinct word-forms.
This is referred to as lemmatisation (c.f. also Section 8.1.8, where we looked at lemma queries in BNCweb), and many programs that produce frequency lists offer this kind of facility.
A similar thing can be done by expanding abbreviations to their full form, but with both lemmatisation and expansion, one always has to bear in mind that the individual forms may potentially also cause a change in meaning of the whole textual unit in which they occur, or may have been used deliberately as a (stylistic) feature of a particular type of genre.
Now, however, this is considered one of the standard five methods of the field, alongside frequency, concordance, n-gram and collocation.
In the former, for example, arguing strongly and argued strongly would both count as cases of the collocation argue strongly.
In the latter, these counts would be kept separate; ‚Ä¢ to account for possible effects of the evenness of dispersion of a collocation within the corpus, each item was also quantified with a DP value (see Chap. 5).
First and overall, frequency and association data were found to be reliable predictors of learners' knowledge of collocation.
For these foreign language learners, it seems that the number of times a collocation occurs is a far more important factor than the strength of association between components.
Finally, the dispersion of a collocation across a reference corpus had only a weak relationship with knowledge (more widely spread collocations were better recognized), and this relationship was significant only in the BNC.
Many quantitative corpus analyses are based on concordance data (though not necessarily all: one could think of, for example, a study that is based on frequency or collocation lists instead, see Chaps.
Corpus, concordance, collocation.
This may include new insights into collocation of multimodal units of meaning across interactions; acquisition of speech-gesture units; and insights into frequencies of specific multimodal units in different contexts.
There are various ways of processing (e.g., data analytics, concordance, collocation, keyword marking, local-word-grouping, lexical clustering, lemmatization, morphological processing, sentence processing, and named entity marking), which are applied to a corpus with appropriate technological supports.
By contrast, it is more rarely found in collocation with words indicating a role, as in he played an important role in his failure, in texts destined for children than in texts for adults.
The proper nouns associated with chalet in the corpus show one of the limitations of the collocation analysis.
On the one hand, this definition subsumes the previous two definitions: If we assume that corpus linguistics is essentially the study of the distribution of linguistic phenomena in a linguistic corpus, we immediately understand the central role of the techniques described above: (i) KWIC concordances are a way of displaying the distribution of an expression across different syntagmatic contexts; (ii) collocation tables summarize the distribution of lexical items with respect to other lexical items in quantitative terms, and (iii) frequency lists summarize the overall quantitative distribution of lexical items in a given corpus.
This chapter is dedicated entirely to the discussion of collocation.
There are only limited possibilities of collocation with preceding adjectives, among which the commonest are silly, obstinate, stupid, awful, occasionally egregious.
Any given item thus enters into a range of collocation, the items with which it is collocated being ranged from more to less probable...
Often, we will be interested in the distribution of a word across two specific conditions -in the case of collocation, the distribution across the immediate contexts of two semantically related words.
This has a number of methodological consequences concerning the practicability, the statistical evaluation and the epistemological status of collocation research.
Anyone who decides to become involved in collocation research (or some of the large-scale lexical research areas described in the next chapter), should get acquainted at least with the simple options of automatizing statistical testing offered by spreadsheet applications.
As noted in Section 6.6.1, this is an extremely conservative correction that might make it quite difficult for any given collocation to reach significance.
Of course, the question is how important the role of ùëù-values is in a design where our main aim is to identify collocates and order them in terms of their collocation strength.
Thus, it is probably a good idea to formulate at least some general expectations before doing a large-scale collocation analysis.
Measures of collocation strength differ with respect to the data needed to calcuate them, their computational intensiveness and, crucially, the quality of their results.
As long as we are deriving all our collocation data from the same corpus, this will not make a difference, since the table total will always be the same.
We will use G through much of the remainder of this book whenever dealing with collocations or collocation-like phenomena.
In the following, we will look at some typical examples of collocation research, i.e. cases where both variables consist of (some part of) the lexicon and the values are individual words.
This research is not usually interested in any particular collocation (or set of collocations), or in genuinely linguistic research questions; instead, the focus is on methods (ways of preprocessing corpora, which association measures to use, etc.).
The perspective of these studies tends to be descriptive, often with the aim of showing the usefulness of collocation research for some application area.
This definition has been understood by collocation researchers in two different (but related) ways.
In collocation research, a word (or other element of linguistic structure) typically stands for itself -the aim of the researcher is to uncover the linguistic properties of a word (or set of words).
This case study shows how collocation research may uncover facts that go well beyond lexical semantics or semantic prosody.
In addition to information on collocation and frequency, a corpus will also allow us to examine the extent to which certain types of prescriptive rules are followed.
The final method in the corpus toolbox is collocation.
Unlike the previous four methods, where some minor operational differences that exist in tokenization for frequency lists, concordances, keywords, and n-grams could produce slightly different results in different tools, the collocation method itself is less tightly defined.
Further typical options include whether to consider punctuation as a boundary to collocation window spans or impose minimum frequencies on collocates or node words.
For instance, a concordance can be produced for a certain part-of-speech tag, a frequency list of lemmas, key semantic tags, and calculate collocation statistics for which semantic tags relate to a given word.
In particular, spelling variation causes problems for POS tagging, concordancing, keywords, n-grams, and collocation techniques.
Following this brief introduction, Section 2 explores the state of the art in collocation research, on the basis of which Section 3 presents a cross-linguistic study of the collocational behavior and semantic prosodies of a group of near synonyms in English and Chinese.
The z-score test is a measure which adjusts for the general frequencies of the words involved in a potential collocation and shows how much more frequent the collocation of a word with the node word is than one would expect from their general frequencies.
The first relates to the collocation-via-significance approach.
Second, corpus-based collocation studies have so far focused on, or indeed have largely been confined to, the English language.
There has been little work done on collocation and semantic prosody in languages other than English.
As such, the case study to be presented in the section that follows will explore collocation and semantic prosody in two genetically distant languages, English and Chinese in this case, from a cross-linguistic perspective rather than in a monolingual context.
Before these questions are answered, it is appropriate to introduce the corpora and data analysis method used in this study (Section 3.1), which is followed by a discussion of the collocation and semantic prosodies of the chosen group of near synonyms in English (Section 3.2) and a contrastive analysis of the Chinese group (Section 3.3).
As intuition is usually an unreliable guide to patterns of collocation and semantic prosody, this study takes a corpus-based approach to addressing these research questions.
It is of interest to note that negative appears on the collocation list of result.
It is also important to note that unlike English, in which different forms of a lemma may have different collocates and semantic prosodies (e.g. consequence vs. consequences), Chinese does not have a rich morphology which can affect collocation and semantic prosody in this way.
This chapter has sought to provide a critical account of the current debates in corpus-based collocation research.
The review in this section demonstrates that corpus linguistics has enabled large-scale collocation analysis and foregrounded collocation in linguistic research while corpus-based collocation studies over the past decades have uncovered a range of interesting collocational behavior and semantic prosody which have been hidden from intuition and can only be revealed by examining a large amount of attested data simultaneously.
To demonstrate the kind of research called for, the second main section in this chapter (Section 3) presents a contrastive study of collocation and semantic prosody in English and Chinese, via a case study of a group of near synonyms denoting consequence in the two languages, which suggests that, in spite of some language-specific peculiarities, even genetically distant languages such as English and Chinese display similar collocational behavior and semantic prosody in their use of near synonyms.
In other words, just as collocation is a by-product of the existence of units of meaning, so patterns are a byproduct of frequently occurring semantic sequences.
However, in common with other fields, corpus pragmatics investigates the co-textual patterns of a linguistic item or items, which encompasses lexico-grammatical features such as collocation or semantic prosody.
Items identified from either of these starting points then provide the basis for investigation through collocation and comparisons to see how particular academics and disciplinary communities used these features to express social identities.
One example of this is Hyland and Tse's (2012) study of bios and how collocation allows us to see differences in the ways that senior academics and graduate students refer to themselves in the bios accompanying research articles.
An index analysis collects vital information about each word in the corpus (e.g. dispersion, collocation, and so on), and is necessary if one wishes to run an analysis of recurrent word strings.
There is very little agreement in the collocates recorded in the three collocation dictionaries: only 3 percent of the total number of collocates listed are found in all three dictionaries, and 82 percent appear in only one of the three dictionaries.
The Oxford Collocations Dictionary for Students of English is the only one of the three collocation dictionaries which is corpus-based.
In OALD8 and CALD3, there is no collocation box for verbs of evidence: a limited number of collocations and phraseological units are highlighted in bold in example sentences.
For example, the collocation box under Sense 4 of the verb support ("to show that an idea, statement, theory etc. is true or correct") lists eight abstract nouns that are frequently used as objects of the verb in academic texts: argument, claim, conclusion, contention, hypothesis, idea, theory, and view.
In terms of quantity, for example, the use of corpus-derived collocation boxes needs to be systematized.
So, when we examine the adverb modifiers of adjectives, we know that the collocation scores probably exaggerate the association between the adverb absolutely and the adjective fabulous, as the scores are skewed by the frequent occurrences of the combination of those two words in names.
An outline of collocation and the measurements used to strengthen assumptions will be made from the collocations.
To help with this research collocation is used.
In the second case study, we propose to use interactive visualisation techniques to improve the interpretation and exploration of the collocation method in corpus linguistics.
From this graph a user can select groups of data to compare against within the key word clouds, collocation networks and social network relationships, and how each of these aspects varies over time.
With the CONE and GraphColl prototypes, we have proposed and illustrated a highly dynamic way of exploring collocation networks, as an example of our wish to add dynamic elements to both existing and novel visualisations.
Collocates are words that co-occur with the node in a specifically defined span around the node, which we call the collocation window.
In this case, the span (collocation window) is one word to the left and one word to the right, sometimes abbreviated to 1L, 1R.
In the example given above, within a 1L, 1R collocation window around each occurrence of the node, we can observe the words that co-occur with love, namely, in the order of appearance, my (three times), is (twice), in (once), am (once), will (twice), thee (twice), only (once), and (once) and thou (once).
Some collocation measures such as MI highlight rare exclusivity of the collocational relationship, favouring collocates which occur almost exclusively in the company of the node, even though this may be only once or twice in the entire corpus.
Exclusivity refers to a specific aspect of the collocation relationship where words occur only or predominantly in each other's company.
Collocation graphs and networks build on the idea of collocation introduced in Section 3.2.
A collocation graph is a visual representation of the collocational relationship between a node and its collocates.
In Chapter 3 (see Sections 3.2 and 3.4), we have seen contingency tables that were constructed for collocation and keyword analyses to show all possibilities (contingencies) of word (co)occurrence.
These students often ask me what the best statistical test is to use with corpora, what the best collocation measure is etc.
The basis for this provocative question stemmed from research findings highlighting considerable cross-disciplinary variation in terms of lexical distribution, collocation, and even meaning.
Let us look at one example from the case study chapter below, the collocation of alphabetical order.
We begin with a very simple example: We want to determine the collocation strength of the collocation alphabetical order in the BNC but, for now, in a very simplistic way.
Co-occurrence of words within a short span (i.e. 'collocation') is a traditional concept in Corpus Linguistics, as noted at the beginning of this paper.
It has been widely accepted with Corpus Linguistics that collocation needs to be measured within a fairly short span: + / -5 words from the node is common.
In the same way that key items analysis is based on, and abstracts away from, two frequency lists, so a collocation analysis is based on, and abstracts away from, a concordance search.
The idea that collocation is of high or indeed primary importance to understanding the meaning of a word or other linguistic form originates with J.R. Firth and is very important in the neo-Firthian school of corpus linguistics; but collocation analysis itself is a commonplace of most or all approaches to corpus linguistics.
Second, as with key items, a rigorous collocation analysis will always go beyond the raw list of statistical collocates to look at actual instances of those collocates in usage alongside the node.
For such rank measures, a collocation x y is explored by -computing all AMs for collocations with x, ranking them, and noting the rank for x y; -computing all AMs for collocations with y, ranking them, and noting the rank for x y; -comparing the difference in ranks.
While this sounds promising, the computational effort that goes into these calculations is immense, since the computation of one AM for the collocation x y requires the computation of all AMs for all collocations with x and then separately for all collocations with y.
This shows a very strong likelihood for woman to be preceded by a, meaning the collocation of these words is highly predictable.
In general, as the name collocation implies, we're here dealing with a phenomenon that describes which words tend to occur in proximity (co + location) to one another because they have some kind of 'affinity' to, or 'affiliation' with, one another.
Make sure that you have the collocation measure in the 'Collocates Preferences' set to 'MI' initially and that the 'Sort by Stat' option is selected.
Perhaps the only major limitation here, though, is that the BYU interface only provides a single collocation score measure, which is MI.
Here, sadly, the designers of the architecture have introduced a serious flaw in the system that may well affect the overall calculations of the collocation statistics very strongly, which is to treat punctuation tokens (and their types) as equivalent to words.
As we've seen before, this makes a lot of sense because it not only allows us to distinguish features on different linguistic levels more easily, actually making them countable, but also to possibly exclude some parts of the data from our specific analyses, for instance by ensuring that we don't perform n-gram/collocation analyses across syntactic boundaries.
These are just some of the issues we need to constantly be aware of when we use such tools, so the idea that 'bigger is better', even if it is indeed often important to work with very large amounts of data for such research as collocation analysis in order to be able to find rarer combinations, may not always be fully justified if the quantity of data isn't equally matched by quality.
However, with the advent of electronic corpora, the speed and systematicity with which diachronic -like synchronic -records can be queried has increased tremendously, opening up new research possibilities by dramatically facilitating at least some aspects of data collection.
At the same time, barring corpora of very recent history, diachronic data will always present us with important problems, failing to represent all the variation associated with the users and contexts of use, with especially spoken language as the perennial gap.
In the most general terms, our plea here is one for informed use of diachronic resources.
Apart from that, diachronic corpora differ widely in size, composition, scope, annotation, and the nature of their textual material.
Specific requirements of diachronic research simply need to be met in different ways.
Nevertheless, there are recurrent challenges that both compilers and end users of diachronic corpora have to confront.
In addition to being organized along the temporal dimension, diachronic corpora often include information on other lectal and diatypic properties of the texts they contain.
Though limited to recent periods, another way to counter the biases in the historical record is of course the compilation of diachronic corpora containing actual audio-recorded speech (see Chap. 11 for more information about spoken corpora).
Not only is it difficult to approximate the full range of synchronic variability for a language at a given point in time, there is a further difficulty in doing so without compromising diachronic comparability.
However, even despite efforts to preserve genre balance, any diachronic comparison across this timespan has to take into account that the language used in such a corpus changes from a relatively standardized written language with more or less direct roots in a contemporary vernacular to a functionally impoverished and much more writing-dependent language used mainly in religious contexts and as a lingua franca among European scholars and scientists.
A completely different response to the issue of comparability is to create singlegenre diachronic corpora that cover relatively short time spans and draw their data from a single historical source.
The single-genre approach has the advantage of improving diachronic comparability, but comes at the expense of coverage, both with respect to synchronic variability and time-depth.
Moreover, although diachronic comparability improves, it may still not be optimal, because genres themselves tend to be moving targets.
In a diachronic corpus of narrative fiction this inevitably affects the frequencies of specific grammatical patterns associated with either descriptive or more action-driven narrative passages.
Turning from the level of the texts that make up a corpus to the internal properties of those texts, perhaps the most fundamental question compilers and users of diachronic corpora must ask is to what extent they can rely on methods devised for the annotation and analysis of contemporary data in handling data from older periods. Older texts are in principle somewhat alien.
From the design properties of corpora and their texts, we move to the actual use of diachronic corpora for research.
This holds a fortiori for the complex interrogation of diachronic corpora.
In this section, we will discuss a number of heuristic techniques that can be used to interrogate the syntagmatic-paradigmatic organization across the whole lexicogrammatical continuum, and we will illustrate their potential relevance to diachronic corpus linguistics.
Firstly, predicative and existential matrices are distinguished from each other by the different diachronic realization of the position syntactically enclitic with the finite matrix verb.
By regular addition of synchronic data, a corpus attains a diachronic dimension.
However, there are also diachronic or historical corpora, which contain texts from earlier periods of English.
With diachronic corpora (i.e. corpora used to study historical periods of English), the time frame for texts is somewhat easier to determine, since the various historical periods of English are fairly well-defined.
Contemporary corpora are used for studying language in a synchronic way, that is, at a given moment during its evolution, whereas historical corpora make it possible to carry out studies from a diachronic point of view, that is, on the evolution of language.
Finally, we have seen that corpora can be very diverse in nature, depending on whether they are made up once and for all or incremental, general or specialized, annotated or not, monolingual or multilingual, synchronous or diachronic.
Finally, this corpus should contain a synchronic type of data, corresponding to the current pronunciation, rather than to its diachronic evolution.
These corpora must also contain diachronic information, for example regarding the evolution of pronunciation from the second half of the 20th Century to the present day.
In addition, diachronic corpora only offer a list of certain textual genres, such as literature and legal texts, which also limits the generalization of the conclusions that can be drawn from them.
The Base du fran√ßais m√©di√©val (Guillot-Barbance et al. 2017) offers access to different diachronic corpora.
The second part of the book presents a range of case studies from the domains of lexicology, grammar, text linguistics and metaphor, including variationist and diachronic perspectives.
The case study also demonstrates the importance of corpora in diachronic research, a field of study which, as mentioned in Chapter 1 has always relied on citations drawn from authentic texts, but which can profit from querying large collections of such texts and quantifying the results.
Let us further limit the category to verbs first documented before the 19th century, in order to leave a clear diachronic gap between the established types and the productive types.
This part begins with chapters on the corpus-based description of spoken English, written academic English, and patterns of variation (synchronic and diachronic) among a wider range of spoken and written registers.
Its wide temporal coverage and large scope of lexical types make the OED an ideal basis for studies that investigate diachronic type frequency changes in phenomena such as the way-construction.
However, with regard to diachronic change, the analysis shows that inanimate recipients, as in The herbs gave the soup a nice flavor, have become more acceptable in the ditransitive construction in the twentieth century.
Wolk et al.'s study exemplifies how diachronic corpus studies can precisely document changes in grammatical structure and simultaneously address issues of speakers' knowledge of language.
In a second step, the resulting curve of changing productivity is used to divide the development into diachronic stages.
The third analytical step addresses this question with a quantitative analysis that compares formations with -ment across the diachronic stages with regard to several structural and semantic variables.
We have discussed diachronic comparisons, but the parameters and entities to be compared can be various.
The present chapter discusses the concept of register from a diachronic perspective.
In Section 2, an initial survey demonstrates the importance of the register factor in historical corpus linguistics and introduces a number of central matters that arise when the concept of register is applied to diachronic material.
The register parameter can be taken into account in two ways in diachronic investigations.
A synchronic or diachronic corpus is representative to the extent that the study of that corpus can replace the study of the textual universe it represents.
Even registers with a great deal of diachronic stability, such as religious writing, are subject to change in this regard.
In this section, we will discuss a number of studies that represent prominent aspects of diachronic research from a register perspective.
Walker's study is a good example of a diachronic corpus-based investigation where the author needs to consider the special nature of his or her data and carefully assess the reliability of the data sources used.
The first part of this chapter reviews the growing body of research that analyzes dialect variation in corpora, including research on variation across nations, regions, genders, ages, and classes, in both speech and writing, and from both a synchronic and diachronic perspective, with a focus on dialect variation in the English language.
Due to the history of the project, certain limitations apply with respect to the diachronic bias inherent in individual components and across regional varieties.
Moon's analysis is particularly enlightening in that it offers a diachronic perspective to current lexicographic practice and places emphasis on "the function of phraseological information in relation to the needs and interests of the target users" (2008b: 333).
When Professor Matti Rissanen wrote a short article entitled "Three problems connected with the use of diachronic corpora" for the ICAME Journal in 1989, the landscape of corpus linguistics looked quite different from today.
At the time, Rissanen was leading a group of scholars responsible for the compilation of the first diachronic corpus of English, the Helsinki Corpus of English Texts.
We consequently decided to investigate the frequency of sentence-initial as well in COHA, a corpus that provided us with enough material to examine its diachronic development in recent American English.
Of course, the issue of genre balance also affects smaller diachronic corpora like the Helsinki Corpus, because some genres may disappear and new ones appear over time, and compromises must be made in the compilation process in terms of representativeness vs. diachronic comparability.
Indeed, we wish to emphasize that resources like COHA are invaluable for diachronic linguistic research, and that these resources are even more useful when one knows exactly how they are constructed and annotated.
First, we removed the content posted before 2016 in order to limit the likelihood of picking up diachronic effects; the tweets in the original corpus date back to 2006.
Our framework splits along three orthogonal dimensions: linguistic (lexical, grammar/syntax, semantics), structural (to permit sub-corpora) and temporal (for diachronic corpora).
The final dimension incorporated into our proposed framework is time which will assist with the exploration and visualisation of diachronic corpora.
A study that involves time as a variable is called a diachronic or longitudinal study.
In addition, when considering diachronic representativeness, we need to deal with certain limitations inherent in historical data.
If there is not enough evidence, or the evidence is biased towards, for example, a certain genre or certain types of speakers/writers (see 'diachronic representativeness' above), the patterns we observe are ambiguous.
Yet for other diachronic processes, more complex models (complex curves) are more appropriate.
Third, the fluctuation of the meaning of linguistic forms (diachronic polysemy) is a phenomenon that needs to be carefully considered when looking at the development of language; the same linguistic form often changes meaning (or its set of meanings) over time, so in diachronic analyses we also need to provide an account of the semantic development, not only an overview of changing frequencies of linguistic forms.
They offer an opportunity to look inside each diachronic sampling point and analyse variation between individual texts in the historical period.
Effective visualization using line charts, boxplots, error bars, sparklines, candlestick plots etc. is a useful starting point for any diachronic investigation.
In addition, when dealing with diachronic comparisons, we need to assess whether the observed differences in frequencies are related to change in the discourse/language over time or whether these are related to other sources of variation.
Then, there is a difference between diachronic corpora and synchronic corpora.
In diachronic research, scholars may focus on the specific usage of a word or a structure.
Also, the effects of demographic information on the use of specific linguistic constructions could be explored with the help of a corpus designed as a historical or diachronic corpus.
A historical or diachronic corpus is a collection of texts from different periods.
The Contemporary Historical Corpus of American English (COHA) is an example of a diachronic corpus.
A crucial characteristic of a diachronic corpus is that the texts it contains are comparable across periods.
Even the diachronic ones are, because they've not been designed to be added to later, even if, for example, at some point in time a further Old English epic may somehow be unearthed and could thus theoretically be included in a new edition of the Helsinki Corpus.
However, with the advent of electronic corpora, the speed and systematicity with which diachronic -like synchronic -records can be queried has increased tremendously, opening up new research possibilities by dramatically facilitating at least some aspects of data collection.
Not only is it difficult to approximate the full range of synchronic variability for a language at a given point in time, there is a further difficulty in doing so without compromising diachronic comparability.
This approach prioritizes coverage of synchronic variability, to the best level achievable and with no prior assumptions made.
The single-genre approach has the advantage of improving diachronic comparability, but comes at the expense of coverage, both with respect to synchronic variability and time-depth.
It is also argued that the observed tendencies -such as short-before-long and animate referents firstare in line with synchronic corpus-based and experimental findings about general cognitive principles underlying the framework of probabilistic grammar.
By regular addition of synchronic data, a corpus attains a diachronic dimension.
Contemporary corpora are used for studying language in a synchronic way, that is, at a given moment during its evolution, whereas historical corpora make it possible to carry out studies from a diachronic point of view, that is, on the evolution of language.
Finally, this corpus should contain a synchronic type of data, corresponding to the current pronunciation, rather than to its diachronic evolution.
It should be a synchronic corpus, corresponding to current uses of the language.
This part begins with chapters on the corpus-based description of spoken English, written academic English, and patterns of variation (synchronic and diachronic) among a wider range of spoken and written registers.
LGSWE provides the basis for the selection of linguistic features, and it also gives the point of comparison as the synchronic patterns of expressing stance, in various registers of Present-day English (conversation, fiction, news reports, and academic writing) are established in it.
A synchronic or diachronic corpus is representative to the extent that the study of that corpus can replace the study of the textual universe it represents.
The first part of this chapter reviews the growing body of research that analyzes dialect variation in corpora, including research on variation across nations, regions, genders, ages, and classes, in both speech and writing, and from both a synchronic and diachronic perspective, with a focus on dialect variation in the English language.
Learner corpora can be of different types, including general or specific, written or spoken, synchronic or longitudinal, mono-L1 or multi-L1 data, which can be produced by learners of different origins and different proficiency levels.
Examples for synchronic English data include the Open American National Corpus for spoken and written American English, as well as the ENCOW corpus for World Englishes as used on the web.
Then, there is a difference between diachronic corpora and synchronic corpora.
The Brown family corpora would all be synchronic when considered individually.
The fact that this colligation with its changing realization (that, it) occurs in both predicative and existential matrices suggests that the so-called it-extraposition construction is part of a larger class of evolving complementation constructions, even though, because of the different matrix syntax, reference to the complement is obligatory in predicative and optional in existential matrices.
Therefore, essentially, all the cases of collocations identified through the tscore stat that I've just discussed, strictly speaking, represent cases of colligation.
In other words, what we're really looking into here is a form of colligation, which, however, is probably a little too finegrained for most purposes.
Adding new annotations to a spoken corpus, however, can still constitute a major challenge for the simple lack of suitable tools although great advances have been made in the last decade in terms of the interoperability of the major tools in use for spoken corpus construction: ELAN, Praat, EXMARaLDA and ANVIL (see Sect. 11.4 below) now all have import and export functions for their respective file formats so that it is possible to add new annotations with one of these tools to a spoken corpus that was compiled with another tool (for this Transformer by Oliver Ehmer can also be used).
There are still some artifacts of corpus construction: the codes F and J are used in BROWN to indicate that letter combinations and formulae have been removed.
Corpus building not only allows you to answer a specific research question, but it also gives you experience in corpus construction.
As a result, the handbook includes relatively little discussion of topics that have been fully covered in existing textbooks, such as surveys of existing corpora, or methodological discussions of corpus construction and analysis.
This chapter presents an introductory survey of computational tools and methods for corpus construction and analysis.
However, corpus researchers need an understanding of the issues that arise in corpus construction even if they never build any corpora of their own.
The very fact that the corpus is a sample means that we need to think carefully about how our sample relates to the population, which means we need a detailed and critical awareness of the process of sampling that has been used in corpus construction.
Third, in the process of corpus construction we have to think about the relationship between the population and the sample in a somewhat different way to how this is done in fields where sampling can be done on a random basis.
For instance, raw video data encoded in one format such as MPEG1 will have to be regularly updated to new encoding schemes.
First of all, the 1 in Construction 1+Lemma+Givenness is R's way of encoding the fact that an intercept is part of the model.
Dummy coding is a way of encoding a categorical variable as a R. Sch√§fer distributed around 0.
SEMANTICS could be a factor encoding the semantic classes which nest individual lemmas.
It could also be a (crossed) grouping factor completely unrelated to the lemmas, for example encoding some semantic property of the whole sentence containing the construction.
Development of a speech corpus involves issues and factors like purpose of use, selection of speakers, choice of settings, size of a corpus, use of recording instruments, manner of data sampling, manner of data elicitation, nature of transcription, types of data encoding, management of audio files, editing of input data, processing of spoken texts, annotation of speech, analysis of speech corpus, etc.
Since a corpus contains text samples either from a written or spoken discourse, annotation is done by adopting a value-encoding process, which attaches interpretative information at different levels of a text.
This involves not only collecting data (speech and writing) but encoding it: transcribing recorded speech, for instance, as well as adding annotation to it, such as markup indicating in a conversation when one person's speech overlaps another speaker's, and in writing where such features as paragraph boundaries occur in written texts.
One way to answer this question is to examine the guidelines established by the Text Encoding Initiative (TEI) for the encoding of electronic texts, including linguistic corpora (www.tei-c.org/ release/doc/tei-p5-doc/en/html/CC.html).
Even though the processes of collecting and encoding data for inclusion in a corpus are described above as separate processes, in many senses they are closely connected: After a conversation is recorded, for instance, it may prove more efficient to transcribe it immediately, since whoever made the recording will be available either to transcribe the conversation or to answer questions about the recording to aid in its transcription.
An important element to take into account when downloading a corpus from the Internet is its encoding format (this not to be confused with the compression format, when applicable, that needs to be handled by means of specific archiving tools).
Before opening one or more files for them to be processed with AntConc, we have to make sure that the encoding chosen in AntConc for reading the characters is suitable for reading the file correctly.
By default, AntConc uses UTF-8 encoding.
However, this encoding does not correspond to text files containing French characters, because of accented characters.
When the text of a file is not displayed correctly in AntConc, the encoding used can be changed to make it match the file's encoding, for example, the ISO 8859-1 format (Latin.1).
The header section contains metadata, a description of the file, the encoding, the text profile, mainly the language, the context or participants, and even a history of its revisions.
For example, discourse markers can be identified by looking for lexical elements encoding them as you know, I mean, etc.
XML can also be used for encoding the metadata of a corpus (see Chapter 6, section 6.4).
Different types of texts have different types of character encoding associated with them.
Most corpus tools require plain text versions with optional XML encoding, so where material is sourced in another form, some format conversions will be in order.
It does mean, however, that you would have a hard time finding corpora on paper, in the form of punch cards or digitally in HTML or Microsoft Word document formats; the probably most widely used format consists of text files with a Unicode UTF-8 encoding and XML annotation.
The final distinction I would like to mention at least briefly involves the encoding of the corpus files.
Given especially the predominance of work on English in corpus linguistics, until rather recently many corpora came in the so-called ASCII (American Standard Code for Information Interchange) character encoding, an encoding scheme that encodes 2 7 = 128 characters as numbers and that is largely based on the Western alphabet.
This is an important issue since corpus files these days will typically be in Unicode encoding, specifically UTF-8.
And with yet other files, even some that come with this book, the pattern might be the opposite, and sometimes it's not even clear what encoding a file comes with because that information is not always provided or obvious.
There is a function file, which establishes a connection to a file and allows you to specify an encoding argument, and that connection will then be read with a function called readLines, which does exactly what its name suggests.
The bad news for Windows users is that, with R for Windows at least, it is often not as straightforward to handle Unicode as it is with R for MacOS X or R for Linux, which is mainly due to the fact that Mac OS X and Linux are UTF-8 operating systems so they play well with input files that have the same encoding, which is one of many reasons (see Section 3.8.2 for another one) why I recommend doing corpus processing with R on a Linux machine (and it takes very little time to install a long-term support version of, for instance, Kubuntu or Linux Mint on a desktop computer).
However, since this is an important issue, I would like to encourage you to not only get more familiar with this kind of encoding, but also do Exercise box 3.8 now.
One is that we're now loading a file with Windows's version of Unicode UCS-2LE and so it is particularly useful to load this file with readLines(file(...)) and the encoding argument set to "UCS-2LE".
Part of the reason for this is that, as a practical matter, both can be inserted into the corpus using the same encoding system of tags inserted into the running text.
For languages with other scripts, for example, Cyrillic scripts in many languages of Eastern Europe and Central Asia or various scripts of East Asian languages (Mandarin, Japanese, etc.) corpus builders will either need to use encoding such as Unicode (cf. 5.11) or add a layer of transliteration to the corpus text.
This means you have to make sure that when working with different software or on different machines the text and letters that look the same really are the same in terms of their encoding.
If encoding does not match, you will potentially not find relevant text.
Needless to say, corpus linguistic studies of this sort cannot explain why languages opt for one or the other type of subject and object encoding, and the explanation for this lies in deep and intricate histories of language evolution with many stages of development, each a story worth telling in itself.
The particular flavour we'll want to use for this course is a variable byte encoding called UTF-8, which stores characters in up to six bytes, but generally has the advantage that the basic Latin characters appear at exactly the same positions as in the simple encoding discussed above, so that opening an English text in UTF-8 would essentially be the same as opening it in ASCII or Latin1.
Header elements may for instance be the page title (contained in the <title>‚Ä¶</title> tag, where the forward slash indicates the end of the tag in the closing part), which then appears in the title bar of your browser, or possibly meta-information (<meta>‚Ä¶</meta>), such as the text (content) type or encoding/character set (charset), as well as style (sheet) information/references (<style>‚Ä¶</style> or <link rel="stylesheet" href="./corpus.css" type="text/css" />, in our case) responsible for some of the page formatting.
When those pages, however, are then opened on a computer in another country, and which is set to a different code page, the result may look very similar to, or even worse than, the result we get when we set the exercise to any encoding that is different from UTF-8.
You may optionally also be able to specify an encoding, and if this is available, you should definitely select 'UTF-8 ' here.
What exactly you may need to remove from your data depends on the specific formatting or encoding conventions used for the document.
The latter, strangely, are still part of the text representation of the XML version of BNCweb, despite the fact that they can easily be encoded in UTF-8, which is, after all, the assumed default encoding for XML.
Add the line <?xml version="1.0" encoding="UTF-8" ?> at the very beginning of the file.
The line we just inserted identifies the document as being in XML for any application that's capable of displaying it, at the same time specifying its encoding as UTF-8.
If you need to change the encoding, though, where exactly this can be done, and also how you can see this, may vary.
In Notepad++, you can also specify the default encoding for any files you create under 'Settings‚ÜíPreferences‚ÜíNew Document', where I'd recommend you set the option for 'UTF-8 without BOM', as well as use the additional setting 'Apply to opened ANSI files'.
Corpus linguistics has long championed, and pioneered, markup schemes to permit the systematic encoding of metadata and interpretative analyses within corpora.
For this encoding, each PDV symbol was assigned a unique fourdigit code.
Using this encoding scheme, a PDV-level transcription of any given speaker interview audio file is a sequence of fourdigit codes, and a State-level transcription is a sequence of five-digit codes.
Based on a comparison of our value (32.69) with the distribution, we can make a judgement about the statistical significance (pvalue) of the result.
A p-value smaller than 0.05 is conventionally considered to indicate statistical significance.
It overestimates statistical significance (type I error) in cases where there is no real difference and the variation is purely due to chance; the type I error rates (false hits) ranged from 48 to 99% depending on the linguistic feature investigated.
If the global null hypothesis cannot be rejected at a certain level of statistical significance Œ± (by default, 0.05), no further splits are made.
It is widely recognized that quantitative research failing to reach statistical significance (e.g., p > .05) is less likely to be accepted for publication.
While it is generally accepted that any level below .05 indicates statistical significance, it is quite common for more stringent significance levels to be employed (e.g. p‚â¶:001).
As a related point, statistical significance has nothing to do with the quality of our data.
If we have chosen unrepresentative data or if we have extracted or annotated our data sloppily, the statistical significance of the results is meaningless.
Second, statistical significance has nothing to do with theoretical relevance.
Put simply, if we have no theoretical model in which the results can be interpreted meaningfully, statistical significance does not add to our understanding of the object of research.
We now treat the ùúí 2 component as a ùúí 2 value in its own right, checking it for statistical significance in the same way as the overall ùúí 2 value.
This is well below the level required to claim statistical significance.
As pointed out in connection with the comparison of the TTRs for mini-in the FLOB and the FROWN corpus, we would like to be able to test differences between two (or more) TTRs (and, of course, also two or more HTRs) for statistical significance.
Quantitative frequencies and statistical significance of the differences found were computed to check whether there was a match in the proportional patterns of different qualities for each feature -which was interpreted as a sign of a universal tendency.
The last point to discuss in this section is the appropriate test for statistical significance that can be used with cross-tabulation; a statistical significance test evaluates the amount of evidence against the null hypothesis (see Section 1.3).
The statistical significance of a correlation is directly related to the number of observations (cases).
It is important to remember that even if a result is significantly unlikely to be obtained by chance, it is still possible to be randomly obtained, and that statistical significance in a model does not directly translate to real-world importance.
Below, we will use it to analyze a corpus based measure, namely the type-token ratio (TTR) of different texts that have the same overall length.
For mini-, the type-token ratio is much higher: it occurs in 382 different words, so its TTR is 382 /1702 = 0.2244.
We will refer to this measure as the hapax-token ratio (or HTR) by analogy with the term type-token ratio.
Instead, we need to look at the type-token ratio and the hapax-token ratio.
In terms of how the information is conveyed, we see differences in the type-token ratio.
For example, a frame occurring 200 times with 25 distinct fillers would have a type-token ratio of .
Like its name suggests, the type-token ratio is the ratio of the number of different words in a text (types) to the number of all words in the text (tokens).
For instance, note how the rightmost value is 7, i.e., the type-token ratio of the whole vector (0.7) times the length of the vector (10).
After that we store the results we obtain by sapplying functions to the list to count lengths and compute type-token ratios (with an anonymous/inline function); we compute a mean type-token ratio using all types and tokens, and a mean of all lengths of utterances with mean, but to avoid the effect that outliers might have we use the argument trim=0.05 to discard both the smallest and the largest 5 percent of the data.
We use lines to add the type-token ratio plot and add smoothers with lines(lowess(...)).
The general argument is that texts produced by native language users should get priority over texts produced by nonnative users in a general and monitor corpus.
On the other hand, in case of a monitor corpus, which tries to represent all possible text varieties, we may include all non-standard and ordinary text samples along with standard and established text samples.
A heterogeneous monitor corpus includes text samples from all subject areas and the individuality of the source of a text and its writer are well attested.
Thus, a monitor corpus becomes a worthy representation of texts that are ever produced in a language.
Diversity is a useful safeguard for a monitor corpus against skewed representation.
The first distinction we can make among all the existing corpora is the one that classifies them into a sample corpus and a monitor corpus.
One monitor corpus that allows us to see changes in spoken corpora and also provides more current spoken data is COCA.
A monitor corpus is a corpus allowing researchers to track current changes synchronically in a particular language.
On top of that, being a monitor corpus, it also makes it possible to track potential changes across different periods in the same way.
In addition, sample corpus analyses are included to illustrate the many theoretical and practical applications that linguistic corpora have.
The first distinction we can make among all the existing corpora is the one that classifies them into a sample corpus and a monitor corpus.
This issue, too, can be illustrated with the help of the same sample corpus from Hiltunen (2021), Corpus A, which comprises complete issues of the four newspapers sampled at 10-year intervals.
This ensures the extracted sample corpus is reasonably clean and can therefore be expected to provide reliable output for standard corpus linguistic tasks.
A sampling frame is determined by identifying a specific population that one wishes to make generalizations about.
The idea behind this sampling frame was to ensure that the corpus contained samples of speech representing the ways that people speak in different regions of the country and in different contexts.
Therefore, geographic region becomes a relevant trait for the sampling frame of a spoken French corpus.
As previously mentioned, a better method might be to define a sampling frame and to divide the samples to be collected depending on the important properties of such a frame.
For example, if the sampling frame for a corpus of French spoken in Switzerland includes different criteria such as gender, age, socio-economic level or place of residence, an equivalent number of samples should be chosen to match each selected criterion.
These categories are called the sampling frame.
When selecting the sampling frame we are often predisposed to see as salient the text types that have traditionally been included in language corpora such as those in the Brown family sampling frame (see ‚Ä¢ Legal considerations bias: corpus designers often face the problem of copy- right; this is especially the case when corpus creators want to share their corpora with other researchers.
However, this does not necessarily mean that the text must be excluded from the corpus, since there is annotation that can be included in a corpus indicating that certain sections of a sample are "extra-corpus" material; that is, material not considered part of the corpus for purposes of word counts, generating KWIC (key word in context), and so forth.
Concordancing programs retrieve structures and present them in KWIC (key word in context) concordances.
For instance, the opening of the section in ECL1 on "Structural Markup" makes reference to ASCII (American Standard Code for Information Interchange) text formats and standard generalized markup language (SGML).
And once you've reached a more advanced level in your research, you might also want to consider using a 'proper' markup language, such as HTML or XML, for annotating and categorising your data.
This first official type of markup language, however, as we'll see further below, was relatively complex and also had a number of serious drawbacks.
After Tim Berners-Lee had invented the World Wide Web (WWW) in 1989, it was necessary to develop a new, and simpler, markup language in order to take full advantage of the new hypertext medium.
On the one hand, there are many advantages to letting learners use corpora by themselves, for example, by teaching them to search for word occurrences using a concordancer.
The main advantage of downloadable corpora is their great flexibility for carrying out word or structure searches using a concordancer (see section 5.6).
For example, the AntConc concordancer that we discuss can only process text format files (or files with XML or HTML tags which can also be treated as text files).
We will then focus on the freely available AntConc concordancer, and introduce some of its functionalities.
Above all, a concordancer is a tool that makes it possible to look up words in their context of use.
For instance, in the Litt√©racie avanc√©e corpus described above, a search for the French word avis (opinion) by means of a concordancer indicates that the students have used this word 61 times.
More specifically, the concordancer determines which of the words are used significantly more (or even less used in the case of negative keywords) in the search corpus compared to the reference corpus.
This concordancer is compatible with the various current operating systems (Linux, MacOS or Windows).
When corpora can be downloaded, a concordancer should be used in order to explore them systematically.
But in this case, a version without misspellings should also be included so that the words can be found by a concordancer.
No matter the way of acquiring data, an important point is to save the corpus files into a format which can then be used by a concordancer.
In the case of corpus studies, this is not a problem, however, since the files will not be read by humans, but processed by a concordancer.
In its recent versions, the WordSmith concordancer also offers a similar function.
For this information to be made available for future users of the corpus without separating it from the rest of each sample portion, it should be possible to include it in the files, but in such a way that it is not taken into account by a concordancer when counting the words of the corpus.
A possible solution could be to insert these marking elements inside tags, something that the concordancer will be able to ignore.
In the AntConc concordancer, discussed in Chapter 5, it is possible to inform the program about the existence of tags and not to consider the information they contain.
Within the transcripts, it might be useful to include an annotation of the non-standard words used, for example, in verlan, with their equivalent standard so that they can be found in a concordancer.
In most cases, identification must be done on raw data, sowhenever possible -it is important to find one or more lexical elements that can be automatically identified by a concordancer.
Once annotated, the elements of the corpus can be searched by their POS tags by means of a concordancer.
For example, we can retrieve all the discourse markers using a concordancer, and then annotate their function in a file.
It is still common practice, for instance, to first retrieve data representing a particular linguistic phenomenon from an electronic corpus (e.g. by means of a concordancer tool or query script) and subsequently manually categorize the collected examples into different functional-semantic groups (e.g. animate/inanimate; literal/figurative; agent/patient/instrument/ ‚Ä¶).
This is obviously useful if you are using one of the systems mentioned above, but if not, the transparency of the syntax should allow you to translate the query into whatever possibilities your concordancer offers you.
The concordancer is an excellent way of locating examples of such prosodic clash.
On the other hand, if you have a ready-made concordancer, you click a few buttons (and enter a search term) to get the job done.
For our present purposes, it suffices to distinguish two broad varieties of concordancer that are used today.
In some cases, an analyst may not have much choice in whether to use a desktop concordancer or a client-server system: the choice is determined by what data they wish to use.
In contrast, a line-based concordancer, such as the one built into my Simple Corpus Tool (downloadable from martinweisser.org), will only extract and display the immediate context found on the same line, plus a number of surrounding lines specified.
The other thing is that our expectations concerning the results may differ from the actual output of the concordancer, as you will hopefully have noticed while concordancing on the words this and in.
Thus, generally the concordancer will not be able to 'understand' that you may be looking for a word with a particular meaning if there are homographs or polysemous forms, as in the case of round, which may be classified as an adjective, an adverb, a noun, a verb, or preposition.
However, as the examples from the exercise will hopefully have shown you, if we use a concordancer to investigate enough data, we're bound to find examples that may run counter to our imagination, and also help us deepen our knowledge/understanding of how words are used, which is one of the reasons why concordances are not only useful for research in lexicology or lexicography, but also as a means for native and non-native speakers to develop their language skills.
Thus, to be able to work more efficiently with a concordancer or similar search tool, it would be very useful to be able to specify more complex patterns that we could then look for all at once, and also make use of the other useful options we've explored before, such as sorting, etc., to help us simplify our analysis procedures.
As groupings normally capture, and we don't want whatever is supposed to be constraining our grouping to become part of the match, and thus be highlighted by the concordancer, these constructs actually use a special syntax that excludes them from being captured, which is that the opening bracket of the grouping parentheses is immediately followed by a question mark, i.e. (?‚Ä¶).
Unfortunately, though, as useful as AntConc is for most purposes, since it's a stream-based concordancer it'll ignore all line breaks and match more than we want to see in our concordance, so using it here is not an option.
A much simpler way to search the data, though, is to use a line-based concordancer, such as my own Simple Corpus Tool, where you can both look at the whole file easily once it's been loaded and also devise a suitable regex that will match two occurrences of the tag used for prepositions occurring in a row at the end of the line.
And, of course, you could always also use smaller corpora (or parts of the BNC itself) and investigate these in a concordancer like AntConc, where the sorting options make things easier for you.
In one sense, the distinction between corpus-driven and corpus-based research methods can be misleading.
At least with respect to a register functional approach, any feature that is identified through a corpus-driven approach will still merit closer scrutiny and analysis in the corpus.
As we have discussed many times throughout the book, one can follow a) a corpus-driven or b) a corpus-based (often called corpus-informed) approach to linguistic analysis.
While it is relatively easy to carry out lexical studies with corpus-driven approaches (whether you rely on existing corpora or analyze your own corpus), as available tools allow you to extract lexical patterns from the texts, it is quite difficult to apply corpus-driven approaches to do a full lexico-grammatical analysis of texts.
You can continue to look for lexico-grammatical patterns that you find interesting, or you can carry out corpus-based studies that rely on the results of previous, corpus-driven studies.
Unlike the corpus-driven approaches illustrated in this book, more advanced corpus-driven register studies have identified co-occurring linguistic features that have emerged through corpus analyses.
Research of this typereferred to as a "corpus-driven" approach -identifies strong tendencies for words and grammatical constructions to pattern together in particular ways, while other theoretically possible combinations rarely occur.
Sections 2.1 and 2.2 contrast studies based on their major methodological approaches, distinguishing between corpus-based studies of pre-selected lexical expressions versus corpus-driven studies to identify the full set of important multi-word sequences in a corpus.
The main distinguishing characteristic of corpus-driven studies of phraseological patterns is that they do not begin with a pre-selected list of theoretically interesting words or phrases.
One of the earliest corpus-driven studies of this type was Salem's (1987) analysis of repeated lexical phrases in a corpus of French government resolutions.
In the late 1990s, corpus-driven studies of recurrent lexical phrases in English registers began to appear.
As a result, corpus-driven studies of lexical phrases (both continuous and discontinuous) have shown that lexical patterning is ubiquitous in English, and basic to the discourse structure of both spoken and written texts.
These methodological considerations are generally disregarded in corpus-driven studies of phraseology.
This fully corpus-driven approach is rather resource-intensive, yet is theoretically important because it makes it possible to account for frequent discontinuous sequences of words that are not associated with a moderately frequent lexical bundle.
Because of the computational resources required to process and store all potential discontinuous sequences (frames) in large corpora (c. 10 million words in total) from a strictly corpus-driven approach, we used a MySQL database to store all possible four-word sequences in the two subcorpora (specifically all four-word sequences that did not cross (a) punctuation boundaries in the written corpus, and (b) turn boundaries in the spoken corpus).
The fully corpus-driven approach employed in this case study shows that some of the gaps in previous findings were simply an artifact of the corpusbased methodology at the first stage of the research.
In our case study, we illustrated how this distinction is not absolute: some previous studies of discontinuous lexical frames (like Biber 2009 and Ro ¬®mer 2010) are intermediate along this continuum, beginning with a corpus-driven approach to identify a set of continuous lexical sequences, but carrying out the corpus-based investigation of only those sequences, to analyze the extent to which they occur as discontinuous frames.
In contrast, the exploratory investigation that we present in our case study employs a corpus-driven approach to directly identify the important discontinuous frames in speech and writing.
However, these frames can be identified through direct corpus-driven analysis.
By thus linking description to theory, Gries argues that his corpus-driven study not only describes, but explains and predicts the way the choice between option (5a) and option (5b) is made.
It also offers a starting point for corpus-driven investigations of academic corpora by generating list of items which can be further explored in more detail using concordance analyses.
The study is a good example of a fully corpus-driven analysis which allows generalizations to emerge bottom-up from the learner data.
The PDEV is an ongoing corpus-driven project in which a procedure called Corpus Pattern Analysis (CPA) is applied to identify the various patterns in which a verb is used and then discover how exactly meanings arise from each of the patterns.
Linguists normally look askance at 'bag of words' studies, but I do think they offer interesting new ways of carrying out 'corpus-driven' research.
The literature draws a distinction between corpus-driven and corpusbased linguistics, where the former is taken to be a scientific paradigm in the sense that behaviourist and generative linguistics are paradigms, that is, ontologies in terms of which linguistic theories can be stated, and the latter to be a methodology, that is, a collection of techniques for the formulation and testing of hypotheses within some already-existing paradigm.
The significance test calculates the significance of the difference in frequency between the word in the target data and in the reference corpus.
The last point to discuss in this section is the appropriate test for statistical significance that can be used with cross-tabulation; a statistical significance test evaluates the amount of evidence against the null hypothesis (see Section 1.3).
