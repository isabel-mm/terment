List of Figures

Introduction

Linguistics is a science, and should therefore use scientific methodology. The currently dominant methodology is the hypothetico-deductive one associated in the philosophy of science with Karl

1. Some aspect of the natural world, that is, a domain of interest, is selected for study, and a research question that will substantially further scientific knowledge of the domain is posed. 2. A hypothesis that answers the research question is stated. 3. The hypothesis is tested by observation of the domain. If it is incompatible with observation the hypothesis must either be emended to make it compatible or, if this is not possible, must be abandoned. If it is compatible then the hypothesis is said to be supported but not proven; no scientific hypothesis is ever proven because it is always open to falsification by new evidence from observation. On this model, the science of the selected aspect of the domain of interest at any given time is a collection of hypotheses that are valid with respect to observations of the domain made up to that time, or, in other words, a collection of best guesses about what that aspect of the natural world is like.

Because falsifiable hypotheses are central in science, it is natural to ask how they are generated. The consensus in philosophy of science is that hypothesis generation is non-algorithmic, that is, not reducible to a formula, but is rather driven by human intellectual creativity in response to a research question

In linguistics the research domain is text, where 'text' is understood generically as output from the human language faculty. In branches of linguistics concerned with chronological, geographical, and social language variation, text takes the form of collections of spoken and / or written language, or corpora. In those concerned with the structure of the language faculty, and in generative linguistics more specifically, the role of text is less obvious but no less fundamental: native speaker grammaticality judgment has historically been used as the basis for hypothesis generation and confirmation instead of evidence derived from corpora, but such judgment is based on example sentences generated by the researcher's language faculty, not on direct access to the language faculty within the human cognitive system itself, and therefore on text in the above sense. The use of corpora in Western linguistics began in the late eighteenth century with the postulation of an Indo-European protolanguage and its reconstruction based on examination of numerous living languages and of historical written documents; after two centuries of development the importance of corpora in linguistics research has increased to the extent that a subdiscipline has come into being, corpus linguistics, whose remit is to develop methodologies for creating collections of natural language speech and text, abstracting data from them, and analysing that data with the aim of generating or testing hypotheses about the structure of language and its use in the world.

Throughout the nineteenth and much of the twentieth centuries corpus linguistics was exclusively paper-based. The collections at the root of the discipline were in the form of hand-written or printed documents, and research using such collections involved reading through the documents, often repeatedly, creating data by noting features of interest on some paper medium such as index cards, inspecting the data directly, and on the basis of that inspection drawing conclusions that were published in printed books or journals. The advent of digital electronic technology in the second half of the twentieth century and its evolution since then have increasingly rendered this traditional methodology obsolete. On the one hand, the possibility of representing language electronically rather than as visual marks on paper, together with the development of electronic media, infrastructure, and computational tools for creation, emendation, storage, and transmission of electronic text have led to a rapid increase in the number and size of corpora available to the linguist, and these are now at or beyond the limit of what an individual researcher can efficiently use in the traditional way. On the other, data abstracted from very large corpora can themselves be so extensive and complex as to be impenetrable to understanding by direct inspection. Digital electronic technology has been a boon the corpus linguistics community, but in linguistics, as in life, it's possible to have too much of a good thing.

One response to digital electronic text and data overload is to use only corpora of tractable size or, equivalently, subsets of large corpora, but simply ignoring available information is not scientifically respectable. The alternative is to look to related research disciplines for help. The overload in corpus linguistics is symptomatic of a more general trend. Daily use of digital electronic information technology by many millions of people worldwide in their professional and personal lives has generated and continues to generate truly vast amounts of electronic speech and text, and abstraction of information from all but a tiny part of it by direct inspection is an intractable task not only for individuals but also in government and commerce -what, for example, are the prospects for finding a specific item of information by reading sequentially through the huge number of documents currently available on the World Wide Web? In response, research disciplines devoted to information abstraction from very large collections of electronic text have come into being. These go under a variety of names such as informatics, information science, information retrieval, text summarization, data mining, and natural language processing. They overlap to greater or lesser degrees but share an essentially identical remit: to make interpretation of large collections of digital text tractable. To achieve this they draw on concepts and methods from a range of other disciplines including mathematics, statistics, computer science, and artificial intelligence.

An increasingly important class of these concepts and methods is cluster analysis, which is used across a broad range of sciences for hypothesis generation based on identification of structure in data which are too large or complex, or both, to be interpretable by direct inspection. The aim of the present discussion is to show how cluster analysis can be used for corpusbased hypothesis generation in linguistics by applying it to a case study of a dialect corpus, the Diachronic Electronic Corpus of Tyneside English , and thereby to contribute to the development of an empirically-based quantitative methodology for hypothesis generation in the linguistics community.

This aim is realized by presenting the relevant material so as to make it accessible to that community. The implication is that accessibility is a problem, and in the author's view it is, for several reasons.

-The number of available clustering methods is so large that even specifically dedicated surveys of them are selective, more are continually being proposed, and the associated technical literature is correspondingly extensive and growing. Selection of the method or methods appropriate to any given research application requires engagement with this literature, and for the non-specialist the magnitude of the task can be a substantial obstacle to informed use. The present discussion addresses this obstacle by selectivity. There is no prospect of being able to cover all or even very many of the available clustering methods at a level of description sufficient to convey understanding, and as such no attempt is made at comprehensiveness; surveys exist already, and no obvious purpose would be served by a précis of them here. What is offered instead are detailed descriptions of a relatively small selection of methods which are likely to be of most use to corpus linguists, where usefulness is judged on criteria of intuitive accessibility, theoretically and empirically demonstrated effectiveness, and availability of software implementations for practical application. These detailed descriptions are supplemented by references to variations on and alternatives to the methods in question, thereby providing the reader with pointers to the broader range of methods in which the selected ones are embedded. Needless to say, there is a subjective element in this, and a different writer might well have made different choices.

-Understanding of the nature, creation, representation, and properties of data is fundamental to successful cluster analysis. The clustering literature typically assumes familiarity with these topics, however, and consequently tends to deal with them in relatively cursory fashion; anyone lacking the requisite familiarity must acquire it by engaging with the relevant and also very extensive mathematical, statistical, and data processing literatures. To forestall the need for this, the account of the selected cluster analytical methods that follows is preceded by a detailed discussion of data issues. -Cluster analysis and data processing are based on concepts from mathematics, statistics, and computer science, and discussions of them in the above-mentioned literatures are, in general, quite technical. This can be a serious obstacle in that, though it has become less pronounced, the arts / science divide is still with us, and many professional linguists have little or no background in and sometimes even an antipathy to mathematics and statistics; for further discussion of this assertion see Chapter 4 below. Understanding of these concepts and associated formalisms is, however, a prerequisite for informed application of cluster analysis, and so introductory-level explanations of them are provided.

No a priori knowledge of them is assumed, and all are explained before any use is made of them. They are, moreover, introduced in the course of discussion as they are needed, so that the requisite knowledge is built up gradually. This approach itself presents a problem. Some linguists are as mathematically and statistically sophisticated as any in the traditional 'hard' sciences, and to such readers intuitive explanations can seem tedious. The choice, therefore, is between puzzling some readers and probably putting them off the discussion entirely, and boring others. There is no obvious solution. In my experience more linguists need the explanations than not; every effort is made to avoid longwindedness, but where there is a choice between brevity and intuitive clarity, the latter wins every time. The discussion is in six main chapters. The first chapter is the present Introduction. The second chapter motivates the use of of cluster analysis for hypothesis generation in linguistics. The third deals with data creation: the nature of data, its abstraction from text corpora, its representation in a mathematical format suitable for cluster analysis, and transformation of that representation so as to optimize its interpretability. The fourth chapter describes a range of cluster analysis methods and exemplifies their application to data created in Chapter 3. The fifth shows how these methods can serve as the basis for generation of linguistic hypotheses, and the sixth reviews existing applications of cluster analysis in corpus linguistics. In addition, there is an Appendix that identifies software implementations which make the clustering methods described in Chapter 4 available for practical application. Exemplification throughout the discussion is based on data abstracted from the Diachronic Electronic Corpus of Tyneside English .

This book is one of a series entitled Quantitative Linguistics, whose remit is to cover 'all aspects of quantitative methods and models in linguistics, text analysis, and related research fields', and more specifically 'the whole spectrum of theoretical and empirical research, ultimately striving for an exact mathematical formulation and empirical testing of hypotheses: observation and description of linguistic data, application of methods and models, discussion of methodological and epistemological issues, modelling of language and text phenomena'. In addressing methodological issues in linguistic hypothesis generation using mathematically-based data creation and cluster analysis methods, the author's belief is that it satisfies this remit.

As noted in the Introduction, cluster analysis is a tool for hypothesis generation. It identifies structure latent in data, and awareness of such structure can be used to draw the inferences on the basis of which a hypothesis is formulated. To see how this works, let us assume that the domain of interest is a speech community and that one wants to understand the relationship between phonetic usage and social structure within it; for concreteness, that community will be assumed to be Tyneside in north-east England, shown in Figure

Tyneside

Is there systematic phonetic variation in the Tyneside speech community, and, if so, does that variation correlate systematically with social factors?

To answer the question, a representative sample of Tyneside speech is collected and relevant data is abstracted from it. The sample used here is the Diachronic Electronic Corpus of Tyneside English (henceforth DECTE), a collection of interviews with Tyneside English speakers that will be fully described in due course.

A group of 24 speakers is selected at random and a set of phonetic variables descriptive of Tyneside pronunciation is defined. The number of times each speaker uses the phonetic variable or variables of interest is recorded, thereby building up a body of data. To start, each speaker's speech is described by a single variable, the phonetic segment @ 1 ; the labels in the Speaker column of Table

As noted in the Introduction, cluster analysis is a family of computational methods for identification and graphical display of structure in data when the data are too large either in terms of the number of variables or of the number of objects described, or both, to be readily interpretable by direct inspection. All the members of the family work by partitioning a set of objects in the domain of interest into disjoint subsets in accordance with how relatively similar those objects are in terms of the variables that describe them. The objects of interest in Tables 2.1, 2.2, and 2.3 are speakers, and each speaker's phonetic usage is described by a set of variables. Any two speakers' usage will be more or less similar depending on how similar their respective variable values are: if the values are identical then so are the speakers in terms of their usage, and the greater the divergence in values the greater the differences. Cluster analysis of the data in Table

This and other varieties of cluster analysis are described in detail later in the discussion; the aim at this stage is to give an initial impression of how they can be used in linguistic analysis. Figure

Once the structure of the data has been identified by the above procedure it can be used for generation of a hypothesis in response to the research question.

-Is there systematic phonetic variation in the Tyneside speech community?

Since the relative lengths of the branches joining subclusters represents their relative similarity, the speakers included in the analysis can be seen to fall into two main clusters, labeled A and B in the tree, such that the speakers in cluster A are relatively much more similar to one another than any of them are to speakers in cluster B, and vice versa.

A reasonable hypothesis based on this finding would be that there is systematic phonetic variation in the Tyneside speech community, and more specifically that the speakers who constitute that community fall into two main groups. -Does that variation correlate systematically with social factors?

DECTE includes a range of social information for each speaker, such as age, gender, educational level, occupation, and so on. Also included is an indication of whether the speaker comes from Newcastle on the north shore of the river Tyne or Gateshead on the south side; correlating place of residence with the cluster tree in Figure

This hypothesis can be refined on the one hand by correlating the internal structures of clusters A and B with a larger number of social factors, and on the other by identifying the phonetic segments which are most important as determinants of the cluster structure. The former is analogous to what has already been described and does not need to be made explicit at this stage, though subsequent discussion will do so. One approach to the latter is to create summary descriptions of the phonetic characteristics of the two main clusters A and B and then to compare them. This is done by taking the mean of variable values for the speakers in each cluster, as in Table

All the speakers whom the cluster tree assigns to A are collected in the cluster A sub-table of Table

Cluster analysis can be applied to hypothesis generation in any research where the data consists of objects described by variables; since most research uses data of this kind, it is very widely applicable. It can usefully be applied where the number of objects and variables is so large that the data cannot easily be interpreted by direct inspection, as above. The foregoing discussion has sketched one sort of application to linguistic analysis; a few other random possibilities are, briefly:

-A historical linguist might want to infer phonetic or phonological structure in a legacy corpus on the basis of spelling by cluster analyzing alphabetic n-grams for different magnitudes 2, 3, 4 . . . of n. -A generative linguist might want to infer syntactic structures in a littleknown or endangered language by clustering lexical n-grams for different magnitudes of n. -A philologist might want to use cluster analysis of alphabetic n-grams to see if a collection of historical literary texts can be classified chronologically of geographically on the basis of their spelling. Further applications to linguistic analysis are given in Chapter 6, the literature review.

'Data' is the plural of 'datum', the past participle of Latin 'dare', to give, and means things that are given. A datum is therefore something to be accepted at face value, a true statement about the world. What is a true statement about the world? That question has been debated in philosophical metaphysics since Antiquity and probably before, and, in our own time, has been intensively studied by the disciplines that comprise cognitive science

Data are ontologically different from the world. The world is as it is; data are an interpretation of it for the purpose of scientific study. The weather is not the meteorologist's data -measurements of such things as air temperature are. A text corpus is not the linguist's data -measurements of such things as lexical frequency are. Data are constructed from observation of things in the world, and the process of construction raises a range of issues that determine the amenability of the data to analysis and the interpretability of the analytical results. The importance to cluster analysis of understanding such data issues can hardly be overstated

The discussion is in three main sections. The first section deals with data creation, the second presents a geometrical interpretation of data on which all subsequent discussion is based, and the third describes several ways of transforming data prior to cluster analysis in terms of that geometrical interpretation.

3.1

Data creation

Research domain

To state the obvious, data creation presupposes a research domain from which the data are to be abstracted. In corpus-based linguistics the research domain is some collection of natural language utterances. In the present case the domain is the English spoken in the Tyneside region of north-east England

The DECTE corpus

DECTE contains samples of the Tyneside English speech variety dating from the later 20th and the early 21st centuries collected from residents of Tyneside and surrounding areas of North-East England. The main locations in this area represented in the corpus are the cities of Newcastle upon Tyne on the north side of the river Tyne and Gatesheadon the south side, but its geographical reach is currently being extended to include speakers from other regions in the North-East such as County Durham, Northumberland, and Sunderland. It updates the existing Newcastle Electronic Corpus of Tyneside English (NECTE), which was created between 2000 and 2005 and consists of two pre-existing corpora of audio-recorded Tyneside speech

The earlier of the two components of NECTE , the Tyneside Linguistic Survey (TLS) , was created in the late 1960s

The other component of NECTE is the Phonological Variation and Change in Contemporary Spoken English (PVC) project

NECTE amalgamated the TLS and PVC materials into a single Text Encoding Initiative (TEI)-conformant XML-encoded corpus and made them available online in a variety of aligned formats: digitized audio, standard orthographic transcription, phonetic transcription, and part-of-speech tagged.

In 2011-12 the DECTE project combined NECTE with the NECTE2 corpus, which was begun in 2007 and is ongoing. NECTE2 consists of digitized audio recordings and orthographic transcriptions of dyadic interviews together with records of informant social details and other supplementary material, collected by undergraduate and postgraduate students and researchers at Newcastle University. The interviews record the language use of a variety of local informants from a range of social groups and extend the geographical domain covered in the earlier collections to include other parts of the North East of England. Successive cohorts of students add their own interviews to NECTE2. The components of DECTE and their interrelationship are shown schematically in Figure

The DECTE phonetic transcriptions

The main motivation for the TLS project was to see whether systematic phonetic variation among Tyneside speakers of the period could be interestingly correlated with variation in their social characteristics. To this end the project developed a methodology that was radical at the time and remains so today: in contrast to the then-universal and still-dominant theory driven approach, where social and linguistic factors are selected by the analyst on the basis of some combination of an independently-specified theoretical framework, existing case studies, and personal experience of the domain of inquiry, the TLS proposed a fundamentally empirical approach in which salient factors are extracted from the data itself and then serve as the basis for model construction.

To realize this research aim using its empirical methodology, the TLS had to compare the audio interviews it had collected at the phonetic level of representation. This required the analog speech signal to be discretized into phonetic segment sequences, or, in other words, to be phonetically transcribed. The TLS project wanted a very detailed phonetic transcription of its audio files, and the standard International Phonetic Alphabet scheme was not detailed enough. It therefore developed an extended version of the IPA scheme; the sample from the TLS encoding manual in Figure

There are four columns in the encoding table. The first three give symbols for increasingly fine-grained transcription, and the fourth examples of what speech sounds the symbols represent: the OU column lists phonological segments, the PDV ('Putative Diasystemic Variable') column lists IPA-level phonetic segments, and the State column lists the TLS's detailed elaboration of the IPA scheme.

The graphical symbols had to be numerically encoded for computational processing. For this encoding, each PDV symbol was assigned a unique fourdigit code. A fifth digit was then added to any given PDV code to give a five-digit State code. This fifth digit was the State symbol's position in the left-to-right sequence in the State column. For example, the PDV code for [i:] is 0002; the State code for, say, [ i -] is 00024, because [ i -] is fourth in the left-to-right State symbol sequence. Using this encoding scheme, a PDV-level transcription of any given speaker interview audio file is a sequence of fourdigit codes, and a State-level transcription is a sequence of five-digit codes. Reference to the DECTE transcriptions will henceforth be to the PDV level.

Associated with each of the 63 DECTE speakers is a file representing a transcription of about the first 10 minutes of the audio interview and containing a sequence of numerical State-level codes of which only the first four

PDV-level digits are used in what follows. A fragment of one of these is shown in Table

The 63 phonetic transcription files and the social data associated with them constitute the corpus from which the data used in the remainder of this discussion were abstracted. For ease of cross-reference, abbreviated versions of the DECTE naming conventions for speakers together with the phonetic symbols and corresponding numerical codes are used throughout.

A final note: earlier work

Research question

Any aspect of the world can be described in an arbitrary number of ways and to arbitrary degrees of precision. A desktop computer can, for example, be described in terms of its role in an the administrative structure of an organization, its physical appearance, its hardware components, the functionality of the software installed on it, the programs which implement that functionality, the design of the chips on the circuit board, or the atomic and subatomic characteristics of the transistors on the chips, not to speak of its connectivity to the internet or its social and economic impact on the world at large. Which description is best? That depends on why one wants the description. A software developer wants a clear definition of the required functionality but doesn't care about the details of chip design; the chip designer doesn't care about the physical appearance of the machines in which her devices are installed but a marketing manager does; an academic interested in the sociology of computers doesn't care about chip design either, or about circuit boards, or about programs. In general, how one describes a thing depends on what one wants to know about it, or, in other words, on the question one has asked.

The implications of this go straight to the heart of the debate on the nature of science and scientific theories in philosophy of science

In a scientific context, the question one has asked is the research question component of the hypothetico-deductive model outlined earlier. Given a domain of interest, how is a good research question formulated? That, of course, is the central question in science. Asking the right questions is what leads to scientific breakthroughs and makes reputations, and, beyond a thorough knowledge of the research area and possession of a creative intelligence, there is no known guaranteed route to the right questions. What is clear from the preceding paragraph, though, is that a well-defined question is the key precondition to the conduct of research, and more particularly to the creation of the data that will support hypothesis formulation. The research question provides an interpretative orientation; without such an orientation, how does one know what to observe in the domain, what is important, and what is not? A linguist's domain is natural language, but syntacticians want to know different things about it than semanticists, and they ask commensurately different questions. In the present case we will be interested in sociophonetics, and the research question is the one stated earlier:

Is there systematic phonetic variation in the Tyneside speech community, and, if so, does that variation correlate systematically with social variables?

Variable selection

Given that data are an interpretation of some domain of interest, what does such an interpretation look like? It is a description of objects in the domain in terms of variables. A variable is a symbol, that is, a physical entity to which a meaning is assigned by human interpreters; the physical shape A in the English spelling system means the phoneme /a/, for example, because all users of the system agree that it does. The variables chosen to describe a domain constitute the conceptual template in terms of which the domain is interpreted and on which the proposed analysis is based. If the analysis is to be valid with respect to the domain, therefore, it is crucial that the set of selected variables be adequate in relation to the research question, where adequacy is understood as follows:

-The variables should represent all and only those aspects of the domain which are relevant to the research question, that is, relevant aspects of the domain should not be unrepresented in the set of variables, and irrelevant aspects should not be represented. Failure to include relevant aspects in the data renders the description of the domain incomplete and thereby self-evidently compromises the validity of analysis based on it; inclusion of irrelevant aspects is less serious but introduces potentially confounding factors into an analysis. -Each variable should be independent of all the others in terms of what it represents in the domain, that is, the variables should not overlap with one another in what they describe in the domain because such overlap describes the same thing multiple times and can thereby skew the analysis by overemphasizing the importance of some aspects of the domain over others. In general, adequacy so defined cannot be guaranteed in any given research application because neither relevance nor independence is always obvious. Any domain can be described by an essentially arbitrary number of finite sets of variables, as the foregoing example of computer description makes clear. Selection of one particular set can only be done on the basis of personal knowledge of the domain and of the body of scientific theory associated with it, tempered by personal discretion. In other words, there is no algorithm for choosing an adequate set of variables.

The research question defined on DECTE involves phonetic analysis, and that implies phonetic transcription of the audio speaker interviews: a set of variables is defined each of which represents a characteristic of the speech signal taken to be phonetically significant, and these are then used to interpret the continuous signal as a sequence of discrete symbols. The standard way of doing this is to use the symbols defined by the International Phonetic Alphabet (IPA), but the TLS researchers felt that the IPA was too restrictive in the sense that it did not capture phonetic features which they considered to be of interest, and so they invented their own transcription scheme, described earlier. The remainder of this discussion refers to data abstracted from these TLS transcriptions, but it has to be understood that the 156 variables in that scheme are not necessarily optimal or even adequate relative to our research question. They only constitute one view of what is important in the phonetics of Tyneside speech. In fact, as we shall see, many of them have no particular relevance to the research question.

Variable value assignment

Once variables have been selected, a value is assigned to each of them for each of the objects of interest in the domain. This value assignment is what makes the link between the researcher's conceptualization of the domain in terms of the variables s/he has chosen and the actual state of the world, and allows the resulting data to be taken as a valid representation of the domain. The type of value assigned to any given variable depends on its meaning. The fundamental distinction of types is between quantitative, that is, numerical values, and qualitative ones such as binary 'yes / no' or categorial 'poor / adequate / good / excellent'

The objects of interest in DECTE are the 63 speakers, each of whom is described by the values for each of the 156 phonetic variables. What kind of value should be assigned? One possibility is to use qualitative binary ones: the value of any given variable is 'yes' if the speaker in question uses the corresponding phonetic segment in his or her interview, and 'no' if not. Another, and the one adopted here, is to use quantitative values which represent the number of times the speaker uses each of the phonetic segments.

Data representation

If they are to be analyzed using mathematically-based computational methods like cluster analysis, the descriptions of the entities in the domain of interest in terms of the selected variables must be mathematically represented. A widely used way of doing this in, for example, information retrieval

The most basic characteristic of data is that they be complete and accurate, where 'complete' means that all variables for all cases in the data have values associated with them, and 'accurate' that all values assigned to variables faithfully reflect the reality they represent. These are stringent requirements: most datasets large enough to have cluster analysis usefully applied to them probably contain error, known as 'noise', to greater or lesser degrees. Measurement error arises in numerous ways -tolerances in measuring instruments, human inaccuracy in the use of the instruments, corruption at one or more points in data transmission, and so on.

Because error in data distorts analytical results, it has to be eliminated as much as possible. This is a two-step process, the first step of which is to determine the amount and nature of the error, and the second to mitigate or remove it. Methods for error identification and correction are standardly discussed in statistics and multivariate analysis textbooks, for example by

The DECTE data are generated by counting the frequency of phonetic segments in interviews, so completeness and accuracy should not be issues if the survey is carefully done using a reliable procedure; manual counting of features in physical text by direct observation is in general far less accurate than the software equivalent for electronic text.

Data geometry

Data matrices have a geometrical interpretation, and the remainder of the discussion is based on it. This section first presents a few relevant mathematical and geometrical concepts and then shows how data can be represented and interpreted in terms of them.

Space

In colloquial usage, the word 'space' denotes a fundamental aspect of how humans understand their world: that we live our lives in a three-dimensional space, that there are directions in that space, that distances along those directions can be measured, that relative distances between and among objects in the space can be compared, that objects in the space themselves have size and shape which can be measured and described. The earliest geometries were attempts to define these intuitive notions of space, direction, distance, size, and shape in terms of abstract principles which could, on the one hand, be applied to scientific understanding of physical reality, and on the other to practical problems like construction and navigation. Basing their ideas on the first attempts in ancient Mesopotamia and Egypt, Greek philosophers from the sixth century BCE onwards developed such abstract principles systematically, and their work culminated in the geometrical system attributed to Euclid (floruit ca. 300 BCE), which remained the standard for more than two millennia thereafter

In the nineteenth century CE the validity of Euclidean geometry was questioned for the first time both intrinsically and as a description of physical reality. It was realized that the Euclidean was not the only possible geometry, and alternative ones were proposed in which, for example, there are no parallel lines and the angles inside a triangle always sum to less than 180 degrees. Since the nineteenth century these alternative geometries have continued to be developed without reference to their utility as descriptions of physical reality, and as part of this development 'space' has come to have an entirely abstract meaning which has nothing obvious to do with the one rooted in our intuitions about physical reality. A space under this construal is a mathematical set on which one or more mathematical structures are defined, and is thus a mathematical object rather than a humanly-perceived physical phenomenon

Cartesian product

Given two sets A and B, the Cartesian product

Vector space

If the mathematical structures of addition and scalar multiplication, that is, multiplication by a single number, are defined on the n-tuples of a Cartesian product X , then X together with these two structures is a vector space V subject to a range of conditions which are for present purposes assumed to apply

Given an n-dimensional vector space V , n vectors can be selected from the space to constitute a basis for it, and the set of these n vectors is so called because all other vectors in V can be generated from it using the operations of addition and scalar multiplication, as described below. Selection of basis vectors is constrained by certain conditions explained in any and every linear algebra textbook, but understanding of these constraints is unnecessary for present purposes because we shall be using orthogonal bases, and such bases automatically satisfy the conditions. Orthogonal basis vectors have the property that their inner product is zero; the inner product of n-dimensional vectors v and w, also called the dot product and written v.w, is defined in Equation (3.1).

v

that is, corresponding components of v and w are multiplied and all the products summed. For n = 2, the inner product of, say, v = [2.2, 3.5] and w = [1.9, 6.0] is (2.2 × 1.9) + (3.5 × 6.0) = 25.18, and so v and w are not orthogonal, but the inner product of v = [12.89, 0] and w = [0, 3.8] is 0, and in this case they are.

For orthogonal basis vectors v 1 , v 2 , . . . v n and scalars s 1 , s 2 , . . . s n , a linear combination of the v 1 . . . v n generates a new vector x of the same dimensionality, as given in Equation (3.2):

where, in multiplication of a vector by a scalar, each component of the vector is multiplied by the scalar, and in vector addition corresponding components are added. For example, take V to be based on a twofold Cartesian product A = R × R of the set of real numbers R. Select any two orthogonal vectors from V , say v 1 = [12.89, 0] and v 2 = [0, 3.8], adopting the convention that vectors are shown between square brackets and components are commaseparated. Table

Vector spaces have a geometrical interpretation. Under this interpretation, orthogonal vectors are perpendicular to one another, as in Figure

Vectors generated by linear combination of the basis vectors of an ndimensional space are conceptualized as coordinates of points in the space. Thus, for the linear combination of the basis vectors v =

Finally, note in the above examples that the non-zero components of basis vectors can be any real number. It is, however, convenient to have a standard basis for every dimensionality n = 1, 2, 3 . . . . These standard bases, called orthonormal bases, consist of vectors whose values are restricted to 0 and 1, so that the basis for n = 2 is (1, 0), (0, 1), for n = 3 (1, 0, 0), (0, 1, 0), (0, 0, 1) and so on. This restriction does not affect the capacity of the bases to generate all other n-dimensional vectors in the space.

Manifolds in vector space

Given a set A and an n-fold Cartesian product X of A, a relation is a subset of X . The subset may be random or made on the basis of some explicit criterion. In the latter case, for example, if A is the set of all people in some city, then X = A × A is the set of all possible pairings of people in the city. If one now defines a selection criterion, say 'loves', then the subset of pairs which satisfy the criterion constitute the relation: it is the set of all pairs of people one of whom loves the other.

In a vector space, a relation defined on an n-fold Cartesian product is a subset of vectors in the space. Geometrically, such a subset is a manifold

What has been said about manifolds in two-dimensional space applies straightforwardly to arbitrary dimensionality n; for n > 3 lines are referred to as hypercurves and planes and nonlinear surfaces as hyperplanes and hypersurfaces. Hyper-objects cannot be directly visualized or even conceptualized except by analogy with two and three dimensionalshapes, but as mathematical objects they are unproblematical.

Proximity in vector space

The geometrical proximity of two vectors v and w in a vector space V is determined by a combination of the size of the angle between the lines joining them to the origin of the space's basis, and by the lengths of those lines. Assume that v and w have identical lengths and are separated by an angle θ , as in Figure

The angle between two vectors v and w can be found by first finding its cosine and then translating that into the corresponding angle using standard trigonometric tables. The cosine is found using the formula given in Equation (3.3):

where:

1. θ is the unknown angle between v and w.

2. |v| and |w| are the lengths or 'norms' of v and w, that is, the lengths of the lines connecting them to the origin in the basis, as in Figure

3. The division of a vector v by its length |v| is always a unit vector, that is, a vector of length 1. 4. The dot between the two division terms designates the inner product, as described earlier. The formula for finding the cosine of the angle between two vectors is based on the observation that, if the lengths of the vectors are the same, then the sole determinant of the distance between them is the angle, as noted. The formula rescales both vectors to the same length, that is, 1, and the inner product of the rescaled vectors is the cosine of the angle between them.

To see why the inner product of length-normalized vectors should be the cosine of the angle between them, recall that the cosine of either one of the non-right angles in a right-angled triangle is defined as the ratio of the length of the side adjacent to the angle of interest to the hypotenuse, as in Figure

The vector v is rotated 30 degrees about the origin keeping its length constant; the new coordinates are [0.87, 0.50] as in Figure

Moving on to proximity measurement by distance, the distance between two vectors v and w in a vector space V can be measured in terms of a metric. Given a set X , a metric (cf.

1. d(x, y) ≥ 0, that is, the distance between any two vectors in the space is non-negative.

2. d(x, y) = 0 if and only if x = y, that is, the distance from a vector to itself is 0, and for vectors which are not identical is greater than 0. 3. d(x, y) = d(y, x), that is, distances are symmetrical. 4. d(x, y) ≤ d(x, y) + d(y, z), that is, the distance between any two vectors is always less than or equal to the distance between them and a third vector. This is the triangle inequality, shown diagrammatically in Figure

. . . A metric space M(V, d) is a vector space V on which a metric d is defined in terms of which the distance between any two points in the space can be measured. Numerous distance metrics exist

1. Linear metrics, where the distance between two points in a manifold is taken to be the length of the straight line joining the points, or some approximation to it, without reference to the shape of the manifold. 2. Nonlinear metrics, where the distance between the two points is the length of the shortest line joining them along the surface of the manifold and where this line can but need not be straight. This categorization is motivated by the earlier observation that manifolds can have shapes which range from perfectly flat to various degrees of curvature. Where the manifold is flat, as in Figure

. . . . Distance in vector space will figure prominently in the discussion from this point onwards, and as such it is discussed in some detail; see further

The most commonly used linear metric is the Minkowski, given in Equation (3.5); for others see

where 1. M is a matrix each of whose n-dimensional row vectors specifies a point in n-dimensional metric space. 2. i and j index any two row vectors in M. 3. p is a real-valued user-defined parameter in the range 1 . . . ∞. 4. |M i, j -M j,k | is the absolute difference between the coordinates of i and j in the space. 5. ∑ k=1..n |M i, j -M j,k | generalizes to n dimensions the Pythagorean theorem for the length of the hypotenuse of a right angled triangle in two dimensions, which is the shortest distance between any two 2 dimensional vectors. Three parameterizations of Minkowski distance are normally used in clustering contexts. Where p = 1 the result is the Manhattan or city-block distance in

which simplifies to Equation (3.7)

where the vertical bars | . . . | indicate absolute value, so that, for example, |2 -4| is 2 rather than -2; this captures the intuition that distances cannot be negative. If, for example, n = 2, M i =

By far the most often used parameterization of the Minkowski metric is p = 2, the Euclidean distance, shown in Equation (3.8).

This is just the Pythagorean rule known, one hopes, to all schoolchildren, that the length of the hypotenuse of a right-angled triangle is the square root of the sum of the squares of the lengths of the other two sides. Again for n = 2, M i =

which simplifies to Equation (3.10)

The parameter p can be any positive real-number value: where 1 < p < 2, the Manhattan approximation approaches the Euclidean distance, and where p > 2 the approximation moves away from the Euclidean and approaches the Chebyshev. Like the Euclidean distance, the Manhattan distance is based on the differences between any pair of row vectors M i and M j in their n dimensions, but it merely sums the absolute values of the differences without using them to calculate the shortest distance, and is thereby an approximation to the shortest distance; Chebyshev takes the distance between any pair of row vectors M i and M j to be the maximum absolute difference across all n dimensions and, like Manhattan, is therefore an approximation to linear distance.

There are many nonlinear metrics

. . Mathematically, geodesic distance is a generalization of linear to nonlinear distance measurement in a space: the geodesic distance g x,y is the shortest distance between two points x and y on a manifold measured along its possibly-curved surface

A spanning tree for G is an acyclic subgraph of G which contains all the nodes in G and some subset of the arcs of G

and so on. The graph distance table and the Euclidean one from which it was derived in this way are shown in Tables 3.6 and 3.7. As expected, the sum of distances and mean distance for the graph matrix are both substantially greater than for the Euclidean one, and the graph distance between M 1 and M 7 is three times larger than for the Euclidean, which Figure

In general, the graph approximation of geodesic distance is constrained to follow the shape of the manifold by the need to visit its nodes in the course of minimum spanning tree traversal. Intuitively, this corresponds to approximating the geodesic distance between any two cities on the surface of the Earth, say from New York to Beijing in Figure

Geometrical interpretation of data

Data are a description of objects from a domain of interest in terms of a set of variables such that each variable is assigned a value for each of the objects. We have seen that, given m objects described by n variables, a standard representation of data for computational analysis is a matrix M in which each of the m rows represents a different object, each of the n columns represents a different variable, and the value at M i j describes object i in terms of variable j, for i = 1 . . . m, j = 1 . . . n. The matrix thereby makes the link between the researcher's conceptualization of the domain in terms of the semantics of the variables s/he has chosen and the actual state of the world, and allows the resulting data to be taken as a representation of the domain based on empirical observation.

Once data are represented as a matrix M, the foregoing geometrical concepts apply directly to it. Specifically:

-The dimensionality of M, that is, the number n of columns representing the n data variables, defines an n-dimensional data space. -The sequence of n numbers comprising each row vector of M specifies the coordinates of the vector in the space, and the vector itself is a point at the specified coordinates; because the row vectors represent the objects in the research domain, each object has a specified location in the data space. -The set of all data vectors in the space constitutes a manifold; the shape of the manifold is the shape of the data. -Distance between the data vectors comprising the manifold can be measured linearly or nonlinearly. The issue of whether the data manifold is linear or nonlinear will be prominent in the discussion to follow because it reflects a corresponding distinction in the characteristics of the natural process that the data describe. Linear processes have a constant proportionality between cause and effect. If kicking a ball is a linear system and kicking it x hard makes it go y distance, then a 2x kick will make it go 2y distance, a 3x kick 3y distance and so on for nx and ny. Experience tells us that reality is not like this, however: air and rolling resistance become significant factors as the ball is kicked harder and harder, so that for a 5x kick it only goes, say, 4.9y, for 6x 5.7y, and again so on until it bursts and goes hardly any distance at all. This is nonlinear behaviour: the breakdown of strict proportionality between cause and effect. Such nonlinear effects pervade the natural world, giving rise to a wide variety of complex and often unexpected, including chaotic, behaviours

Conceptualizing data as a manifold in n-dimensional space is fundamental to the discussion of clustering that follows for two main reasons. On the one hand, it becomes possible to visualize the degrees of similarity of data vectors, that is, the rows of a data matrix, as clusters in a geometrical space, thereby greatly enhancing intuitive understanding of structure in data. And, on the other, the degrees of similarity among data vectors can be quantified in terms of relative distance between them, and this quantification is the basis for most of the clustering methods presented later on.

Data transformation

Once a data matrix has been constructed, it can be transformed in a variety of ways prior to cluster analysis. In some cases such transformation is desirable in that it enhances the quality of the data and thereby of the analysis. In others the transformation is not only desirable but necessary to mitigate or eliminate characteristics in the matrix that would compromise the quality of the analysis or even render it valueless. The present section describes various types of transformation and the motivations for using them.

Variable scaling

The variables selected for a research project involving cluster analysis may require measurement on different scales. This is not an issue with respect to MDECTEbecause all its variables measure phonetic segment frequency and are thus on the same scale, but it is not difficult to think of cases in corpusbased linguistics where it can be. In sociolinguistics, for example, speakers might be described by a set of variables one of which represents the frequency of occurrence of some phonetic segment in interviews, another one speaker age, and a third income. Because these variables represent different kinds of thing in the world, they are measured in numerical units and ranges appropriate to them: phonetic frequency in the integer range, say, 1..1000, age in the integer range 20..100, and income in some currency in the real-valued range 0..50000. Humans understand that one can't compare apples and oranges and, faced with different scales, use the variable semantics to interpret their values sensibly. But cluster analysis methods don't have common sense. Given an m × n data matrix M in which the m rows represent the m objects to be clustered, the n columns represent the n variables, and the entry at M i j (for i = 1 . . . m, j = 1 . . . n) represents a numerical measure of object i in terms of variable j, a clustering method has no idea what the values in the matrix mean and calculates the degrees of similarity between the row vectors purely on the basis of the relative numerical magnitudes of the variable values, as we shall see. As a consequence, variables whose scales permit relatively larger magnitudes can have a greater influence on the cluster analysis than those whose scales restrict them to relatively small values, and this can compromise the reliability of the analysis, as has often been noted -cf., for example,

Table

In Table

The trees in Table

That the result of cluster analysis should be contingent on the vagaries of scale selection is self-evidently unsatisfactory both in the present case and also more generally in research applications where variables are measured on different scales. Some way of eliminating scale as a factor in such applications is required; a way of doing this follows.

Relative to a data matrix M, a solution to the above problem is to standardize the variables by transforming the values in the column vectors of M in such a way that variation in scale among them is removed: if all the variables are measured on the same scale, none can dominate. The textbook method for doing this is via standard score, also known as z-score and autoscaling -cf., for example,

For the ith value in any given vector x, the z-standardization is defined as in Equation (3.11)

where -µ(x) is the mean of the values in the vector. Given a variable x whose values are represented as a vector of n numerical values distributed across some range, the mean or average of those values is the value at the centre of the distribution. The values in Table

where µ is the conventional symbol for 'mean', ∑ denotes summation, and n is the number of values in x: the mean of a set of n values is their sum divided by n. In the case of Table

Given a variable x whose values are represented as a vector of n values [x 1 , x 2 . . . x n ], variance is calculated as follows.

• The mean of the values µ is (

• The amount by which any given value x i differs from µ is then x i -µ. • The average difference from µ across all values is therefore ∑ n i=1 x i -µ/n. • This average difference of variable values from their mean almost corresponds to the definition of variance. One more step is necessary, and it is technical rather than conceptual. Because µ is an average, some of the variable values will be greater than µ and some will be less. Consequently, some of the differences (x i -µ) will be positive and some negative. When all the (x i -µ) are added up, as above, they will cancel each other out. To prevent this, the (x i -µ) are squared. • The definition of variance for n values

where δ is the standard deviation. The standard deviation of A is the square root of 594.44 = 24.38, which tells one that, on average, the marks for A vary by that amount, and by 2.83 for B, both of which are readily interpretable in terms of their respective runs of marks. The z-standardization of an arbitrary vector x is shown in Table

Application of z-standardization transforms any vector into one having a mean of 0 and a standard deviation of 1, and, because division by a constant is a linear operation, the shape of the distribution of the original values is preserved, as is shown by the pre-and post-standardization plots in Table

When z-standardization is applied to each of the column vectors of a matrix, any variation in scale across those variables disappears because all the variables are now expressed in terms of the number of standard deviations from their respective means. Tables 3.12a-c show, for example, zstandardization of the matrices in Table

Application of z-standardization appears to be a good general solution to the problem of variation in scaling among data variables, and it is in fact widely used for that purpose. It is, however, arguable that, for cluster analysis, z-standardization should be used with caution or not at all, again as others have observed

The argument against z-standardization for cluster analysis depends on making a distinction between three properties of a variable:

-The absolute magnitude of values of a variable is the numerical size of its values, and can for present purposes be taken as the absolute maximum of those values. For Frequency in Table

Coefficient of Variation

The intuition gained from direct inspection of the matrices in Table

How does this relate to the use of z-standardization of data for cluster analysis? It is a general property of every z-standardized vector, noted above, that its standard deviation is 1. Application of z-standardization to multiple columns of a matrix therefore imposes a uniform absolute magnitude of variability on them. This is shown in Table

Because the absolute magnitude of variability determines the degree of a variable's effect on clustering, the implication is that all the column vectors in a z-standardized matrix have an equal influence; we have already seen an example of this above. This obviously eliminates any possibility of dominance by variables with relatively high absolute magnitudes of variability, but there is a price, and that price might be felt to be too high in any given research application. Intuitively, real-world objects can be distinguished from one another in proportion to the degree to which they differ: identical objects cannot be distinguished, objects that differ moderately from one another are moderately easy to distinguish, and so on. Data variables used to describe real-world objects to be clustered are therefore useful in proportion to the variability in their values: a variable with no variability says that the objects are identical with respect to the characteristic it describes and can therefore contribute nothing as a clustering criterion , a variable with moderate variability says that the corresponding objects are moderately distinguishable with respect to the associated characteristic and is therefore moderately useful as a clustering criterion , and again so on. Variables v 1 and v 2 in Table

For multivariate data whose variables are measured on different scales, what is required is a standardization method that, like z-standardization, eliminates the distorting effect of disparity of variable scale on clustering but, unlike z-standardization, also preserves the relativities of size of the pre-standardization intrinsic variabilities in the post-standardization absolute magni- tudes of variability. In other words, what is required is a method that generates standardized variable vectors such that the ratios of their absolute magnitudes of variability are identical to those of the intrinsic variabilities of the unstandardized ones. In this way the standardized variables can influence the clustering in proportion to the real-world distinguishability of the objects they describe. Such a method follows.

The literature

The right-hand side of Table

Since, therefore, (i) the coefficient of variation is a scale-independent measure of variability, and (ii) the standard deviation of a mean-standardized variable is always identical to the coefficient of variation of the unstandardized variable, and (iii) the standard deviation of a variable is what measures its absolute magnitude of variability, mean-standardization fulfils the above-stated requirements for a general standardization method: that it eliminate the distorting effect of disparity of variable scale on clustering while preserving the ratios of the intrinsic variabilities of the unstandardized variables in the ratios of the absolute magnitudes of variation of the standardized ones. The absolute magnitudes of variation of mean-standardized variables are identical to the intrinsic variabilities of the unstandardized ones, and hence so are the ratios.

Figures 3.23a-3.23c compare the cluster trees for the unstandardized, zstandardized, and mean-standardized versions of the matrix in Table

Direct inspection of the unstandardized matrix in Figures 3.23a-3.23c reveals three value-groups for v 1 , four groups for v 2 , and small random variations on a constant for v 3 . The primary clustering in Figure3.23a is by v 1 because it has the highest absolute magnitude of variability and subclustering within the three primary clusters is by v 2 , with the effect of v 3 invisible, all as expected. The cluster tree for the z-standardized matrix is much more complex, and any sense of the groups observable in v 1 and v 2 is lost as the clustering algorithm takes account of the numerically much-enhanced random variation in v 3 generated by z-standardization; the tree in Figure

Most statistics, data processing, and cluster analysis textbooks say something about standardization. The z-standardization procedure is always mentioned and, when different methods are cited or proposed, there is typically little discussion of the relative merits of the alternatives, though, as noted earlier, quite a few express reservations about z-standardization. The relatively few studies that are devoted specifically to the issue are empirical, that is, they assess various methods' effectiveness in allowing clustering algorithms to recover clusters known a priori to exist in specific data sets, and their conclusions are inconsistent with one another and with the results of the present discussion.

Normalization

This section deals with a problem that arises when clustering is based on frequency data abstracted from multi-document corpora and there is substantial variation in the lengths of the documents. The discussion is in three main parts. It first shows why variation in document length can be a problem for frequency-based clustering, then goes on to describe a matrix transformation or 'normalization' designed to deal with the problem, and finally shows that such normalization is ineffective where documents are too short to provide reliable probability estimates for data variables. The 63 interviews that comprise the DECTE corpus differ substantially in length and so, consequently, do the phonetic transcriptions of them. Figure

The labels (A)-(C) in Figure

The reason for this effect is easy to see. Whatever set of linguistic features one is counting, be they phonetic, phonological, morphological, lexical, syntactic, or semantic, it is in general probable that a longer document will contain more instances of those features than a shorter one: a newspaper will, for example, contain many more instances of, say, the word 'the' than an average-length email. If frequency profiles for varying-length documents are constructed, as here for the phonetic usage of the DECTE speakers, then the profiles for the longer documents will, in general, have relatively high values and those for the shorter documents relatively low ones. The preceding discussion of scaling has already observed that clustering is strongly affected by the relative magnitudes of variable values. When, therefore, the rows of a frequency matrix are clustered, the profiles are grouped according to relative frequency magnitude, and the grouping will thus be strongly influenced by document length.

The solution to the problem of clustering in accordance with document length is to transform or 'normalize' the values in the data matrix in such a way as to mitigate or eliminate the effect of the variation. Normalization is accomplished by dividing the values in the matrix by some constant factor which reflects the terms in which the analyst wants to understand the data; a statistician, for example, might want to understand data variation in terms of standard deviation, and so divides by that. In the present case the normalization factor is document length, so that the frequency values representing any given document are divided by its length or by the mean length of all the documents in the collection to which it belongs. Such normalization is an important issue in Information Retrieval because, without it, longer documents in general have a higher probability of retrieval than shorter ones relative to any given query.

The associated literature consequently contains various proposals for how such normalization should be done -for example

Normalization by mean document length (Spärck Jones, Walker, and Robertson 2000) is used as the basis for discussion in what follows because of its intuitive simplicity. Mean document length normalization involves transformation of the row vectors of the data matrix in relation to the average length of documents in the corpus being used, and, in the present case, transformation of the row vectors of MDECTE in relation to the average length of the m = 63 DECTE phonetic transcriptions, as in Equation (3.17).

where M i is the matrix row representing the frequency profile of i'th DECTE transcription T i , length(T i ) is the total number of phonetic segments in T i , and µ is the mean number of phonetic segments across all transcriptions T in DECTE , as in Equation (3.18).

The values in each row vector M i are multiplied by the ratio of the mean number of segments per transcription across the set of transcriptions T to the number of segments in transcription T i . The longer the document the numerically smaller the ratio, and vice versa; the effect is to decrease the values in the vectors that represent long documents, and increase them in vectors that represent short ones, relative to average document length.

MDECTEwas normalized by mean document length and then cluster analyzed using the same clustering method as for Figure

The tree in Figure

Caveat emptor, however. Mean document length normalization has eliminated variation in transcription length as a factor in clustering of the DECTE speakers. There is a limit to the effectiveness of normalization, however, and it has to do with the probabilities with which the linguistic features of interest occur in the corpus. Given a population E of n events, the empirical interpretation of probability

Applying these observations to the present case, each of the constituent transcriptions of T is taken to be a sample of the population of all Tyneside speakers. The longer the transcription the more likely it is that its estimate of the population probabilities of the 156 phonetic segment types in T will be accurate, and, conversely, the shorter the transcription the less likely this will be. It is consequently possible that a very short transcription will give very inaccurate probability estimates for the segment types. The normalization procedure will then accentuate this inaccuracy, and this will in turn affect the validity of the clustering. The obvious solution to the problem of poor population probability estimation by short documents or transcriptions is to determine which documents in the collection of interest are too short to provide reasonably good estimates and to eliminate the corresponding rows from the data matrix. But how short is too short? The answer lies in statistical sampling theory; for further details see

Dimensionality reduction

The dimensionality of data is the number of variables used to describe the data objects: data describing humans in terms of height and weight are twodimensional, the weather in terms of temperature, atmospheric pressure, and wind speed are three-dimensional, and so on to any number of dimensions n.

Reducing the dimensionality of data as much as possible with as little loss of information as possible is a major issue in data analysis across a wide range of research disciplines

As will be seen, cluster analysis is based on measurement of proximity between and among data objects in n-dimensional space, and the discussion of data geometry has presented some proximity measures. For low-dimensional spaces, that is, for spaces where n = 2 and n = 3 which can be graphically represented, these measures are intuitively reasonable. In Figure

The manifold for MDECTEconsists of 63 vectors in a 156-dimensional vector space; it cannot be shown as in Figure

-For a fixed number of data vectors m and a uniform and fixed variable value scale, the manifold becomes increasingly sparse as their dimensionality n grows. To see this, assume some bivariate data in which both variables take values in the range 0..9: the number of possible vectors like (0, 9), (3, 4), and so on is 10 × 10 = 100. For trivariate data using the same range the number of possible vectors like (0, 9, 2) and (3, 4, 7) is 10× 10× 10 = 1000. In general, the number of possible vectors is r d , where r is the measurement range (here 0..9) and d the dimensionality. The r d function generates an extremely rapid increase in data space size with dimensionality: even a modest d = 8 for a 0..9 range allows for 100, 000, 000 vectors. This very rapid increase in data space size with dimensionality is widely known as the 'curse of dimensionality', discussed in, for example,

What about using more data? Let's say that 24 percent occupancy of the data space is judged to be adequate for manifold resolution. To achieve that for the 3-dimensional case one would need 240 vectors, 2400 for the 4-dimensional case, and 24,000,000 for the 8-dimensional one. This may or may not be possible. And what are the prospects for dimensionalities higher than 8?

-As dimensionality grows, the distances between pairs of vectors in the space become increasingly similar. In the relevant information retrieval and data mining literature, proximity between vectors in a space is articulated as the 'nearest neighbour' problem: given a set V of ndimensional vectors and an n-dimensional vector w not in V , find the vector v in V that w is closest to in the vector space. This is an apparently straightforward problem easily soluble by, for example, calculating the Euclidean distance between w and each of the v in V , and selecting the shortest one.

As dimensionality increases, however, this straightforward approach becomes increasingly unreliable because "under certain broad conditions . . . as dimensionality increases, the distance to the nearest neighbour approaches the distance to the farthest neighbour. In other words, the contrast in differences to different data points becomes nonexistent"

To demonstrate this, a sequence of 1000 matrices, each with 100 rows containing random values in the range 0 . . . 1, was generated such that the first matrix had dimensionality k = 1, the second had dimensionality k = 2, and so on to dimensionality k = 1000, as shown in Table

The implication for clustering is straightforward: because the most popular cluster analysis methods group vectors on the basis of their relative distances from one another in a vector space, as the distances between vectors in the space approach uniformity it becomes less and less possible to cluster them reliably.

One response to these characteristics of high-dimensional data is to use it as is and live with the consequent unreliability. The other is to attempt to mitigate their effects by reducing the data dimensionality. The remainder of this section addresses the latter alternative by presenting a range of dimensionality reduction methods.

The foregoing discussion of data creation noted that variables selected for a research project are essentially a first guess about how best to describe the domain of interest, and that the guess is not necessarily optimal. It may, therefore, be the case that the initial selection can be refined and, more specifically, that the number of variables can be reduced without losing too much relevant information. Given the importance of dimensionality reduction in data processing generally, there is an extensive literature on it and that literature proposes numerous reduction methods. The following account of these methods cannot be exhaustive. Instead, the aim is to provide an overview of the main approaches to the problem and a discussion of the methods that are most often used and / or seem to the author to be most intuitively accessible and effective for corpus linguists.

The methods for dimensionality reduction are of two broad types. One type selects a subset of the more important data variables and eliminates the remainder from the analysis, using some definition of importance. The other type abstracts a new and usually much smaller set of variables on the basis of the existing ones. In the relevant machine learning, artificial intelligence, and cognitive science literatures these approaches to dimensionality reduction are called 'feature selection' and 'feature extraction', but the present discussion has so far used the more generic term 'variable' for what these disciplines call features, and will continue to do so.

The dimensionality of data can be reduced by retaining variables which are important and eliminating those which are not, relative to some criterion of importance; for data in vector space format, this corresponds to eliminating the columns representing unimportant variables from the data matrix. The literature on variable selection is extensive -for summary accounts see

The point of cluster analysis is to group objects in a domain of interest in terms of their relative degrees of similarity based on the variables used to describe them. Intuitively, a variable is useful for this purpose if it has the following characteristics: frequency, variability, and nonrandomness. We will first briefly discuss these three characteristics in general before presenting ways of selecting variables for each.

-Frequency: In general, a variable should represent something which occurs often enough for it to make a significant contribution to understanding of the domain. For example, in the DECTE interviews the two most frequent phonetic segments occur 12454 and 8255 times respectively out of a total of 157116 segment tokens across all 63 speakers, but 13 segments occur only once. The frequent segments are prominent features which any attempt to understand the phonetics of Tyneside speech must take into account, whereas the infrequent ones tell one little about Tyneside speech and may well be just noise resulting from speaker mispronunciation or transcription error. -Variability: The values which the variable takes should vary substantially. As the discussion of variable scaling has already noted, realworld objects can be distinguished from one another in proportion to the degree to which they differ: identical objects cannot be distinguished, objects that differ moderately from one another are moderately easy to distinguish, and so on. Data variables used to describe real-world objects are therefore useful for distinguishing the objects they describe in proportion to the variability in their values: a variable with no variability says that the objects are identical with respect to the characteristic it describes and can therefore contribute nothing to distinction of objects, a variable with moderate variability days that the corresponding objects are moderately distinguishable with respect to the associated characteristic and therefore moderately useful for the purpose, and again so on. -Nonrandomness: The variation in the values which the variable takes should be nonrandom. Random variation of the aspect of the domain which the variable describes means that there is no systematic variation among objects, and all one can say on this basis is that, in this respect, the objects differ, which is obvious from the outset. A variable is, therefore, useful for clustering to the extent that the values which it takes have a nonrandom distribution of variability among objects.

The remainder of this section presents ways of selecting variables for each of these criteria, then identifies associated problems, and finally proposes a way of resolving the problems.

Frequency

An m × n frequency matrix F is constructed in which the value at F i j is the number of times variable j (for j = 1 . . . n) occurs in document i (for i = 1 . . . m). The frequency of occurrence of variable j across the whole corpus is given in

Frequencies for all the columns of F are calculated, sorted, and the less frequent variables are removed from F, thereby reducing the dimensionality of F. MDECTE is a frequency matrix, so the summation and sorting process can be applied directly; the result of doing so is shown in Figure

Variability

The degree of variability in the values of a variable is described by its variance or, expressed in the original units of measurement, its standard deviation. Given a data matrix in which the rows are the objects of interest and the Note that, where variables are measured on different scales, conclusions about their relative variabilities based on the magnitudes of their variances can be misleading. The foregoing discussion of variable scaling made a distinction between absolute and intrinsic variability, where the first is the amount of variation in values expressed in terms of the scale on which those values are measured and the second is the amount of variation expressed independently of scale. Absolute magnitude of variability is measured by standard deviation, and comparison of the standard deviations of a set of variables therefore offers a scale-dependent assessment of their variabilities. The discussion of variable scaling also showed why scale dependence can be misleading -essentially, because the magnitude of a variable's standard deviation is strongly influenced by the magnitude of its values, so that, judged by its standard deviation, a variable with a relatively lower intrinsic variability but relatively larger values can appear to have greater variability than one with relatively higher intrinsic variability but relatively smaller values. For this reason, intrinsic variability as measured by the coefficient of variation, introduced earlier, should used as the criterion for variable selection where variables are measured on different scales. All the variables in MDECTE are measured on the same scale, segment frequency, and so this problem does not arise.

Nonrandomness

Two approaches to assessing nonrandomness in the distribution of variability are considered here: Poisson distribution and term frequency -inverse document frequency.

A widely used measure of nonrandomness is the ratio of the variance of a set of values to their mean. To understand this measure it is first necessary to understand the Poisson distribution, a statistical model of randomness. This part of the discussion briefly introduces the Poisson distribution, then shows how the variance-to-mean ratio relates to it, and finally describes the application of the ratio to dimensionality reduction.

The Poisson distribution models the number of times that a random and rare event occurs in some specified spatial or temporal interval; see for example

where:

p is a probability.

x is the variable in question.

r is the number of events that occur over an interval i, and r! is r factorial.

e is the base of the natural logarithm, that is, 2.71828.

-λ is the mean value of x over many intervals i. For a Poisson process whose mean rate of occurrence of events λ over the designated interval i is known, therefore, this function gives the probability that some independently specified number r of events occurs over i. For example, assume that 7 cars pass through a rural intersection on Thursday, 3 on Friday, and 5 on Saturday; the mean number λ of cars passing through the intersection on a given day is 5. What is the probability that 4 cars will pass through on Sunday? The calculation is given in Equation (3.21)

The Poisson distribution can be used to test whether the values in a given data variable are random or not: if there is a close fit between the data and the theoretical distribution, it is probable that the data was generated by a random process.

How can degrees of adherence to the Poisson distribution be determined with respect to a set of variable values? A characteristic of the theoretical Poisson distribution is that its mean and variance are identical. Given a frequency matrix whose columns represent the variables of interest, therefore, the degree to which any column j diverges from Poisson can be determined by calculating the degree to which j's mean and variance differ. This ratio is known as the 'variance-to-mean ratio' (vmr) and is defined on a vector x by

V mr is also known as the 'index of dispersion', which indicates its use as a measure of dispersion of some set of values relative to a statistical distribu-tion. Relative to a Poisson distribution it measures degree of divergence from randomness.

The vmr can be used for dimensionality reduction as follows; a document collection D containing m documents is assumed. The production of a natural language document, and more specifically the successive occurrence of tokens of variables which constitutes the document, is taken to be a Poisson process. For each of the n variables x j describing the documents of

-The mean rate of occurrence λ j of x j in the m documents is the total number of occurrences of x j in D divided by m. -The actual number of occurrences of x j in document

-The question being asked with respect to x j is: since the documents are taken to be generated by a Poisson process, and therefore that each document d i is expected, on average, to contain λ j tokens of x j , how probable is the actual number of occurrences r i j in each of the d i ? -If the probability of x j is high across all the d i , then it fits the Poisson distribution, that is, the occurrence pattern of x j is random and it can therefore be eliminated as a variable. If, however, the probability of x j is low for one or more of the documents, then x j diverges from the distribution -in other words, x j occurs nonrandomly to a greater or lesser degree and should therefore be retained. In the ideal case the probability of x j is low for a proper subset of the documents in D and high elsewhere, indicating that its occurrence pattern is nonrandom in some documents and random in the remainder and that it is therefore a good criterion for document classification. The assumption that any natural language document or document collection is generated by a stochastic process is, of course, unjustified

The vmr values for the column vectors of MDECTEwere calculated, sorted in descending order of magnitude, and plotted as shown in Figure

Spärck

In short, word frequency on its own is not a reliable clustering criterion . The most useful words are those whose occurrences are, on the one hand, relatively frequent, and on the other are not, like 'computer', more or less randomly spread across all collection documents but rather occur in clumps such that a relatively few documents contain most or all the occurrences and the rest of the collection few or none; the word 'debug', for example, can be expected to occur frequently in documents that are primarily about computer programming and compiler design, but only infrequently if at all in those about, say, word processing. On this criterion, the usefulness of lexical types is assessed in accordance with their 'clumpiness' of occurrence across documents in a collection.

When she proposed clumpiness of distribution as a criterion, Spärck Jones also provided a method for calculating it. That method, together with some emendments to it made by

where d f j is the document frequency, that is, the number of documents belonging to D in which t j occurs. The inverse document frequency of a term, therefore, is the ratio of the total number of documents in a collection to the number of documents in which the term occurs; log 2 is not conceptually part of id f , but merely scales the m/d f j ratio to a convenient interval.

There is a problem with id f : it says that terms which occur only once in a corpus are the most important for document classification. Assuming a 1000-document collection, the id f of a term that occurs in one document is log 2 (1000/1) = 9.97, for a term that occurs in two documents log 2 (1000/2) = 8.97 and so on in a decreasing sequence. This is counterintuitive -does one really want to agree with id f that a lexical type which occurs once in a single document is a better criterion for document classification than one which occurs numerous times in a small subset of documents in the collection? It also contradicts the empirically-based and widely used principle

where t f (t j ) is the frequency of term t j across all documents in D. Using this formulation, the t fid f of some lexical type A that occurs once in a single document is 1× log 2 (1000/1) = 9.97, and the t fid f of a type B that occurs 400 times across 3 documents is 400 × log 2 (1000/3) = 3352.3, that is, B is far more useful for document differentiation than A, which is more intuitively satisfying than the alternative

The notion of clumpiness in the distribution of lexical items across document collections extends naturally to other types of variables such as the MDECTE phonetic segments. Its application to dimensionality reduction is analogous to that of the methods already presented: the columns of the data matrix are sorted in descending order of t fid f magnitude, the t fid f values are plotted, the plot is used to select a suitable threshold k, and all the columns below that threshold are eliminated. The plot for MDECTE is given in Figure

Though t fid f has been and is extensively and successfully used in Information Retrieval, it has a characteristic which compromises its utility for dimensionality reduction. Because clumpiness in t fid f relative to some Where the id f is near 0 the t fid f of v is very small, and when it is at 0that is, where v occurs in every document -the t fid f remains 0 irrespective of v's frequency. It is, however, possible that v is nonrandomly distributed across the documents even where it occurs in every document -some documents might, for example, contain only one or two tokens of v, while others might contain scores or hundreds -and t fid f cannot identify such a distribution. Use of t fid f for dimensionality reduction therefore runs the risk of eliminating distributionally-important variables on account of the definition of clumpiness on which it is based. A small change to the formulation of t fid f prevents the id f term evaluating to zero and this allows relative frequency to remain a factor, as shown in Equation (3.25).

Since (m + 1) must always be greater than d f the id f and consequently the t fid f are always greater than 0.

All the methods for dimensionality reduction presented so far, from frequency through to t fid f , suffer two general problems. The first is that selection of a threshold k below which variables are discarded is problematic. Visual intuition based on plotting indicates that, the further to the left of the plot one goes, the more important the variable. But where, exactly, should the threshold be drawn? In Figure

The second problem is incommensurateness. The selection criteria focus on different aspects of data, and as such there is no guarantee that, relative to a given data matrix, they will select identical subsets of variables. Indeed, the expectation is that they will not: a variable can have high frequency but little or no variability, and even if it does have significant variability, that variability might or might not be distributed nonrandomly across the matrix rows. This expectation is fulfilled by MDECTE, as shown in Figure

Each of the rows (a)-(d) of Figure

Given that the four variable selection criteria in the foregoing discussion can be expected to, and for MDECTE do, select different subsets of variables, which of them is to be preferred or, alternatively, how can their selections be reconciled? With respect to threshold selection, the literature appears to contain no principled resolution, and none is offered here; selection of a suitable threshold remains at the discretion of the researcher. The remainder of this discussion deals with the problem of incommensurateness.

In any given research application there might be some project-specific reason to prefer one or another of the four variable selection criteria. Failing this, there is no obvious way of choosing among them. The alternative is to attempt to reconcile them. The reconciliation proposed here attempts to identify and eliminate the variables which fail to satisfy the principles of fre-quency, variability, and nonrandomness set out at the start of the discussion. This can be done by calculating frequency, variance, vmr, and t fid f values for each column of the data matrix, sorting each set of values in descending order of magnitude, z-standardizing for comparability, and then co-plotting. For MDECTE this amounts to co-plotting the frequency, variance, vmr, and t fid f curves from The variables with high-frequency, high-variance, and high-nonrandomness values are on the left of the plot, and these values diminish smoothly as one moves to the right. If a threshold is now selected, the variables to the right of it can be discarded and those to the left retained, yielding the required dimensionality reduction; as before, threshold selection is subjective, and in Figure

It is, moreover, possible to further reduce dimensionality by refining the selection to the left of the threshold. This refinement is based on tabulation of the retained variables, as shown for MDECTE in Table

-Column 3 contains high-frequency, high-variance variables, but the variance is near-randomly distributed. -Column 4 contains variables whose values are nonrandomly distributed, but they are low-frequency and low-variance. -Column 5 contains a high-frequency variable with little variance, and such variance as it has is near-random. -Column 6 contains a low-frequency, low-variance variable whose values, on one measure, are nonrandomly distributed. -Column 7 contains quite a large number of low-frequency, low-variance variables whose values on the t fid f measure are nonrandomly distributed. The initial selection based on the threshold in Figure

Finally, the various dimensionality reduction methods described thus far are general in the sense that they are applicable to any data matrix in which the rows represent objects to be clustered and the columns the variables describing those objects. Where the variables are lexical, however, there is additional scope for dimensionality reduction via stemming and elimination of so-called stop-words. This is a substantial topic in its own right, and is not discussed here; for further information see for example

As noted, the methods for dimensionality reduction are of two broad types. One type selects a subset of the more important data variables and eliminates the remainder from the analysis, using some definition of importance. The other type abstracts a new and usually much smaller set of variables on the basis of the existing ones. The preceding discussion has dealt with the first of these; the remainder of this section moves to the second, variable extraction.

The discussion of data creation noted that, because selection of variables is at the discretion of the researcher, it is possible that the selection in any given application will be suboptimal in the sense that there is redundancy among them, that is, that they overlap with one another to greater or lesser degrees in terms of what they represent in the research domain. Where there is such redundancy, dimensionality reduction can be achieved by eliminating the repetition of information which redundancy implies, and more specifically by replacing the researcher-selected variables with a smaller number of non-redundant variables that describe the domain as well as, or almost as well as, the originals. Slightly more formally, given an n-dimensional data matrix, dimensionality reduction by variable extraction assumes that the data can be described, with tolerable loss of information, by a manifold in a vector space whose dimensionality is lower than that of the data, and proposes ways of identifying that manifold.

For example, data for a study of student performance at university might include variables like personality type, degree of motivation, score on intelligence tests, scholastic record, family background, class, ethnicity, age, and health. For some of these there is self-evident redundancy: between personality type and motivation, say, or between scholastic record and family background, where support for learning at home is reflected in performance in school. For others the redundancy is less obvious or controversial, as between class, ethnicity, and score on intelligence tests. Variable extraction methods look for evidence of such redundancy between and among variables and use it to derive new variables which give a non-redundant, reduceddimensionality representation of the domain. In the foregoing example, the researcher-defined variables personality type, motivation, scholastic record, and score on intelligence tests might be replaced by a 'general intelligence' variable based on similarity of variability among these variables in the data, and family background, class, ethnicity, age, and health with a 'social profile' one, thereby reducing data dimensionality from nine to two.

The following discussion of variable extraction first gives a precise definition of redundancy, then introduces the concept of intrinsic dimension, and finally presents some variable extraction methods.

If there is little or no redundancy in variables then there is little or no point to variable extraction. The first step must, therefore, be to determine the level of redundancy in the data of interest to see whether variable extraction is worth undertaking. The methods for doing this described below are all based on assessing the degrees of overlap between data variables in terms of the information about the domain that they represent, and they do this by measuring the similarity between and among the column vectors of the data matrix which represent the variables.

We have seen that the values in an n-dimensional vector are the coordinates of its location in n-dimensional space. The similarity of values in any two vectors in the space will consequently be reflected in the distance between them: vectors with very similar values will be close together, and to the extent that the differences in values increase they will be further apart. By calculating the distances between all unique pairings of column vectors in a data matrix, it is possible to identify degrees of similarity and therefore of redundancy between them. The Euclidean distances between all unique pairings of the 156 column vectors of MDECTE in 63-dimensional space were calculated, sorted in descending order of magnitude, and plotted in

Angle has an advantage over distance as an indicator of degree of redundancy. The magnitude of distances measured between vectors is determined by the scale on which the variables are measured, and as such it is difficult to know how to interpret a given distance in terms of degree of redundancy: does a distance of, say, 50 represent a lot of redundancy or only a little? A given distance is diagnostically useful only in relation to other distances, as in Figure

A third and frequently used way of measuring redundancy is correlation. In probability theory two events A and B are said to be independent if the occurrence of A has no effect on the probability of B occurring, or vice versa, and dependent otherwise. Given two variables x and y and an ordered sequence of n observations at times t 1 ,t 2 . . .t n for each, if the measured value for x at time t i (for i = 1 . . . n) has no predictive effect on what the measured value for y at t i will be, then those variables are independent, or, failing this condition, dependent. In statistics, variables that are dependent are said to be associated, and the degree of association is the degree to which they depart from independence. Statistics provides various measures of association, the most often used of which, Pearson's product-moment correlation coefficient, or 'Pearson's correlation coefficient' for short, is described below.

To understand Pearson's correlation coefficient, one first has to understand the concept of covariance between any two variables x and y, which is a measure of the degree to which there is a linear relationship between the values taken at successive observations in the time sequence t 1 ,t 2 . . .t n : as the observed values of x change in the sequence, do the values of y at each corresponding observation change in a constant proportion? Figure

In Figure

where where µ x and µ y are the means of x and y respectively, n is the number of observations in x and y, and the (x i -µ x )(y i -µ y ) expression is the inner product of vectors x and y adjusted by subtracting their respective means. Using this formula, the covariances of the variables in Figure

Pcorr divides x and y by their respective standard deviations, thereby transforming their values to a common scale and so eliminating the effect of scale. This is shown in Table

The Pearson coefficient captures the intuition gained from examination of the plots in Figure

Figure

Though they derive from different conceptual frameworks, that is, from vector space geometry and probability, cosine similarity and Pearson correlation are very closely related in that, with respect to determination of vector redundancy, both are based on the inner product of the vectors, both recognize that disparity of scale between the vectors is a problem, and both address this problem by normalization. The only substantive difference between them is that Pearson correlation subtracts their respective means from the vectors, with the result that the vectors are now mean-centred, that is, the sums of their values are 0; if vectors are mean-centred prior to cosine calculation, cosine similarity and Pearson correlation are equivalent. The theoretical basis for variable extraction is the concept of intrinsic dimension. We have seen that an m × n matrix defines a manifold in n-dimensional space. In such a space, it is possible to have manifolds whose shape can be described in k dimensions, where k < n. Figure

The data in Figure

Another example is a plane in three-dimensional space, shown in Figure

This plane can be redescribed in two-dimensional space, as in Figure

This account of variable extraction methods first presents the standard method, linear principal component analysis (PCA), and then goes on to survey a range of other methods.

Because real-world objects can be distinguished from one another by the degree to which they differ, the data variables used to describe those objects are useful for clustering in proportion to how well they describe that variability, as already noted. In reducing dimensionality, therefore, a reasonable strategy is to attempt to preserve variability, and that means retaining as much of the variance of the original data in the reduced-dimensionality representation as possible. Redundancy, on the other hand, is just repeated variance, and it can be eliminated from data without loss of information. PCA reduces dimensionality by eliminating the covariance while preserving most of the variance in data. Because it is the standard dimensionality reduction method, PCA is described in greater or lesser degrees of detail and clarity by most publications in the field. The standard reference works are those of

Given an n-dimensional data matrix containing some degree of redundancy, linear PCA replaces the n variables with a smaller set of k uncorrelated variables called principal components which retain most of the variance in the original variables, thereby reducing the dimensionality of the data with only a relatively small loss of information. It does this by projecting the ndimensional data reduction into a k-dimensional vector space, where k < n and closer than n to the data's intrinsic dimensionality. This is a two-step process: the first step identifies the reduced-dimensionality space, and the second projects the original data into it.

Figure

Vectors v 1 and v 2 both have a substantial degree of variance, as shown both by the standard deviation and the scatter plot, and the coefficient of determination shows that they are highly redundant in that they share 90 percent of their variance. The aim is to reduce the dimensionality of this data from 2 to 1 by eliminating the redundancy and retaining the total data variance, that is, the combined variance of v 1 and v 2 .

The first step is to centre the data on 0 by subtracting their respective means from v 1 and v 2 . This restates the data in terms of a different orthogonal basis but does not alter either the variable variances or their covariance. The mean-centred variables and corresponding plot are shown in Figure

The basis vectors are now rotated about the origin, preserving their orthogonality, so that one or the other of them -in this case the horizontal onebecomes the line of best fit to the data distribution, as shown in Figure

The line of best fit is the one that minimizes the sum of squared distances between itself and each of the data points. Two of the distances are shown by tered. Almost all the variance after rotation is in v 1 and very little in v 2 , as shown in the data table in Figure

This idea extends to any dimensionality n and always proceeds in the same three steps:

-The data manifold is mean-centred. -A new orthogonal basis for the mean-centred data is found in which the basis vectors are aligned as well as possible along the main directions of variance in the manifold. -Dimensionality is reduced by identifying and discarding the variables corresponding to the basis vectors with negligible variance. The second step in this procedure is the key, and that is what PCA offers: it finds an orthogonal basis for any given n-dimensional data matrix such that the basis vectors lie along the main directions of variance in the data manifold. These basis vectors are the principal components of the data. Given a data matrix D whose m rows represent the m objects of interest and whose n columns represent the n variables describing those objects, PCA creates two matrices which we shall call EV ECT and EVAL:

-EV ECT is an n × n matrix whose column vectors are the principal components of D and constitute an orthonormal basis for D. -EVAL is an n×n diagonal matrix, that is, one in which the only nonzero values are in the diagonal from its upper left to its lower right corner. These diagonal values are the lengths of the basis vectors in EV ECT , and represent the magnitudes of the directions of variance in the data manifold. The diagonal values in EVAL are sorted in descending order of magnitude and are synchronized with EV ECT such that, for j = 1 . . . n, EVAL j is the length of basis vector EV ECT j . Using EVAL, therefore, less important directions of variance can be identified and the corresponding basis vectors eliminated from EV ECT , leaving a k < n dimensional space into which the original data matrix D can be projected. Where such elimination is possible, the result is a dimensionality-reduced data matrix.

In the following example D is taken to be a fragment of length-normalized MDECTE small enough for convenient exposition, and is shown in Table

The covariance of each unique pair of columns DMC i and DMC j (for i, j = 1 . . . n) is calculated as described in earlier in the discussion of redundancy and stored in C, where C is an n × n matrix in which both the rows i and columns j represent the variables of DMC, and the value at C i, j is the covariance of variable column i and variable column j in DMC. The values on the main diagonal are the 'covariances' of the variables with themselves, that is, their variances. Table

This orthonormal basis is found by calculating the eigenvectors of C. Calculation of eigenvectors is a fairly complex matter and is not described here because the details are not particularly germane to the discussion. Most linear algebra textbooks provide accessible accounts; see for example

Table

The orthonormal basis is n-dimensional, just like the original data matrix. To achieve dimensionality reduction, a way has to be found of eliminating any basis vectors that lie along relatively insignificant directions of variance. The criterion used for this is the relative magnitudes of the eigenvalues associated with the eigenvectors in EVAL.

The calculation of eigenvectors associates an eigenvalue with each eigenvector, as already noted, and the magnitude of the eigenvalue is an indication of the degree of variance represented by the corresponding eigenvector. Since the eigenvalues are sorted by magnitude, all the eigenvectors whose eigenvalues are below some specified threshold can be eliminated, yielding a k dimensional orthogonal basis for C, where k < n. This is shown in Table

Once the reduced-dimensionality space has been found, the mean-centred version DMC of the original n-dimensional data matrix D is projected into the reduced k-dimensional space, yielding a new n × k data matrix D reduced that still contains most of the variability in D. This is done by multiplying DMC T on the left by the reduced-dimensionality eigenvector matrix EV ECT T reduced , where the superscript T denotes matrix transposition, that is, re-shaping of the matrix whereby the rows of the original matrix become columns and the columns rows. This multiplication is shown in Equation (3.28).

D T reduced can now be transposed again to show the result in the original format, with rows representing the data objects and columns the variables, as in Table

Most of the variance in MDECTE is captured by the first few basis vectors, and virtually all of it is captured in the first 20 or so. MDECTE can, therefore, be reduced from 156-dimensional to 20-dimensional with a small loss of information about the phonetic usage of the speakers that it represents. Or, seen another way, 20 or so variables are sufficient to describe the phonetic usage that the original 156 variables described redundantly.

Several issues arise with respect to PCA:

-selection: Probably the most important issue is selection of a dimensionality threshold below which components are eliminated. There is no known general and optimal way of determining such a threshold, and selection of one is therefore subjective. There are, however, criteria to guide the subjectivity.

• A priori criterion: The number k of dimensions to be selected is known in advance, so that the eigenvectors with the k largest eigenvalues are chosen. If, for example, one wants to represent the data graphically, only the first two or three dimensions are usable. The obvious danger here is that too few dimensions will be selected to retain sufficient informational content from the original matrix, with potentially misleading results, but this is a matter of judgement in particular applications. • Eigenvalue criterion: Only eigenvectors having an eigenvalue ≥ 1 are considered significant and retained on the grounds that significant dimensions should represent the variance of at least a single variable, and an eigenvalue < 1 drops below that threshold. • Scree test criterion: The scree test is so called by analogy with the erosion debris or scree that collects at the foot of a mountain. The eigenvalues are sorted in descending order of magnitude and plotted; the 'scree' descends from the 'mountain' at the left of the plot to the 'flat' on the right, and the further to the right one goes the less important the eigenvalues become.  The first 20 eigenvectors capture 90 percent of the variance in the original data; the first 30 capture 95 percent, and the first 61 100 percent. Even the safest option of keeping the first 61 eigenvectors would result in a very substantial reduction in dimensionality, but one might take the view that the small gain in terms of data loss over, say, 20 or 30 is not worth the added dimensionality.

-Variable interpretation: In any data matrix the variables typically have labels that are semantically significant to the researcher in the sense that they denote aspects of the research domain considered to be relevant. Because PCA defines a new set of variables, these labels are no longer applicable to the columns of the dimensionality-reduced matrix. This is why the variables in the PCA-reduced matrices in the foregoing discussion were given the semantically-neutral labels v 1 . . . v 4 ; the values for these variables are self-evidently not interpretable as the frequencies of the original data since some of them are negative. In applications where the aim is simply dimensionality reduction and semantic interpretation of the new variables is not an issue, this doesn't matter. There are, however, applications where understanding the meaning of the new variables relative to the original ones might be useful or essential, and there is some scope for this.

Singular value decomposition (SVD)

SVD

where -U , S, and V are the matrices whose product gives D.

-The column vectors of U are an orthonormal basis for the column vectors of D. -The column vectors of V are an orthonormal basis for the row vectors of D; the T superscript denotes transposition, that is, V is rearranged so that its rows become columns and its columns rows. -S is a diagonal matrix, that is, a matrix having nonzero values only on the diagonal from S 1 , 1 to S m , n, and those values in the present case are the singular values of D in descending order of magnitude. These singular values are the square roots of the eigenvectors of U and V . Because the column vectors of V are an orthonormal basis for D and the values in S are ranked by magnitude, SVD can be used for dimensionality reduction in exactly the same way as PCA. Indeed, when D is a covariance or correlation matrix, SVD and PCA are identical. SVD is more general than PCA because it can be applied to matrices of arbitrary dimensions with unre-stricted numerical values whereas PCA is restricted to square matrices containing covariances or correlations, but in practice it is a straightforward matter to calculate a covariance or correlation matrix for whatever data matrix one wants to analyze, so the choice between SVD and PCA is a matter of preference.

Factor Analysis (FA)

FA is very similar to PCA, and the two are often conflated in the literature. Relative to a redundant n-dimensional matrix D, both use eigenvector decomposition to derive a set of basis vectors from the variance / covariance matrix for D, both use the relative magnitudes of the eigenvalues associated with the eigenvectors to select a reduced number of basis vectors k < n, and the k vectors are taken to constitute a reduced-dimensionality basis into which D can be projected in order to reduce its dimensionality from n to k. They differ, however, both conceptually and, as a consequence, in how variability in data is analyzed.

PCA is a formal mathematical exercize that uses patterns of covariance in redundant data to find the main directions and magnitudes of variance, and these directions are expressed as a set of non-redundant synthetic variables in terms of which the original variables can be re-stated. These synthetic variables may or may not have a meaningful interpretation relative to the research domain that the original variables describe, but there is no explicit or implicit claim that they necessarily do; PCA is simply a means to a dimensionality reduction end. FA differs in that it does make a substantive claim about the meaningfulness of the variables it derives. Specifically, the claim is that observed data represent significant aspects of the natural process which generated them in a way that is obscured by various kinds of noise and by suboptimal selection of redundant variables, that these significant aspects are latent in the observed data, and that the factors which FA derives identifies these aspects. As such, FA offers a scientific hypothesis about the natural process to which it relates.

The basis for this claim is what distinguishes FA mathematically from PCA: in deriving components, PCA uses all the variance in a data matrix, but FA uses only a portion of it. To understand the significance of this, it is first necessary to be aware of the distinction between different kinds of variance that FA makes. Relative to a given observed data variable v i in an n-dimensional matrix D, where i is in the range 1 . . . n:

-The common variance of v i is the the variance that v i shares will all the other variables in D; this is referred to as its communality. -Specific variance of v i is the variance unique to v i . -Error variance of v i is the variance due to the noise factors associated with data. -Total variance of v i is the sum of its common, specific, and error variances. PCA analyzes total variance, but FA analyzes common variance only, on the grounds that common variance reflects the essentials of the natural process which the data describes, and that analysis of the patterns of covariance in the data calculated only on communality allows scientifically meaningful factors to be extracted.

FA has two main disadvantages relative to PCA: (i) the common variance on which FA is based is complicated to isolate whereas the total variance on which PCA is based is straightforward, and (ii) the factors which FA generates are not unique, whereas PCA generates a unique and optimal summary of the variance in a matrix. On the other hand, FA has the advantage when meaningful interpretation of the derived variables is required by the researcher. Where, therefore, the aim is simply dimensionality reduction and interpretation of the extracted variables is not crucial, PCA is the choice for its simplicity and optimality, but where meaningful interpretation of the extracted variables is important, FA is preferred.

For discussions of FA see

Multidimensional Scaling (MDS)

PCA uses variance preservation as its criterion for retaining as much of the informational content of data as possible in dimensionality reduction. MDS uses a different criterion, preservation of proximities among data objects, on the grounds that proximity is an indicator of the relative similarities of the real-world objects which the data represents, and therefore of informational content; if a low-dimensional representation of the proximities can be constructed, then the representation preserves the informational content of the original data. Given an m × m proximity matrix P derived from an m × n data matrix M, MDS finds an m × k reduced-dimensionality representation of M, where k is a user-specified parameter. MDS is not a single method but a family of variants. The present section describes the original method on which the variants are ultimately based, classical metric MDS , and a variant, metric least squares MDS .

Classical MDS requires that the proximity measure on which it is to operate be Euclidean distance. Given an m × n data matrix M, therefore, the first step is to calculate the m × m Euclidean distance matrix E for M. Thereafter, the algorithm is:

-Mean-centre E by calculating the mean value for each row E i (for i = 1 . . . m) and subtracting the mean from each value in E i . -Calculate an m × m matrix S each of whose values S i, j is the inner product of rows E i and E j , where the inner product is the sum of the product of the corresponding elements as described earlier and the T superscript denotes transposition:

-Calculate the eigenvectors and eigenvalues EV ECT and EVAL of S, as already described. -Use the eigenvalues, as in PCA, to find the number of eigenvectors k worth retaining. -Project the original data matrix M into the k-dimensional space, again as in PCA: M T reduced = EV ECT T reduced xM T . This algorithm is very reminiscent of PCA, and it can in fact be shown that classical MDS and PCA are equivalent and give identical results -cf.

Classical MDS and PCA both give exact algebraic mappings of data into a lower-dimensional representation. The implicit assumption is that the original data is -free. This is, however, not always and perhaps not even usually the case with data derived from real-world observation, and where noise is present classical MDS and PCA both include it in calculating their lowerdimensional projections. Metric least squares MDS recognizes this as a problem, and to compensate for it relaxes the definition of the mapping from higher-dimensional to lower-dimensional data as algebraically exact to approximate: it generates an m×k representation matrix M ′ of an m×n numericalvalued matrix M by finding an M ′ for which the distances between all distinct pairings of row vectors i, j in M ′ are as close as possible to the proximities p i j between corresponding row vectors of M, for i, j = 1 . . . m. The reasoning is that when the distance relations in M and M ′ are sufficiently similar, M ′ is a good reduced-dimensionality representation of M. Metric least squares MDS operates on distance measurement of proximity. This can be any variety of distance measure, but for simplicity of exposition it will here be assumed to be Euclidean.

The mapping f from M to M ′ could in principle be explicitly defined but is in practice approximated by an iterative procedure using the following algorithm:

1. Calculate the Euclidean distance matrix D(M) for all distinct pairs (i, j) of the m rows of M, so that δ i, j ∈ D(M) is the distance from row i to row j of M, for i, j = 1 . . . m. so that the distances between their new locations in the k-space more closely approximate the corresponding ones in D(M), and return to step (3). Finding M ′ is, in short, a matter of moving its row vectors around in the kspace until the distance relations between them are acceptably close to those of the corresponding vectors in M.

The stress function is based on the statistical concept of squared error. The difference or squared error e 2 between a proximity δ i, j in M and a distance d i, j in M ′ is given in Equation (3.30).

The total difference between all δ i j and d i j is therefore as in Equation (3.31).

This measure is not as useful as it could be, for two reasons. The first is that the total error is a squared quantity and not easily interpretable in terms of the original numerical scale of the proximities, and the solution is to unsquare it; the reasoning here is the same as that for taking the square root of variance to obtain a more comprehensible measure, the standard deviation. The second is that the magnitude of the error is scale-dependent, so that a small difference between proximities and distances measured on a large scale can appear greater than a large difference measured on a small scale, and the solution in this case is to make the error scale-independent; the reasoning in this case is that of the discussion of variable scaling earlier on. The reformulation of the squared error expression incorporating these changes is called stress, which is given in

This is the stress function used to measure the similarity between D(M) and D(M ′ ). By iterating steps (3) and (4) in the above MDS algorithm, the value of this stress function is gradually minimized until it reaches the defined threshold and the iteration stops. Minimization of the stress function in MDS is a particular case of what has become an important and extensive topic across a range of science and engineering disciplines, function optimization. Various optimization methods such as gradient descent are available but all are complex and presentation would serve little purpose for present concerns, so nothing further is said about them here; for details of their application in MDS see

As with other dimensionality reduction methods, a threshold dimensionality k must be determined for MDS. The indicator that k is too small is nonzero stress. If k = n, that is, the selected dimensionality is the same as the original data dimensionality, the stress will be zero. Any k less than the (unknown) intrinsic dimension will involve some increase in stress; the question is what the dimensionality should be to give an acceptable level. The only obvious answer is empirical. Starting with k = 1, MDS is applied for monotonically-increasing values of k, and the behaviour of the stress is observed: when it stops decreasing significantly with increasing k, an approximation to the intrinsic dimension of the data, and thus of optimal k, has been reached (ibid.: 4f.). Figure

The indication is that an appropriate dimensionality for MDECTE is in the range 20 . . . 30. Having generated a reduced-dimensionality representation, it is natural to ask how good a representation of the original data it is. The degree of stress is an obvious indicator, though an ambiguous one because there is no principled criterion for what stress value constitutes an acceptable threshold of closeness

For MDS see

Sammon's Mapping

Sammon's mapping is a nonlinear variant of metric least squares MDS. It differs from MDS in a single modification to the stress function shown in Table

The manifold is nonlinear, but linear measurement does not capture the geodesic distances between the points on it equally well. The distance from f to e, for example, is relatively short and the linear measure is a good approximation to the geodesic distance; from f to c and f to a it is longer and the linear measure is a less good approximation; from a to k it is less good still. The best way to capture the shape of the manifold is to add the distances a → b, b → c, and so on, and simply to disregard the remaining distances. Sammon's mapping is based on this idea. When the normalization term, that is, the distance δ i, j , has a relatively large value, the value of the stress function is relatively small, but if δ i, j is relatively small the stress function value is relatively large; because the iterative procedure underlying metric least squares MDS and Sammon minimizes the stress function, this means that the minimization is based much more on the smaller than on the larger linear distances in the data: the larger the value of the stress function the greater the adjustment to the output matrix in the MDS algorithm. In other words, in generating the reduced-dimensionality matrix Sammon's mapping concentrates on the smaller linear distances in the input matrix because these are better approximations to the shape of whatever nonlinearity there is in the data, and incrementally ignores the distances as they grow larger.

As with metric least squares MDS, the reduced dimensionality k is user specified and can be estimated by applying Sammon's mapping to the data for incrementally increasing values of k, recording the stress for each k, and then plotting the stress values to see where they stop decreasing significantly. Figure

Also as with metric least squares MDS, the goodness of the reduceddimensionality representation can be assessed by calculating the degree of correlation between the distances of the original data and the distances of the reduced-dimensionality representation, which can be stated as a correlation coefficient or visualized as a scatter plot, or both. Though still close, the correlation between distances in the original-dimensionality and reduced-dimensionality spaces is here slightly less good than for MDS relative to the same data. Formally, therefore, the MDS result is better than the Sammon one, even if only marginally. This does not, however, warrant the conclusion that the MDS dimensionality reduction is better than the Sammon. Such a conclusion assumes that the original linear Euclidean distances accurately represent the shape of the MDECTE data manifold. But, as we have seen, MDECTE contains substantial nonlinearity. The Sammon dimensionality-reduced matrix represents that nonlinearity and is for that rea-son to be preferred; the slightly less good formal indicators arise because, in taking account of the nonlinearity, Sammon loses some linear distance information.

The original paper for Sammon's mapping is

Isomap

Isomap is a variant of MDS which reduces dimensionality by operating on a nonlinear rather than on a linear distance matrix. Given a linear distance matrix D L derived from a data matrix M, Isomap derives a graph-distance approximation to a geodesic distance matrix D G from D L , and D G is then the basis for dimensionality reduction using either the classical or the metric least squares MDS procedure; graph distance approximation to geodesic distance has already been described in the foregoing discussion of data geometry. The Isomap approximation differs somewhat from the procedure already described, however, in that it uses the topological concept of neighbourhood. The present discussion gives a brief account of this concept before going on to describe Isomap and applying it to the MDECTE data.

Topology

Topology replaces the concept of metric and associated coordinate system with relative nearness of points to one another in the manifold as the mathematical structure defined on the underlying set; relative nearness of points is determined by a function which, for any given point p in the manifold, returns the set of all points within some specified proximity to p. But how, in the absence of a metric and a coordinate system, is the proximity characterized? The answer is that topological spaces are derived from metric ones and inherit from the latter the concept of neighbourhoods. In a metric space, a subset of points which from a topological point of view constitutes a manifold can itself be partitioned into subsets of a fixed size called neighbourhoods, where the neighbourhood of a point p in the manifold can be defined either as the set of all points within some fixed radius ε from p or as the k nearest neighbours of p using the existing metric and coordinates; in Figure

Topological spaces are supersets of metric spaces, so that every metric space is also a topological one. This observation is made for convenience of reference to geometrical objects in subsequent discussion: these are referred to as manifolds irrespective of whether they are embedded in a metric space or constitute a topological space without reference to a coordinate system.

Assume now the existence of an m × n data manifold M embedded in a metric space and a specification of neighbourhood size as a radius ε or as k nearest neighbours; in what follows, only the k nearest neighbour specification is used to avoid repetition. Isomap first transforms M into a topological manifold by constructing a set of k-neighbourhoods. This is done in two steps:

1. A matrix of linear distances between the data objects, that is, the rows of M, is calculated; assume that the measure is Euclidean and call the matrix D. 2. A neighbourhood matrix N based on D is calculated which shows the distance of each of the data objects M i (i = 1 . . . m) to its k nearest neighbours. This is exemplified with reference to the small randomly generated twodimensional matrix M whose scatterplot is shown with row labels in   M and D are self-explanatory in the light of the discussion so far. N is less so. Note that, apart from 0 in the main diagonal, each row of N has exactly 4 numerical values, corresponding to k = 4. The numerical value at N i, j indicates both that j is in the k-neighbourhood of i and the distance between i and j; the k-neighbourhood of N 1 , for example, includes N 2 , N 4 , N 5 , and N 8 , which can be visually confirmed by Figure

Isomap now interprets N as a graph in which data objects are nodes, the numerical values are arcs labelled with distances between pairs of nodes, and the in f values indicate no arc. In graph representation, the N of Table

Using the graph, the shortest node-to-node distance between any two points in the data manifold can be calculated using one of the standard graph traversal algorithms

It remains, finally, to consider an important problem with Isomap. The size of the neighbourhood is prespecified by the user, and this can be problematical for Isomap in two ways. If k or ε is too small the neighbourhoods do not intersect and the graph becomes disconnected; Figure

The way to deal with this problem is incrementally to increase k until the matrix of all distances G no longer contains in f entries. This potentially creates the second of the above-mentioned problems: too large a neighbourhood leads to so-called short-circuiting, where the connectivity of the neighbourhoods fails correctly to represent the manifold shape. To show what this involves, a simplified version of the Swiss roll manifold of Figure

The geodesic distance from P to R in Figure

The short-circuiting problem was identified by

Isomap was proposed by

Identification of nonlinearity

The foregoing discussion has made a distinction between dimensionality reduction methods appropriate to linear data manifolds and methods appropriate to nonlinear ones. How does one know if a manifold is linear or nonlinear, however, and therefore which class of reduction methods to apply? Where the data are low-dimensional the question can be resolved by plotting, but this is impossible for higher-dimensional data; this section describes various ways of identifying nonlinearity in the latter case.

Data abstracted from a natural process known to be linear are themselves guaranteed to be linear, but data abstracted from a known nonlinear process are not necessarily nonlinear. To see why, consider the sigmoid function used to model a range of processes such as population growth in the natural world, shown in Figure

The linearity or otherwise of data based on empirical observation of a natural process is determined by what part of the process is observed. If observation comes from the linear part (A) of the theoretical distribution in Figure

In practice, data abstracted from observation are likely to contain at least some noise, and it is consequently unlikely that strictly linear relationships between variables will be found. Instead, one is looking for degrees of deviation from linearity. Three ways of doing this are presented.

The graphical method is based on pairwise scatter-plotting of variables and subsequent visual identification of deviation from linearity. In Figure

where n is the number of variables; for n = 100, there would be 4950 different variable pairs to consider. This can be reduced by examining only a tractable subset of the more important variables in any given application, and so is not typically an insuperable problem; for what is meant by important variables and how to select them, see

-Because the aim is simply to decide whether given data are linear or nonlinear rather than to find the optimal mathematical fit, the discussion confines itself to parametric regression, where a specific mathematical model for the relationship between independent and dependent variables is proposed a priori, and does not address nonparametric regression, where the model is inferred from the data. -The discussion is based on the simplest case of one independent variable; the principles of regression extend straightforwardly to multiple independent variables. The first step in parametric regression is to select a mathematical model that relates the values of the dependent variable y to those of the independent variable x. A linear model proposes a linear relationship between x and y, that is, a straight line of the general form given in Equation (3.34).

where a and b are scalar constants representing the slope of the line and the intercept of the line with the y-axis respectively. In regression a and b are unknown and are to be determined. This is done by finding values for a and b such that the sum of squared residuals, that is, distances from the line of best fit to the dependent-variable values on the y-axis, shown as the vertical lines from the data points to the line of best fit in Figure

where the a n . . . a 0 are constants and n is the order of the polynomial; where n = 1 the polynomial is first-order, where n = 2 it as second-order and so on, though traditionally orders 1, 2, and 3 are called 'linear', 'quadratic', and 'cubic' respectively. As with linear regression, nonlinear regression finds the line best fit by calculating the coefficients a n . . . a 0 which minimize the sum of squared residuals between the line and the y values; Figure

The upper plot in Figure

Various goodness-of-fit statistics can be used to corroborate the above graphical methods. Some often-used ones are briefly outlined below; others are discussed in

-Runs test: The runs test is a quantification of the intuitions underlying residual plots. A run is a series of consecutive data points that are either all above or all below the regression line, that is, whose residuals are all positive or all negative. If the residuals are randomly distributed above and below the regression line, then one can calculate the expected number of runs: where N a of the number of points above the line and N b the number of points below, one expects to see the number of runs given by Equation (3.36).

where n is the number of data points, the y i are the actual values of the dependent variable, and the ŷi the corresponding values on the regression line. The smaller the SSE the less deviation there is in dependent variable values from the corresponding ones on the regression line, and consequently the better the fit. In Figure

where rd f is the residual degrees of freedom, defined as the number n of data points minus the number of fitted coefficients c in the regression: rd f = nc. For the n = 120 data points in Figure

SSE is defined as above, and SST is the sum of squares of y-value deviations from their mean, as in

where ȳ is the mean of the dependent variable values and y i is the ith of those values. The SSE/SST term is therefore the ratio of the variability of the dependent variable relative to the regression model and its total variability relative to its mean. The numerically smaller the ratio, and hence the larger the R 2 , the better the fit: if the model fits perfectly then there is no residual variability, SSE = 0, and R 2 = 1, but if not SSE approaches SST as the fit becomes less and less good, and R 2 approaches 0. R 2 is a widely used measure of goodness of fit, but discussions of it in the literature generally advise that it be so used only in combination with visual inspection of data plots. This caution derives from

Knowledge of the likelihood and scale of noise in the domain from which the data were abstracted can help in deciding, but this is supplemented by literature offering an extensive range of model selection methods (ibid.: Ch. 5). Two of the more frequently used methods are outlined and exemplified here, one based on statistical hypothesis testing and the other on information theory.

-Extra sum-of-squares F-test -cf.

For the data in Figure

-Akaike's Information Criterion (AIC) -cf.

p = e -0.5∆ 1 + e -0.5∆

(3.41)

where e is the entropy of the model and ∆ is the difference in AIC scores between the two models. When the two AIC scores are identical, both models have a 0.5 probability of being correct. Otherwise the difference in probabilities can serve as the basis for model selection. In the case of the data for Figure

An alternative to regression is to make the ratio of mean nonlinear to mean linear distances among points on the data manifold the basis for nonlinearity identification. This is motivated by the observation that the shape of a manifold represents the real-world interrelationship of objects described by variables, and curvature in the manifold represents the nonlinear aspect of that interrelationship. Linear metrics ignore the nonlinearity and will therefore always be smaller than nonlinear ones; a disparity between nonlinear and linear measures consequently indicates nonlinearity, and their ratio indicates the degree of disparity.

The ratio of mean geodesic to mean Euclidean distance between all pairs of nodes in a graph gives a measure of the amount of nonlinearity in a data manifold. If the manifold is linear then the two means are identical and the ratio is 1; any nonlinearity makes the mean of geodesic distances greater than the Euclidean mean, and the ratio is greater than 1 in proportion of the degree of nonlinearity. Figure

The linear manifold in Figure

An apparent problem with the graph-based approach is computational tractability. Central in theoretical computer science is the study of the intrinsic complexity of computing mathematical functions, and in particular the classification of mathematical functions according to the time and memory space resources which computation of them requires

Calculation of a minimum spanning tree scales in time as O(elog(v)), where e is the number of arcs in the graph and v is the number of nodes

The horizontal axis represents the dimensionality of the Euclidean distance matrix and the vertical axis the number of seconds required calculate the minimum spanning tree and pairwise graph distances for each 10-increment graph. Using a conventional desktop computer running Matlab at 2.6 GHz, for the 2000-row / column matrix 10.11 seconds were required and, extrapolating, 26.43 seconds are required for 3000, 63.37 seconds for 5000, 246.22 for 10000, and 1038.81 for 20000. These requirements do not seem excessive, and they could easily be reduced by using a faster computer. Eventually, of course, the shape of the curve guarantees that execution of the algorithm will become prohibitively time consuming for very large matrices. But 'large' is a relative concept: in research applications involving matrices up to, say, dimensionality 20000 computational!complexity is not a significant factor in using the proposed approach to nonlinearity identification and graph distance calculation.

A potential genuine disadvantage of the graph distance-based approach is that it does not make the distinction between model and noise that the regression-based approach makes, and treats the data matrix as a faithful representation of the domain from which the data was abstracted. Unless the data is noiseless, therefore, the graph distance-based approach includes noise, whether random or systematic, in its calculations, which may or may not be a problem in relation to the application in question.

Using the graphical and regression-based methods outlined above, no strictly or even approximately linear relationships between pairs of variables were found in MDECTE. In a relatively few cases the relationships looked random or near-random, but most showed a discernible pattern; the pair O: Using O: as the independent variable and A: as the dependent, a selection of polynomials was used to model the nonlinear relationship. These are shown in Figure

The Euclidean 63 × 63 distance matrix E was calculated for MDECTE, the minimum spanning tree for E was found, and the graph distance matrix G was derived by tree traversal, all as described in the foregoing discussion. The distances were then linearized into vectors, sorted, and co-plotted to get a graphical representation of the relationship between linear and graph distances in the two matrices. This is shown in Figure

The Introduction described cluster analysis as a family of mathematicallybased computational methods for identification and graphical display of structure in data when the data are too large either in terms of the number of variables or of the number of objects described, or both, for them to be readily interpretable by direct inspection. Chapter 2 sketched how it could be used as a tool for generation of hypotheses about the natural process from which the data were abstracted, and Chapter 3 used it to exemplify various data issues, in both cases without going into detail on how it actually works. The present chapter now provides that detail.

The discussion is in three main parts: the first part attempts to define what a cluster is, the second presents a range of clustering methods, and the third discusses cluster validation, that is, the assessment of how well a clustering result has captured the intrinsic structure of the data. As in Chapter 3, the MDECTE matrix provides the basis for exemplification, but in a dimensionality-reduced form. Specifically, dimensionality is reduced to 51 using the variable selection method which combines the frequency, variance, vmr and t fid f selection criteria; variable selection rather than extraction was used for dimensionality reduction because the original variables will be required for hypothesis generation later in the discussion.

Cluster definition

In cluster analytical terms, identification of structure in data is identification of clusters. To undertake such identification it is necessary to have a clear idea of what a cluster is, and this is provided by an innate human cognitive capability. Human perception is optimized to detect patterning in the environment

Direct perception of pattern is the intuitive basis for understanding what a cluster is and is fundamental in identifying the cluster structure of data, but it has two main limitations. One limitation is subjectivity and consequent unreliability. Apart from the obvious effect of perceptual malfunction in the observer, this subjectivity stems from the cognitive context in which a given data distribution is interpreted: the casual observer brings nothing to the observation but innate capability, whereas the researcher who compiled the data and knows what the distribution represents brings prior knowledge which potentially and perhaps inevitably affects interpretation. In Figure

The obvious way to address these limitations is by formal and unambiguous definition of what a cluster is, relative to which criteria for cluster membership can be stated and used to test perceptually-based intuition on the one hand and to identify non-visualizable clusters in higher-dimensional data spaces on the other. Textbook and tutorial discussions of cluster analysis uniformly agree, however, that it is difficult and perhaps impossible to give such a definition, and, if it is possible, that no one has thus far succeeded in formulating it. In principle, this lack deprives cluster analysis of a secure theoretical foundation. In practice, the consensus is that there are intuitions which, when implemented in clustering methods, give conceptually useful results, and it is on these intuitions and implementations that contemporary cluster analysis is built.

The fundamental intuition underlying cluster analysis is that data distributions contain clusters when the data objects can be partitioned into groups on the basis of their relative similarity such that the objects in any group are more similar to one another than they are to objects in other groups, given some definition of similarity. In terms of the geometric view of data on which the present discussion is based, the literature conceptualizes similarity in two ways: as distance among objects in the data space, and as variation in the density of objects in the space. Two quotations from a standard textbook

-"A cluster is an aggregation of points in the test space such that the distance between any two points in the cluster is less than the distance between any point in the cluster and any point not in it". -"Clusters may be described as connected regions of multi-dimensional space containing a relatively high density of points, separated from other such regions by a region containing a relatively low density of points". These distance and density views of similarity may at first sight appear to be a distinction without a difference: data points spatially close to one another are dense, and dense regions of a space contain points spatially close to one another. There is, however, a substantive difference, and it corresponds to that between the metric space and topological geometries introduced in the discussion of data in the preceding chapter. The distance conceptualization of similarity uses a metric to measure the proximities of data points relative to a set of basis vectors which define the embedding space of the data manifold, whereas the density one uses relative proximity of points on the data manifold without reference to an embedding space. As we shall see, the difference is important because clustering methods based on density are able to identify a greater range of cluster shapes than at least some of the ones based on distance in metric space.

Clustering methods

The Introduction noted that a large number clustering methods is available and that only a selection of them can be included in the present discussion. It also gave some general selection criteria: intuitive accessibility, theoretically and empirically demonstrated effectiveness, and availability of software implementations for practical application. The discussion has now reached the stage where these criteria need to be applied. Clustering methods assign the set of given data objects to disjoint groups, and the literature standardly divides them into two categories in accordance with the kind of output they generate: hierarchical and nonhierarchical. Given an m × n dimensional data matrix D, hierarchical methods regard the m row vectors of D as a single cluster C and recursively divide each cluster into two subclusters each of whose members are more similar to one another than they are to members of the other on the basis of some definition of similarity, until no further subdivision is possible: at the first step C is divided into subclusters c 1 and c 2 , at the second step c 1 is divided into two subclusters c 1.1 , c 1.2 , and c 2 into c 2.1 , c 2.2 , at the third step each of c 1.1 , c 1.2 , c 2.1 , c 2.2 is again subdivided, and so on. The succession of subdivisions can be and typically is represented as a binary tree, and this gives the hierarchical methods their name. Nonhierarchical methods partition the m row vectors of D into a set of clusters C = c 1 , c 2 ..c k such that the members of cluster c i (i = 1 . . . k) are more similar to one another than they are to any member of any other cluster, again on the basis of some definition of similarity. Both hierarchical and nonhierarchical methods partition the data; the difference is that the nonhierarchical ones give only a single partition into k clusters, where k is either pre-specified by the user or inferred from the data by the method, whereas the hierarchical ones offer a succession of possible partitions and leave it to the user to select one of them. Because these two categories offer complementary information about the cluster structure of data, examples of both are included in the discussion to follow, starting with non-hierarchical ones.

As might be expected from the foregoing comments, the literature on clustering is vast

1. The literature subcategorizes hierarchical and non-hierarchical methods in accordance with the data representation relative to which they are defined: graph-based methods treat data as a graph structure and use concepts from graph theory to define and identify clusters, distributional methods treat data as a mixture of different probability distributions and use concepts from probability theory to identify clusters by decomposing the mixture, and vector space methods treat data as manifolds in a geometric space and use concepts from linear algebra and topology. The discussion of data creation and transformation in the preceding chapter was based on vector space representation, and to provide continuity with that discussion only vector space clustering methods are included in this chapter. For information about graphbased methods see

Unfortunately there is some inconsistency of terminological usage, particularly with respect to 'classification' and 'clustering / cluster analysis'. The focus of this book is on hypothesis generation based on discovery of structure in data. As such it is interested in the second of the above types of categorization and uses the terms 'clustering' and 'cluster analysis' with respect to it throughout the discussion, avoiding 'classification' altogether to forestall confusion.

Nonhierarchical clustering methods

As noted, given m n-dimensional row vectors of a data matrix D, nonhierarchical methods partition the vectors into a clustering C consisting of a set of k clusters C = c 1 . . . c k such that the members of cluster c i (i = 1 . . . k) are more similar to one another than they are to any member of any other cluster.

The theoretical solution to finding such a partition is to define an objective function f , also called an error function or a criterion function, which measures the goodness of a partition relative to some criterion in order to evaluate each possible partition of the m vectors into k clusters relative to f , and, having done this, to select the partition for which the value of f is optimal. In practice, such exhaustive search of all possible clusterings for the optimal one rapidly becomes intractable. The rather complex combinatorial mathe-matics of this intractability are discussed in

Projection clustering

The dimensionality reduction methods described in the preceding chapter can be used for clustering by specifying a projection dimensionality of 2 or 3 and then scatter plotting the result. Figure

The following account of the SOM is in five parts: the first part describes its architecture, the second exemplifies its use for cluster analysis by applying it to the MDECTE data, the third discusses interpretation of the lowdimensional projection which it generates, the fourth surveys advantages and disadvantages of the SOM for clustering, and the fifth gives pointers to developments of the basic SOM architecture. The standard work on the SOM is

A good intuition for how the SOM works can be gained by looking at the biological brain structure it was originally intended to model: sensory input systems (Van Hulle 2000),

Figure

The mathematical model corresponding to the above physical one has three components together with operations defined on them:

-An n dimensional input vector R, for some arbitrary n, which represents the retina. -A p × q output matrix M which represents the sensory cortex, henceforth referred to as the lattice. -A p × q × n matrix C which represents the connections, where C i, j,k is the connection between the neuron at M i, j (for i = 1 . . . p, j = 1 . . . q) and the one at R k (for k = 1 . . . n). Three-dimensional matrices like C have not previously been introduced. They are simply a generalization of the familiar two-dimensional ones, and can be conceptualized as in Figure

. . .  For data clustering a SOM works as follows, assuming an m × n data matrix D is given. For each row vector D i (for i = 1 . . . m) repeat the following two steps:

1. Present D i as input to R.

2. Propagate the input along the connections C to selectively activate the cells of the lattice M; in mathematical terms this corresponds to the inner product of R with each of the connection vectors at C i, j . As described earlier, the inner product of two vectors involves multiplication of corresponding elements and summation of the products, yielding a scalar result. For two vectors

). Once all the data vectors have been processed there is a pattern of activations on the lattice M, and this pattern is the cluster analysis of the data matrix D.

Thus far, an important issue has been left latent but must now be addressed. In biological systems, evolution combined with individual experience of a structured environment determines the pattern of connectivity and cortical response characteristics which implement the mapping from highdimensional sensory inputs to a two-dimensional representation. An artificially constructed model of such a system must specify these things, but the SOM does not do this explicitly. Instead, like other artificial neural network architectures

In terms of the physical model, SOM learning tries to find a pattern of connection strength variation such that similar high-dimensional 'sensory' input signals are mapped to spatially close regions in the 'cortex', that is, so that the similarity relations among the input signals are preserved in their low-dimensional representation. In terms of the corresponding mathematical model, this can be restated as the attempt to find a set of connection vectors C such that the inner products of the c ∈ C and the set of input vectors d ∈ D generates in the output lattice M a pattern of activation that represents the neighbourhood relations of D with minimum distortion.

Given a set of input vectors D, SOM learning is a dynamic process that unfolds in discrete time steps t 1 ,t 2 . . .t p , where p is the number of time steps required to learn the desired mapping. At each time step t i , a vector d j ∈ D is selected, usually randomly, as input to the SOM, and the connection strength matrix C is modified in a way that is sensitive to the pattern of numerical values in d j . At the start of the learning process the magnitude of modifica-tions to C is typically quite large, but as the SOM learns via the modification process the magnitude decreases and ultimately approaches 0, at which point the learning process is stopped -the p'th time step, as above. The question, of course, is how exactly the input-sensitive connection strength modification works, and the answer involves looking at the learning algorithm in detail. At the start of learning, the SOM is parameterized with user-specified values:

-Dimensionality of the input vectors R. This is the same as the dimensionality of the data to be analyzed. -Dimensionality and shape of the output lattice M. In theory the output lattice can have any dimensionality and any shape, though in practice its dimensionality is usually 2 and its shape is usually rectangular or hexagonal. There is evidence that lattice shape can affect the quality of results (Ultsch and Herrmann 2005); 2-dimensional rectangular or hexagonal lattices are assumed in what follows. -Size of the output lattice M. This is the number of cells in M.

-The following further parameters are explained below: neighbourhood shape, initial neighbourhood size, neighbourhood decrement interval, initial learning rate, and learning rate decrement size and interval. In addition, the values in the connection matrix C are initialized in such a way that they are non-uniform; uniform connections would preclude the possiblity of learning. This initialization can be random or can use prior information which allows the SOM to learn more quickly

1. An input vector d k ∈ D is selected.

2. The propagation of the input signal through the connections to the lattice in the physical SOM is represented as the inner product of d k and the connection vector C i, j for each unit of the lattice. The result of each inner product is stored in the corresponding cell of the lattice matrix M i, j , as above. Once all the inner products have been calculated, because the connections strengths in C were initialized to be non-uniform, the result is a non-uniform pattern of activation across the matrix. 3. The lattice matrix M is now searched to identify the cell with the largest numerical activation value. We shall call this cell u i j , where i and j are its coordinates in the lattice. 4. The connection strengths in C are now updated. This is the crucial step, since it is how the SOM learns the connections required to carry out the desired mapping. This update proceeds in two steps:

(a) Update of the connections linking the most highly activated cell u i j in M to the input vector d k . This is done by changing the connection vector associated with u i j according to Equation (4.2):

where t denotes the current time step.

-C i, j,k (t + 1) is the new value of the kth element of the connection vector. -C i, j,k (t) is the current value of the kth element of the connection vector. l(t) is the learning rate, a parameter which controls the size of the modification to the connection vector. More is said about this below. -(d k -C i j ) is the difference between the current input vector and the connection vector. This difference is the sum of element-wise subtraction:

where k is the length of the input and connection vectors. In essence, therefore, the update to the connection vector C i j associated with the most active cell u i j is the current value of C i j plus some proportion of the difference between C i j and the input vector, as determined by the learning rate parameter. The effect of this is to make the connection vector increasingly similar to the input vector. Figure 4.5 gives an example for some d k and associated C i j : Before update the input and connection vectors differ significantly; to make this clearer the difference between vectors is shown as differences in grey-scale components in Figure

Summarizing, the SOM's representation of high dimensional data in a low-dimensional space is a two-step process. The SOM is first trained using the vectors comprising the given data. Once training is complete all the data vectors are input once again in succession, this time without training. The aim now is not to learn but to generate the two-dimensional representation of the data on the lattice. Each successive input vector activates the unit in the lattice with which training has associated it together with neighbouring units, though to an incrementally diminishing degree; when all the vectors have been input, there is a pattern of activations on the lattice, and the lattice is the representation of the input manifold in two-dimensional space.

A SOM with a 11 × 11 output lattice and random initialization of the connections was used to cluster MDECTE, and the results are shows in

Which of these two, if either, is the preferred interpretation of the layout of vector labels on the lattice? Without further information there is no way to decide: the lattice gives no clear indication where the cluster boundaries might be. The eye picks out likely concentrations, but when asked to decide whether a given data item is or is not part of a visually-identified cluster, one is often at a loss. An apparent solution is to fall back on knowledge of the subject domain as a guide. But using a priori knowledge of the data to disambiguate the lattice is not a genuine solution, for two reasons. Firstly, it misses the point of the exercize: the lattice is supposed to reveal the structure of the data, not the other way around. And, secondly, the structure of large, complex, real-world data sets to which the SOM is applied as an analytical tool is not usually recoverable from mere inspection -if it were, there would be little point to SOM analysis in the first place. To be useful as an analytical tool, the SOM's representation of data structure has to be unambiguously interpretable on its own merits, and the problem is that an activation lattice like that in the above figures does not contain enough information to permit this in the general case. The problem lies in the subjectivity of visual interpretation with which this chapter began: humans want to see pattern, but the pattern any individual sees is determined by a range of personal factors and, among researchers, by degrees of knowledge of the domain which the lattice represents. Some objective interpretative criterion is required, and this is what the following section provides.

Actually, the problem is even worse than it looks. When a SOM is used for cluster analysis, inspection of the pattern of activation on the lattice can not only be subjective but can also be based on a misleading assumption. There is a strong temptation to interpret the pattern spatially, that is, to interpret any groups of adjacent, highly activated units as clusters, and the distance between and among clusters on the lattice as proportional to the relative distances among data items in the high-dimensional input space, as with, for example, MDS . That temptation needs to be resisted. The SOM differs from these other methods in that the latter try to preserve relative distance relations among objects on the data manifold, whereas the SOM tries to preserve the manifold topology, that is, the neighbourhood relations of points on the manifold -cf.

We have seen that each lattice cell has an associated vector which represents its connections to the input vector. Since the dimensionality of the connection vectors is the same as that of the inputs, and the dimensionality of the inputs is that of whatever n-dimensional input space is currently of interest, the connection vectors are, in fact, coordinates of points in the ndimensional space. Assume that there is a data manifold in the input space and that the connection vectors have been randomly initialized. In this initial state, there is no systematic relationship between the points specified by the set of connection vectors and the surface of the manifold. By incrementally bringing the connection vectors closer to training vectors taken from the data manifold, a systematic relationship is established in the sense that the connection vectors come to specify points on the manifold; at the end of training, the connection vectors map each of the points on the manifold specified by the training vectors to a particular lattice cell. Moreover, it can and usually does happen that data vectors which are close together on the manifold activate the same unit u i j , as described earlier. In this case, the connection vector for u i j has to be brought closer not only to one but to some number k of input vectors. Since these k vectors are close but not identical, the SOM algorithm adjusts the connection vector of u i j so that it becomes a kind of average vector that specifies a point on the manifold which is intermediate between the k input vectors. In this way, u i j becomes associated not only with a single point on the data manifold, but with an area of the manifold surface containing the k vectors that map to it. This is shown in Figure

1. The process of finding an 'average' vector, or 'centroid', which is intermediate between k vectors was described in Chapter 2 is known as vector quantization: for a set V of k vectors, find a 'reference' vector v r of the same dimension as those in V such that the absolute difference d = |v rv i | between each v i ∈ V and v r is minimized. The SOM training algorithm quantizes the input vectors, and the connection vectors are the result -cf.

2. The partition of a manifold surface into areas surrounding reference vectors is a tesselation. TheSOM algorithm implements a particular type, the Voronoi tesselation , in which a reference vector is the centroid, and the area of k associated vectors surrounding the centroid is a neighbourhood; the neighbourhood of a given reference vector v r in a Voronoi tesselation is defined as the set of vectors closer to v r than to any other reference vector (ibid.: 59f.). 3. The set of neighbourhoods defined by the Voronoi tesselation is the manifold's topology, as discussed in the preceding chapter. And because, finally, the SOM algorithm adjusts the connection vector not only of the most-activated unit u i j but also of units in a neighbourhood of gradually diminishing radius, it ensures that adjacent manifold neighbourhoods map to adjacent lattice units.

How does all this relate to cluster interpretation of a SOM lattice? As noted, a Voronoi tesselation is an instance of a topology, that is, a manifold and a discrete collection of subsets of points on the manifold called neighbourhoods. When it is said that a SOM preserves the topology of the input space, what is meant is that it represents the neighbourhood structure of a manifold: when data is input to a trained SOM, the vectors in a given Voronoi neighbourhood are mapped to the same lattice cell, and the vectors in adjoining Voronoi neighbourhoods are mapped to adjacent lattice cells. The result of this topology preservation is that all vectors close to one another in the input space in the sense that they are in the same or adjoining neighbourhoods will be close on the SOM output lattice.

The problem, though, is this: just because active cells are close together on the SOM lattice does not necessarily mean that the vectors which map to them are topologically close in the input space. This apparently-paradoxical situation arises for two reasons -see discussion in, for example, Ritter, Martinetz, and Schulten (1992: Ch. 4).

1. The topology of the output manifold to which the SOM maps the input one must be fixed in advance. In the vast majority of applications the SOM output topology is a two-dimensional plane, that is, a linear manifold, with rectangular or hexagonal neighbourhoods which are uniform across the lattice except for at the edges, where they are necessarily truncated. There is no guarantee that the intrinsic dimensionality of the input manifold is as low as two, and therefore no guarantee that the output topology will be able to represent the input manifold well. In theory, the SOM is not limited to two-dimensional linear topology, and various developments of it, cited later, propose other ones, but where the standard one is used some degree of distortion in the lattice's representation must be expected -cf.

How can a SOM lattice be interpreted so as to differentiate cells which are spatially close because they are topologically adjacent in the input space and therefore form a cluster, and cells which are spatially close but topologically more or less distant in the input space? The answer is that it cannot be done reliably by visual inspection alone; interpretation of a SOM lattice by visual inspection is doubly unreliable -a subjective interpretation of an ambiguous data representation. This is a well known problem with SOMs

The U -matrix representation of SOM output uses relative distance between reference vectors to find cluster boundaries. Specifically, given an m × n output lattice M, the Euclidean distances between the reference vector associated with each lattice cell M i j (for i = 1..m, j = 1 . . . n) and the reference vectors of the immediately adjacent cells M i-1, j , M i+1, j , M i, j-1 , and M i, j+1 are calculated and summed, and the result for each is stored in a new matrix U i j having the same dimensions as M. If the set of cells immediately adjacent to M i j is designated as M ad jacent(i, j) , and d represents Euclidean distance, then, according to Equation (4.3)

U is now plotted using a colour coding scheme to represent the relative magnitudes of the values in U i, j . Any significant cluster boundaries will be visible. Why? The reference vectors are the coordinates of the centroids of the Voronoi tesselation of the data manifold and thus represent the manifold's topology, as we have seen. Where the sum of distances between the reference vector associated with M i j and those associated with M ad jacent(i j) is small, the distance between those centroids on the manifold is small; conversely, a large sum indicates a large distance between centroids on the manifold. Low-magnitude regions in U thus represent topologically close regions on the manifold, and high-magnitude ones topologically distant regions on the manifold. Assuming a grayscale colour coding scheme, therefore, clusters appear on the lattice as regions containing dark gray cells, and boundaries between clusters as regions containing light gray or white ones, or vice versa. Consider, for example, the U -matrix representation of the SOM lattice for the trivial data in Table

A SOM with an 11 × 11 lattice was trained on these data, with the result that the four row vectors in Table

The SOM has three major advantages for cluster analysis. The first and most important is that it takes account of data nonlinearity in its projection: because the Voronoi tesselation follows the possibly-curved surface of the data manifold and the neighbourhood relations of the tesselation are projected onto the output lattice, the SOM captures and represents any nonlinearities present in the structure of the manifold. The second is that, unlike most other clustering methods, the SOM requires no prior assumptions about the number and shapes of clusters: clusters on the SOM lattice represent relative densities of points, that is, of clusters in the input manifold, whatever their number and shape -see:

-The default lattice structure, the linear two-dimensional rectangular grid, restricts its ability to represent data manifolds having nonlinear shapes and intrinsic dimensionalities higher than 2 without topological distortion; the foregoing discussion of lattice interpretation shows the consequence of this for SOM-based cluster analysis. -There is no theoretical framework within which initial values for the fairly numerous SOM parameters can be chosen. Empirical results across a large number of applications have shown that, when parameter values which are 'sensible' in relation to existing rules of thumb are selected, the choice is not crucial in the sense that the SOM usually converges on identical or similar results for different initializations. This is not invariably the case, however, and different combinations, in particular different initializations of the Voronoi centroids and differences in the order of presentation of the training vectors can and do generate different clusterings -cf.

-Lattice structure: The foregoing discussion has noted that the default SOM lattice structure, the linear two-dimensional rectangular grid, restricts its ability to represent data manifolds of higher intrinsic dimensionality and nonlinear shape directly without topological distortion. One type of development addresses this restriction by proposing more flexible lattice structures which can better represent the input topology -cf.

Clustering results from the GTM and the SOM are typically very similar, which is unsurprising given their close similarity. What the GTM offers, however, is on the one hand an understanding of results in terms of a well developed probability theory, and on the other an objective measure for assessing the goodness of those results. Finally, a fairly recent development of projection clustering must be mentioned: subspace clustering. The foregoing discussion of dimensionality reduction has described linear and nonlinear ways of reducing data of observed dimensionality n to an approximation of its intrinsic dimensionality k, where k is less than n. This assumes that all the data objects are best described using the same number k of latent variables, which is not necessarily the case. Subspace clustering groups variables in accordance with the optimal number of latent variables required to describe them, or, put another way, of the i dimensional subspace (for i = 1 . . . n) of the original n-dimensional data space in which they are embedded. This approach to clustering has in recent years found extensive application in areas like computer vision, motion segmentation, and image processing, and there is now a substantial literature devoted to it. Recent surveys are available in

Proximity-based clustering

Nonhierarchical proximity-based approaches treat clustering as a mathematical optimization problem, where only a small subset of all possible partitions is examined in the hope of finding the optimal one. An initial k-cluster partition is defined and an iterative procedure is used in which, at each step, individual data points are moved from cluster to cluster to form a new partition and the result is evaluated in relation to the objective function f : if the value of f shows an improvement over the preceding one the new partition is retained, and if not it is discarded and another one is tried. Such iterative procedures are widely used and are known as gradient descent or gradient ascent procedures depending on whether optimality of the objective function is defined by a minimum or maximum value. Ideally, the procedure will gradually converge on a partition for which no change leads to an improvement in the value of f , at which point the partition is taken to be optimal. This assumption does not always hold, however, because gradient procedures can and often do converge on local maxima or minima, that is, where further iteration produces no improvement in the value of f but the true maximum or minimum has not been reached. Figure

Since it was first proposed in the mid-1960s

k-means is based on the idea that, for a given set of data objects O, each cluster is represented by a prototype object, and a cluster is defined as the subset of objects in O which are more similar to, or in distance terms closer to, the prototype than they are to the prototype of any other cluster. An objective function is used find a set of clusters each of which optimally meets this criterion. For a data set O comprising m n-dimensional data points, O is partitioned into k prototype-centred clusters by the following iterative procedure:

1. Initialize the procedure by selecting k n-dimensional prototype locations in the data space; these can in principle be anywhere in the space, so that they might correspond to data points but need not. The prototypes are the initial estimate of where the clusters are centred in the space, and their locations are refined in subsequent steps. Placement of initial prototypes and selection of a value for k, that is, of the number of required clusters, is problematical, and is further discussed below. 2. Assign each of the m data points to whichever of the k prototypes it is closest to in the space using a suitable proximity measure. This yields k clusters. 3. Calculate the centroid of each of the k clusters resulting from

where x is a data point, C i is the i'th of k clusters, and p i is the prototype of the i'th cluster. This expression says that the SSE is the sum, for all k clusters, of the Euclidean distances between the cluster prototypes and the data points associated with each prototype. For k-means to have optimized this function, the prototypes have to be placed in the data space so that the Euclidean distances between them and their associated data points is globally minimized across all clusters. It is easy to see that this is what k-means does: the procedure converges on stable cluster centroids, and a centroid is by definition the minimum distance from the all the points on which it is based. Use of k-means is not restricted to Euclidean distance, though this is the most frequently used measure. A variety of different measures an associated objective functions can be used. For example, cosine similarity might be more appropriate for some kinds of data, and in that case a different objective function shown in Equation (4.5), Total Cohesion, can be used instead of SSE:

Cosine similarity is an attractive alternative to Euclidean distance when the data has not been normalized, as described earlier, because by basing proximity measurement solely on the angles between pairs of vectors the magnitudes of vector values (or, equivalently, vector lengths) are eliminated as a factor in clustering. The implication of using cosine similarity, however, is that vector length doesn't matter. There are undoubtedly applications where it does not, in which case cosine proximity is the obvious alternative to data normalization, but there are also applications where it does. With respect to MDECTE, for example, use of cosine proximity implies that all phonetic segments, from very frequent to very infrequent, are equally important in distinguishing speakers from one another, but as the foregoing discussion of data has argued, a variable should in principle represent something which occurs often enough for it to make a significant contribution to understanding of the research domain. The frequent segments in the DECTE interviews are prominent features which any attempt to understand the phonetics of Tyneside speech must consider, whereas the very infrequent ones tell one little about Tyneside speech and may well be just noise resulting from speaker mispronunciation or transcription error. Cosine proximity measurement eliminates the distinction, and is therefore unsuitable in this application. This observation applies equally to the use of cosine proximity measurement with other clustering methods as an alternative to measures which take vector magnitude into account.

Relative to the selection criteria for inclusion in this discussion, k-means is a prime candidate: it is intuitively accessible in that the algorithm is easy to understand and its results are easy to interpret, it is theoretically well founded in linear algebra, its effectiveness has repeatedly been empirically demonstrated, and computational implementations of it are widely available. In addition, -Its computational time complexity grows with data space size as O(nkdt), where n is the number of data vectors, k is the number of clusters, d is the data dimensionality, and t is the number of iterations. This means that k-means essentially grows linearly with data size, unlike other clustering methods to be considered in what follows, and is therefore suitable for clustering very large data sets in reasonable time -cf.

The procedure of k-means also has several well known problems, however.

-Initialization. k-means requires two user-supplied parameter values: the number of clusters k and the locations of the k initial centroids c 1 . . . c k in the data space. These values crucially affect the clustering result. On the one hand, if the value chosen for k is incompatible with the number of clusters in the data, then the result is guaranteed to mislead because k-means will deliver k clusters whatever the actual number of clusters intrinsic to the data, including none. For example, Figure

Various ways of selecting an initialization compatible with the intrinsic cluster structure of the data exist. The obvious one is to base the initialization on reliable a priori knowledge of the domain from which the data comes, where available. Failing this, a projection clustering method can be used to visualize the data and thereby to gain some insight into its cluster structure, or one of the range of initialization heuristics proposed in the literature can be applied

The PCA and MDS two-dimensional visualizations of MDECTE derived earlier in Figure

-Initialization. No general and reliable method for selecting initial parameter values for the number of clusters and placement of prototypes in known, and given the crucial role that these play in determining the k-means result, it is unsurprising that initialization remains a research focus. Several approaches to the problem have already been mentioned.

For initial prototype placement one of these approaches was to use of various parameter value selection heuristics. The simplest of these heuristics is random selection; others use a variety of criteria derived from the data.

To work well in practice, however, Isodata requires optimization of no fewer than six threshold parameters, and the extra complication might not be thought worthwhile relative to simply finding an optimal value of k empirically, as described above. -Convergence. As noted, the standard k-means procedure does not guarantee convergence to a global optimum. Stochastic optimization techniques like simulated annealing, genetic algorithms, and neural networks

Density-based clustering

Standard k-means fails to identify non-linearly-separable clusters because it partitions the data into clusters without reference to its density structure. The obvious solution is to take account of density, and this is what density-based clustering methods do; for general discussions see:

Dbscan is based on a topological view of data manifolds, which was introduced in the discussion of Isomap in the preceding chapter. On this view, a data manifold is defined not in terms of the positions of its constituent points in an n-dimensional space relative to the n basis vectors, but rather as a set of adjacent, locally-Euclidean neighbourhoods. The essential idea is that clusters are collections of sufficiently dense adjacent neighbourhoods, and that neighbourhoods which are insufficiently dense are noise, given some definition of 'sufficiently dense'. This is shown in Figure

To implement this idea Dbscan requires predefinition of two parameters: the radius r, called E ps, which defines the size of the neighbourhood, and the threshold number of points for sufficient density, called MinPts. Relative to these parameters, three types of point are distinguished:

-Core points, whose neighbourhood contains MinPts or more points.

-Border points, whose neighbourhood contains fewer than MinPts but which are themselves in the neighbourhood of one or more core points. -Noise points, which are all points that are not either core or border points. Assuming m data points, the Dbscan algorithm is as follows:

1. Visit each data point m i , i = 1 . . . m, labelling each as a core, border, or noise point in accordance with the above definitions 2. Eliminate all the noise points.

3. Link all pairs of core points within a radius E ps of one another. 4. Abstract the clusters, where a cluster is the set of all linked core points. 5. Assign the border points to the clusters. If a border point is in the neighbourhood of only one core point, assign it to the cluster to which the core point belongs. If it is in more than one neighbourhood, assign it arbitrarily to one of them. Like k-means, Dbscan was selected for inclusion because it is a easy to understand and interpret, is mathematically well founded, has an established user base, and is readily available in software implementations. It has important advantages over k-means, however. One is that Dbscan does not require and in fact does not permit prespecification of the number of clusters, but rather infers it from the data; selection of k is one of the main drawbacks of k-means, as we have seen. Another is that Dbscan can find non-linearlyseparable clusters, which extends its range of applicability beyond that of kmeans. The k-means procedure was, for example, able to identify the linearly separable clusters in Figures 4.18a  Dbscan's computational time complexity is somewhat more demanding than that of k-means, though still reasonably moderate in ranging from O(mlogm) to a worst-case O(m 2 ), where m is the number of data points

-Selection of parameter values. As with k-means, selection of suitable parameter values strongly affects the ability of Dbscan to identify the intrinsic data cluster structure. Given some value for MinPts, increasing the size of E ps will allow an increasing number of points to become core points and thereby include noise points in the clusters, as in Fig-

Core

The problem is that if E ps is small enough to keep the lower cluster separate from the upper one then it is too small to allow the upper one to be identified as a single cluster, and if E ps is large enough for the upper one to be identified as a single cluster then it is too large to keep the upper cluster separate from the lower one. These problems are related. Various combinations of E ps with different MinPts additional to those shown in Figure

Because Dbscan can identify a superset of data shapes identifiable by kmeans, it is tempting simply to dispense with k-means and to use Dbscan as the default analytical method. The foregoing discussion of problems with Dbscan and its application to MDECTE show, however, that this would be illadvised. Where it is known or strongly suspected that the data density structure is linearly separable, the more reliable k-means method should be used, and if the data is non-linearly separable then results from Dbscan should, in view of its initialization and sparsity problems, be corroborated using some other clustering method or methods.

That variation in data density is a problem for Dbscan was quickly recognized, and proposals for addressing it have appeared, including G Gdbscan

A fairly obvious approach to identification of density in data is to cover the data space with a grid and count the number of data objects in each cell, as shown in Figure

For general discussions of grid-based clustering see: Jain and Dubes (ibid.: Ch. 3.3.5),

Kernel-based clustering is based on concepts from statistical density function estimation, the aim of which is to find a mathematical function that generates some given data distribution

Hierarchical clustering

Given an m × n data matrix D which represents m objects in n-dimensional space, hierarchical analysis does not partition the m objects into k discrete subsets like the clustering methods described so far. Instead, it constructs a constituency tree which represents the distance relations among the m objects in the space and leaves it to the user to infer a partition from the tree. Hierarchical clustering is very widely used, and so is covered in most accounts of cluster analysis, multivariate analysis, and related disciplines like data mining. A selection of discussions is

The method

Construction of a hierarchical cluster tree is a two-step process: the first step abstracts a proximity table from the data matrix to be analyzed, and the second constructs the tree by successive transformations of the table. An intuition for how tree construction proceeds is best gained by working through an example; the example presented in what follows is based on MDECTE, and more specifically on a subset of MDECTE small enough to render illustrative tables and figures tractable for graphical representation. The concept of proximity among data objects has already been described in the foregoing discussion of data geometry. Euclidean distance is used to exemplify its application to construction of a proximity table for the first 6 of the full 63 rows of MDECTE. The Euclidean distances between all possible pairings of these 6 rows were calculated and stored in a 6 × 6 matrix D, shown in Table

Initially, each row vector of the data matrix is taken to be a cluster on its own; clusters here and henceforth are shown in brackets. Table

(1,3) 0 (

Table

The matrix in Table

1. Rows and columns ((((1,3),6),4) and (

Table

Variants

For a matrix with m rows there will at any step in the above tree-building sequence be a set of p clusters, for p in the range 2 . . . m, available for joining, and two of these must be selected. At the first step in the clustering sequence, where all the clusters contain a single object, this is unproblematical: simply choose the two clusters with the smallest distance between them. At subsequent steps in the sequence, however, some criterion for judging relative proximity between composite and singleton cluster pairs or between composite pairs is required, and it is not obvious what the criterion should be. The one exemplified in the foregoing sequence is such a criterion , known as Single Linkage, but there are various others

For simplicity of exposition, it is assumed that a stage in the tree building sequence has been reached where there are p = 3 clusters remaining to be joined. This is shown in Figure

-The Single Linkage criterion defines the degree of closeness between any pair of clusters (X ,Y ) as the smallest distance between any of the data points in X and any of the data points in Y : if there are x vectors in X and y vectors in Y, then, for i = 1 . . . x, j = 1 . . . y, the Single Linkage distance between X and Y is defined as in Equation (4.6).

SingleLinkageDistance(X

where dist(X i ,Y j ) is the distance between the i'th vector in X and the j'th vector in Y stated in terms of whatever metric is being used, such as Euclidean distance. The Single Linkage distances between all unique pairs of the p vectors remaining to be clustered are calculated, and the pair with the smallest distance is joined. This is exemplified for the three clusters of The arrowed lines in Figure

where dist(X i ,Y j ) is the distance between the ith vector in X and the jth vector in Y stated in terms of whatever metric is being used, such as Euclidean distance. The Complete Linkage distances between all unique pairs of the p vectors remaining to be clustered are calculated, and the pair for which the Complete Linkage distance is smallest is joined. This is exemplified for the three clusters of The arrowed lines in Figure

where dist is defined as above. The centroid distances between all unique pairs of the p vectors remaining to be clustered are calculated,and the pair for which the distance is smallest is joined.

The centroid distances for all unique pairings of the p clusters are calculated using the proximity matrix, and the pair for which centroiddistance(A, B) is smallest is joined. This is exemplified for the three clusters of The arrowed lines in 4.32a represent distances between centroids in cluster pairs (A, B), (A,C), and (B,C), which are shown as crosses; the one between A and B is shortest, so these two clusters are joined, as in Figure

where dist is defined as previously; note that distances of objects to themselves are not counted in this calculation, and neither are symmetric ones on the grounds that the distance from, say X i to Y j is the same as the distance from Y j to X i . -Increase in Sum-of-Squares Linkage (Ward's Method) defines the degree of closeness between any pair of clusters (X ,Y ) in terms of minimization of variability using an objective function. To describe it, two measures need to be defined. The error sum of squares (ESS) is the sum of squared deviations of the vectors in A from their centroid. If A contains m vectors, then ESS is defined by Equation (4.10):

The total error sum of squares (TESS) of a set of p clusters is the sum of the error sum of squares of the p clusters. At each step in the treebuilding sequence, the ESS for each of the p clusters available for joining at that step is calculated. For each unique combination of cluster pairs the increase in TESS is observed, and the pair which results in the smallest increase in TESS is joined. Finally, hierarchical variants are standardly divided into agglomerative vs. divisive methods. Agglomerative tree construction was exemplified above: it begins by partitioning the set of data objects so that each member of the set is a cluster on its own, and then builds the tree incrementally by joining pairs of clusters at each step until no more pairs remain and the tree is complete. Because it incrementally builds trees of increasing complexity from simpler components, agglomerative clustering is also called bottom-up clustering. Divisive tree construction begins with a single cluster consisting of all the data objects, and builds the tree incrementally by partitioning that cluster into subtrees at each step until each cluster contains a single data object and no more division is possible, at which point the tree is complete; because it incrementally subdivides a set into subsets, divisive clustering is also known as topdown clustering. Divisive clustering, described in detail by

Though the above tree-building sequence makes it obvious, it nevertheless seems worth making explicit the distinction between the tree generated by a hierarchical analysis and its graphical representation: the tree is the table of joining distances (c) in Table

Because a cluster tree represents constituency only, the sequential ordering of constituents has no interpretative significance. Given the tree in Table 4.9d, for example, any pair of constituents can be rotated about its axis, thereby reversing the sequencing, without affecting its constituency structure, as in Figure

Note also that dendrograms are more often shown sideways, than in the 'icicle' or downward-facing format more familiar to linguists from phrase structure trees. This is a purely practical matter: an icicle format rapidly broadens out as the number of data objects grows, making it impossible to display on a page.

Issues

The main and considerable advantage of hierarchical clustering is that it provides an exhaustive and intuitively accessible description of the proximity relations among data objects, and thereby provides more information that a simple partitioning of the data generated by the non-hierarchical methods covered thus far. It has also been extensively and successfully used in numerous applications, and is widely available in software implementations. There are, however, several associated problems.

-How many clusters? Given that a hierarchical cluster tree provides an exhaustive description of the proximity relations among data objects, how many clusters do the data 'really' contain? As already noted, it is up to the user to decide. Looking at a dendrogram like the one in Figure

In Figure

One way of constraining tree selection is to observe that there is a fundamental difference between the Single Linkage criterion and the oth-ers listed above: the latter are distance-based clustering methods, and the former is a density-based one. Complete Linkage, Average Linkage, Centroid Linkage, and Ward's Method all build clusters on the basis of linear distance between data points and cluster centres relative to the coordinates of the metric space in which the data is embedded.

The result is a tree each level of which is a Voronoi partition of the data space: starting at the root, the first level divides the space into two partitions each of which contains the data points closer to their centre than they are to the centre of the other partition, the second level divides the space into four such partitions, and so on. This partitioning is, moreover, done without reference to the density structure of the data. Single Linkage, on the other hand, builds clusters solely on the basis of local neighbourhood proximity and without reference to cluster centres; it is to the other kinds of hierarchical clustering, therefore, as Dbscanis to k-means. As such, the expectation is that the non-Single Linkage group will, like k-means, correctly identify the cluster structure of data when its dense regions are linearly separable but not otherwise, whereas Single Linkage will be able to identify non-linearly separable clusters.  This difference between Single Linkage clustering and the others underlies the commonly-made observation in the literature that the non-Single Linkage criteria have a strong predisposition to find roughly spherical clusters in data even where clusters of that shape are known not to be present or indeed where the data are known not to have any meaningful cluster structure at all; see for example

Where this is not known, selection of the best cluster tree, that is, the one which best captures the intrinsic cluster structure of the data, must be guided by the cluster validation methods discussed later in this chapter. -Outliers and noise. All the joining criteria are affected by outliers and noise to different degrees and for different reasons; see for example

Developments

Hierarchical methods are not parameterized, and so initialization is not a problem, but cluster shape, data size, noise, and outliers are.

-Cluster shape. As noted, all the hierarchical methods except Single Linkage are predisposed to find linearly separable clusters in data even where the data do not actually contain that structure or indeed any cluster structure at all. The obvious solution is to use Single Linkage instead, but as we have seen this has a predisposition to generate uninformative chained structures when the data contains noise. Ideally, one would want a hierarchical method which is not limited to linearly separable structures on the one hand and not compromised by chaining on the other. CURE

-Linear separability. The non-single linkage hierarchical methods are limited to data with a linearly-separable density structure because the Minkowski distance measures that the literature associates with them are linear. These methods neither know nor care how the values in the distance matrices on the basis of which they construct cluster trees were derived, and there is no obstacle in principle to using values generated by a nonlinear metric like the geodesic one described in the discussion of data geometry. This allows non-linearly separable regions to be separated nonlinearly and removes the linear separability limitation on non-single linkage hierarchical methods. Compare, for example, the Average Linkage trees for Euclidean and geodesic distance measures in Figure

As usual, however, there is a caveat. The foregoing discussion of nonlinearity detection pointed out a potential disadvantage of the graph approximation to geodesic distance measurement: that it does not make the distinction between model and noise which the regression-based approach makes, and treats the data matrix as a faithful representation of the domain from which the data was abstracted. Because the graph distance-based approach includes noise, whether random or systematic, in its calculations, this may or may not be a problem in relation to the application in question.

Cluster validation

The fundamental purpose of a cluster analysis method is to identify structure that might be present in data, that is, any non-regular or non-random distribution of points in the n-dimensional space of the data. It is, however, a commonplace of the cluster analysis literature that no currently available method is guaranteed to provide this with respect to data in general, and the foregoing discussion of a selection of methods confirms this: projection methods based on dimensionality reduction can lose too much information to be reliable, and the linear ones together with k-means and linear hierarchical methods fail to take account of any nonlinearity in the data; the reliability of the SOM, k-means, and Dbscan depends on correct parameterization; different hierarchical joining criteria can assign data points to different clusters and typically impose different constituency structures on the clusters. In addition, some methods impose characteristic cluster distributions on data even when the data are known not to contain such distributions or indeed to have any cluster structure at all.  Except for the limiting case where each point is regarded as a cluster on its own, the distribution in Figure

Given these sources of unreliability, validation of cluster analytical results is required. One obvious and often-used approach to validation is to generate a series of results using methods based on different clustering criteria in the hope that they will mutually support one another and converge on a consistent solution: if a range of methods based on dimensionality reduction, topology preservation, proximity, and density give identical or at least compatible results, the intuition is that the reliability of the solution is supported by consensus. It would, however, be useful to supplement such a consensus with one or more alternative validating criteria. And, of course, there might not be a consensus, in which case a selection must be made, which implies selection criteria. This section presents a range of quantitative ones.

The discussion is in two parts. The first part considers ways of determining the degree to which any given data has a cluster structure prior to application of clustering methods, known in the literature as 'clustering tendency'. The motivation here is the observation that, if data is known to contain little or no cluster structure, then there is no point to attempting to analyze it, and, if an analysis is carried out despite this, then the result must be an artefact of the method. The second part then presents a range of validation criteria for results from application of different analytical methods to data known to contain cluster structure.

Clustering tendency

Figure

Graphical tests for clustering tendency

Where data are two or three-dimensional they can be scatter-plotted directly, and visual interpretation of the plot will reveal the presence or absence of structure. It is usual in the literature to express distrust in the subjectivity of graphical interpretation, but this subjectivity is a matter of degree. It would be unreasonable to deny that Figure

Where the data is high-dimensional it can be reduced to dimensionality 2 or 3 for scatter-plotting, but, depending on the intrinsic dimensionality of the data, this might lose too much information to provide a reliable result. Visual Assessment of cluster Tendency (VAT)

The very distinct clusters of Figure

Statistical tests for clustering tendency

Statistical identification of clustering tendency in data is based on testing the null hypothesis that the data are random and therefore have no cluster structure. This testing is done by comparing the data to what they would look like if they were random relative to some criterion. If the criterion indicates that the data deviate sufficiently from randomness, then the null hypothesis is falsified and the alternative hypothesis, that the data have cluster structure, is adopted with some specified degree of confidence. Otherwise, the null hypothesis stands and the conclusion is that the data very probably contain no cluster structure.

There are several ways of defining randomness in data

Relative to some m x n matrix M, where m is the number of data objects and n the dimensionality, the Hopkins statistic determines whether the distribution of the m objects in the n-dimensional space deviates significantly from a random distribution of objects in that space. It does this on the basis of two measurements. The first measurement randomly selects k < m row vectors of M, and for each of these k vectors the distance to one or more of its nearest neighbours in M is calculated using some distance metric; the k nearest-neighbour distances are then summed and designated p. The second measurement generates k vectors randomly distributed in the data space, and the nearest-neighbour distances between each of these k vectors to the row vectors of M are calculated and summed, and designated q. The Hopkins statistic H is then defined by Equation (4.11):

H is calculated multiple times and summed, each time using a new random selection of k data vectors and a new set of k random vectors, and the mean of the sum is taken to be an indication of how far the distribution is from randomness. The reasoning here is as follows. If the data distribution is random then p and q will, on average, be the same, and the above formula gives 0.5; this is the baseline value for random data. If the data distribution is not random then p will on average be smaller than q because the nearestneighbour distances between clustered points in a data space are on average smaller than between randomly-distributed ones. In such a case the value of the (p + q) term is smaller than when p = q, which gives increasingly larger H-values as p approaches 0 up to a maximum H = 1.

The mean value of H was calculated for both the distributions of Figure

For general discussions of clustering tendency see

Validation

We have seen that clusters can have a variety of shapes, and that clustering methods do not cope equally well with identifying different varieties. Because each method has different criteria for identifying clusters, each has a predisposition to find certain kinds of structure, and this predisposition can prevent it from identifying other kinds of structure or cause it to impose the structure it is designed to find on data

One possible response to this is that it doesn't matter when, as here, cluster analysis is used as a tool for hypothesis generation, because the aim is to stimulate ideas which lead to hypotheses about the research domain from which the clustered data was abstracted, and a poor hypothesis based on a poor analysis can be falsified and eliminated in subsequent hypothesis testing. There is no obvious objection to this in principle, but in practice it is inefficient. A more efficient alternative is to base hypothesizing on clustering results in whose accuracy one can have a reasonable degree of confidence. Cluster analysts have increasingly taken the latter view and have developed a general methodology for providing that confidence, whereby a given data set is multiply analyzed using different clustering methods and different parameterizations of each method, and the result which best represents its intrinsic cluster structure is selected. The problem, of course, is knowing which is best; this section outlines the current state of cluster validation strategies and methods designed for that purpose.

A traditional and commonly-used criterion for cluster validation is domain knowledge, whereby a subject expert selects the analysis which seems most reasonable in terms of what s/he knows about the research area. This has some validity, but it is also highly subjective and runs the risk of reinforcing preconceptions and discounting the unexpected and potentially productive insights which are the prime motivation for use of cluster analysis in hypothesis generation, as already noted. Domain knowledge needs to be supported by objective criteria for what counts as 'best'; the remainder of this section outlines such criteria.

The foregoing account of clustering tendency has already dealt with part of the problem of cluster validation: that the given data are random, and that any clustering result is therefore an artefact of the analytical method used. The discussion to follow assumes that the result to be validated is based on data known to be sufficiently non-random to be meaningfully clustered, and that the aim of validation is to determine how close it has come to identifying the non-random structure, that is, to finding the correct number of clusters on the one hand, and to assigning the correct data objects to each of the clusters on the other. The literature contains numerous methods for doing this. Decades ago

Note that the methods described in what follows apply only to so-called 'crisp' clustering, where each data object is assigned to one and only one cluster, and not to fuzzy clustering, where a data object may belong to one or more cluster; fuzzy clustering

Cluster validation with respect to cluster compactness and separation

The validation methods in this category assess a given clustering in terms of how well it adheres to the proximity-based cluster definition with which this chapter began: a cluster is an aggregation of points in the test space such that the distance between any two points in the cluster is less than the distance between any point in the cluster and any point not in it. They are therefore applicable only when the underlying data density structure is linearly separable. The two criteria most often used are compactness and separation -cf., for example,

-Compactness. Compactness is the degree to which the members of a proposed cluster are similar to one another. The Root Mean Square Standard Deviation (RMSSTD) validity index -cf.

The RMSSTD is then the square root of the mean SS, as in Equation (4.13):

RMSST D = SS kn (4.13) Figure

The essence of the measure remains the same across these specifics, however.

The smaller the distance between clusters and the larger the size of the largest cluster, the less compact and well-separated the clusters are and the smaller the value of Dunn(C). Conversely, the larger the distance between clusters and the smaller the size of the largest cluster the more compact and well separated they are, and the larger the value of Dunn(C). Using average Euclidean distance between and within clusters to measure both cluster separation and compactness, the Dunn index value for the compact and well-separated clusters of Figure

Another traditional index that combines cluster compactness and separation is that proposed by

where δ i and δ j are the average distances of all vectors in clusters c i and c j from their respective cluster centroids, and distance is the distance between the centroids of c i and c j . The δ terms measure the compactness of two clusters in terms of the degree of dispersion of their members, and the Davies-Bouldin (DB) index is therefore the average cluster dispersion to cluster distance ratio. The greater the dispersion and the smaller the distance the less compact and well-separated the clusters, and the larger the value of DB(C).

And, conversely, the smaller the dispersion and the larger the distance the more compact and better-separated the clusters, and the smaller the index value. Applied to the data underlying the plots in Figure

Methods similar to Dunn and Davies-Bouldin are reviewed in

The cophenetic correlation coefficient (

Table

Once the cophenetic distance matrix is constructed it can be compared to the Euclidean distance matrix to see how similar they are and thereby the degree to which the clustering from which the cophenetic matrix was derived preserves the distance relations among the data objects as represented by the Euclidean distance matrix. This can be done in various ways, the simplest of which is to row-wise linearize the two matrices and then to calculate their Pearson correlation coefficient, which gives the cophenetic correlation coefficient; the linearization is shown in Table

The cophenetic correlation coefficient can be used to select the best from a set of hierarchical analyses of the same data.  As expected, the tree structures in Figure

The cophenetic correlation coefficient is a measure of the strength of the linear relationship between distance and cophenetic matrices, but though it is widely used its validity for assessing the relative goodness of hierarchical clusteringresults has been disputed.

Cluster validation of topology preservation

Preservation of manifold topology differs from preservation of linear distances among the data points which constitute it, as explained earlier, and as such the SOM or any other topology-preserving clustering method cannot be assessed by validation indices designed for distance-preserving methods. This section outlines two alternative indices for the SOM.

The foregoing discussion of the SOM noted that, where the intrinsic dimensionality of data is greater than the dimensionality of the output lattice, some distortion of the topology of the input manifold is to be expected. The discussion also noted that different selections of initial parameter values such as the locations of the Voronoi centroids, lattice size and shape, and different sequencings of training data items can generate different cluster results, and this calls the status of the result as a reliable representation of the intrinsic data structure into question. One approach to SOM assessment is to attempt to identify an objective function which the SOM optimizes and, using that function, to select the optimal result in any specific application, as with MDS, Sammon's Mapping, and Isomap. It has been shown that a general objective function applicable across all SOM parameterizations does not exist

Two alternative criteria that have been developed are quantization error and topology preservation error; for these and others see De Bodt,

-Quantization error. As noted in the discussion of the SOM, the process of finding a centroid vector which is intermediate between the k vectors of a Voronoi partition in the course of SOM training is known as vector quantization: for a set V of k vectors, find a 'reference' vector v r of the same dimension as those in V such that the absolute difference d = |v rv i | between each v i ∈ V and v r is minimized. The SOM training algorithm quantizes the input vectors, and the reference vectors are the result

The one proposed in

where u is a function that returns 0 if the best matching unit and second best matching unit are adjacent in the lattice and 1 otherwise. Selecting the best from a set of SOM analyses generated using different parameter values is a matter of choosing the one that minimizes both the quantization and topology preservation errors. This is exemplified in Fig-

Stability assessment

Stability assessment successively applies a given clustering scheme, that is, some combination of clustering method and parameterization , to a given data matrix and to multiple versions of that matrix which have been perturbed in some way, and observes the behaviour of the cluster result with respect to the original matrix and to each of the perturbed ones. If the cluster result does not change it is stable, and if it does then the result is unstable to proportion to the degree of change. The relative stability of the result is on statistical grounds taken to indicate the extent to which the cluster result captures the intrinsic cluster structure of the data (Ben-Hur, Elisseeff, and Guyon 2002; There are various ways of perturbing data, but

In most practical applications all one has is a single sample S 0 and repeated sampling of the population from which it came is not feasible, but the k-sample sequence can be simulated using bootstrapping, which works as follows. Assume that the data matrix D 0 abstracted from S 0 contains m rows and n columns. Then D 1 is an m x n matrix each of whose rows is randomly selected from D 0 ; repeated selection of the same row is allowed, so D 1 may be but probably is not identical to D 0 . Each successive D 2 . . . D k in constructed in the same way. This looks at first glance like what

Once the D i have been generated, the original data matrix D 0 is clustered to give C 0 , and each of the bootstrap data matrices D 1 , D 2 . . . D k is clustered to give C 1 ,C 2 . . .C k . The similarity of each of the C i to C 0 is then calculated to give an index value I(C i ,C 0 ), the k index values are summed, and the mean of the sum is taken to be a measure of the stability of C: the larger the mean, the greater the similarity of the bootstrapped clusterings to C 0 , and therefore the greater the stability of C 0 across multiple bootstrapped samples. There are many ways of measuring the similarity between clusters

where ∩ is the set-theoretic intersection and ∪ the union of A and B. In the present application, the Jaccard coefficient is used to compare two clusters, A from C 0 and B from C i , and is the ratio of the number of objects which are both in A and B to the number of objects in either A or B or both. For two clusterings C 0 and C i , each cluster in C 0 is paired with the corresponding cluster in C i and the Jaccard index is calculated for each pair; the index values are summed, and the mean of the sum is the Jaccard index for C 0 and C i . The stability index can be used just like the other types of validation index already discussed to look for an optimal clustering scheme by applying the above procedure to each such scheme and then selecting the scheme that generates the most stable clustering result.

Validation was until fairly recently the Cinderella of cluster analysis. Some early work on the topic has been cited; surveys of it are available in

What emerges from these discussions and others like them is that, at present, cluster validation methods are far from foolproof. Much like the clustering methods they are intended to validate, they have biases which arise on account of the assumptions they make about the nature of clusters and about the shapes of the manifolds which clustering purports to describe

The preceding chapters have -identified a research domain: the speech community of Tyneside in north-east England; -asked a research question about the domain: is there systematic phonetic variation in the Tyneside speech community, and, if so, does that variation correlate systematically with social variables? -abstracted phonetic frequency data from the DECTE sample of speakers and represented them as a matrix MDECTE; -normalized MDECTE to compensate for variation in interview length and reduced its dimensionality; -described and exemplified application of a selection of cluster analytical methods to the transformed MDECTE data. The present chapter develops a hypothesis in answer to the research question based on cluster analysis of MDECTE. The discussion is in two main parts: the first part reviews and extends the clustering results presented so far, and the second uses these extended results to formulate the hypothesis.

Cluster analysis of MDECTE

Apart from Dbscan, where variation in data density was a problem, all the cluster analyses in the preceding chapter agreed that there are two highly distinctive clusters in MDECTE: a larger one containing speakers g01 to g56, and a smaller one containing speakers n57 to n63. These are not exhaustively repeated here; the MDS result in Figure

-   The SOM topology preservation indices given in the validation section of the previous chapter indicate that the 25 × 25 lattice given in Figure

For PCA the first two dimensions represented in Figure

Nonhierarchical partitional methods

Application of Dbscan to MDECTE56 required experimentation with numerous combinations of MinPts and E ps parameters to determine the optimal ranges of both, where optimality was judged as lying between combinations which designated all data points as noise points on the one hand, and those which included all data points in a single cluster. The number of clusters is not prespecified for Dbscan but is inferred from the density structure of the data, and for no combination of parameters was anything other than a twocluster structure found. The optimal range for MinPts was found to be 2 . . . 5, and for E ps 59 . . . 61; the partition given for MinPts = 4 and E ps = 60 in Table

The relationship between the k-means and Dbscan results is straightforward:

-For k = 2 Dbscan cluster 1 is a subset of k-means cluster 1, and Dbscan cluster 2 is a subset of k-means cluster 2, with the exception of g04 which is in different clusters in Dbscan and k-means. -For k = 3 the result is the same as for k = 2 in that none of the k-means cluster 2 data points are in either Dbscan cluster.  The initial visual impression is that these trees are all very different except for the Euclidean and geodesic Single Linkage ones, which are identical, but closer examination of them reveals regularities common to them all: among others, for example, the sets of data points (4 14 19 24 27 31 46 48 50 51) and

Hierarchical partitional methods

Comparison of results

Having applied the different clustering method categories to MDECTE56 individually, the next step is to correlate the results in order to determine the degree to which they are mutually supporting with respect to the cluster structure of the data.

Comparison of projection and nonhierarchical partition results

Comparison of the k-means k = 2 and k = 3 results with the projection ones are dealt with separately, beginning with the former; because the Dbscan clustering is a subset of the k-means k = 2 one, the k-means k = 2 comparison subsumes it and there is consequently no need to deal with it separately.  show the k-means partition for k = 2 on each of the projections, using the '*' symbol to represent data points belonging to k-means cluster 1, and '+' for k-means cluster 2.       In each case k-means for k = 2 partitions the projections into two regions, which are demarcated by dashed lines for convenience of reference. This is unsurprising because, as we have seen, k-means clusters linearly separable regions of the data space, and the regions of the projections in Figures 5.11-5.15 can be regarded as linearly separable when the distortions introduced by the projection methods are taken into account.

As can be seen, k-means k = 3 differs from k = 2 only in dividing the k = 2 cluster 2 into two subclusters. Shown in relation to the PCA projection in Figure

Comparison of projection and nonhierarchical partition with hierarchical clustering results

In this section the hierarchical analyses are correlated with the Dbscan and k-means partitions of MDECTE56. Figures 5.17    Using the additional information that the correlation with the Dbscan and k-means provides, the relationships among the various trees becomes clearer.

-The Euclidean and geodesic distance based Single Linkage trees are identical, and differ from all the other trees in their characteristic chained structure. That chaining keeps all the data points corresponding to those in Dbscan cluster 1 and all the points corresponding to those in Dbscan cluster 2 together, and the rest of the structure is an apparently random mix of remaining points from the k-means clusters.

Single Linkage has, in other words, identified the same clusters as Dbscan, which is unsurprising because, as noted earlier, Single Linkage is a density-based clustering method unlike the other varieties of hierarchical analysis, which are distance-based. -Complete, Average, and Ward Linkage trees based on Euclidean distance are very similar though not identical, and correspond quite closely to the k-means partition for k = 3. In each case there are two main clusters corresponding to k-means clusters 1 and 3 which are, respectively, supersets of Dbscan clusters 1 and 2. There is also a small number of data points clustered separately from the main ones. These latter points are observable as outliers or at the periphery of the main data cloud in the projection plots, and though the selection of points differs somewhat from tree to tree, they correspond essentially to k-means cluster 2; the distributions are shown in Table

Hypothesis formulation

The first part of the research question, Is there systematic phonetic variation in the Tyneside speech community?, can now be answered affirmatively on the basis of the foregoing results. The DECTE speakers fall into two main clusters, a larger one containing speakers g01 . . . g56 and a smaller one containing speakers n57 . . . n63. The g01 . . . g56 cluster itself has a subcluster structure, for which the foregoing analyses have revealed two alternatives. All the projection and nonhierarchical partition results as well as all the hierarchical results apart from Average Linkage based on geodesic distance agree that there are two main subclusters, but that these do not include all the speakers:

1. The Single Linkage tree and Dbscan partition a minority of the data points into two clusters and regard the rest of the data as noise. 2. The Euclidean distance based Complete, Average, and Ward Linkage trees group a small number of speakers separately from the two main clusters in slightly different ways; these speakers are observable at the periphery of the main data cloud in the projection plots or as outliers to it, and correspond substantially to the smallest of the clusters in the k-means result for k = 3. 3. The geodesic distance based Complete and Ward Linkage trees are partially compatible with (2) in partitioning most of the speakers into two main clusters and the remaining speakers into a small one, but differ from (

It remains to consider the second part of the question: Does that variation correlate systematically with associated social variables?. To answer it, the cluster results are supplemented with social data associated with the speakers in the DECTE corpus.

The unanimous partition of the 63 speakers into two subclusters g01 . . . g56 and n57 . . . n63 corresponds to speakers from Gateshead on the south bank of the river Tyne, betokened by the 'g' prefix, and those from Newcastle on the north bank betokened by the 'n' prefix. DECTE does not include any social data for the Newcastle speakers, though surviving members of the original Tyneside Linguistic Survey team have confirmed that the n57 . . . n63 speakers were in fact the academics who comprised the team. The Gateshead speakers, on the other hand, were with a few exceptions working class with the minimum legal level of education and in manual skilled and unskilled employment. The primary clustering of the DECTE speakers therefore has a clear sociolinguistic interpretation based on educational level and employment type.

For the Gateshead subcluster the two alternatives identified above are available for interpretation. Since the aim here is methodological, that is, to exemplify the application of cluster analysis to hypothesis generation, only the first of them is addressed, though clearly the second would also have to be considered if the aim were an exhaustive investigation of the Geordie dialect as represented by DECTE . For clarity of presentation, the hierarchical result with the best cophenetic index, the Average Linkage tree based on Euclidean distance, is used as the basis for discussion.

Initially the full range of social variables provided by DECTE was correlated with the Average Linkage tree, and variables with no discernible systematic correlation with the tree structure were eliminated, The surviving variables are shown in Figure

The two main clusters, labelled C and D in Figure

The second part of the research question can now also be answered affirmatively on the above evidence. The primary distinction in phonetic usage in DECTE separates a small group of highly-educated, middle-class Newcastle professionals from a much larger Gateshead group whose members were predominantly speakers in manual employment and with a minimal or sometimes slightly higher level of education. The indication is that the primary differentiating factor among the Gateshead speakers is gender, though the existence of cluster B suggests that educational level and type of employment are also factors.

The hypothesis developed thus far can be augmented by identification of the phonetic segments which are chiefly responsible for differentiating the DECTE speakers into clusters. This is done using cluster centroids, as introduced in Chapter 2. The centroid of any given cluster represents the average usage of its constituent speakers; variable-by-variable comparison of any two centroids reveals the degree to which the clusters differ, on average, for every variable, and allows those with the largest differences to be identified. Centroids for successive pairs of clusters were calculated and compared using bar plots to represent the comparisons graphically. Figure

There is systematic phonetic variation in the Tyneside speech community, and this variation correlates in a sociolinguistically significant way with educational level, employment type, and gender. The phonetic segments shown in As already noted, a full rather than the present expository study of DECTE would have to go into greater detail, investigating such matters as the variation in the cluster structuring and allocation of speakers to clusters found in the foregoing discussion, the possible relevance of additional social factors, and centroid analysis of additional subcluster differences. The next step is to test this hypothesis with respect to data drawn from speakers not included in DECTE .

In principle, this discussion of hypothesis generation based on cluster analysis of DECTE should stop here, but it happens that existing work on the phonetics of Tyneside English provides results based on samples apart from the TLS component of DECTE

Variation in the GOAT vowel is a prominent feature of Tyneside English. Basing their results on the somewhat more recent PVC component of DECTE ,

-PVC [o:] / DECTE [O:] is the unmarked variant preferred by all speakers apart from working-class males in the PVC corpus. -PVC [8:] / DECTE [A:] is the almost exclusive preserve of working class males, and is described by

All of these are consistent with the relevant column pairs in Figures 5.22 These confirmations indicate that cluster analysis has generated an empirically supportable hypothesis in the present case, and suggest that it can do so both for other phonetic segments in DECTE and in corpus linguistic applications more generally. This chapter reviews the use of cluster analysis in corpus linguistics to date. As such, the following review sets a tractable limit on the literature it surveys, and that limit is the literature specific to corpus linguistics. The Introduction defined corpus linguistics for the purposes of the present discussion as a methodology in the service of the science of language. This implic-itly excludes a range of language technologies such as information retrieval, document classification , data mining, and speech processing, as well as areas of artificial intelligence like natural language generation / understanding and machine learning. These technologies work with natural language text and speech and thereby have much in common methodologically with corpus linguistics, including application to cluster analysis to text corpora; indeed, many of the concepts and techniques presented in the foregoing chapters come from their literatures. Their aims are, however, not language science but language engineering, and they therefore fall outside the self-imposed remit of this review. Also excluded on these grounds are quantitative stylometry and author attribution, whose aims are literary rather than linguistic.

The review is in two main parts: the first part outlines work on quantitative methods in corpus linguistics to serve as a context for the second, which deals with the cluster analytic work specifically.

Context

The hypothesis generation methodology described in the foregoing chapters is intended as a contribution to corpus linguistics, whose remit the Introduction described as development of methodologies for creating collections of natural language speech and text, abstracting data from them, and analysing those data with the aim of generating or testing hypotheses about the structure of language and its use in the world. This is a not-uncontroversial position. The literature draws a distinction between corpus-driven and corpusbased linguistics, where the former is taken to be a scientific paradigm in the sense that behaviourist and generative linguistics are paradigms, that is, ontologies in terms of which linguistic theories can be stated, and the latter to be a methodology, that is, a collection of techniques for the formulation and testing of hypotheses within some already-existing paradigm. The present discussion does not engage with this debate, and simply adopts the corpus-based view of corpus linguistics; for recent discussions see

Despite its relatively well-developed state, corpus linguistic methodology has historically been unevenly distributed across the linguistics research community as a whole. This is especially the case for application of quantitative methods: simple quantifications such as frequencies, means, and percentages together with uni-and bivariate graphs are fairly extensively represented in the literature, but application of anything more complex than that is much rarer.

In grammatical linguistics, here understood as the study of the architecture of human language, statistical analysis of data derived from empirical observation was part of the standard methodology in the heyday of behaviourism in the first half of the twentieth century. Linguists at this time studied ways of inferring grammatical structure from linguistic performance using distributional information extracted from natural language corpora. A major figure in this approach to linguistics was Zellig Harris, who in a series of publications -for example

The eclipse of corpus-based methodology is characteristic of linguistics as practised in the United States and Britain. In continental Europe the application of mathematical and statistical concepts and methods to analysis of corpus data for derivation of linguistic laws has continued to be developed. An exhaustive listing of the relevant earlier European literature, most of which is not in English and therefore little known in the English-speaking linguistics world, is given in

Quantitative methods have, moreover, always been central in some linguistics subfields concerned with the architecture of language: quantitative analysis of empirical and, increasingly, corpus-derived data is fundamental in psycholinguistics -cf.

Variationist linguistics, here understood as the study of how language use varies in chronological, social, and geographical domains, is fundamentally empirical in that it uses data abstracted from observation of language use either to infer hypotheses about patterns of linguistic variation in or to test hypotheses about a language community. These three domains have historically been the preserves of historical linguistics, sociolinguistics, and dialectology respectively, though there is substantial overlap among them. Because they are based on analysis of data abstracted from language use, all three are naturally suited to quantitative and more specifically statistical methods, and, as corpus linguistics has developed in recent years, the corresponding research communities have increasingly adopted it, albeit unevenly. This unevenness is particularly evident with respect to quantitative methods, as sampling of the recent literatures in a way analogous to that for grammatical linguistics above testifies. are:

-The use of quantitative methods in chronological variation research has been concentrated in linguistic phylogeny, the study of relatedness among languages. This study has historically been associated primarily with the Indo-European group of languages, and was throughout the nineteenth and most of the twentieth centuries dominated by the Neogrammarian paradigm. This paradigm assumed that the Indo-European language interrelationships and, latterly, language interrelationships in general could be modelled by acyclic directed graphs, or trees, and used the Comparative Method to construct such trees. The Comparative Method has been and remains largely non-quantitative; quantitative comparative methods were first introduced in the midtwentieth century via lexicostatistics, which involves calculation of proportions of cognates among related languages, and glottochronology, which posits a constant rate of linguistic change analogous to that of biological genetic mutation -cf.

2),

-Textbooks, monographs, and tutorial papers:

Because many modern-day corpus linguists have been trained as linguists, not statisticians, it is not surprising that they have been reluctant to use statistics in their studies. Many corpus linguists come from a tradition that has provided them with ample background in linguistic theory and the techniques of linguistic description but little experience of statistics. As they begin doing analyses of corpora they find themselves practising their linguistic tradition in the realm of numbers, the discipline of statistics, which many corpus linguists find foreign and intimidating. As a consequence, many corpus linguists have chosen not to do any statistical analysis, and work instead with frequency counts. . .

Cluster analysis in corpus linguistics

Several of the above-cited textbooks and monographs on quantitative methods in linguistics include accounts of cluster analysis and its application. The following discussion of specific application areas adopts the foregoing categorization of subdisciplines into grammatical and variationist linguistics, and the latter into social, geographical, and chronological variation .

Cluster analysis in grammatical research

In the aftermath of Chomsky's critique of empirical approaches to the study of language in his 1959 review of Skinner's Verbal Behaviour

Empirical work continued to be done, however, and some of it used cluster analysis.

It was not until the later 1980s, however, that there was a resurgence of interest in empirical and more specifically statistical approaches among linguists as a consequence partly of the advent of 'connectionist' cognitive science with its emphasis on the empirical learning of cognitive functions by artificial neural networks, partly of the success of stochastic methods like Hidden Markov Models in the natural language and speech processing communities, and partly of the increasing availability of large-scale digital electronic natural language corpora from which to extract distributional information reliably, including corpora augmented with grammatical information such as the Penn Treebank and WordNet. Since ca. 1990 the volume of empirical corpus-based linguistic research has increased quite rapidly, as sketched above, and with it the use of cluster analysis. Most of the work has concentrated on lexis, but there is also some on phonology, syntax, and language acquisition

Over the past two decades or so a large amount of work has been done on inference of grammatical and semantic lexical categories from text (Korhonen 2010), driven mainly by the requirements of natural language processing tasks like computational lexicography, parsing, word sense disambiguation, and semantic role labelling, as well as by language technologies like information extraction, question answering, and machine translation systems. This work is based on the intuition that there are restrictions on which words can co-occur within some degree of contiguity in natural language strings, and that the distribution of words in text can therefore be used to infer grammatical and semantic categories and category relatedness for them. This intuition underlies the "distributional hypothesis" central to Harris' work on empirically-based inference of grammatical objects

Representative examples of more recent work in this area are:

-

Work in other areas of grammar involving cluster analysis is less extensive, as noted.

Cluster analysis in chronological variation research

As noted, the use of quantitative methods in chronological variation research has been concentrated in linguistic phylogeny, and application of cluster analysis in this subfield shares that orientation.

Examples of earlier work are as follows.

Since about 2000, language classification has increasingly been associated with cladistic language phylogeny, and cluster analysis is one of a variety of quantitative methods used in cladistic research. The general focus in this work is not so much achievement of a definitive result for some language group, but methodological discussion of data creation and clustering issues.

-

As diachronic corpora have begun to appear, so have applications of cluster analysis in chronological variation research apart from language phylogeny

Cluster analysis in geographical variation research

Applications of cluster analysis in geographical variation research are associated primarily with the universities of Salzburg and Groningen. These are reviewed first, and work by other individuals and groups is covered subsequently.

In a series of articles from 1971 onwards, Goebl developed a dialectometric methodology which culminated in three books: Dialektometrie. Prinzipien und Methoden des Einsatzes der Numerischen Taxonomie im Bereich der Dialektgeographie

Since the late 1990s Nerbonne and his collaborators have developed a methodology based on the dialectometry pioneered by Séguy and Goebl, whose defining characteristics are creation of data based on quantification of observed language use and multivariate analysis of that data, and which incorporates the most sophisticated application of cluster analysis in current linguistic research. To gain an understanding of that methodology a good place to begin is with

Quantification of observed language use

The discussion in the foregoing chapters of this book has been based on quantification of observed language use in terms of the vector space model: a set of n variables was defined to describe a set of m objects -in the case of DECTE , speakers -in the research domain, and the frequency of occurrence of each of the variables in the domain for each of the objects or speakers was recorded. This approach to quantification has been used in dialectometry by, for example,

Given its centrality in the Groningen group's methodology, the Levenshtein distance is described in greater or lesser degrees of detail in many of its publications; see for example

Multivariate analysis

Multivariate analysis in dialectometric terms is the simultaneous use of multiple linguistic features to infer dialect distributions from data. The Groningen group describes this simultaneous use of linguistic features as 'aggregation', and contrasts it with the traditional dialectological procedure of analyzing the distribution of a single or at most a very few features. The reasons for the group's preference for aggregation-based analysis are made explicit in

Cluster analysis is an important class of multivariate method, and has been fundamental to the Groningen methodology from the outset; see

For the period before 2000, the earliest example of the use of cluster analysis for geographical variation research appears to be that of

Examples of work since 2000 are:

Cluster analysis in social variation research

Several researchers have used cluster analysis for sociolinguistic interpretation of data relating to variation in phonetic usage among speakers. In chronological order: -

-

Recent research has seen the development of sociolectometry as a parallel to dialectometry, which studies lexical variation across language varieties in relation to social factors using vector space models based on the distributional hypothesis referred to earlier, and a variety of clustering methods such as multidimensional scaling, principal component analysis, hierarchical clustering, and clustering by committee

The foregoing discussion has proposed cluster analysis as a tool for generating linguistic hypotheses from natural language corpora. The motivation for doing so was practical: as the size and complexity of corpora and of data abstracted from them have grown, so the traditional paper-based approach to discerning structure in them has become increasingly intractable, and cluster analysis offers a solution. Hypothesis generation based on cluster analysis has two further advantages in terms of scientific methodology, however. These are objectivity and replicability

-Objectivity. The question of whether humans can have objective knowledge of reality has been central in philosophical metaphysics and epistemology since Antiquity and, in recent centuries, in the philosophy of science. The issues are complex, controversy abounds, and the associated academic literatures are vast -saying what an objective statement about the world might be is anything but straightforward, as Chapter 2 has already noted. The position adopted here is that objective knowledge is ultimately impossible simply because no observation of the natural world and no interpretation of such observation can be independent of the constraints which the human cognitive system and the physiological structures which implement it impose on these things. On this assumption, objectivity in science becomes a matter of attempting to identify sources of subjectivity and to eliminate them as factors in formulating our species-specific understanding of nature. An important way of doing this in science is to use generic methods grounded in mathematics and statistics, since such methods minimize the chance of incorporating subjective assumptions into the analysis, whether by accident or design. -Replicability is a foundational principle of scientific method. Given results based on a scientific experiment, replicability requires that the data creation and analytical methods used to generate those results are sufficiently well specified to allow the experiment to be reproduced and for that reproduction to yield results identical to or at least compatible with the originals. The obvious benefit is elimination of fraud.

There have been and presumably always will be occasional cases of scientific fraud, but this is not the main motivation for the replicability requirement. The motivation is, rather, avoidance of error: everyone makes mistakes, and by precise and comprehensive specification of procedures the researcher enables subject colleagues suspicious of the validity of results to check them. Cluster analysis and the associated data representation and transformation concepts are objective in the above sense in that they are mathematically grounded, and analyses based on them are replicable as a consequence in that experimental procedures can be precisely and comprehensively specified. In the foregoing search for structure in the DECTE corpus the initial selection of phonetic variables was subjective, but the data representation and the transformation methods used to refine the selection were generic, as were the clustering methods used to analyze the result. The data and clustering methodology was, moreover, specified precisely enough for anyone with access to the DECTE corpus to check the results of the analysis.

Given the methodological advantages of cluster analysis for hypothesis generation, the hope is that this book will foster its adoption for that purpose in the corpus linguistics community. This Appendix lists software implementations of the clustering methods presented earlier. The coverage is not exhaustive: only software known to the author to be useful either via direct experience or online reviews is included.

8.1

Cluster analysis facilities in general-purpose statistical packages

Most general-purpose statistics / data analysis packages provide some subset of the standard dimensionality reduction and cluster analysis methods: principal component analysis, factor analysis, multidimensional scaling, k-means clustering, hierarchical clustering, and sometimes others not covered in this book. In addition, they typically provide an extensive range of extremely useful data creation and transformation facilities. A selection of them is listed in alphabetical order below; URLs are given for each and are valid at the time of writing. A similar case for programming is made by

There are numerous programming languages, and in principle any of them can be used for corpus linguistic applications. In practice, however, two have emerged as the languages of choice for quantitative natural language processing generally: Matlab and R. Both are high-level programming languages in the sense that they provide many of the functions relevant to statistical and mathematical computation as language-native primitives and offer a wide range of excellent graphics facilities for display of results. For any given algorithm, this allows programs to be shorter and less complex than they would be for lower-level, less domain-specific languages like, say, Java or ••, and makes the languages themselves easier to learn.

Matlab ( ØØÔ »»ÛÛÛºÑ Ø ÛÓÖ ×ºÓºÙ ») is described by its website as "a high-level language and interactive environment for numerical computation, visualization, and programming". It provides numerous and extensive libraries of functions specific to different types of quantitative computation such as signal and image processing, control system design and analysis, and computational finance. One of these libraries is called "Math, Statistics, and Optimization", and it contains a larger range of dimensionality reduction and cluster analysis functions than any of the above software packages: principal component analysis, canonical correlation, factor analysis, singular value decomposition, multidimensional scaling, Sammon's mapping, hierarchical clustering, k-means, self-organizing map, and Gaussian mixture models. This is a useful gain in coverage, but the real advantage of Matlab over the packages is twofold. On the one hand, Matlab makes it possible for users to contribute application-specific libraries to the collection of language-native ones. Several such contributed libraries exist for cluster analysis, and these substantially expand the range of available methods. Some examples are: ØØÔ »»ÛÛÛº ×º ÙØº »ÔÖÓ Ø×»×ÓÑØÓÓÐ ÓÜ» On the other hand, because the user has access to the program code both for the native Matlab and the contributed libraries, functions can be modified according to need in line with current research requirements, or, as a last resort, the required functions can be written ab initio using the rich collection of already-existing mathematical and statistical ones. Finally, there is a plethora of online tutorials and Matlab textbooks ranging from introductory to advanced, so accessibility is not a problem. R ( ØØÔ »»ÛÛÛºÖ¹ÔÖÓ ØºÓÖ ») is described by its website as "a free software environment for statistical computing and graphics", and it has the same advantages over the clustering software packages as Matlab. R provides an extensive range of dimensionality reduction and cluster analysis functions, which are listed at the following websites:

- Corpus linguistic-specific applications using clustering 311

There are numerous online tutorials for R generally and for cluster analysis using R specifically, as well as a range of textbooks. Of particular interest to corpus linguists are:

In recent years R has been emerging as the preferred language for quantitative natural language processing and corpus linguistics, not least because, unlike Matlab, it is available online free of charge.

8.4

Corpus linguistic-specific applications using clustering

The dialectometric methodology of the Groningen group, described in the foregoing literature review, has been implemented in Gabmap, described as "a Web application aimed especially to facilitate explorations in quantitative dialectology -or dialectometry -by enabling researchers in dialectology to conduct computer-supported explorations and calculations even if they have relatively little computational expertise"

composite 11, 204-208 consistency 178 cophenetic distance 242 criterion 61, 85, 226 cluster merge, 211 Dbscan see also

S