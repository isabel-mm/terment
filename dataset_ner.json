[
    {
        "text": "For three of the verbsencourage, support, and fear -use in one of the searched constructions is rare (less than 1% of occurrences of the verb) (continued) 7 Analyzing Co-occurrence Data 151 and not listed as a possible form in the corpus-based Collins Cobuild English Language Dictionary.",
        "entities": [
            [
                231,
                243,
                "TERM"
            ]
        ]
    },
    {
        "text": "Each time you run the search, even with the same keyword (by choosing the button \"KWIC\" in the top left corner of COCA), it will display a different set of randomly selected examples.",
        "entities": [
            [
                49,
                56,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are many different descriptive and theoretical frameworks that are used in corpus linguistics.",
        "entities": [
            [
                81,
                99,
                "TERM"
            ]
        ]
    },
    {
        "text": "The body of the text mainly contains tags, which are destined to delimit text units such as paragraphs or even sentences.",
        "entities": [
            [
                16,
                20,
                "TERM"
            ],
            [
                73,
                77,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this small sample, we not only see differences in nouns (\"English\" is the third most frequent word in the individual corpus and \"students\" is the fourth most frequent word in the collaborative corpus) but also differences in function words (\"in\" is the fourth most frequent word in the individual texts and \"of\" is the fifth most frequent words in the collaborative texts).",
        "entities": [
            [
                120,
                126,
                "TERM"
            ],
            [
                196,
                202,
                "TERM"
            ],
            [
                14,
                20,
                "TERM"
            ]
        ]
    },
    {
        "text": "First, any text, spoken or written, will lose not only its communicative context (the discourse of which it was originally a part), but also some of its linguistic and paralinguistic properties when it becomes part of a corpus.",
        "entities": [
            [
                220,
                226,
                "TERM"
            ],
            [
                11,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "The special meaning of the angled brackets < and > means that actual less-than or greater-than signs in the text itself must be represented by the special codes &lt; and &gt;.",
        "entities": [
            [
                108,
                112,
                "TERM"
            ]
        ]
    },
    {
        "text": "Following this brief introduction, Section 2 explores the state of the art in collocation research, on the basis of which Section 3 presents a cross-linguistic study of the collocational behavior and semantic prosodies of a group of near synonyms in English and Chinese.",
        "entities": [
            [
                78,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "This will return you to the basic single-corpus COCA interface.",
        "entities": [
            [
                41,
                47,
                "TERM"
            ]
        ]
    },
    {
        "text": "First, the corpus should contain French literature, which restricts the literary subject to works written in original French, rather than translations.",
        "entities": [
            [
                11,
                17,
                "TERM"
            ]
        ]
    },
    {
        "text": "Instead of a list of collocates displayed in a table (the usual format), the graph shows the relationship between the node and the collocates by displaying collocates closer to or further apart from the node.",
        "entities": [
            [
                118,
                122,
                "TERM"
            ],
            [
                203,
                207,
                "TERM"
            ]
        ]
    },
    {
        "text": "English discourse particles -evidence from a corpus.",
        "entities": [
            [
                45,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, before, say, \"pressing down\" can be used as an operational definition, at least three questions need to be asked: first, what type of object is to be used for pressing (what material it is made of and what shape it has); second, how much pressure is to be applied; and third, how the \"difficulty\" of pressing down is to be determined.",
        "entities": [
            [
                135,
                139,
                "TERM"
            ]
        ]
    },
    {
        "text": "To do this, they search multiple linguistic variables at the same time in a corpus.",
        "entities": [
            [
                76,
                82,
                "TERM"
            ]
        ]
    },
    {
        "text": "In contrast, the inductive approach can be applied to a large data set because it requires no a priori annotation.",
        "entities": [
            [
                103,
                113,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is necessary to neutralize the differences in the type of data used in order to bring out the differences between languages.",
        "entities": [
            [
                53,
                57,
                "TERM"
            ]
        ]
    },
    {
        "text": "Given the straightforward logic underlying the notion of dispersion, the huge impact it can have, and the fact that dispersion can correlate as strongly as frequency with experimental data (see Gries 2010c), dispersion and corpus homogeneity should be at the top of the to-do list of research on corpus-linguistic statistics.",
        "entities": [
            [
                223,
                229,
                "TERM"
            ],
            [
                296,
                302,
                "TERM"
            ]
        ]
    },
    {
        "text": "During the earliest period of the current research scope, between 1997 and 2001, the research topics related to new perspectives on grammar were the central issue in corpus linguistics.",
        "entities": [
            [
                166,
                184,
                "TERM"
            ]
        ]
    },
    {
        "text": "Then, there is a difference between diachronic corpora and synchronic corpora.",
        "entities": [
            [
                59,
                69,
                "TERM"
            ],
            [
                36,
                46,
                "TERM"
            ]
        ]
    },
    {
        "text": "For many questions, the raw data retrieved from a corpus will not be sufficient.",
        "entities": [
            [
                50,
                56,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this section, we will consider three examples of text archives -Lexis-Nexis (representing a wide range of similar text archives, such as ProQuest or EBSCO archives, other newspaper archives, or archives like Literature Online or Project Gutenberg), the Web (via Google), and Google Books.",
        "entities": [
            [
                52,
                56,
                "TERM"
            ],
            [
                117,
                121,
                "TERM"
            ]
        ]
    },
    {
        "text": "Starting our investigation with particles occurring one position to the right of the node verb form, we can choose the position '1 Right' and set the restriction to 'any preposition'.",
        "entities": [
            [
                85,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "In corpora that do not include whole texts but only text samples (like the Brown family) it is therefore important to achieve a balance in terms of the different sections of texts represented.",
        "entities": [
            [
                128,
                135,
                "TERM"
            ],
            [
                52,
                56,
                "TERM"
            ]
        ]
    },
    {
        "text": "With parametric tests, like the Two-Way ANOVA, we can generalize to the population that the sample was drawn from.",
        "entities": [
            [
                92,
                98,
                "TERM"
            ]
        ]
    },
    {
        "text": "In a diachronic corpus of narrative fiction this inevitably affects the frequencies of specific grammatical patterns associated with either descriptive or more action-driven narrative passages.",
        "entities": [
            [
                5,
                15,
                "TERM"
            ],
            [
                16,
                22,
                "TERM"
            ]
        ]
    },
    {
        "text": "Above all, a concordancer is a tool that makes it possible to look up words in their context of use.",
        "entities": [
            [
                13,
                25,
                "TERM"
            ]
        ]
    },
    {
        "text": "For end users to make appropriate use of a corpus it is instrumental that they understand how it has been compiled.",
        "entities": [
            [
                43,
                49,
                "TERM"
            ]
        ]
    },
    {
        "text": "Of course, XML doesn't need to be viewed inside a browser or transformed in any way, but can either be viewed, or even -at least to some extent -meaningfully searched, in one of our basic editors, or manipulated in other ways through programming languages, too.",
        "entities": [
            [
                11,
                14,
                "TERM"
            ]
        ]
    },
    {
        "text": "Our aim in comparing those two types of grammar books is twofold, viz. to find out the similarities and differences between these materials, and see which corpus findings have been included in corpusinformed materials.",
        "entities": [
            [
                155,
                161,
                "TERM"
            ]
        ]
    },
    {
        "text": "Digital corpus is now a known thing to us and we have come to a common agreement to understand what counts as a 'corpus', what are its characteristics, how it can be built and classified, how it can be annotated and processed, and how it can be analyzed and utilized.",
        "entities": [
            [
                8,
                14,
                "TERM"
            ],
            [
                113,
                119,
                "TERM"
            ]
        ]
    },
    {
        "text": "The annotation manual should also list the rules that have been applied to certain borderline or ambiguous cases in the corpus in order to deal with them systematically.",
        "entities": [
            [
                4,
                14,
                "TERM"
            ],
            [
                120,
                126,
                "TERM"
            ]
        ]
    },
    {
        "text": "This method, probably the most widespread corpus-linguistic tool, is the concordance.",
        "entities": [
            [
                73,
                84,
                "TERM"
            ],
            [
                42,
                48,
                "TERM"
            ]
        ]
    },
    {
        "text": "Yet, in order to ascertain the 'spokenness' of a text/corpus until such time, we may often want to handle contractions as single units.",
        "entities": [
            [
                54,
                60,
                "TERM"
            ],
            [
                49,
                53,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this section, we summarize the evidence gathered across the case studies, and show which classification method is expected to give strongest results in similar semi-automatic annotation setups.",
        "entities": [
            [
                178,
                188,
                "TERM"
            ]
        ]
    },
    {
        "text": "The review in this section demonstrates that corpus linguistics has enabled large-scale collocation analysis and foregrounded collocation in linguistic research while corpus-based collocation studies over the past decades have uncovered a range of interesting collocational behavior and semantic prosody which have been hidden from intuition and can only be revealed by examining a large amount of attested data simultaneously.",
        "entities": [
            [
                45,
                63,
                "TERM"
            ],
            [
                167,
                179,
                "TERM"
            ],
            [
                88,
                99,
                "TERM"
            ],
            [
                126,
                137,
                "TERM"
            ],
            [
                180,
                191,
                "TERM"
            ]
        ]
    },
    {
        "text": "Let us define a corpus somewhat crudely as a large collection of authentic text (i.e., samples of language produced in genuine communicative situations), and corpus linguistics as any form of linguistic inquiry based on data derived from such a corpus.",
        "entities": [
            [
                158,
                176,
                "TERM"
            ],
            [
                16,
                22,
                "TERM"
            ],
            [
                245,
                251,
                "TERM"
            ],
            [
                75,
                79,
                "TERM"
            ]
        ]
    },
    {
        "text": "These differences inevitably induce a certain bias towards specific text categories.",
        "entities": [
            [
                68,
                72,
                "TERM"
            ]
        ]
    },
    {
        "text": "Quite commonly one and the same verb takes different kinds of complement with different relative frequencies, such that one type is preferred and other ones are more marginal.",
        "entities": [
            [
                124,
                128,
                "TERM"
            ]
        ]
    },
    {
        "text": "Studies in lexical grammar are currently pulling in two directions, and any research project has to find a balance between the two.",
        "entities": [
            [
                107,
                114,
                "TERM"
            ]
        ]
    },
    {
        "text": "As presented in Chaps. 1 and 2, annotations and metadata are sometimes relatively simple: markup can be added to set them apart from the text and control their inventories.",
        "entities": [
            [
                48,
                56,
                "TERM"
            ],
            [
                90,
                96,
                "TERM"
            ],
            [
                137,
                141,
                "TERM"
            ]
        ]
    },
    {
        "text": "Of particular interest is the sub-corpus from WordPress and Blogger, which includes 95 million words of blog posts and a further 86 million words of reader comments on those posts.",
        "entities": [
            [
                34,
                40,
                "TERM"
            ]
        ]
    },
    {
        "text": "If possible, try to create a corpus that can be published freely under an open license.",
        "entities": [
            [
                29,
                35,
                "TERM"
            ]
        ]
    },
    {
        "text": "These corpora are certainly impressive in terms of their size, even though they typically contain mere billions rather than trillions of 2 What is corpus linguistics? words.",
        "entities": [
            [
                147,
                165,
                "TERM"
            ]
        ]
    },
    {
        "text": "The indication is that the primary differentiating factor among the Gateshead speakers is gender, though the existence of cluster B suggests that educational level and type of employment are also factors.",
        "entities": [
            [
                168,
                172,
                "TERM"
            ]
        ]
    },
    {
        "text": "Occasionally, people refer to such collections as error corpora, but we will not use the term corpus for these.",
        "entities": [
            [
                94,
                100,
                "TERM"
            ]
        ]
    },
    {
        "text": "Current conceptions of corpus linguistics started with the creation of the Quirk Corpus (which contained print samples of spoken and written English) in 1955 at the Survey of English Usage at University College London.",
        "entities": [
            [
                23,
                41,
                "TERM"
            ]
        ]
    },
    {
        "text": "First, a frequency list may provide the frequencies of all words together with the words with their letters reversed.",
        "entities": [
            [
                9,
                23,
                "TERM"
            ]
        ]
    },
    {
        "text": "While they often make it possible to search A. Ädel the archive, they may not make the text files downloadable other than one by one by clicking a hyperlink.",
        "entities": [
            [
                87,
                91,
                "TERM"
            ]
        ]
    },
    {
        "text": "The mean rates for nouns drawn from every possible sample of size n that can be drawn from the population constitute the sampling distribution for this parameter.",
        "entities": [
            [
                51,
                57,
                "TERM"
            ]
        ]
    },
    {
        "text": "The SEC corpus is coded for these features as well as temporal alignment at the level of the phoneme.",
        "entities": [
            [
                8,
                14,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this language, the elements of a text are marked up using named tags including one or more attributes.",
        "entities": [
            [
                36,
                40,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this section, we will discuss a number of heuristic techniques that can be used to interrogate the syntagmatic-paradigmatic organization across the whole lexicogrammatical continuum, and we will illustrate their potential relevance to diachronic corpus linguistics.",
        "entities": [
            [
                249,
                267,
                "TERM"
            ],
            [
                238,
                248,
                "TERM"
            ]
        ]
    },
    {
        "text": "As we saw above, this is easy to remedy by simply determining the exact number of times that the phenomenon in question occurs in a given sample.",
        "entities": [
            [
                138,
                144,
                "TERM"
            ]
        ]
    },
    {
        "text": "Although all this is interesting information which is conventionally represented inside the document, as well as affecting its pagination/layout, it generally does not form part of the meaning potential of the text itself, which is what we're usually most interested in when we analyse texts from a linguistic point of view.",
        "entities": [
            [
                210,
                214,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, syntactic annotation tools are still imperfect and sometimes too complex to implement or use.",
        "entities": [
            [
                19,
                29,
                "TERM"
            ]
        ]
    },
    {
        "text": "In many cases, this piece of information can be obtained by contacting the corpus creators.",
        "entities": [
            [
                75,
                81,
                "TERM"
            ]
        ]
    },
    {
        "text": "This bias can be introduced either indirectly by the topic of the task in that, for example, a large batch of essays on the topic of friendship in a corpus will lead to a disproportionally high frequencies of words and phrases that belong to the lexical field of friendship.",
        "entities": [
            [
                149,
                155,
                "TERM"
            ]
        ]
    },
    {
        "text": "In contrast, the type frequency of an affix is a fairly direct reflection of the importance of the affix for the lexicon of a language: obviously an affix that occurs in many different words is more important than one that occurs only in a few words.",
        "entities": [
            [
                17,
                21,
                "TERM"
            ]
        ]
    },
    {
        "text": "We know that the maximum value of CV depends on the number of corpus parts and can be calculated as ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi no: of corpus parts À 1 p .",
        "entities": [
            [
                62,
                68,
                "TERM"
            ],
            [
                247,
                253,
                "TERM"
            ]
        ]
    },
    {
        "text": "The central premise of corpus linguistics is that all of the language-internal andexternal contextual features are relevant to the way that people use language.",
        "entities": [
            [
                23,
                41,
                "TERM"
            ]
        ]
    },
    {
        "text": "For each category that we discuss, sample clusters of tweets are provided in order to illustrate the contrast between the contact-related use -the target of our analysis -and the noise that we identified along the way.",
        "entities": [
            [
                35,
                41,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is nonetheless true that a corpus can only show that which it contains, and therefore the absence of evidence that a word or a structure exists in a corpus cannot constitute definitive proof of their absence from the language.",
        "entities": [
            [
                30,
                36,
                "TERM"
            ],
            [
                152,
                158,
                "TERM"
            ]
        ]
    },
    {
        "text": "We also introduced some basic principles regarding sample collection and balancing.",
        "entities": [
            [
                51,
                57,
                "TERM"
            ]
        ]
    },
    {
        "text": "Bootstrapping could be used as a method for evaluating the linguistic representativeness of a corpus (cf. Chap. 1).",
        "entities": [
            [
                70,
                88,
                "TERM"
            ],
            [
                94,
                100,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is therefore important that we always ensure we understand how the text of a corpus is internally structured, and to critically relate these to our linguistic analyses and categories.",
        "entities": [
            [
                80,
                86,
                "TERM"
            ],
            [
                70,
                74,
                "TERM"
            ]
        ]
    },
    {
        "text": "Systems such as Voicewalker was used for the Santa Barbara corpus and SoundScriber was used for compiling the Michigan Corpus of Academic Spoken English (MICASE).",
        "entities": [
            [
                59,
                65,
                "TERM"
            ]
        ]
    },
    {
        "text": "Comment This chapter will take you through the steps to complete a corpus project.",
        "entities": [
            [
                67,
                73,
                "TERM"
            ]
        ]
    },
    {
        "text": "The major idea here is to avoid massive skewing in results by over-representing just a single or very few text types.",
        "entities": [
            [
                106,
                110,
                "TERM"
            ]
        ]
    },
    {
        "text": "For these foreign language learners, it seems that the number of times a collocation occurs is a far more important factor than the strength of association between components.",
        "entities": [
            [
                73,
                84,
                "TERM"
            ]
        ]
    },
    {
        "text": "As the term suggests, it is a ratio of two probabilities from the crosstab table comparing the probability of a particular linguistic outcome (e.g. the definite article) occurring in one context type relative to the same outcome occurring in the other context type.",
        "entities": [
            [
                195,
                199,
                "TERM"
            ],
            [
                260,
                264,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first of these is the one you have already seen, a text form in natural human language.",
        "entities": [
            [
                55,
                59,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, we can calculate the mean rate of occurrence for nouns in a single corpus sample of size n.",
        "entities": [
            [
                80,
                86,
                "TERM"
            ],
            [
                87,
                93,
                "TERM"
            ]
        ]
    },
    {
        "text": "Finally, annotation and analysis should be separated from normative, oftentimes (over)prescriptive interpretation.",
        "entities": [
            [
                9,
                19,
                "TERM"
            ]
        ]
    },
    {
        "text": "Text A comes from a newspaper, text B from a novel.",
        "entities": [
            [
                31,
                35,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, since it's somewhat of a nuisance having to do this all the time, apart from being error-prone, it's quite useful to be able to specify a search for all different forms of a headword or lemma.",
        "entities": [
            [
                195,
                200,
                "TERM"
            ]
        ]
    },
    {
        "text": "In doing so, he determines per text (and thus per speaker) what he calls its referential density (RD).",
        "entities": [
            [
                31,
                35,
                "TERM"
            ]
        ]
    },
    {
        "text": "Second, whenever possible, we should go beyond the inclusion of translated novels, news, and international organizations' legal and administrative texts, and strive for the inclusion of more genres and text types, especially those that are dominant in today's translation market, to which corpus compilers have had limited access to date, for obvious reasons of confidentiality and/or copyright clearance.",
        "entities": [
            [
                289,
                295,
                "TERM"
            ],
            [
                202,
                206,
                "TERM"
            ]
        ]
    },
    {
        "text": "More bottom-up text-based approaches to text classification, for instance, can reduce the need to rely on more aprioristic classifications.",
        "entities": [
            [
                15,
                19,
                "TERM"
            ],
            [
                40,
                44,
                "TERM"
            ]
        ]
    },
    {
        "text": "We will then focus on the freely available AntConc concordancer, and introduce some of its functionalities.",
        "entities": [
            [
                51,
                63,
                "TERM"
            ]
        ]
    },
    {
        "text": "As Cook (1995: 37) notes, a spoken text is made meaningful by more than the words one finds in a transcription: How a conversation is interpreted depends crucially upon such contextual features as paralanguage (e.g. gestures and facial expressions), the knowledge the participants have about the cultural context in which the conversation takes place, their attitudes towards one another, and so forth.",
        "entities": [
            [
                35,
                39,
                "TERM"
            ]
        ]
    },
    {
        "text": "Many of the classic and larger corpora of English contain PoS tagging, for example, COCA or the Brown family corpora.",
        "entities": [
            [
                62,
                69,
                "TERM"
            ]
        ]
    },
    {
        "text": "Starting with micro f1 and its accompanying standard deviation, we find relatively high scores for all of the classification tasks for each lemma individually as well as for the grouped set.",
        "entities": [
            [
                140,
                145,
                "TERM"
            ]
        ]
    },
    {
        "text": "To corpus linguists, however, the database also presents multiple issues, which need to be addressed to make full use of it.",
        "entities": [
            [
                3,
                9,
                "TERM"
            ]
        ]
    },
    {
        "text": "Remember that the most powerful package to handle XML data in R is XML, but also remember that it has a bad memory leak when used on Windows systems.",
        "entities": [
            [
                50,
                53,
                "TERM"
            ],
            [
                67,
                70,
                "TERM"
            ]
        ]
    },
    {
        "text": "To some extent, these shortcomings are addressed by Gale's Digital Scholar Lab, 5 a text analytics platform through which the British Library Newspapers database can also be accessed.",
        "entities": [
            [
                84,
                88,
                "TERM"
            ]
        ]
    },
    {
        "text": "To summarize these three points: bootstrapping cannot provide better data or more data from the population than what is contained in the sample.",
        "entities": [
            [
                137,
                143,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the corpus linguistics field, it is also known as lexical bundles, recurrent combinations or clusters.",
        "entities": [
            [
                7,
                25,
                "TERM"
            ]
        ]
    },
    {
        "text": "Furthermore, as real corpus linguistics is not just about getting some 'impressive' numbers but should in fact allow you to gain real insights into different aspects of language, you should always try to relate your results to what you know from established theories and other methods used in linguistics, or even other related disciplines, such as for example sociology, psychology, etc., as far as they may be relevant to answering your research questions.",
        "entities": [
            [
                21,
                39,
                "TERM"
            ]
        ]
    },
    {
        "text": "By contrast, there are other configurations that are found significantly more often than expected in their respective corpus periods.",
        "entities": [
            [
                118,
                124,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is evident that the accuracy of the text depends crucially on the quality of the image used as the basis of the digitised text.",
        "entities": [
            [
                39,
                43,
                "TERM"
            ],
            [
                125,
                129,
                "TERM"
            ]
        ]
    },
    {
        "text": "This corpus includes four original works in French and their translation into English, as well as four original works in English and their translation into French.",
        "entities": [
            [
                5,
                11,
                "TERM"
            ]
        ]
    },
    {
        "text": "Other things you'll need to delete are single letters followed by dots, as well as abbreviations, such as i.e. or e.g., and any genuine words that are followed by dots, as the dots don't form part of the general token definition we're using, and most of these forms (apart from the abbreviations) are probably the results of tokenisation errors in the BNC, anyway.",
        "entities": [
            [
                212,
                217,
                "TERM"
            ]
        ]
    },
    {
        "text": "As with representativeness, the issues around balance are fraught with difficultly, but the critical thing for the user of any corpus is to be actively aware of how that corpus is balanced, to think carefully about what this means for the generalisability of results based on that corpus.",
        "entities": [
            [
                8,
                26,
                "TERM"
            ],
            [
                46,
                53,
                "TERM"
            ],
            [
                127,
                133,
                "TERM"
            ],
            [
                170,
                176,
                "TERM"
            ],
            [
                281,
                287,
                "TERM"
            ]
        ]
    },
    {
        "text": "Given the increasing availability of historical corpora and regionally-stratified corpora, this method may therefore be a useful addition to the corpus-linguistic toolkit.",
        "entities": [
            [
                145,
                151,
                "TERM"
            ]
        ]
    },
    {
        "text": "There is very little agreement in the collocates recorded in the three collocation dictionaries: only 3 percent of the total number of collocates listed are found in all three dictionaries, and 82 percent appear in only one of the three dictionaries.",
        "entities": [
            [
                71,
                82,
                "TERM"
            ]
        ]
    },
    {
        "text": "Including different versions of the same translation would also prove to be rewarding (e.g. draft, unedited, and edited versions of the translated text).",
        "entities": [
            [
                147,
                151,
                "TERM"
            ]
        ]
    },
    {
        "text": "The corpus is available on request from the authors.",
        "entities": [
            [
                4,
                10,
                "TERM"
            ]
        ]
    },
    {
        "text": "Instead, linguistic theories can be used to explain why particular frequencies and examples actually exist in a corpus: in other words, to discuss how theory and data interact.",
        "entities": [
            [
                112,
                118,
                "TERM"
            ]
        ]
    },
    {
        "text": "Within a text, some words may be restricted to particular sections, which is also useful to know.",
        "entities": [
            [
                9,
                13,
                "TERM"
            ]
        ]
    },
    {
        "text": "Imagine, for example, that you have a (untagged) corpus from which you want to retrieve all mentions of former and current Presidents of the United States.",
        "entities": [
            [
                49,
                55,
                "TERM"
            ]
        ]
    },
    {
        "text": "In corpus linguistics, it is also often used to measure the strength of association between phonemes or between a word and a construction it can occur in.",
        "entities": [
            [
                3,
                21,
                "TERM"
            ]
        ]
    },
    {
        "text": "In addition to informative writing, the corpus contains differing types of imaginative prose, such as general fiction and science fiction.",
        "entities": [
            [
                40,
                46,
                "TERM"
            ]
        ]
    },
    {
        "text": "This information can be considered comparable to metadata for texts in a corpus or to demographic data in a study with human participants.",
        "entities": [
            [
                49,
                57,
                "TERM"
            ],
            [
                73,
                79,
                "TERM"
            ]
        ]
    },
    {
        "text": "While voice recognition software has made tremendous progress recently, it still works best on monologic speech: an individual speaking slowly into a microphone that is then transferred into written text in a document.",
        "entities": [
            [
                199,
                203,
                "TERM"
            ]
        ]
    },
    {
        "text": "This may for example happen when the corpus is in a language that uses a different alphabet from the standard Western European ones that are supported on all computers by default.",
        "entities": [
            [
                37,
                43,
                "TERM"
            ]
        ]
    },
    {
        "text": "Similarly, a person working on comparative studies between two or more languages requires a multilingual comparable corpus than a monitor one.",
        "entities": [
            [
                116,
                122,
                "TERM"
            ]
        ]
    },
    {
        "text": "If this were not identified in advance as a semantically meaningful chunk meaning 'to ridicule or parody', then separate word counts for 'send' and 'up' would be observed and merged with the other occurrences of those words in the corpus, potentially incorrectly inflating their frequencies.",
        "entities": [
            [
                231,
                237,
                "TERM"
            ]
        ]
    },
    {
        "text": "In order to study one of these areas specifically, it is preferable to resort to a specialized corpus.",
        "entities": [
            [
                95,
                101,
                "TERM"
            ]
        ]
    },
    {
        "text": "The corpus was also divided into two sub-corpora: one consisting of collaboratively written essays (N = 51) and another consisting of individual essays (N = 102).",
        "entities": [
            [
                4,
                10,
                "TERM"
            ]
        ]
    },
    {
        "text": "If this type of search is not enabled by the interface, all the verbal forms must be looked up one by one with their exact forms.",
        "entities": [
            [
                8,
                12,
                "TERM"
            ]
        ]
    },
    {
        "text": "This means that at least some rank values will occur more than once, which is a typical situation for corpus-linguistic research involving ordinal data.",
        "entities": [
            [
                102,
                108,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, if tags are present, they can be used instead of or as well as a word pattern -for instance, to search for can as a noun versus can as a verb, or to search for all adverbs in the corpus.",
        "entities": [
            [
                188,
                194,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, the bulk of the corpus contains various kinds of informative prose, including press reportage, editorials, and reviews; government documents; differing types of learned writing; learned writing from, for instance, the humanities and social sciences.",
        "entities": [
            [
                30,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "We believe that the applications of bootstrapping in corpus linguistics we have discussed in the previous four sections are just a beginning for corpus linguistics.",
        "entities": [
            [
                53,
                71,
                "TERM"
            ],
            [
                145,
                163,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, to fully describe a word or phrase in a corpus, we need to introduce another concept, namely dispersion.",
        "entities": [
            [
                49,
                55,
                "TERM"
            ]
        ]
    },
    {
        "text": "Our aim in carrying out the case study was not to come up with the conclusion that all grammar textbooks should include corpus-based information on all grammar points.",
        "entities": [
            [
                120,
                132,
                "TERM"
            ]
        ]
    },
    {
        "text": "If one computes the MI of in spite of in the untagged Brown corpus by comparing the observed frequency of in spite of of 54 against an expected frequency based on complete independence, MI becomes an extremely high value of 12.25.",
        "entities": [
            [
                60,
                66,
                "TERM"
            ]
        ]
    },
    {
        "text": "When creating a spoken corpus, one also needs to think about whether an orthographic representation of the text will be sufficient, whether the corpus should be represented in phonetic transcription, or whether it should support annotation on various different levels (see Chapter 11).",
        "entities": [
            [
                229,
                239,
                "TERM"
            ],
            [
                23,
                29,
                "TERM"
            ],
            [
                144,
                150,
                "TERM"
            ],
            [
                107,
                111,
                "TERM"
            ]
        ]
    },
    {
        "text": "To summarize, there is no one way to deal with all the variables affecting the gender balance of a corpus.",
        "entities": [
            [
                86,
                93,
                "TERM"
            ],
            [
                99,
                105,
                "TERM"
            ]
        ]
    },
    {
        "text": "Before we start, please watch the tutorial on how to use the keyword feature in AntConc: www.youtube.com/watch?v=SludW4FHatI&list=PL iRIDpYmiC0SjJeT2FuysOkLa45HG_tIu&index=10.",
        "entities": [
            [
                61,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "The virtue of working with a DIY corpus is that the design of the data can be tailored directly to the specific research question at hand.",
        "entities": [
            [
                33,
                39,
                "TERM"
            ]
        ]
    },
    {
        "text": "Thus, the following sections will try to provide you with a rough overview of what exactly PoS tagging is and how it can be carried out, where its strengths and weaknesses lie, and how you may be able to use it with your own data.",
        "entities": [
            [
                95,
                102,
                "TERM"
            ]
        ]
    },
    {
        "text": "With the state of the art in corpus-pragmatic research established, we now turn to a more fine-grained discussion of exemplar studies in the areas outlined above.",
        "entities": [
            [
                29,
                35,
                "TERM"
            ]
        ]
    },
    {
        "text": "While the study of pragmatic items can be challenging in a corpus, it is eminently possible.",
        "entities": [
            [
                59,
                65,
                "TERM"
            ]
        ]
    },
    {
        "text": "This work is based on the intuition that there are restrictions on which words can co-occur within some degree of contiguity in natural language strings, and that the distribution of words in text can therefore be used to infer grammatical and semantic categories and category relatedness for them.",
        "entities": [
            [
                192,
                196,
                "TERM"
            ]
        ]
    },
    {
        "text": "A portion of the corpus, including works from the 18th to the 20th Century, can be downloaded for free from the Ortolang website.",
        "entities": [
            [
                17,
                23,
                "TERM"
            ]
        ]
    },
    {
        "text": "While we acknowledge the use and importance of combining these two approaches, in the rest of the book we will focus on the quantitative approach to corpus linguistics, which poses its own theoretical and methodological challenges.",
        "entities": [
            [
                149,
                167,
                "TERM"
            ]
        ]
    },
    {
        "text": "In particular, spelling variation causes problems for POS tagging, concordancing, keywords, n-grams, and collocation techniques.",
        "entities": [
            [
                105,
                116,
                "TERM"
            ],
            [
                58,
                65,
                "TERM"
            ]
        ]
    },
    {
        "text": "The bad news for Windows users is that, with R for Windows at least, it is often not as straightforward to handle Unicode as it is with R for MacOS X or R for Linux, which is mainly due to the fact that Mac OS X and Linux are UTF-8 operating systems so they play well with input files that have the same encoding, which is one of many reasons (see Section 3.8.2 for another one) why I recommend doing corpus processing with R on a Linux machine (and it takes very little time to install a long-term support version of, for instance, Kubuntu or Linux Mint on a desktop computer).",
        "entities": [
            [
                304,
                312,
                "TERM"
            ],
            [
                401,
                407,
                "TERM"
            ]
        ]
    },
    {
        "text": "As illustrated in some of the corpus projects that compared COCA with the BYU-BNC corpus, the unequal sizes of these two corpora did not allow for straight frequency comparisons between the two corpora.",
        "entities": [
            [
                30,
                36,
                "TERM"
            ],
            [
                82,
                88,
                "TERM"
            ]
        ]
    },
    {
        "text": "Generally speaking, the larger and the more similar the reference corpus is to the corpus of interest the more reliable and focused the comparison is.",
        "entities": [
            [
                56,
                72,
                "TERM"
            ],
            [
                83,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "This is legitimate if the goal is to investigate that particular variety, but if the corpus were meant to represent the standard language in general (which the corpus creators explicitly deny), it would force us to accept a very narrow understanding of standard.",
        "entities": [
            [
                85,
                91,
                "TERM"
            ],
            [
                160,
                166,
                "TERM"
            ]
        ]
    },
    {
        "text": "Their main conclusion of the first case study is that \"D values based on 1,000 corpus parts completely fail to discriminate among words with uniform versus skewed distributions in naturalistic data\" (p. 454).",
        "entities": [
            [
                79,
                85,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, there is a potential problem that we need to take into account: the values of the variables Sex and 6.6 Complex research designs Age and their intersections are not necessarily distributed evenly in the subpart of the BNC used here; although the makers of the corpus were careful to include a broad range of speakers of all ages, sexes (and class memberships, ignored in our study), they did not attempt to balance all these demographic variables, let alone their intersections.",
        "entities": [
            [
                416,
                423,
                "TERM"
            ],
            [
                269,
                275,
                "TERM"
            ]
        ]
    },
    {
        "text": "The p-value represents the chance that the null hypothesis would be true if we observed this sample of data.",
        "entities": [
            [
                93,
                99,
                "TERM"
            ]
        ]
    },
    {
        "text": "In other words, I use it as a superordinate term for text-linguistic terms like genre, register, style, and medium as well as sociolinguistic terms like dialect, sociolect, etc.",
        "entities": [
            [
                53,
                57,
                "TERM"
            ]
        ]
    },
    {
        "text": "One additional highly important attribute of the corpus is the fact that it contains a very large amount of meta-information, i.e. information about who produced which type of document(s) and in which contexts.",
        "entities": [
            [
                49,
                55,
                "TERM"
            ],
            [
                168,
                172,
                "TERM"
            ]
        ]
    },
    {
        "text": "The concepts that we normally apply for this purpose are representativeness and balance.",
        "entities": [
            [
                57,
                75,
                "TERM"
            ],
            [
                80,
                87,
                "TERM"
            ]
        ]
    },
    {
        "text": "A corpus, unless it is a fixed one, should grow regularly.",
        "entities": [
            [
                2,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "One might argue that this second step can be seen as corpus-based because it identifies features in the corpus that merit further attention.",
        "entities": [
            [
                53,
                65,
                "TERM"
            ],
            [
                104,
                110,
                "TERM"
            ]
        ]
    },
    {
        "text": "What is interesting here is that even if the value of 120 in text no. 10 was replaced by 1,200, the median would be the same as before.",
        "entities": [
            [
                61,
                65,
                "TERM"
            ]
        ]
    },
    {
        "text": "When working with corpora based on language documentations, corpus linguists need to work with what they have, and this may often require flexibility.",
        "entities": [
            [
                60,
                66,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the example of a sociolinguistic research looking at the use of swearwords by men and women we would get a p-value that would give us the probability of seeing the observed or even more extreme difference between the two groups if the null hypothesis were true, that is, if there really was no difference in the population and the difference observed in the corpus (sample) was merely due to chance as the result of a sampling error.",
        "entities": [
            [
                361,
                367,
                "TERM"
            ],
            [
                369,
                375,
                "TERM"
            ]
        ]
    },
    {
        "text": "The problem, after all, is not so much that each data point becomes less reliable if we add more annotation categories, but rather that we often do not have enough data points to obtain a truly informative picture when addressing research questions that require us to take many categories into account simultaneously.",
        "entities": [
            [
                97,
                107,
                "TERM"
            ]
        ]
    },
    {
        "text": "Additionally, some data models add grouping mechanisms to annotation graphs, often referred to as 'annotation layers', which can be used to lump together annotations that are somehow related.",
        "entities": [
            [
                58,
                68,
                "TERM"
            ],
            [
                99,
                109,
                "TERM"
            ]
        ]
    },
    {
        "text": "One example from Cameron's writing is the significantly above-average use of is; which was the fifth most frequent keyword in her corpus.",
        "entities": [
            [
                115,
                122,
                "TERM"
            ],
            [
                130,
                136,
                "TERM"
            ]
        ]
    },
    {
        "text": "When we calculate the range 2 of w 1 , we'll get the same number as for word w (i.e. 5 out of 6 or 83.3%), although the large majority of all occurrences of w 1 are in only one part (Part 1) whereas word w is, in comparison, more evenly spread out across the corpus.",
        "entities": [
            [
                259,
                265,
                "TERM"
            ]
        ]
    },
    {
        "text": "And there is some evidence to suggest that this is the appropriate approach to take in creating a corpus.",
        "entities": [
            [
                98,
                104,
                "TERM"
            ]
        ]
    },
    {
        "text": "In principle, the former only applies to corpora for general use, though, as it's often assumed that these ought to provide an equal amount of materials from many different genres or areas of relevance that allow us to investigate a representative and realistic sample of the language and draw more or less universally applicable conclusions.",
        "entities": [
            [
                262,
                268,
                "TERM"
            ]
        ]
    },
    {
        "text": "But of all the early electronic corpora, the first computerized corpus of written English, the Brown Corpus (described earlier), was really the corpus that ushered in the modern-day era of corpus linguistics.",
        "entities": [
            [
                189,
                207,
                "TERM"
            ],
            [
                64,
                70,
                "TERM"
            ],
            [
                144,
                150,
                "TERM"
            ]
        ]
    },
    {
        "text": "Referents that are important in a culture are more likely to be talked and written about than those that are not; thus, in a sufficiently large and representative corpus, the frequency of a linguistic item may be taken to represent the importance of its referent in the culture.",
        "entities": [
            [
                163,
                169,
                "TERM"
            ]
        ]
    },
    {
        "text": "Here again, whilst we agree that newspapers usually contain more passive forms than some other text types and that many learners (who often underuse the passive) need practice in using passive sentences, we feel that the exercise presented here might be counterproductive.",
        "entities": [
            [
                95,
                99,
                "TERM"
            ]
        ]
    },
    {
        "text": "Two spans were used: four words either side of the node and nine words either side of the node; • whether counts were based on lemmatized or non-lemmatized counts.",
        "entities": [
            [
                51,
                55,
                "TERM"
            ],
            [
                90,
                94,
                "TERM"
            ]
        ]
    },
    {
        "text": "We will then provide more details on exactly which type of information is presented in the corpus-informed books, but also how is it presented.",
        "entities": [
            [
                91,
                97,
                "TERM"
            ],
            [
                51,
                55,
                "TERM"
            ]
        ]
    },
    {
        "text": "Such disambiguation remains both a theoretical and practical challenge for corpus-based frequency list research.",
        "entities": [
            [
                88,
                102,
                "TERM"
            ],
            [
                75,
                87,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this paper, we present the development of a webbased corpus from Twitter posts, named ILiAD: An Interactive Corpus for Linguistic Annotated Data.",
        "entities": [
            [
                56,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "This value is known as the median -a value that splits a sample or population into a higher and a lower portion of equal sizes.",
        "entities": [
            [
                57,
                63,
                "TERM"
            ]
        ]
    },
    {
        "text": "For some phenomena such as the annotation of morphosyntactic categories or speech acts, every word or sentence in the corpus will be involved.",
        "entities": [
            [
                31,
                41,
                "TERM"
            ],
            [
                118,
                124,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, he considered such factors as whether the formality of the particular type of writing influenced the choice of either retaining or deleting that; whether that deletion was less common in formal writing than in informal writing; whether the particular verb influenced use or non-use of that; and whether including or not including that resulted in a difference in meaning.",
        "entities": [
            [
                84,
                88,
                "TERM"
            ]
        ]
    },
    {
        "text": "Moreover, if corpus pragmatics is concerned with the interpretation of meaning in context, another disadvantage associated with the relationship between corpus linguistics and pragmatics is that many larger corpora are impoverished both textually and contextually (Ru ¨hlemann 2010).",
        "entities": [
            [
                153,
                171,
                "TERM"
            ],
            [
                13,
                19,
                "TERM"
            ]
        ]
    },
    {
        "text": "Sometimes this is not possible, as in the case of the problem-solution corpus described above.",
        "entities": [
            [
                71,
                77,
                "TERM"
            ]
        ]
    },
    {
        "text": "In ECL1, it is noted that \"as more and more corpora have been created, we have gained considerable knowledge of how to construct a corpus that is balanced and representative and that will yield reliable grammatical information\" (138).",
        "entities": [
            [
                131,
                137,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, if we change the composition of the corpus to make it more homogeneous, this ought to change very quickly.",
        "entities": [
            [
                45,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "Part-of-speech (POS) tagging is the assignment to each word of a single label indicating that word's grammatical category membership.",
        "entities": [
            [
                21,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "Like the SBC, a corpus created with ELAN will have two components, namely, a set of media files with the raw audio and/or audio-visual data and a corresponding set of ELAN document files (a special kind of XML file).",
        "entities": [
            [
                16,
                22,
                "TERM"
            ],
            [
                206,
                209,
                "TERM"
            ]
        ]
    },
    {
        "text": "The top content items from the Swales corpus are research, genre, English, academic, writing, non-native speakers of English, and the concept of discourse community which similarly encompass his key areas of contribution.",
        "entities": [
            [
                38,
                44,
                "TERM"
            ]
        ]
    },
    {
        "text": "The text is clearly about fish in the two rivers.",
        "entities": [
            [
                4,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "The simplest way of solving the problem of different sample sizes is to create samples of equal size for the purposes of comparison.",
        "entities": [
            [
                53,
                59,
                "TERM"
            ]
        ]
    },
    {
        "text": "We can convert it into something called the sample standard deviation, however, by taking its square root.",
        "entities": [
            [
                44,
                50,
                "TERM"
            ]
        ]
    },
    {
        "text": "Again, variables from Group B share a communicative function, which can be labelled as 'academic-type description' with passives and many modified nouns and nominal forms ending in -tion, -ment, -ness and -ity.",
        "entities": [
            [
                97,
                101,
                "TERM"
            ]
        ]
    },
    {
        "text": "Speech corpora are based on spoken language but necessitate detailed annotation including not only written transcription but transcription in phonetic alphabets and careful connections with the time course of speaking.",
        "entities": [
            [
                69,
                79,
                "TERM"
            ]
        ]
    },
    {
        "text": "This is still very much a grey area, with different laws applying across the world, and anyone planning to distribute a web-derived corpus should seek local advice.",
        "entities": [
            [
                132,
                138,
                "TERM"
            ]
        ]
    },
    {
        "text": "All the occurrences of A as NP were extracted from the BNC-XML, amounting to 1,819 tokens.",
        "entities": [
            [
                59,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "Second, statistical significance has nothing to do with theoretical relevance.",
        "entities": [
            [
                8,
                32,
                "TERM"
            ]
        ]
    },
    {
        "text": "Here items are classified according to a particular scheme, and an arithmetical count is made on the number of items that belong to each class in the scheme within a corpus.",
        "entities": [
            [
                166,
                172,
                "TERM"
            ]
        ]
    },
    {
        "text": "To use large, digitised archives as data for corpus linguistic research, it is essential to critically think about the ways in which they have been collected and processed, as well as what kinds of software tools are available to search them.",
        "entities": [
            [
                45,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "Time-alignment means that the annotation -of whatever kind -is directly linked to the rendition of the speech signal.",
        "entities": [
            [
                30,
                40,
                "TERM"
            ]
        ]
    },
    {
        "text": "But corpus linguistics was not only developed thanks to the creation of such tools.",
        "entities": [
            [
                4,
                22,
                "TERM"
            ]
        ]
    },
    {
        "text": "The more types have already occurred, the more types there are to be reused (put simply, speakers will encounter fewer and fewer communicative situations that require a new type), which makes it less and less probable that new types (including new hapaxes) will occur.",
        "entities": [
            [
                173,
                177,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first step is to identify which occurrences will be annotated in the corpus.",
        "entities": [
            [
                73,
                79,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, would you want the corpus to contain a specific section of a newspaper (e.g. editorials).",
        "entities": [
            [
                33,
                39,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, we use a different type of language when talking informally to friends than when we are asked to write a research report.",
        "entities": [
            [
                32,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "By that I do not only mean that corpus linguists need to use more different statistical tests (while that is generally true, the choice of a particular test is of course mostly dictated by the particular research question), but also that there needs to be a growing awareness that some choices that corpus linguists traditionally make may be pro blematic and would benefit from a different perspective.",
        "entities": [
            [
                32,
                38,
                "TERM"
            ],
            [
                299,
                305,
                "TERM"
            ]
        ]
    },
    {
        "text": "We consequently decided to investigate the frequency of sentence-initial as well in COHA, a corpus that provided us with enough material to examine its diachronic development in recent American English.",
        "entities": [
            [
                152,
                162,
                "TERM"
            ],
            [
                92,
                98,
                "TERM"
            ]
        ]
    },
    {
        "text": "As an alternative, corpora can be stored in external data services such as clouds with corpus access for example by compressed media streaming.",
        "entities": [
            [
                87,
                93,
                "TERM"
            ]
        ]
    },
    {
        "text": "These annotation steps are often referred to as \"tagging\" or \"coding\" in the literature.",
        "entities": [
            [
                6,
                16,
                "TERM"
            ],
            [
                49,
                56,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this chapter so far, we have described the origins and motivations for the development of the keywords method in corpus linguistics, and shown two representative studies using the technique.",
        "entities": [
            [
                116,
                134,
                "TERM"
            ]
        ]
    },
    {
        "text": "A corpus of spontaneous conversations would need to contain unrehearsed conversations between two or more individuals in the types of settings in which such conversations are held, such as the family dinner table, a party, a chat between two friends, and so forth.",
        "entities": [
            [
                2,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "Based on a general corpus, we like to produce texts and reference materials that will guide in spelling, pronunciation, grammar (i.e., syntax), meaning and usage.",
        "entities": [
            [
                19,
                25,
                "TERM"
            ]
        ]
    },
    {
        "text": "Traditional corpus consultation is in some ways a relatively marginal activity, to be found in few classrooms around the world.",
        "entities": [
            [
                12,
                18,
                "TERM"
            ]
        ]
    },
    {
        "text": "More specifically, when researchers reuse a corpus created by other teams, they must mention in their publication the Internet link where the data were downloaded or retrieved from.",
        "entities": [
            [
                44,
                50,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, we know that the use of nouns and adjectives in text is strongly correlated.",
        "entities": [
            [
                61,
                65,
                "TERM"
            ]
        ]
    },
    {
        "text": "As the above discussion already indicated, however, the distinction between corpora and text archives is often blurred.",
        "entities": [
            [
                88,
                92,
                "TERM"
            ]
        ]
    },
    {
        "text": "Second, subsequent corpus users can much more easily re-evaluate the accuracy of annotations and potentially modify them or add further annotation detail, as required for any research agenda.",
        "entities": [
            [
                136,
                146,
                "TERM"
            ],
            [
                19,
                25,
                "TERM"
            ]
        ]
    },
    {
        "text": "An example of a domain-specific literary corpus would be the collected works of an author, which can be used to investigate the style of this particular author, or even to verify disputes about the authorship of a piece of literature where this may be contentious.",
        "entities": [
            [
                41,
                47,
                "TERM"
            ]
        ]
    },
    {
        "text": "Thus, if the user chose \"XML\" then menu made that a 1 and switch then assigns TRUE to xml.version.",
        "entities": [
            [
                25,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "Regardless of the stylistic genre targeted, the use of quantitative methods linked to corpus linguistics has led to many advances in lexicography and has been one of its main application areas.",
        "entities": [
            [
                86,
                104,
                "TERM"
            ]
        ]
    },
    {
        "text": "If the text is annotated or corrected by hand then this could form the basis of a training corpus for an automatic tagging system which can then learn from the human annotators in order to attempt to replicate their coding later on larger amounts of data.",
        "entities": [
            [
                115,
                122,
                "TERM"
            ],
            [
                91,
                97,
                "TERM"
            ],
            [
                7,
                11,
                "TERM"
            ]
        ]
    },
    {
        "text": "Indeed, it is no exaggeration to say that corpus linguistics using large computer-readable language data has established itself as the main methodology in historical pragmatics.",
        "entities": [
            [
                42,
                60,
                "TERM"
            ]
        ]
    },
    {
        "text": "Then we calculate the distances between the individual occurrences of the word (w 1 ) in the corpusto do this we need to use the corpus positions.",
        "entities": [
            [
                129,
                135,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the current version, we have selected a relatively small number of users in the corpus, as compared to other larger projects with similar goals.",
        "entities": [
            [
                83,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "Should you choose to do a slide show presentation, try not to put too much text on your slides.",
        "entities": [
            [
                75,
                79,
                "TERM"
            ]
        ]
    },
    {
        "text": "Because of these advantages, corpus-based dialect studies have greatly expanded our knowledge of dialect variation, showing that social and regional linguistic variation are far more complex and pervasive than has previously been assumed.",
        "entities": [
            [
                29,
                41,
                "TERM"
            ]
        ]
    },
    {
        "text": "The components highlighted as interesting from a linguistic perspective are features such as style, intended readership, intended function and the context in which the text is intended to fulfill a communicative purpose.",
        "entities": [
            [
                168,
                172,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are many linguistic and technical issues relating to corpus sanitation (e.g., text normalization, orthographic error correction, spelling error correction, real word-error correction, grammatical error correction, punctuation error removal, and tokenization).",
        "entities": [
            [
                59,
                65,
                "TERM"
            ],
            [
                84,
                88,
                "TERM"
            ]
        ]
    },
    {
        "text": "A further layer of annotation vital for many corpora is a free translation into a major world language (Schultze-Berndt 2006).",
        "entities": [
            [
                19,
                29,
                "TERM"
            ]
        ]
    },
    {
        "text": "For the time being, it is not possible for linguists to observe how the French language works through the study of a single corpus.",
        "entities": [
            [
                124,
                130,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, this argument ignores the fact that, before any annotation is finished, it repeatedly, and often for very long periods of time, needs to be read and edited by humans, so that readability does indeed represent an issue in annotation.",
        "entities": [
            [
                57,
                67,
                "TERM"
            ],
            [
                230,
                240,
                "TERM"
            ]
        ]
    },
    {
        "text": "As this textbook is more practical in nature than other textbooks on corpus linguistics, at the end of almost all chapters, I've also added a section entitled 'Sources and Further Reading'.",
        "entities": [
            [
                69,
                87,
                "TERM"
            ]
        ]
    },
    {
        "text": "Then, we characterize each observation for its features, which in this case would be syntactic position and type of article.",
        "entities": [
            [
                108,
                112,
                "TERM"
            ]
        ]
    },
    {
        "text": "We need to expand corpus studies into multimodal academic genres where writing is frequently used with graphical and visual semiotic forms, such as academic websites and textbooks.",
        "entities": [
            [
                18,
                24,
                "TERM"
            ]
        ]
    },
    {
        "text": "We can describe the quality of a data set that we have retrieved from a corpus in terms of two measures.",
        "entities": [
            [
                72,
                78,
                "TERM"
            ]
        ]
    },
    {
        "text": "If we compare the reports of university students with a year of the Le Monde newspaper rather than the rest of their academic work, the keyword list includes élèves, activité, formation, réflexivité, écriture, évaluation, résumé, pensée, enseignants, savoir, etc., because the topics covered vary more widely between the two corpora than between the different types of university work.",
        "entities": [
            [
                136,
                143,
                "TERM"
            ]
        ]
    },
    {
        "text": "Sociolinguistics is a subfield of linguistics where corpus studies are a possible methodology, alongside surveys and experiments and more.",
        "entities": [
            [
                52,
                58,
                "TERM"
            ]
        ]
    },
    {
        "text": "Another consideration in sharing corpus resources involves how to make these accessible to others and how to preserve digital data.",
        "entities": [
            [
                33,
                39,
                "TERM"
            ]
        ]
    },
    {
        "text": "Again, each corpus processing tool used at the retrieval stage should be listed, described and properly referred to.",
        "entities": [
            [
                12,
                18,
                "TERM"
            ]
        ]
    },
    {
        "text": "As far as possible, the chosen tag sets should be based on annotation standards which have been broadly accepted in the literature.",
        "entities": [
            [
                59,
                69,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first relates to the collocation-via-significance approach.",
        "entities": [
            [
                25,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "It represents information of a kind that many corpus creators have expressed an interest in, but which few corpus projects have included (see Chap. 16 for more information).",
        "entities": [
            [
                46,
                52,
                "TERM"
            ],
            [
                107,
                113,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are different ways to calculate lexical dispersion in a corpus.",
        "entities": [
            [
                62,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "Yet, even well documented spoken corpora are often not immediately (re-)usable to the wider research community, especially if the intended linguistic use is not the original one of the corpus compilers.",
        "entities": [
            [
                185,
                191,
                "TERM"
            ]
        ]
    },
    {
        "text": "In contrast, a written text will contain features of orthography, such as paragraph boundaries or font changes.",
        "entities": [
            [
                23,
                27,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this section I shall outline some of the most important subfields of historical pragmatics where corpus-linguistic methods have yielded novel insights into central research questions.",
        "entities": [
            [
                100,
                106,
                "TERM"
            ]
        ]
    },
    {
        "text": "That is, if we did a difference type of test (e.g., like ANOVA), there would not be a significant difference between teacher and student presentations in terms of the use of \"I mean\".",
        "entities": [
            [
                32,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "Other fields have had long and intense discussions about these things -corpus linguistics, unfortunately, has not.",
        "entities": [
            [
                71,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "The replication crisis in linguistics is highly relevant to corpus-based research: Many corpus studies are not directly replicable as the data on which they are based are not readily available.",
        "entities": [
            [
                60,
                72,
                "TERM"
            ],
            [
                88,
                94,
                "TERM"
            ]
        ]
    },
    {
        "text": "Such results can only be obtained by dense corpus studies allowing us to detect relatively rare phenomena such as passive constructions.",
        "entities": [
            [
                43,
                49,
                "TERM"
            ]
        ]
    },
    {
        "text": "The degrees of freedom is a complex concept, which is used when dealing with calculations based on a sample (corpus) rather than a population; in corpus linguistics, this is the case most of the time.",
        "entities": [
            [
                146,
                164,
                "TERM"
            ],
            [
                109,
                115,
                "TERM"
            ],
            [
                101,
                107,
                "TERM"
            ]
        ]
    },
    {
        "text": "Retrieval and annotation are discussed in detail in Chapter 4.",
        "entities": [
            [
                14,
                24,
                "TERM"
            ]
        ]
    },
    {
        "text": "For each request, the interface returns an indication of the frequency rank of the word looked up in the corpus.",
        "entities": [
            [
                105,
                111,
                "TERM"
            ]
        ]
    },
    {
        "text": "Furthermore, concordancers, such as AntConc, generally also allow us to search for XML tags because they simply represent plain searchable text, but may also sometimes provide options to hide them when we don't want to display them.",
        "entities": [
            [
                139,
                143,
                "TERM"
            ],
            [
                83,
                86,
                "TERM"
            ]
        ]
    },
    {
        "text": "To do the latter on a subcorpus you've created, you can simply select your corpus from the BNCweb start page from the dropdown list next to where it reads 'Restrictions'.",
        "entities": [
            [
                75,
                81,
                "TERM"
            ]
        ]
    },
    {
        "text": "There is a function file, which establishes a connection to a file and allows you to specify an encoding argument, and that connection will then be read with a function called readLines, which does exactly what its name suggests.",
        "entities": [
            [
                96,
                104,
                "TERM"
            ]
        ]
    },
    {
        "text": "The syntax is therefore: [lemma = \"film\"] [tag = \"ADJ\"].",
        "entities": [
            [
                26,
                31,
                "TERM"
            ]
        ]
    },
    {
        "text": "For the moment, the important point to bear in mind is that corpus linguistics often resorts to a quantitative methodology (see section 1.5) so as to be able to generalize the conclusions observed on the basis of a linguistic sample to the whole of the language, or belonging to a particular language register.",
        "entities": [
            [
                60,
                78,
                "TERM"
            ],
            [
                226,
                232,
                "TERM"
            ]
        ]
    },
    {
        "text": "By contrast, it is more rarely found in collocation with words indicating a role, as in he played an important role in his failure, in texts destined for children than in texts for adults.",
        "entities": [
            [
                40,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "Discrete variables have measurements that cannot be divided, like token counts or word lengths in characters.",
        "entities": [
            [
                66,
                71,
                "TERM"
            ]
        ]
    },
    {
        "text": "In a corpus of spoken dialogues, for instance, it is necessary to include tags that identify who is speaking or which sections of speaker turns contain overlapping speech.",
        "entities": [
            [
                5,
                11,
                "TERM"
            ]
        ]
    },
    {
        "text": "A 'random sample' is a sample where every member of the population has Random sample equal probability of being included in the sample.",
        "entities": [
            [
                10,
                16,
                "TERM"
            ],
            [
                23,
                29,
                "TERM"
            ],
            [
                78,
                84,
                "TERM"
            ],
            [
                128,
                134,
                "TERM"
            ]
        ]
    },
    {
        "text": "The standard deviation is an indicator of the amount of variation in a sample (or sub-sample) that is frequently reported; it is good practice to report standard deviations whenever we report means.",
        "entities": [
            [
                71,
                77,
                "TERM"
            ],
            [
                86,
                92,
                "TERM"
            ]
        ]
    },
    {
        "text": "This type of information is included in the \"headers\" of the text but will not be read by the concordance software.",
        "entities": [
            [
                94,
                105,
                "TERM"
            ],
            [
                5,
                9,
                "TERM"
            ],
            [
                61,
                65,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is a great baseline corpus for your own research as well.",
        "entities": [
            [
                23,
                29,
                "TERM"
            ]
        ]
    },
    {
        "text": "Conversely, even the most sophisticated quantitative analysis typically entails item-by-item annotation of each data point, which equates to a qualitative analysis of sorts; likewise, the results of a quantitative analysis demand interpretation, which typically requires a qualitative approach.",
        "entities": [
            [
                93,
                103,
                "TERM"
            ]
        ]
    },
    {
        "text": "In order to be able to draw quantitative conclusions, it is necessary to carry out inferential statistical tests, which will indicate whether the results obtained on the corpus sample can be generalized to the entire population.",
        "entities": [
            [
                170,
                176,
                "TERM"
            ],
            [
                177,
                183,
                "TERM"
            ]
        ]
    },
    {
        "text": "Although this iterative process is often not reported in final publications, it is evident from the many textbook descriptions of corpus linguistics.",
        "entities": [
            [
                130,
                148,
                "TERM"
            ]
        ]
    },
    {
        "text": "Online you can find websites that help you practice your regular expressions on sample data.",
        "entities": [
            [
                80,
                86,
                "TERM"
            ]
        ]
    },
    {
        "text": "This is because this exercise is really only one (complicated) regular expression: The goal is to match all kinds of formats of numbers, which is something that can easily come up when you generate frequency lists of (large) corpora and want to avoid having potentially tens of thousands of frequency list entries that are really just different numbers -in such a situation, you would probably want just one entry \"_NUM_\" or something similar; thus, this is a realistic situation.",
        "entities": [
            [
                291,
                305,
                "TERM"
            ]
        ]
    },
    {
        "text": "Many corpora nowadays still do not adequately enable corpus users to set up queries that would exclude items appearing in names when needed to improve precision of the searches.",
        "entities": [
            [
                53,
                59,
                "TERM"
            ]
        ]
    },
    {
        "text": "The LLC tagset is much larger than the Brown tagset, comprising 204 tags, reflecting finer-grained and additional grammatical distinctions, for example, the case forms of pronouns, and including cliticised forms whereby clitic and host are treated as one 'contracted' token word (cf 2.2.2).",
        "entities": [
            [
                268,
                273,
                "TERM"
            ]
        ]
    },
    {
        "text": "The conversion of a corpus into a database of syntactic tree structures or a treebank remains a problematic and laborious task, which is associated with error or failure rates far greater than those associated with taggers.",
        "entities": [
            [
                20,
                26,
                "TERM"
            ]
        ]
    },
    {
        "text": "A selection of occurrences of the vowels to be studied, representing the different periods, should then be looked up in the corpus.",
        "entities": [
            [
                124,
                130,
                "TERM"
            ]
        ]
    },
    {
        "text": "For corpus linguists, it is of course important to know that vectors can also contain character strings -the only difference to numbers is that the character strings have to be put either between double or single quotes.",
        "entities": [
            [
                4,
                10,
                "TERM"
            ]
        ]
    },
    {
        "text": "The standard deviations are calculated according to equation (2.11) from Chapter 2 (standard deviation sample).",
        "entities": [
            [
                103,
                109,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are many applications of the principles and methods of bootstrapping that need to be further explored by corpus researchers.",
        "entities": [
            [
                111,
                117,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, the first reference corpora (such as the Brown corpus developed for American English in the early 1960s) were about this size.",
        "entities": [
            [
                61,
                67,
                "TERM"
            ]
        ]
    },
    {
        "text": "As long as we are deriving all our collocation data from the same corpus, this will not make a difference, since the table total will always be the same.",
        "entities": [
            [
                35,
                46,
                "TERM"
            ],
            [
                66,
                72,
                "TERM"
            ]
        ]
    },
    {
        "text": "As you saw above, the function table takes as an argument one or more vectors or factors and provides the token frequency of each type or each combination of types.",
        "entities": [
            [
                106,
                111,
                "TERM"
            ],
            [
                130,
                134,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is worthwhile mentioning that the concepts of sampling density and coverage used above conflate two aspects of sampling which are of great relevance for theory and practice, viz. sampling intervals (e.g. one month between samples) and durations of recordings (e.g. 2 hours per sample).",
        "entities": [
            [
                280,
                286,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the case of a speech corpus, on the other hand, it is linked with the total number of sentences, unique words (types), and total words (tokens) in a corpus.",
        "entities": [
            [
                24,
                30,
                "TERM"
            ],
            [
                152,
                158,
                "TERM"
            ]
        ]
    },
    {
        "text": "Part III, then, is organized in terms of the range of varieties that have been studied from a corpus perspective.",
        "entities": [
            [
                94,
                100,
                "TERM"
            ]
        ]
    },
    {
        "text": "We can search for a specific lexeme in a corpus and determine its collocates, that is, a list of lexemes that co-occurs with it.",
        "entities": [
            [
                41,
                47,
                "TERM"
            ]
        ]
    },
    {
        "text": "The word bungalow is the least frequent word, with 1,100 occurrences in the corpus, corresponding to frequency rank number 53,542 and frequency class number 16.",
        "entities": [
            [
                76,
                82,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, because of their availability and size, many corpus linguists use them as resources, and as long as one bears their limitations in mind in terms of representativity etc., there is little reason not to.",
        "entities": [
            [
                54,
                60,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the case of a spoken corpus in particular, it is essential for participants to know that they are being recorded and that their data will later be used for linguistic analyses.",
        "entities": [
            [
                24,
                30,
                "TERM"
            ]
        ]
    },
    {
        "text": "In a purely lexical corpus (i.e. a corpus containing just the text), only individual words (or sequences of words) can be searched for.",
        "entities": [
            [
                20,
                26,
                "TERM"
            ],
            [
                35,
                41,
                "TERM"
            ],
            [
                62,
                66,
                "TERM"
            ]
        ]
    },
    {
        "text": "This particular quantitative information about lexis and grammar suggests a complex interaction of grammar, lexis, register, and phraseology in relation to frequency (ibid.: 459).",
        "entities": [
            [
                47,
                52,
                "TERM"
            ],
            [
                108,
                113,
                "TERM"
            ]
        ]
    },
    {
        "text": "Video recordings also allow us to document sign languages; see Chapter 4.3 for some details on their corpus-based investigation.",
        "entities": [
            [
                101,
                113,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, as we have seen, the keyword list is a result of multiple decisions in the process starting with the selection of the reference corpus and finishing with the choice of the particular statistic.",
        "entities": [
            [
                127,
                143,
                "TERM"
            ],
            [
                30,
                37,
                "TERM"
            ]
        ]
    },
    {
        "text": "Speaking meaningfully about corpus frequencies is not straightforward.",
        "entities": [
            [
                28,
                34,
                "TERM"
            ]
        ]
    },
    {
        "text": "Again, clicking on the item and investigating the concordance lines will soon tell us that s isn't only used to mark a particular speaker, but of course also represents the clitic (contraction) forms of is (as in that's) and us (as in let's), although there are no possessive markers in the corpus.",
        "entities": [
            [
                50,
                61,
                "TERM"
            ],
            [
                291,
                297,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is important to point out from the outset that the term parallel corpus is to some extent ambiguous, because it is sometimes used to refer to comparable original texts in two or more languages, especially texts that 258 M.-A. Lefer belong to comparable genres or text types and deal with similar topics (e.g. Italian and German newspaper articles about migration or English and Portuguese medical research articles).",
        "entities": [
            [
                68,
                74,
                "TERM"
            ],
            [
                266,
                270,
                "TERM"
            ]
        ]
    },
    {
        "text": "A second step may be to manipulate the actual linguistic data (that is, what the A. Ädel people represented in the corpus said or wrote) by changing also names and places mentioned which could in some way give away the source. In the case of image data, this would involve masking participants' identity in various ways.",
        "entities": [
            [
                115,
                121,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, we have to remember that the type/token ratio is very sensitive to the length of the text; it decreases as the text becomes longer and more words get used again (recycled).",
        "entities": [
            [
                43,
                48,
                "TERM"
            ],
            [
                38,
                42,
                "TERM"
            ],
            [
                94,
                98,
                "TERM"
            ],
            [
                120,
                124,
                "TERM"
            ]
        ]
    },
    {
        "text": "When a researcher taking part in corpus-based research is recognized, his/her names are employed as key terms to deal for research offspring in basic scholastic data.",
        "entities": [
            [
                33,
                45,
                "TERM"
            ]
        ]
    },
    {
        "text": "Linguists normally look askance at 'bag of words' studies, but I do think they offer interesting new ways of carrying out 'corpus-driven' research.",
        "entities": [
            [
                123,
                136,
                "TERM"
            ]
        ]
    },
    {
        "text": "Importantly, the exact search expressions should be reported and the settings used should be specified (e.g. list of word separators, minimum frequency or dispersion threshold for word lists; cf. Part II for the settings typically associated with different corpus methods).",
        "entities": [
            [
                257,
                263,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first of these is that they act as a means to reflect the hierarchical structure and logic of the text.",
        "entities": [
            [
                102,
                106,
                "TERM"
            ]
        ]
    },
    {
        "text": "We will also discuss best practices to follow when making a manual annotation and present the different ways to assess the reliability of such annotations.",
        "entities": [
            [
                67,
                77,
                "TERM"
            ]
        ]
    },
    {
        "text": "Letters, for instance, have several advantages as a source of historical text material.",
        "entities": [
            [
                73,
                77,
                "TERM"
            ]
        ]
    },
    {
        "text": "The remainder of the chapter then focuses more specifically on the individual methodological considerations (e.g. ensuring that a corpus is \"balanced\") that anyone planning to create a corpus needs to address.",
        "entities": [
            [
                130,
                136,
                "TERM"
            ],
            [
                185,
                191,
                "TERM"
            ]
        ]
    },
    {
        "text": "In total, this corpus includes 200 articles which correspond to approximately 400,000 words.",
        "entities": [
            [
                15,
                21,
                "TERM"
            ]
        ]
    },
    {
        "text": "This research specifies that all forms of the lemma acteur will be retrieved from the corpus when a word tagged as an adjective appears to its right.",
        "entities": [
            [
                86,
                92,
                "TERM"
            ],
            [
                46,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "Focusing on the first kind for a moment, it would be possible of course to manually annotate texts using any standard word processor, but here it is useful to have software tools that check annotation as it is added, e.g. to ensure that typos in tags or category labels do not occur, and to allow standard mark-up formats (such as XML) to be employed consistently and correctly in the resulting corpus, e.g. as in the Dexter software.",
        "entities": [
            [
                190,
                200,
                "TERM"
            ],
            [
                395,
                401,
                "TERM"
            ],
            [
                331,
                334,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, with a small corpus, there is probably a lot less of this \"junk\" to throw out.",
        "entities": [
            [
                22,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "As we will see below, in the field of stylistics, corpus linguistics tools do not seek to replace qualitative analysis, but only aim to guide the choice of themes and excerpts to analyze.",
        "entities": [
            [
                50,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, the British National Corpus (BNC) has annotation that shows the corpus compilers considered of course, for example, for instance, according to, irrespective of, etc. to be one lexical item each, which means one would count of course, not of and course separately.",
        "entities": [
            [
                51,
                61,
                "TERM"
            ],
            [
                77,
                83,
                "TERM"
            ]
        ]
    },
    {
        "text": "Once a draft version of a text contains \"metadata\" and \"textual markup\", it can then be placed into a \"lexical (pending proofreading)\" directory to indicate that the text will be ready for use as a lexical version of the corpus once it has been proofread.",
        "entities": [
            [
                41,
                49,
                "TERM"
            ],
            [
                64,
                70,
                "TERM"
            ],
            [
                221,
                227,
                "TERM"
            ],
            [
                26,
                30,
                "TERM"
            ],
            [
                166,
                170,
                "TERM"
            ]
        ]
    },
    {
        "text": "After that we store the results we obtain by sapplying functions to the list to count lengths and compute type-token ratios (with an anonymous/inline function); we compute a mean type-token ratio using all types and tokens, and a mean of all lengths of utterances with mean, but to avoid the effect that outliers might have we use the argument trim=0.05 to discard both the smallest and the largest 5 percent of the data.",
        "entities": [
            [
                179,
                195,
                "TERM"
            ],
            [
                111,
                116,
                "TERM"
            ],
            [
                106,
                110,
                "TERM"
            ]
        ]
    },
    {
        "text": "Conrad argued that three changes prompted by corpus-based studies of grammar had \"the potential to revolutionize the teaching of grammar\" (2000: 549): first, monolithic descriptions of English grammar would be replaced by register-specific descriptions; second, the teaching of grammar would become more integrated with the teaching of vocabulary; and third, the emphasis would shift from structural accuracy to the appropriate conditions of use for alternative grammatical constructions.",
        "entities": [
            [
                45,
                57,
                "TERM"
            ]
        ]
    },
    {
        "text": "For the genitive alternation, the text ID and the possessor head lemma are used as crossed random effects.",
        "entities": [
            [
                65,
                70,
                "TERM"
            ],
            [
                34,
                38,
                "TERM"
            ]
        ]
    },
    {
        "text": "A corpus with more or longer texts will allow more words in them.",
        "entities": [
            [
                2,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, if the goal is to reach a sample of 200,000 words per text genre, this number of words can almost be instantly reached by including one or two entire books in the sample.",
        "entities": [
            [
                39,
                45,
                "TERM"
            ],
            [
                176,
                182,
                "TERM"
            ],
            [
                67,
                71,
                "TERM"
            ]
        ]
    },
    {
        "text": "We can already see, therefore, from this relatively short sample of text that a failure to deal with this feature could cause issues in a number of places throughout the text.",
        "entities": [
            [
                58,
                64,
                "TERM"
            ],
            [
                68,
                72,
                "TERM"
            ],
            [
                170,
                174,
                "TERM"
            ]
        ]
    },
    {
        "text": "Most forms of traditional non-corpus-assisted discourse analysis have practiced the close-reading (that is, \"qualitative analysis\") of single texts or a small number of texts in the attempt to highlight both textual structures and also how meanings are conveyed.",
        "entities": [
            [
                30,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first thing you obviously need to consider is what type of spoken data you may want to analyse.",
        "entities": [
            [
                55,
                59,
                "TERM"
            ]
        ]
    },
    {
        "text": "On closer inspection, however, it becomes apparent that we may be dealing with a different type of exception here: the word pavement has additional senses to the one cited in (5a) above, one of which does exist in American English.",
        "entities": [
            [
                91,
                95,
                "TERM"
            ]
        ]
    },
    {
        "text": "Recall that corpus linguistics includes both quantitative and qualitative analysis.",
        "entities": [
            [
                12,
                30,
                "TERM"
            ]
        ]
    },
    {
        "text": "Looking at the words that exclusively occur in the general corpus, we can see some interesting types, namely concerning and regarding, that have been classified as prepositions despite the fact that they don't look like typical prepositions because they are in fact ing-forms, that is, they clearly still retain some verbal character.",
        "entities": [
            [
                59,
                65,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, it could turn out that an annotation set used for a corpus is based on false assumptions.",
        "entities": [
            [
                39,
                49,
                "TERM"
            ],
            [
                65,
                71,
                "TERM"
            ]
        ]
    },
    {
        "text": "Something similar, albeit not to signal a parenthetical but instead some kind of pseudo-punctuation, happens again for \"Yes-or\" a little further down in the text.",
        "entities": [
            [
                157,
                161,
                "TERM"
            ]
        ]
    },
    {
        "text": "This chapter has highlighted the growing recognition of the crucial role played by the register parameter in historical corpus linguistics.",
        "entities": [
            [
                120,
                138,
                "TERM"
            ]
        ]
    },
    {
        "text": "Perspectives on the matter vary between scholarly approaches: the criteria for determining what is \"good data\" -for example, considerations on the accessibility and processing of the data -can be quite different in Digital Humanities and corpus linguistics.",
        "entities": [
            [
                238,
                256,
                "TERM"
            ]
        ]
    },
    {
        "text": "This problem can be handled with annotation during the actual transcription of the recording that marks certain sections as inaudible.",
        "entities": [
            [
                33,
                43,
                "TERM"
            ]
        ]
    },
    {
        "text": "Again the POS-tagged VOICE corpus clarifies the situation.",
        "entities": [
            [
                27,
                33,
                "TERM"
            ]
        ]
    },
    {
        "text": "Finally, we should make sure that the sample is acceptable from an ethical point of view, in the sense that it respects the right to anonymity of the person involved and that it does not contain inappropriate content.",
        "entities": [
            [
                38,
                44,
                "TERM"
            ]
        ]
    },
    {
        "text": "They annotated 2,986 attestations captured as concordance lines for 14 variables that were previously shown to impact native speakers' choices, including the semantic relation encoded by the noun phrases, the morphological number marking on the noun phrases, their animacy, specificity, complexity, and, crucially, the L1 background of the learners, among others.",
        "entities": [
            [
                46,
                57,
                "TERM"
            ]
        ]
    },
    {
        "text": "Alternatively, we may apply selective sampling methods that are used in the Brown Corpus and the LOB corpus or consider more suitable methods of text representation keeping in mind the goal and purpose of a corpus.",
        "entities": [
            [
                101,
                107,
                "TERM"
            ],
            [
                207,
                213,
                "TERM"
            ],
            [
                145,
                149,
                "TERM"
            ]
        ]
    },
    {
        "text": "Since the late 1990s when linguists began to turn to the web as a corpus, search engines have actually become less advanced in terms of the options they offer.",
        "entities": [
            [
                66,
                72,
                "TERM"
            ]
        ]
    },
    {
        "text": "Section 3 focuses on some pitfalls related to big corpora, and our examples in this section concern both the reliability of the semi-automated sampling of such resources and the comparability of the research results when new genres are introduced to the corpus.",
        "entities": [
            [
                178,
                191,
                "TERM"
            ],
            [
                254,
                260,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is helpful to know the relationship between variables in your corpus generally.",
        "entities": [
            [
                65,
                71,
                "TERM"
            ]
        ]
    },
    {
        "text": "But in addition to PoS tagging syntactic annotations also pick up more aspects of a syntactic structure.",
        "entities": [
            [
                23,
                30,
                "TERM"
            ]
        ]
    },
    {
        "text": "Like the phase 2 work outlined above, there is an attempt to rely on information that emerges from the text ('trust the text', as Sinclair says), rather than on information that is presupposed.",
        "entities": [
            [
                103,
                107,
                "TERM"
            ],
            [
                120,
                124,
                "TERM"
            ]
        ]
    },
    {
        "text": "Monospaced font indicates instructions/text to be typed into the computer, such as a search string or regular expression.",
        "entities": [
            [
                39,
                43,
                "TERM"
            ]
        ]
    },
    {
        "text": "This chapter will guide you through the steps and procedures to actually put the corpus to use and to report on your research findings.",
        "entities": [
            [
                81,
                87,
                "TERM"
            ]
        ]
    },
    {
        "text": "After the loop, we'll pick the most frequent n adjectives (something like n = 2,000 for the learner data case study and n = 5,000 for the Brown corpus case study) occurring in it and tag all occurrences of these forms in the untagged corpus files, and then we will retrieve sequences of two adjective tags and whatever they tag from these corpus files; with the ICLE corpus, we will actually save the tagged corpus files before we search them, with the Brown corpus example, we'll tag the files and immediately search them while they are still in memory.",
        "entities": [
            [
                144,
                150,
                "TERM"
            ],
            [
                234,
                240,
                "TERM"
            ],
            [
                339,
                345,
                "TERM"
            ],
            [
                367,
                373,
                "TERM"
            ],
            [
                408,
                414,
                "TERM"
            ],
            [
                459,
                465,
                "TERM"
            ]
        ]
    },
    {
        "text": "Many quantitative corpus analyses are based on concordance data (though not necessarily all: one could think of, for example, a study that is based on frequency or collocation lists instead, see Chaps.",
        "entities": [
            [
                164,
                175,
                "TERM"
            ],
            [
                47,
                58,
                "TERM"
            ],
            [
                18,
                24,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, the opening of the section in ECL1 on \"Structural Markup\" makes reference to ASCII (American Standard Code for Information Interchange) text formats and standard generalized markup language (SGML).",
        "entities": [
            [
                188,
                203,
                "TERM"
            ],
            [
                150,
                154,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, a researcher who is interested in spoken workplace discourse could document demographic information about speakers' job titles and ages and whether interactions involve peers or managers/subordinates, and then include in the corpus a predetermined proportion of texts from each category.",
        "entities": [
            [
                238,
                244,
                "TERM"
            ]
        ]
    },
    {
        "text": "As backchannels constitute text where another speaker simply provides feedback to the current speaker who holds the turn, but doesn't really interrupt to take over, it also makes sense to incorporate this via an empty element.",
        "entities": [
            [
                27,
                31,
                "TERM"
            ]
        ]
    },
    {
        "text": "To study press reportage, for instance, it is only necessary to take from a given corpus all samples of press reportage, and to study within this sub-corpus whatever one wishes to focus on.",
        "entities": [
            [
                82,
                88,
                "TERM"
            ],
            [
                150,
                156,
                "TERM"
            ]
        ]
    },
    {
        "text": "This type can be seen as the prototype of the early borrowings with which the word-formation process originated.",
        "entities": [
            [
                5,
                9,
                "TERM"
            ]
        ]
    },
    {
        "text": "Given that the information was public at the time of crawling, one could make a case that including it in the corpus is unproblematic, but it is quite easy to imagine scenarios in which the publication of data crawled from the web can lead to legally or ethically challenging situations.",
        "entities": [
            [
                110,
                116,
                "TERM"
            ]
        ]
    },
    {
        "text": "We'll discuss options of how to actually mark (up) and distinguish larger units of text in Chapter 11, when we talk about annotation.",
        "entities": [
            [
                122,
                132,
                "TERM"
            ],
            [
                83,
                87,
                "TERM"
            ]
        ]
    },
    {
        "text": "XML can also be used for encoding the metadata of a corpus (see Chapter 6, section 6.4).",
        "entities": [
            [
                38,
                46,
                "TERM"
            ],
            [
                25,
                33,
                "TERM"
            ],
            [
                52,
                58,
                "TERM"
            ],
            [
                0,
                3,
                "TERM"
            ]
        ]
    },
    {
        "text": "In practice, some corpus tools let us set the minimum cut-off limits for the frequencies of words in C and R before considering them in the keyword procedure.",
        "entities": [
            [
                140,
                147,
                "TERM"
            ],
            [
                18,
                24,
                "TERM"
            ]
        ]
    },
    {
        "text": "Of course, different tagging programs will have different terminology and provide varying levels of detail about the items being tagged.",
        "entities": [
            [
                21,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "Nevertheless, looking for elements in a corpus, even a computerized one, by using a simple word processing tool is rather inconvenient.",
        "entities": [
            [
                40,
                46,
                "TERM"
            ]
        ]
    },
    {
        "text": "But these texts also tend to be long enough for reasonable quantitative corpus-linguistic analysis.",
        "entities": [
            [
                72,
                78,
                "TERM"
            ]
        ]
    },
    {
        "text": "A corpus is useful to observe the variation of these properties in constellation with each other and see how each of these independent variables tend to affect our dependent variable.",
        "entities": [
            [
                2,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "Mastering these terms will make reading of the rest of the book, and many papers in corpus linguistics, much easier.",
        "entities": [
            [
                84,
                102,
                "TERM"
            ]
        ]
    },
    {
        "text": "Even when working within a text genre, we should aim to diversify its sources as much as possible.",
        "entities": [
            [
                27,
                31,
                "TERM"
            ]
        ]
    },
    {
        "text": "The primary goal of semantic annotation is to identify the sense a word denotes when it is used in a text.",
        "entities": [
            [
                29,
                39,
                "TERM"
            ],
            [
                101,
                105,
                "TERM"
            ]
        ]
    },
    {
        "text": "In addition, from frequency rank number 4,077 onwards, the words in the corpus only have one occurrence.",
        "entities": [
            [
                72,
                78,
                "TERM"
            ]
        ]
    },
    {
        "text": "As we saw in the preceding chapter, corpora fell out of favor just as linguistics grew into an academic discipline in its own right and as a result, corpus-based studies of language were relegated to the margins of the field.",
        "entities": [
            [
                149,
                161,
                "TERM"
            ]
        ]
    },
    {
        "text": "Due to the practical and theoretical difficulties of defining and measuring complexity, the vast majority of corpus-based studies operationalize Weight in terms of some measure of Word Length even if they theoretically conceptualize it in terms of complexity.",
        "entities": [
            [
                109,
                121,
                "TERM"
            ]
        ]
    },
    {
        "text": "The overarching goal of statistical analysis is to estimate parameters (e.g. mean, standard deviation) of a population by measuring those parameters in a sample.",
        "entities": [
            [
                154,
                160,
                "TERM"
            ]
        ]
    },
    {
        "text": "Indeed, we speculate that in years to come, much, if not all, pragmatics research will involve corpus linguistics.",
        "entities": [
            [
                95,
                113,
                "TERM"
            ]
        ]
    },
    {
        "text": "The corpus is made up of 11 sub-sections containing at least 10 texts each, produced under similar conditions, namely by students of the same level.",
        "entities": [
            [
                4,
                10,
                "TERM"
            ]
        ]
    },
    {
        "text": "Even the diachronic ones are, because they've not been designed to be added to later, even if, for example, at some point in time a further Old English epic may somehow be unearthed and could thus theoretically be included in a new edition of the Helsinki Corpus.",
        "entities": [
            [
                9,
                19,
                "TERM"
            ]
        ]
    },
    {
        "text": "These meet the criterion of having been produced in a natural setting because journalists write the article to be published in newspapers and to communicate something to their readers, not because they want to fill a linguist's corpus.",
        "entities": [
            [
                228,
                234,
                "TERM"
            ]
        ]
    },
    {
        "text": "We already saw that the issue of data annotation is extremely complex even in the case of individual lexical items, and the preceding chapter discussed some more complicated examples.",
        "entities": [
            [
                38,
                48,
                "TERM"
            ]
        ]
    },
    {
        "text": "One commonly used tagset is CLAWS (Constituent Likelihood Automatic Word-tagging System), available in different versions (e.g., CLAWS 5 contains just over 60 tags, while CLAWS 7 contains over 160).",
        "entities": [
            [
                73,
                80,
                "TERM"
            ]
        ]
    },
    {
        "text": "Essentially, the option we just explored has relatively little to do with keywords as calculated through the options from the top part of the same page, as all it really does is eliminate all word types both corpora share, and then display whatever remains as a frequency list.",
        "entities": [
            [
                262,
                276,
                "TERM"
            ]
        ]
    },
    {
        "text": "In all cases, it is necessary to study the rights of use indicated by the respective sites before starting to compile the corpus.",
        "entities": [
            [
                122,
                128,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this section, we want to explore how to use a form of XML that I refer to as 'Simple XML' in order to do annotation on multiple levels.",
        "entities": [
            [
                108,
                118,
                "TERM"
            ],
            [
                57,
                60,
                "TERM"
            ],
            [
                88,
                91,
                "TERM"
            ]
        ]
    },
    {
        "text": "Now, some corpora (e.g., COCA) have POS tags attached to each word, and some corpora (e.g., Michigan Corpus of Academic Spoken English [MICASE], and Michigan Corpus of Undergraduate Student Papers [MICUSP]) do not have that feature in addition to the actual words in a corpus.",
        "entities": [
            [
                269,
                275,
                "TERM"
            ]
        ]
    },
    {
        "text": "For both the spoken and written parts of the corpus, not all samples are exactly 2,000 words: a sample is not broken off in mid-sentence but at a point (often over or just under the 2,000-word limit) where a natural break occurs.",
        "entities": [
            [
                45,
                51,
                "TERM"
            ],
            [
                96,
                102,
                "TERM"
            ]
        ]
    },
    {
        "text": "That being done, bilingual concordancers search directly for the occurrences of a word in one of the two languages of the corpus, and simultaneously extract the matching sentence in the other language.",
        "entities": [
            [
                122,
                128,
                "TERM"
            ]
        ]
    },
    {
        "text": "On the other hand, the history of many fiction genres, including drama and short stories, is certainly longer than the coverage of COHA, so their proportions could have been more optimally balanced in the corpus; considering the automated sampling of COHA, their better representation in the most recent periods probably reflects their increased availability in digital form.",
        "entities": [
            [
                205,
                211,
                "TERM"
            ]
        ]
    },
    {
        "text": "The architecture chosen for a certain corpus refers to the conceptual division of different types of objects contained in a corpus, such as texts, annotations and metadata.",
        "entities": [
            [
                163,
                171,
                "TERM"
            ],
            [
                38,
                44,
                "TERM"
            ],
            [
                124,
                130,
                "TERM"
            ]
        ]
    },
    {
        "text": "Longitudinal corpora imply regular recordings over at least several months, transcriptions of each utterance (ideally not only of the target child, but of all interlocutors), and a multitude of different annotation levels, depending on the questions of the respective project.",
        "entities": [
            [
                204,
                214,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this chapter, we have discussed the main elements to consider when creating a new corpus.",
        "entities": [
            [
                85,
                91,
                "TERM"
            ]
        ]
    },
    {
        "text": "The frequency list based on lemmas confirms our initial assumption (based on Zipf's law) about the number of words with the frequency of 30 and over: there are 3,196 such lemmas in the corpus.",
        "entities": [
            [
                4,
                18,
                "TERM"
            ],
            [
                185,
                191,
                "TERM"
            ]
        ]
    },
    {
        "text": "The fact that this colligation with its changing realization (that, it) occurs in both predicative and existential matrices suggests that the so-called it-extraposition construction is part of a larger class of evolving complementation constructions, even though, because of the different matrix syntax, reference to the complement is obligatory in predicative and optional in existential matrices.",
        "entities": [
            [
                19,
                30,
                "TERM"
            ]
        ]
    },
    {
        "text": "Now we will consider the defining characteristics of corpus linguistics as they will be used in this book.",
        "entities": [
            [
                53,
                71,
                "TERM"
            ]
        ]
    },
    {
        "text": "The more frequent a linguistic phenomenon, the better it can be studied on the basis of a small corpus.",
        "entities": [
            [
                96,
                102,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, if the sample chosen includes 500 students met at the exit of a university building, the sample obtained will most likely not correspond to the actual result of the election, since this sample is not representative.",
        "entities": [
            [
                20,
                26,
                "TERM"
            ],
            [
                102,
                108,
                "TERM"
            ],
            [
                199,
                205,
                "TERM"
            ]
        ]
    },
    {
        "text": "Type I and type II errors are part and parcel of the procedure.",
        "entities": [
            [
                11,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "This research focuses on the word rather than on the lemma in order to exclude the masculine occurrences, acteur.",
        "entities": [
            [
                53,
                58,
                "TERM"
            ]
        ]
    },
    {
        "text": "Buysse (2020) compared the oral productions of French-speaking and Dutch-speaking learners of English in the LINDSEI corpus with the productions of native English speakers in the LOCNEC corpus, a corpus which was compiled to be comparable with the LINSDEI (see Chapter 4 for a definition of the concept of comparability).",
        "entities": [
            [
                306,
                319,
                "TERM"
            ],
            [
                117,
                123,
                "TERM"
            ],
            [
                186,
                192,
                "TERM"
            ],
            [
                196,
                202,
                "TERM"
            ]
        ]
    },
    {
        "text": "In its recent versions, the WordSmith concordancer also offers a similar function.",
        "entities": [
            [
                38,
                50,
                "TERM"
            ]
        ]
    },
    {
        "text": "Six of these are text types (instructive, administrative, etc.) and three are translation-related varieties (non-translated, translated from English, translated from French).",
        "entities": [
            [
                17,
                21,
                "TERM"
            ]
        ]
    },
    {
        "text": "While corpus linguistics has very little to say to those with interests in, for instance, generative grammar, it is of great value to those who are interested in studying pragmatics: how language is used.",
        "entities": [
            [
                6,
                24,
                "TERM"
            ]
        ]
    },
    {
        "text": "A more recent corpus, the Corpus of Web-Based Global English, is 1.9 billion words in length.",
        "entities": [
            [
                14,
                20,
                "TERM"
            ]
        ]
    },
    {
        "text": "Just like Firefox, it also breaks longer paragraphs into shorter lines, thus adding extra line breaks that do not form part of the original text.",
        "entities": [
            [
                140,
                144,
                "TERM"
            ]
        ]
    },
    {
        "text": "Development of a speech corpus involves issues and factors like purpose of use, selection of speakers, choice of settings, size of a corpus, use of recording instruments, manner of data sampling, manner of data elicitation, nature of transcription, types of data encoding, management of audio files, editing of input data, processing of spoken texts, annotation of speech, analysis of speech corpus, etc.",
        "entities": [
            [
                351,
                361,
                "TERM"
            ],
            [
                263,
                271,
                "TERM"
            ],
            [
                24,
                30,
                "TERM"
            ],
            [
                133,
                139,
                "TERM"
            ],
            [
                392,
                398,
                "TERM"
            ]
        ]
    },
    {
        "text": "If our corpus represents a sample rather than the population (which is typically the case), we should consider the amount of evidence we have in the sample.",
        "entities": [
            [
                7,
                13,
                "TERM"
            ],
            [
                27,
                33,
                "TERM"
            ],
            [
                149,
                155,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are several projects gathering very large corpora on a broader range of web-accessible text.",
        "entities": [
            [
                93,
                97,
                "TERM"
            ]
        ]
    },
    {
        "text": "As is obvious from the above, a lot of corpus-linguistic work does not discuss all their methodological aspects in sufficient detail, but in order to at least begin to approach the ideal of reproducibility, it is essential that all this information be provided at a sufficient level of detail.",
        "entities": [
            [
                39,
                45,
                "TERM"
            ]
        ]
    },
    {
        "text": "Also, the effects of demographic information on the use of specific linguistic constructions could be explored with the help of a corpus designed as a historical or diachronic corpus.",
        "entities": [
            [
                165,
                175,
                "TERM"
            ],
            [
                130,
                136,
                "TERM"
            ],
            [
                176,
                182,
                "TERM"
            ]
        ]
    },
    {
        "text": "Following this procedure, the sample corresponding to the first 18-year-old male participant from Marseille registered in the corpus would be saved in a file called \"221001.txt\".",
        "entities": [
            [
                126,
                132,
                "TERM"
            ],
            [
                30,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "Consequently, different corpora contained different kinds of markup to describe the same linguistic phenomena.",
        "entities": [
            [
                61,
                67,
                "TERM"
            ]
        ]
    },
    {
        "text": "We must be careful as to how we select them in the corpus, though -and the best way to do so is to get a set of randomly selected relative clause sentences and continue our classification based on that.",
        "entities": [
            [
                51,
                57,
                "TERM"
            ]
        ]
    },
    {
        "text": "While in corpus linguistics concordancing has become a mainstream method, in literary criticism it does not seem to play a major role.",
        "entities": [
            [
                9,
                27,
                "TERM"
            ]
        ]
    },
    {
        "text": "The only 'word' that was apparently not an issue in this exercise is very, but I said \"apparently\" above because of course the same issue as for this also applies to very, only that you may not have noticed it because no form of very with an initial capital letter occurs in the text.",
        "entities": [
            [
                279,
                283,
                "TERM"
            ]
        ]
    },
    {
        "text": "Everybody who has ever worked in corpus linguistics knows that no corpus will ever be perfect, and that the quality of a corpus depends less on the competence of the researchers involved than on the resources they were able to put into it.",
        "entities": [
            [
                33,
                51,
                "TERM"
            ],
            [
                66,
                72,
                "TERM"
            ],
            [
                121,
                127,
                "TERM"
            ]
        ]
    },
    {
        "text": "Following this indication that corpus work could help these learners expand their lexicons, a scaled-up version of the project was prepared using two levels of learner, both experimental and control groups, two outcome measures corresponding to experimental and control conditions, and a learning target of 200 new word families per week for twelve weeks (or 2,400 words, roughly the number these learners would need to have a chance of reading for content in English).",
        "entities": [
            [
                31,
                37,
                "TERM"
            ]
        ]
    },
    {
        "text": "Measures of collocation strength differ with respect to the data needed to calcuate them, their computational intensiveness and, crucially, the quality of their results.",
        "entities": [
            [
                12,
                23,
                "TERM"
            ]
        ]
    },
    {
        "text": "First, it does not follow the principle of separating distinct semantic layers (such as segments vs. morpheme functions) in the annotation syntax, thus making this format harder to read, process, and convert than others.",
        "entities": [
            [
                128,
                138,
                "TERM"
            ]
        ]
    },
    {
        "text": "Sections 2.1 and 2.2 contrast studies based on their major methodological approaches, distinguishing between corpus-based studies of pre-selected lexical expressions versus corpus-driven studies to identify the full set of important multi-word sequences in a corpus.",
        "entities": [
            [
                173,
                186,
                "TERM"
            ],
            [
                109,
                121,
                "TERM"
            ],
            [
                259,
                265,
                "TERM"
            ]
        ]
    },
    {
        "text": "In such cases, we might have to either fall back on commercial corpora or compile our own corpus.",
        "entities": [
            [
                90,
                96,
                "TERM"
            ]
        ]
    },
    {
        "text": "Co-occurrence of words within a short span (i.e. 'collocation') is a traditional concept in Corpus Linguistics, as noted at the beginning of this paper.",
        "entities": [
            [
                50,
                61,
                "TERM"
            ]
        ]
    },
    {
        "text": "We, however, need to critically evaluate the assumptions of the tests before applying them; in the context of corpus linguistic analyses, the assumption of independence of observations is of utmost importance (see Sect. 20.2.2).",
        "entities": [
            [
                110,
                116,
                "TERM"
            ]
        ]
    },
    {
        "text": "As we have explained in Chapter 2, texts have different properties on two different dimensions, text-internal/linguistic and text-external/situational.",
        "entities": [
            [
                96,
                100,
                "TERM"
            ],
            [
                125,
                129,
                "TERM"
            ]
        ]
    },
    {
        "text": "This focus is closely related to key concerns in corpus linguistics showing that frequent patterns are not necessarily those that language users are aware of.",
        "entities": [
            [
                49,
                67,
                "TERM"
            ]
        ]
    },
    {
        "text": "The SBCSAE and the LLC cannot easily be combined into a larger corpus, since they mark prosodic features at very different levels of detail.",
        "entities": [
            [
                63,
                69,
                "TERM"
            ]
        ]
    },
    {
        "text": "Another distinction that can be made regarding the types of existing corpora relates to the type of processing carried out on the linguistic data of the corpus.",
        "entities": [
            [
                153,
                159,
                "TERM"
            ],
            [
                92,
                96,
                "TERM"
            ]
        ]
    },
    {
        "text": "Extremely large text archives, such as Google Books 3 6.",
        "entities": [
            [
                16,
                20,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, it would be quite difficult to create the type of student specific corpus mentioned above.",
        "entities": [
            [
                76,
                82,
                "TERM"
            ],
            [
                51,
                55,
                "TERM"
            ]
        ]
    },
    {
        "text": "This is well below the level required to claim statistical significance.",
        "entities": [
            [
                47,
                71,
                "TERM"
            ]
        ]
    },
    {
        "text": "The coefficient of variation (CV) describes the amount of variation relative to the mean relative frequency of a word or phrase in the corpus.",
        "entities": [
            [
                135,
                141,
                "TERM"
            ]
        ]
    },
    {
        "text": "You can see the actual words from the text in these three bands on the right-hand side.",
        "entities": [
            [
                38,
                42,
                "TERM"
            ]
        ]
    },
    {
        "text": "Only when corpus compilers have received the written consent of the speakers recorded for the corpus that their data can be used for research and be disseminated can corpora be used and shared.",
        "entities": [
            [
                10,
                16,
                "TERM"
            ],
            [
                94,
                100,
                "TERM"
            ]
        ]
    },
    {
        "text": "In order to analyze gender variation in this corpus, the gender of the author of each letter was predicted based on their first name, as listed in the byline of the letter.",
        "entities": [
            [
                45,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "The chapter advocates for more detailed annotation of named entities in large linguistic corpora that are already available.",
        "entities": [
            [
                40,
                50,
                "TERM"
            ]
        ]
    },
    {
        "text": "The motivation was to prepare it for the linguistic analysis within the corpus.",
        "entities": [
            [
                72,
                78,
                "TERM"
            ]
        ]
    },
    {
        "text": "In a general sense, a corpus can refer to any collection of texts that serve as the basis for analysis.",
        "entities": [
            [
                22,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "All of the tools listed require manual transcription, annotation and analysis of data: these processes are not automated.",
        "entities": [
            [
                54,
                64,
                "TERM"
            ]
        ]
    },
    {
        "text": "Thus, it is tremendously helpful if a corpus is extensible, that is, existing annotations can be amended or improved and new ones can be added, both by the original creators and by people who reuse the data.",
        "entities": [
            [
                38,
                44,
                "TERM"
            ]
        ]
    },
    {
        "text": "While it is available in a machine-readable format, it is too short to be representative of the types of sentences occurring in news stories, and whether the random sampling of sentences produced a balanced corpus is questionable.",
        "entities": [
            [
                207,
                213,
                "TERM"
            ]
        ]
    },
    {
        "text": "On the other hand, this type of research is corpus-based, because it starts from a hypothesis (e.g. \"passive sentences tend to be used more frequently with state verbs\"), and seeks to verify it in the corpus, which, in that way, only works as an analysis tool.",
        "entities": [
            [
                44,
                56,
                "TERM"
            ],
            [
                201,
                207,
                "TERM"
            ],
            [
                24,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "If you have chosen your favourite keyword list for American English, you might be interested in knowing which procedure was used to identify these keywords.",
        "entities": [
            [
                34,
                41,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this case, XML editing software may be required to simplify the process and check for consistency of the results.",
        "entities": [
            [
                14,
                17,
                "TERM"
            ]
        ]
    },
    {
        "text": "These are just some of the issues we need to constantly be aware of when we use such tools, so the idea that 'bigger is better', even if it is indeed often important to work with very large amounts of data for such research as collocation analysis in order to be able to find rarer combinations, may not always be fully justified if the quantity of data isn't equally matched by quality.",
        "entities": [
            [
                227,
                238,
                "TERM"
            ]
        ]
    },
    {
        "text": "You can see from these examples that 'text' in this sense is broader than a document or what you produce when typing in a text editor.",
        "entities": [
            [
                38,
                42,
                "TERM"
            ],
            [
                122,
                126,
                "TERM"
            ]
        ]
    },
    {
        "text": "A corpus can be designed to be the key material for many different research projects for a long time to come, or it can be created with a single project in mind, with no concrete plan to make it available to others.",
        "entities": [
            [
                2,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "That said, our analysis identified clear trends regarding the use of semantic shifts based on a large amount of data, further confirming the potential that corpus-based analyses have in understanding the patterns behind complex linguistic behaviors.",
        "entities": [
            [
                156,
                168,
                "TERM"
            ]
        ]
    },
    {
        "text": "For this encoding, each PDV symbol was assigned a unique fourdigit code.",
        "entities": [
            [
                9,
                17,
                "TERM"
            ]
        ]
    },
    {
        "text": "The English noun issue was used 1,957 times in the TED conference corpus.",
        "entities": [
            [
                66,
                72,
                "TERM"
            ]
        ]
    },
    {
        "text": "Let us assume that we find 67 hits in the female-speaker sample and 71 hits in the male-speaker sample to be such sentences.",
        "entities": [
            [
                57,
                63,
                "TERM"
            ],
            [
                96,
                102,
                "TERM"
            ]
        ]
    },
    {
        "text": "In all these fields, the use of text corpora has been obvious for a long time and corpora use was never interrupted as a result of Chomsky's work.",
        "entities": [
            [
                32,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "This word clearly refers to Tony Blair, a former Prime Minister of Great Britain, and will be more prominent in British than in American press reporting (64 times in nine texts versus one time in one text, respectively).",
        "entities": [
            [
                200,
                204,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, most popular part-of-speech taggers are trained on newspaper text, a register which tends to follow fairly strict style guides and contain few errors.",
        "entities": [
            [
                74,
                78,
                "TERM"
            ]
        ]
    },
    {
        "text": "A study of the word (in either or both of its senses) would obviously require that we look at the lemma PAVEMENT, comprising at least the word forms pavement (singular), pavements (plural) and, depending on how the corpus is prepared, pavement's (possessive).",
        "entities": [
            [
                215,
                221,
                "TERM"
            ],
            [
                98,
                103,
                "TERM"
            ]
        ]
    },
    {
        "text": "On the one hand, from the point of view of quantitative L2 analysis, these could thus be considered unwanted items or 'false positives' as their inclusion in word counts and concordance analyses will affect findings on what structures learners may have acquired and are able to produce.",
        "entities": [
            [
                174,
                185,
                "TERM"
            ]
        ]
    },
    {
        "text": "Before a multi-dimensional analysis can be performed, a corpus needs to be both lexically tagged and then analyzed by a particular statistical method termed factor analysis.",
        "entities": [
            [
                56,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "Moreover, privacy rights require corpus compilers to anonymise the disseminated data (cf. Chap. 1): while this is easily achieved in the transcriptions, where references to people and places can be removed, complete anonymization in audio files, i.e. the changing of the voice quality, would run counter and make impossible many research purposes of the corpus.",
        "entities": [
            [
                33,
                39,
                "TERM"
            ],
            [
                354,
                360,
                "TERM"
            ]
        ]
    },
    {
        "text": "A corpus of 100-articles would suffice for meeting a hypothetical 85% threshold for a list of 750 words, and that level of reliability could be achieved even for a full 1,000 words with corpora of ≥ 200-articles.",
        "entities": [
            [
                2,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "Second, just like a corpus, a speaker's linguistic experience is limited to certain language varieties: most English speakers have never been to confession or planned an illegal activity, for example, which means they will lack knowledge of certain linguistic structures typical of these situations.",
        "entities": [
            [
                20,
                26,
                "TERM"
            ]
        ]
    },
    {
        "text": "The multiple annotation schemes of SCOPIC are organised along functional categories.",
        "entities": [
            [
                13,
                23,
                "TERM"
            ]
        ]
    },
    {
        "text": "Various annotation systems have been developed for these different domains.",
        "entities": [
            [
                8,
                18,
                "TERM"
            ]
        ]
    },
    {
        "text": "The more specific corpus tools and methods that are employed comprise relatively basic techniques: the retrieval of clusters, key comparisons, concordance searches, and the identification of (significant) collocates.",
        "entities": [
            [
                143,
                154,
                "TERM"
            ],
            [
                18,
                24,
                "TERM"
            ]
        ]
    },
    {
        "text": "As intuition is usually an unreliable guide to patterns of collocation and semantic prosody, this study takes a corpus-based approach to addressing these research questions.",
        "entities": [
            [
                112,
                124,
                "TERM"
            ],
            [
                59,
                70,
                "TERM"
            ]
        ]
    },
    {
        "text": "Pasting the token counts from the 'Make/edit subcorpora' page also presents no problem because, this time, we only have one single piece of 'text' without HTML codes, so there's nothing to misinterpret for the spreadsheet program.",
        "entities": [
            [
                12,
                17,
                "TERM"
            ],
            [
                141,
                145,
                "TERM"
            ]
        ]
    },
    {
        "text": "The success of MD also directly depends on the reliability of the automatic identification of linguistic variables (tagging) in corpora.",
        "entities": [
            [
                116,
                123,
                "TERM"
            ]
        ]
    },
    {
        "text": "For a start, AntConc can only read text format files.",
        "entities": [
            [
                35,
                39,
                "TERM"
            ]
        ]
    },
    {
        "text": "This way, the actual effort of generating a frequency list, a collocate display, a dispersion plot, etc. often reduces to about the time you need with a concordance program.",
        "entities": [
            [
                44,
                58,
                "TERM"
            ],
            [
                153,
                164,
                "TERM"
            ]
        ]
    },
    {
        "text": "The most important thing to remember/look for in such a program is a menu item that either provides us with a 'Save as…' or 'Export' option from the 'File' menu to save the text as plain text, as we've seen for web pages earlier, or some item on a different menu that will probably contain the word 'extract', such as in older versions of Adobe Acrobat or GSview.",
        "entities": [
            [
                173,
                177,
                "TERM"
            ],
            [
                187,
                191,
                "TERM"
            ]
        ]
    },
    {
        "text": "The results indicated that male characters have a much longer speaking time than the female characters, more than four times more words in the corpus.",
        "entities": [
            [
                143,
                149,
                "TERM"
            ]
        ]
    },
    {
        "text": "As a rule of thumb, however, the highest number of matching or similar metadata field values might be a good starting point.",
        "entities": [
            [
                71,
                79,
                "TERM"
            ]
        ]
    },
    {
        "text": "The main one is probably that, in addition text-processing capabilities, R offers a large number of ready-made functions for the statistical evaluation and graphical representation of data, which allows you to perform just about all corpus-linguistic tasks within only one programming environment.",
        "entities": [
            [
                233,
                239,
                "TERM"
            ],
            [
                43,
                47,
                "TERM"
            ]
        ]
    },
    {
        "text": "Let's assume that we have asked two raters, a religious person and an atheist, to code the concordance lines from the 'Think about' task.",
        "entities": [
            [
                91,
                102,
                "TERM"
            ]
        ]
    },
    {
        "text": "By training computer algorithms on extensive corpora, researchers have been able to develop language models that possess the ability to understand and generate text that resembles human language.",
        "entities": [
            [
                160,
                164,
                "TERM"
            ]
        ]
    },
    {
        "text": "These logistical realities explain why 90 percent of the BNC (a second-generation corpus) contains written texts and only 10 percent spoken texts.",
        "entities": [
            [
                82,
                88,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this chapter, we have shown how corpus linguistics can be of use in different areas of applied linguistics.",
        "entities": [
            [
                35,
                53,
                "TERM"
            ]
        ]
    },
    {
        "text": "This does not commit one to distributing a corpus in this format: the ICAME CD-ROM (2nd ed.) allows users to work with an entire corpus saved in a single file.",
        "entities": [
            [
                43,
                49,
                "TERM"
            ],
            [
                129,
                135,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, there are also diachronic or historical corpora, which contain texts from earlier periods of English.",
        "entities": [
            [
                24,
                34,
                "TERM"
            ]
        ]
    },
    {
        "text": "Logistic regression models are easier to fit and easier to interpret than multinomial regression, and are what you will see most commonly in multivariate quantitative corpus linguistics.",
        "entities": [
            [
                167,
                185,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, a \"draft\" directory is useful for early stages of corpus development and can contain spoken texts that are in the process of being transcribed or written texts that have been computerized but not proofread.",
        "entities": [
            [
                64,
                70,
                "TERM"
            ]
        ]
    },
    {
        "text": "Try to access an annotated text and find the respective word for 'woman' .",
        "entities": [
            [
                27,
                31,
                "TERM"
            ]
        ]
    },
    {
        "text": "Finally, an annotation of the grammatical categories associated with each word requires establishing a list of these categories.",
        "entities": [
            [
                12,
                22,
                "TERM"
            ]
        ]
    },
    {
        "text": "After testing the appropriateness of the corpus for the research, the researcher needs to find suitable software tools to conduct the study, code the results and then analyze them through appropriate statistical tests.",
        "entities": [
            [
                41,
                47,
                "TERM"
            ]
        ]
    },
    {
        "text": "This function outputs random or pseudo-random samples, which is useful when, for example, you have a large number of matches or a large table and want to access only a small, randomly chosen sample or when you have a vector of words and want it re-sorted in a random order.",
        "entities": [
            [
                191,
                197,
                "TERM"
            ]
        ]
    },
    {
        "text": "As mentioned, Wmatrix performs both automatic annotation and retrieval.",
        "entities": [
            [
                46,
                56,
                "TERM"
            ]
        ]
    },
    {
        "text": "In a corpus of general song lyrics, you would want to include lyrics from different types of music (rock, rap, country, popular music, etc.) in order to achieve balance in your corpus.",
        "entities": [
            [
                161,
                168,
                "TERM"
            ],
            [
                5,
                11,
                "TERM"
            ],
            [
                177,
                183,
                "TERM"
            ]
        ]
    },
    {
        "text": "The 10 most frequent content words are the following: This list illustrates the fact that the most frequent words in a corpus are those belonging to functional categories such as prepositions and determiners.",
        "entities": [
            [
                119,
                125,
                "TERM"
            ]
        ]
    },
    {
        "text": "Other directories can be created to fit the needs of the research team building a particular corpus.",
        "entities": [
            [
                93,
                99,
                "TERM"
            ]
        ]
    },
    {
        "text": "On the other hand, in case of a monitor corpus, which tries to represent all possible text varieties, we may include all non-standard and ordinary text samples along with standard and established text samples.",
        "entities": [
            [
                32,
                46,
                "TERM"
            ],
            [
                86,
                90,
                "TERM"
            ],
            [
                147,
                151,
                "TERM"
            ],
            [
                196,
                200,
                "TERM"
            ]
        ]
    },
    {
        "text": "Another scientific requirement corpus linguists follow in principle is replicability of results.",
        "entities": [
            [
                31,
                37,
                "TERM"
            ]
        ]
    },
    {
        "text": "The main, \"create_kwic_concordance\" function is designed to accept five parameters: (1) the corpus location, (2) an option to ignore case when searching, (3) the search term, (4) a context size that defines how many tokens to the left and right of the search term will be shown in the KWIC results, and (5) a file path for the results file (lines 15-20).",
        "entities": [
            [
                92,
                98,
                "TERM"
            ]
        ]
    },
    {
        "text": "The app presents a wide range of visualizations and analyses from the Twitter corpus.",
        "entities": [
            [
                78,
                84,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are a number of reasons why a sample might be biased.",
        "entities": [
            [
                36,
                42,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is necessary to decide if a corpus should contain all types of written text samples or specific sets of text samples.",
        "entities": [
            [
                31,
                37,
                "TERM"
            ],
            [
                74,
                78,
                "TERM"
            ],
            [
                107,
                111,
                "TERM"
            ]
        ]
    },
    {
        "text": "Notable differences between lists would suggest limitations to their generalizability, and, ultimately, to the representativeness of the corpus upon which they were based.",
        "entities": [
            [
                111,
                129,
                "TERM"
            ],
            [
                137,
                143,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the ICE Project, such incomplete utterances are given an orthographic spelling that best reflects the pronunciation of the incompletely uttered word, and then the incomplete utterance is enclosed in markup, <.> i </.>, that labels the expression as an instance of an incomplete word.",
        "entities": [
            [
                202,
                208,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, we cannot take for granted that this method yields a balanced sampling, especially in the case of a small corpus.",
        "entities": [
            [
                115,
                121,
                "TERM"
            ]
        ]
    },
    {
        "text": "If you set the argument lines.around to a number greater than zero, then you increase the preceding and subsequent context by that number of corpus elements.",
        "entities": [
            [
                141,
                147,
                "TERM"
            ]
        ]
    },
    {
        "text": "Because token counts differ from tool to tool we should also provide details about the tool and/or how the token count was arrived at.",
        "entities": [
            [
                8,
                13,
                "TERM"
            ],
            [
                107,
                112,
                "TERM"
            ]
        ]
    },
    {
        "text": "This is because what it would in fact generally match is the very first occurrence of the character sequence, followed by the rest of the text, that is, a combination of word characters, whitespaces, punctuation marks, etc., until we reach the end of the text itself, which is -perhaps not so obviously -also a kind of word boundary.",
        "entities": [
            [
                138,
                142,
                "TERM"
            ],
            [
                255,
                259,
                "TERM"
            ]
        ]
    },
    {
        "text": "On the basis of this study and of a previous analysis of newspaper language, Laviosa proposes four \"core patterns of lexical use\" potentially applying to translated English in general, namely that: Translated texts have a relatively lower percentage of content words versus grammatical words (i.e. their lexical density is lower); the proportion of high frequency words versus low frequency words is relatively higher in translated texts; the list head of a corpus of translated texts accounts for a larger area of the corpus (i.e. the most frequent words are repeated more often); The list head of translated texts contains fewer lemmas.",
        "entities": [
            [
                458,
                464,
                "TERM"
            ],
            [
                519,
                525,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, U. Gut the interoperability between tools is still one of the major challenges for spoken corpus use and re-use across linguistic subdisciplines as discussed in Sect. 11.3.",
        "entities": [
            [
                99,
                105,
                "TERM"
            ]
        ]
    },
    {
        "text": "This could be done using the measure of referential distance discussed in Section 3.2 of Chapter 3, which (in slightly different versions) is the most frequently used operationalization in corpus linguistics.",
        "entities": [
            [
                189,
                207,
                "TERM"
            ]
        ]
    },
    {
        "text": "It needs to be explained that pragmatic research which truly focuses on language in use has to be corpus-based because there is no convenient way of making speakers say what you want them to say where language is unpredictable, messy, and extended over many turns.",
        "entities": [
            [
                98,
                110,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, the expression talk about occurred 6 times in the corpus.",
        "entities": [
            [
                64,
                70,
                "TERM"
            ]
        ]
    },
    {
        "text": "Likewise, because written corpora were encoded in text files, there was no way to indicate certain features of orthography, such as italicization or boldface fonts.",
        "entities": [
            [
                50,
                54,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this case, two scenarios are possible: either the conclusions of the study will be limited to the editorial style, or the corpus should be diversified in view of including other types of journalistic texts.",
        "entities": [
            [
                125,
                131,
                "TERM"
            ]
        ]
    },
    {
        "text": "After this brief excursion, let's return to investigating how we can make use of PoS tagging in BNCweb.",
        "entities": [
            [
                85,
                92,
                "TERM"
            ]
        ]
    },
    {
        "text": "Write three 5-gram sequences in English that you think may have a chance of being repeated more than once in a corpus.",
        "entities": [
            [
                111,
                117,
                "TERM"
            ]
        ]
    },
    {
        "text": "It displays the number of times each preposition type is found in a certain context.",
        "entities": [
            [
                49,
                53,
                "TERM"
            ]
        ]
    },
    {
        "text": "As a particular text is being worked on, a log is maintained that notes what work was done on the text and what work needs to be done.",
        "entities": [
            [
                16,
                20,
                "TERM"
            ],
            [
                98,
                102,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is easy for the end user to apply the method to their data, but the researcher's own input shows in the selection of the target corpus and an optimal reference corpus and the interpretation of the machine-produced key word lists, which is not simple at all (see below).",
        "entities": [
            [
                153,
                169,
                "TERM"
            ],
            [
                131,
                137,
                "TERM"
            ]
        ]
    },
    {
        "text": "Metadata is a key component of any corpus: users need to know precisely what is in a corpus.",
        "entities": [
            [
                35,
                41,
                "TERM"
            ],
            [
                85,
                91,
                "TERM"
            ]
        ]
    },
    {
        "text": "If you don't have such skills (yet), perhaps the best solution is to continue doing corpus-based studies.",
        "entities": [
            [
                84,
                96,
                "TERM"
            ]
        ]
    },
    {
        "text": "Data and information should be retrieved from a corpus.",
        "entities": [
            [
                48,
                54,
                "TERM"
            ]
        ]
    },
    {
        "text": "Each time the epistemological position of a researcher guides them to explore a theory through a corpus, the corpus plays a crucial role.",
        "entities": [
            [
                97,
                103,
                "TERM"
            ],
            [
                109,
                115,
                "TERM"
            ]
        ]
    },
    {
        "text": "The point of this case study was not to provide such an explanation but to show how an empirical basis can be provided using token frequencies derived from linguistic corpora.",
        "entities": [
            [
                125,
                130,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is possible to select the texts of a corpus randomly from a population of texts of interest.",
        "entities": [
            [
                40,
                46,
                "TERM"
            ]
        ]
    },
    {
        "text": "But corpus linguistics is also used for conducting research in fundamental areas of linguistics such as the study of syntax, since it makes it possible to identify the types of syntactic structures used in different languages.",
        "entities": [
            [
                4,
                22,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, the measure of Referential Distance discussed in Chapter 3, Section 3.2 yields cardinal data ranging from 0 to whatever maximum distance we decide on and it would be possible, and reasonable, to calculate the mean referential distance of a particular type of referring expression.",
        "entities": [
            [
                264,
                268,
                "TERM"
            ]
        ]
    },
    {
        "text": "Recall that the observed median animacy in our sample was 1 for the spossessive and 5 for the of -possessive, which deviates from the prediction of the H 0 in the direction of our H 1 .",
        "entities": [
            [
                47,
                53,
                "TERM"
            ]
        ]
    },
    {
        "text": "First, it allows us to trace the corpus evidence we see back to its source.",
        "entities": [
            [
                33,
                39,
                "TERM"
            ]
        ]
    },
    {
        "text": "For the spoken parts of a corpus, several additional variables need to be considered: the dialects the individuals speak, the contexts in which they speak, and the relationships they have with those they are speaking with.",
        "entities": [
            [
                26,
                32,
                "TERM"
            ]
        ]
    },
    {
        "text": "One way to understand linguistic analysis and language is through corpus linguistics, which looks at how language is used in certain contexts and how it can vary from context to context.",
        "entities": [
            [
                66,
                84,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is important to mention, however, that generalizing from a corpus will always be an extrapolation -it provides the evidence for interpretations about how language works.",
        "entities": [
            [
                62,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "For other types of text such as letters, text messages, etc., the units are so small that it makes no sense to not fully include them.",
        "entities": [
            [
                19,
                23,
                "TERM"
            ],
            [
                41,
                45,
                "TERM"
            ]
        ]
    },
    {
        "text": "Corpus tools and methods are now being applied very widely to historical data, learner language, and online varieties (Usenet, Emails, Blogs, and Microblogs), so I also consider the effect of non-standard or \"dirty data\" on corpus tools and methods, e.g. where spelling variation affects their robustness.",
        "entities": [
            [
                224,
                230,
                "TERM"
            ]
        ]
    },
    {
        "text": "Dimension scores are calculated for each text sample by adding up standardized frequencies with salient positive loadings and subtracting salient negative loadings on a dimension.",
        "entities": [
            [
                46,
                52,
                "TERM"
            ],
            [
                41,
                45,
                "TERM"
            ]
        ]
    },
    {
        "text": "Therefore, the time saved in using second-party transcripts is worth the tolerance of a certain level of error, provided that some checking is done to ensure overall accuracy in the transcripts included in a corpus.",
        "entities": [
            [
                208,
                214,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are contexts which might be expected to be particularly suitable to particular kinds of lexical relations and which could be used, given a large enough corpus, to identify word pairs in such relations.",
        "entities": [
            [
                158,
                164,
                "TERM"
            ]
        ]
    },
    {
        "text": "A portion of the corpus contains narrative texts produced by CP and CE1 students (6-7 years old), while the other section is made up of a series of argumentative texts produced by CE2 and CM1 students (8-9 years old).",
        "entities": [
            [
                17,
                23,
                "TERM"
            ]
        ]
    },
    {
        "text": "Definition (Final Version) Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus.",
        "entities": [
            [
                203,
                209,
                "TERM"
            ]
        ]
    },
    {
        "text": "Go through the above sample paragraph and make a list of how many elements there are and which type they belong to.",
        "entities": [
            [
                21,
                27,
                "TERM"
            ],
            [
                95,
                99,
                "TERM"
            ]
        ]
    },
    {
        "text": "The sample variance S 2 = P(1-P), and for a very small P value, it is roughly equivalent to P, namely x in this case.",
        "entities": [
            [
                4,
                10,
                "TERM"
            ]
        ]
    },
    {
        "text": "This corpus should be specific to the population of French-speaking Switzerland.",
        "entities": [
            [
                5,
                11,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, fitting a tree with a lower minimal criterion may be useful in a pilot study performed for exploratory purposes; • minimum 20 observations in a node for a split to be considered, specified by minsplit = 20; • according to the default settings, there should be at least seven observations in one node after a split, minbucket = 7.",
        "entities": [
            [
                157,
                161,
                "TERM"
            ],
            [
                308,
                312,
                "TERM"
            ]
        ]
    },
    {
        "text": "Among other things, the metadata appearing in the header should offer information about the main features of the participants: degree of relatedness (parents, friends, colleagues, strangers, etc.), the context in which the conversation took place, the manner in which the recording was captured, etc.",
        "entities": [
            [
                24,
                32,
                "TERM"
            ]
        ]
    },
    {
        "text": "It expresses the probability of the sample data being observed if the null hypothesis were true in the population.",
        "entities": [
            [
                36,
                42,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, an explanatory variable can be the genre/register or date of publication of a text as well as speaker's age, gender and language proficiency, to name only a few.",
        "entities": [
            [
                92,
                96,
                "TERM"
            ]
        ]
    },
    {
        "text": "These modes differ from written texts in that the raw data is not readily amenable to inclusion in our corpus.",
        "entities": [
            [
                103,
                109,
                "TERM"
            ]
        ]
    },
    {
        "text": "We will need the functions switch and menu (which you do not know yet so you may want to briefly look at their help pages -they are not difficult and the script will show you how they are used anyway) to prompt the user to choose the annotation format that will be processed, and we need a conditional with if to then define regular expressions for either choice.",
        "entities": [
            [
                234,
                244,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first part of this chapter surveys the development of corpus-based translation studies (CBTS), from the programmatic proposals in the early 1990s to recent developments and trends.",
        "entities": [
            [
                58,
                70,
                "TERM"
            ]
        ]
    },
    {
        "text": "As a matter of fact, if a certain annotation turns out to be impossible to achieve in a convergent manner for human annotators, this indicates that the phenomenon may be poorly defined or that the categories have been poorly delimited.",
        "entities": [
            [
                34,
                44,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, even if the texts of the corpus have been selected randomly, the sentences and words are not random.",
        "entities": [
            [
                34,
                40,
                "TERM"
            ]
        ]
    },
    {
        "text": "In corpus linguistic studies, lemmas can be particularly useful in making complex searches more tractable, especially when a single word appears in many forms.",
        "entities": [
            [
                3,
                9,
                "TERM"
            ]
        ]
    },
    {
        "text": "The differences between how literary genres are categorized largely reflect the different viewpoints and goals between the fields, with corpus linguistic categorizations aiming to focus on the different communicative purposes of texts, whereas the descriptions of genres by literature scholars are based more on content and stylistic concerns.",
        "entities": [
            [
                136,
                142,
                "TERM"
            ]
        ]
    },
    {
        "text": "XML is a hierarchical format (that lends itself well to representation as a tree that does not allow cross-nesting/overlapping) in which you add to data markup and annotation in the form of either start and end tags (which may contain attribute-value pairs), or just start tags (with attribute-value pairs), and in fact you have seen examples above that are similar to that annotation already.",
        "entities": [
            [
                164,
                174,
                "TERM"
            ],
            [
                374,
                384,
                "TERM"
            ],
            [
                153,
                159,
                "TERM"
            ],
            [
                0,
                3,
                "TERM"
            ]
        ]
    },
    {
        "text": "Posts were inspected to extract only those written by people with cancer (as opposed to family members, carers, or those experiencing symptoms that may be associated with cancer), which resulted in a final corpus comprising 12,757 posts and 1,629,370 words.",
        "entities": [
            [
                206,
                212,
                "TERM"
            ]
        ]
    },
    {
        "text": "Use the links in the online materials on the page 'Understanding File Formats & Their Properties' to go through each of the examples of different types of 'text' documents above and see how easy/difficult it is to identify where the actual text is.",
        "entities": [
            [
                156,
                160,
                "TERM"
            ],
            [
                240,
                244,
                "TERM"
            ]
        ]
    },
    {
        "text": "One of the novel software applications, the Keyword analysis, uses significance tests to distinguish words that are significantly more frequent or significantly less frequent than in a reference corpus; calculations are carried out automatically by the program and it is possible to gain valuable insights into the material that cannot be achieved with qualitative study.",
        "entities": [
            [
                185,
                201,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, raw video data encoded in one format such as MPEG1 will have to be regularly updated to new encoding schemes.",
        "entities": [
            [
                106,
                114,
                "TERM"
            ]
        ]
    },
    {
        "text": "That is, the distribution in the \"sample\" could be projected to the distribution of the \"population\".",
        "entities": [
            [
                34,
                40,
                "TERM"
            ]
        ]
    },
    {
        "text": "Of course, some markup is probably better inserted after a text sample is computerized.",
        "entities": [
            [
                16,
                22,
                "TERM"
            ],
            [
                64,
                70,
                "TERM"
            ],
            [
                59,
                63,
                "TERM"
            ]
        ]
    },
    {
        "text": "Since this era, advances in corpus linguistics have resulted in the creation of many different types of corpora containing authentic texts ranging from spontaneous conversations to technical writing in the social and natural sciences.",
        "entities": [
            [
                28,
                46,
                "TERM"
            ]
        ]
    },
    {
        "text": "I will develop this proposal by successively considering and dismissing alternative characterizations of corpus linguistics.",
        "entities": [
            [
                105,
                123,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, if a corpus contains 80 tokens of the words do/does/did followed by the full form of the word not and 20 tokens of these verbs followed by the contracted form of the word not, then the DO not contraction rate in that corpus is 20 percent.",
        "entities": [
            [
                18,
                24,
                "TERM"
            ],
            [
                230,
                236,
                "TERM"
            ]
        ]
    },
    {
        "text": "Besides, it is also necessary to determine which metadata will be associated with each corpus sample.",
        "entities": [
            [
                49,
                57,
                "TERM"
            ],
            [
                87,
                93,
                "TERM"
            ],
            [
                94,
                100,
                "TERM"
            ]
        ]
    },
    {
        "text": "Once the corpus is created and annotated, the most crucial part will be using the corpus for analysis.",
        "entities": [
            [
                9,
                15,
                "TERM"
            ],
            [
                82,
                88,
                "TERM"
            ]
        ]
    },
    {
        "text": "Because of the computational resources required to process and store all potential discontinuous sequences (frames) in large corpora (c. 10 million words in total) from a strictly corpus-driven approach, we used a MySQL database to store all possible four-word sequences in the two subcorpora (specifically all four-word sequences that did not cross (a) punctuation boundaries in the written corpus, and (b) turn boundaries in the spoken corpus).",
        "entities": [
            [
                180,
                193,
                "TERM"
            ],
            [
                392,
                398,
                "TERM"
            ],
            [
                438,
                444,
                "TERM"
            ]
        ]
    },
    {
        "text": "It should be noted that we should generally be sceptical of an all too clearcut conception of linguistic levels, and it is corpus-linguistic research that has advanced our understanding of interactions between, for instance, syntax, morphology and phonology in the area of clitics (some examples of which we observed above) and affixation.",
        "entities": [
            [
                123,
                129,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, corpus linguists have actually uncovered a number of relationships between words and linguistic phenomena beyond lexicon and grammar without making use of such annotations.",
        "entities": [
            [
                9,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "As a sub-type of this query option, there are also two highly useful menu options for querying in only spoken or written texts.",
        "entities": [
            [
                9,
                13,
                "TERM"
            ]
        ]
    },
    {
        "text": "If the residence history on the biographical form is unclear, it is also possible to interview the individual afterwards, provided that he or she can be located; if the individual does not fit the criteria for inclusion, his or her text can be discarded.",
        "entities": [
            [
                232,
                236,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, we defined American English as \"the language occurring in the BROWN and FROWN corpora\", but we saw that the FROWN corpus contains at least one misclassified text by a British author, and we also saw that it is questionable to assume that all and only speakers of American English produce the language we would want to call \"American English\" (recall the uses of sidewalk by British speakers).",
        "entities": [
            [
                127,
                133,
                "TERM"
            ],
            [
                170,
                174,
                "TERM"
            ]
        ]
    },
    {
        "text": "Moreover, this enables users of the corpus to plan the construction of their queries targeting these annotations.",
        "entities": [
            [
                36,
                42,
                "TERM"
            ]
        ]
    },
    {
        "text": "To make it a little easier to understand what the shortcuts do, just try to remember that the basic keyboard combinations without pressing the 'Shift' key -the one that switches between small and capital letters -will only help you to navigate through the document more efficiently, while combining them with 'Shift' will also select the text spans covered by the shortcuts.",
        "entities": [
            [
                338,
                342,
                "TERM"
            ]
        ]
    },
    {
        "text": "Finally, the chosen corpus should include productions made by adult native speakers.",
        "entities": [
            [
                20,
                26,
                "TERM"
            ]
        ]
    },
    {
        "text": "To be interpreted meaningfully, then, it must therefore nearly always be combined with some form of qualitative analysis -that is, an analysis that involves the linguist interacting with the actual discourse within the corpus and its structure and/or meaning.",
        "entities": [
            [
                219,
                225,
                "TERM"
            ]
        ]
    },
    {
        "text": "It describes how linguistic corpora have played an important role in providing corpus linguists with linguistic evidence to support the claims they make in the particular analyses of language that they conduct.",
        "entities": [
            [
                79,
                85,
                "TERM"
            ]
        ]
    },
    {
        "text": "Make sure to support your analysis with examples from the corpus.",
        "entities": [
            [
                58,
                64,
                "TERM"
            ]
        ]
    },
    {
        "text": "If you wish to show that one text is very similar to another, the higher the overlap the better.",
        "entities": [
            [
                29,
                33,
                "TERM"
            ]
        ]
    },
    {
        "text": "Perhaps most strikingly in need of study are the longer-term or secondary effects of regular concordance work on language awareness and sensitivity, autonomy, motivation, noticing, and other cognitive and metacognitive skills, and so on; their virtual absence in the studies covered here is no doubt due in large measure to the difficulty of assessing such features over time.",
        "entities": [
            [
                93,
                104,
                "TERM"
            ]
        ]
    },
    {
        "text": "The selection of the texts to include in your corpus depends on their suitability and their availability.",
        "entities": [
            [
                46,
                52,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this chapter, we have argued that meta-analysis should be more widely applied within corpus linguistics as a method of synthesizing and empirically reviewing previous corpus-based research.",
        "entities": [
            [
                88,
                106,
                "TERM"
            ],
            [
                170,
                182,
                "TERM"
            ]
        ]
    },
    {
        "text": "Given the wealth of speech that exists, as well as the logistical difficulties involved in recording and transcribing it, collecting data for the spoken part of a corpus is much more labor-intensive than collecting written samples.",
        "entities": [
            [
                163,
                169,
                "TERM"
            ]
        ]
    },
    {
        "text": "Thus, I will attempt in this chapter to sketch out a broad, and, I believe, ultimately uncontroversial characterization of corpus linguistics as an instance of the scientific method.",
        "entities": [
            [
                123,
                141,
                "TERM"
            ]
        ]
    },
    {
        "text": "The RD of a text is determined by counting all possible argument positions (S, A, P, obliques; cf. 11.2.1 and 11.2.2 above) and among these all positions are filled by an overt form (pronoun, lexical NP) rather than being left zero (where a referent is clearly identifiable from preceding context although no overt form appears); then the number of overt argument positions is divided by the number of all argument positions to yield the RD.",
        "entities": [
            [
                12,
                16,
                "TERM"
            ]
        ]
    },
    {
        "text": "This collection of papers focuses on questions of standards for the construction, annotation, searching, archiving and sharing of spoken corpora used in conversation analysis, sociolinguistics, discourse analysis and pragmatics.",
        "entities": [
            [
                82,
                92,
                "TERM"
            ]
        ]
    },
    {
        "text": "Section 3 presents small-scale studies of corpus searches involving words that may be frequently used as named entities, or parts of named entities.",
        "entities": [
            [
                42,
                48,
                "TERM"
            ]
        ]
    },
    {
        "text": "The FINREP corpus is a corpus of corporate financial reports, i.e. reports issued annually by companies providing financial information as well as commentaries about operational and financial performance.",
        "entities": [
            [
                11,
                17,
                "TERM"
            ],
            [
                23,
                29,
                "TERM"
            ]
        ]
    },
    {
        "text": "Though limited to recent periods, another way to counter the biases in the historical record is of course the compilation of diachronic corpora containing actual audio-recorded speech (see Chap. 11 for more information about spoken corpora).",
        "entities": [
            [
                125,
                135,
                "TERM"
            ]
        ]
    },
    {
        "text": "The second part of the book presents a range of case studies from the domains of lexicology, grammar, text linguistics and metaphor, including variationist and diachronic perspectives.",
        "entities": [
            [
                160,
                170,
                "TERM"
            ],
            [
                102,
                106,
                "TERM"
            ]
        ]
    },
    {
        "text": "Although each chapter includes a broad summary of previous research, the primary focus is on a more detailed description of the most important corpus-based studies in this area, with discussion of what those studies found and why they are especially important.",
        "entities": [
            [
                143,
                155,
                "TERM"
            ]
        ]
    },
    {
        "text": "Additional important layers of annotation are translation, morphological glosses where software such as ELAN can be useful (cf. Chapters 6 and 7).",
        "entities": [
            [
                31,
                41,
                "TERM"
            ]
        ]
    },
    {
        "text": "In these cases, changes are natural and inevitable and, if they are carefully made, the integrity of the corpus will not be compromised.",
        "entities": [
            [
                105,
                111,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, in order to estimate the importance of a word in a corpus, frequency is not the only element to take into account.",
        "entities": [
            [
                60,
                66,
                "TERM"
            ]
        ]
    },
    {
        "text": "Although this lemma occurs in a majority of corpus parts (Range 10) it has a very low Juilland's D (0.2).",
        "entities": [
            [
                44,
                50,
                "TERM"
            ],
            [
                14,
                19,
                "TERM"
            ]
        ]
    },
    {
        "text": "After this initial exploration, to answer the research question about the prominent topics in British public discourse we need to look at the weather terms in the context of other words (lemmas) in the corpus.",
        "entities": [
            [
                202,
                208,
                "TERM"
            ]
        ]
    },
    {
        "text": "Nevertheless, based on these observations, it would be highly advisable to inspect concordance lines of the highest-ranking collocations to check for possible skewing effects by names.",
        "entities": [
            [
                83,
                94,
                "TERM"
            ]
        ]
    },
    {
        "text": "These students often ask me what the best statistical test is to use with corpora, what the best collocation measure is etc.",
        "entities": [
            [
                97,
                108,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the simplest case (which we 4.1 Retrieval assumed to hold in the examples discussed in the previous chapter), a corpus will contain plain text in a standard orthography and the software will be able to find passages matching a specific string of characters.",
        "entities": [
            [
                115,
                121,
                "TERM"
            ],
            [
                141,
                145,
                "TERM"
            ]
        ]
    },
    {
        "text": "A welldesigned corpus includes texts that are relevant to the research goals of the study; are saved into a file format that allows different software programs to analyze the texts; and are labeled with enough relevant contextual material so that the different contexts are easily identifiable in the corpus.",
        "entities": [
            [
                15,
                21,
                "TERM"
            ],
            [
                301,
                307,
                "TERM"
            ]
        ]
    },
    {
        "text": "In order for the results to be replicable, corpus linguists need to make their choice of corpora and analytical techniques transparent.",
        "entities": [
            [
                43,
                49,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are 373 stem types occurring with -ic in the LOB corpus, with a mean length of 7.32 and a sample variance of 5.72; there are 153 stem types occurring with -ical, with a mean length of 6.60 and a sample variance of 4.57.",
        "entities": [
            [
                55,
                61,
                "TERM"
            ],
            [
                96,
                102,
                "TERM"
            ],
            [
                201,
                207,
                "TERM"
            ]
        ]
    },
    {
        "text": "She built a small but balanced corpus of randomly selected 30 class sessions from multiple corpora (MICASE, BASE, and others) where male and female instructors and levels of instruction (undergraduate, graduate) were both represented.",
        "entities": [
            [
                31,
                37,
                "TERM"
            ]
        ]
    },
    {
        "text": "With diachronic corpora (i.e. corpora used to study historical periods of English), the time frame for texts is somewhat easier to determine, since the various historical periods of English are fairly well-defined.",
        "entities": [
            [
                5,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "Actually, TED Talks are always made in English; as a result, English is the only source language, contrary to the Europarl corpus, where all languages are alternately source and target.",
        "entities": [
            [
                123,
                129,
                "TERM"
            ]
        ]
    },
    {
        "text": "In these cases, the most common operationalization strategy found in corpus linguistics is reference to a dictionary or lexical database.",
        "entities": [
            [
                69,
                87,
                "TERM"
            ]
        ]
    },
    {
        "text": "Yet, their inclusion in the corpus does serve the community' s interest.",
        "entities": [
            [
                28,
                34,
                "TERM"
            ]
        ]
    },
    {
        "text": "The final distinction I would like to mention at least briefly involves the encoding of the corpus files.",
        "entities": [
            [
                76,
                84,
                "TERM"
            ],
            [
                92,
                98,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first is that you can employ the right mouse button (two-finger tap on touchpads on the Mac) to activate the context menu inside the browser in order to be able to save the material, as otherwise clicking on the link will simply get the text displayed inside a browser window, rather than saved to your computer.",
        "entities": [
            [
                241,
                245,
                "TERM"
            ]
        ]
    },
    {
        "text": "Since the 1960s, a yet more specific meaning for the term has emerged: corpora in this sense are very large, machine-readable collections of spoken and written text that are analysed using computational methods.",
        "entities": [
            [
                160,
                164,
                "TERM"
            ]
        ]
    },
    {
        "text": "You can also determine the relative length and complexity of phrases (understood as dependency structures) by considering the number of head indices in the head column cross-referencing the head word of the phrase in question, or all phrases dependent on the verb node.",
        "entities": [
            [
                264,
                268,
                "TERM"
            ]
        ]
    },
    {
        "text": "One of the major issues we've repeatedly encountered, especially concerning the mega corpora we've worked with, is that the creation of large-scale resources may frequently lead to the compilers taking shortcuts when it comes to ensuring the quality of the data in terms of tokenisation and annotation.",
        "entities": [
            [
                291,
                301,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, as pointed out above, many (if not most) hypotheses in corpus linguistics do not take the form of universal statements (\"All X's are Y\", \"Z's always do Y\", etc.), but in terms of tendencies or preferences (\"X's tend to be Y\", \"Z's prefer Y\", etc.).",
        "entities": [
            [
                64,
                82,
                "TERM"
            ]
        ]
    },
    {
        "text": "In response, research disciplines devoted to information abstraction from very large collections of electronic text have come into being.",
        "entities": [
            [
                111,
                115,
                "TERM"
            ]
        ]
    },
    {
        "text": "In direct commercialization of corpus, one should seek permission from legal copyright holders.",
        "entities": [
            [
                31,
                37,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the field of new media, the CoMeRe database includes communication corpora mediated by networks, such as SMS/text messages, tweets, blogs, etc.",
        "entities": [
            [
                112,
                116,
                "TERM"
            ]
        ]
    },
    {
        "text": "Corpus work is therefore still rare; the databases that have been collected have mostly been small, and are perhaps best counted into the very generic category of \"corpus\" that in traditional philology was used to describe the language data investigated for a study.",
        "entities": [
            [
                164,
                170,
                "TERM"
            ]
        ]
    },
    {
        "text": "The written text produced in two different conditions can be analyzed for variation in linguistic features and interpreted functionally.",
        "entities": [
            [
                12,
                16,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, Brat is an online tool that makes it possible to annotate not only entities in a corpus, but also the relations between them.",
        "entities": [
            [
                94,
                100,
                "TERM"
            ]
        ]
    },
    {
        "text": "With better metadata about individual texts and speakers, we will be in a better position to understand the data, not only to correlate metadata to variation, but also to see more precisely how corpora differ in the case of comparison.",
        "entities": [
            [
                12,
                20,
                "TERM"
            ],
            [
                136,
                144,
                "TERM"
            ]
        ]
    },
    {
        "text": "If we calculate the normalized frequency of first-person pronouns in this text, we get as the result 200 first-person pronouns per 1,000 words.",
        "entities": [
            [
                74,
                78,
                "TERM"
            ]
        ]
    },
    {
        "text": "Searching and replacing text along the lines of what we just practised above is a very useful semi-automatic means of preparing your data efficiently, but you nevertheless constantly need to be aware of potential errors that might be introduced by replacing the wrong things or replacing them in the wrong order.",
        "entities": [
            [
                24,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "By default, AntConc uses UTF-8 encoding.",
        "entities": [
            [
                31,
                39,
                "TERM"
            ]
        ]
    },
    {
        "text": "The next step is to assign each corpus a set of independent labels.",
        "entities": [
            [
                32,
                38,
                "TERM"
            ]
        ]
    },
    {
        "text": "When comparing the information in the manuals, you'll hopefully spot that the composition of the different corpora is roughly modelled on that of the Brown Corpus, with only small variations in categories and numbers of sample texts.",
        "entities": [
            [
                220,
                226,
                "TERM"
            ]
        ]
    },
    {
        "text": "Firstly, corpus linguists need to be clear about their own epistemologies.",
        "entities": [
            [
                9,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, where corpus pragmatics' \"added value\" lies is in its insistence that these patterns be considered in light of the context -the situational, interpersonal, and cultural knowledge that interactional participants share.",
        "entities": [
            [
                15,
                21,
                "TERM"
            ]
        ]
    },
    {
        "text": "We then provide a register analytical framework for interpreting corpus findings (Chapter 2).",
        "entities": [
            [
                65,
                71,
                "TERM"
            ]
        ]
    },
    {
        "text": "Although clearly out of date now and fraught with a number of issues, including the corpus on which it was based, and subjective decisions regarding what should be included or not, there may be a baby in that bathwater.",
        "entities": [
            [
                84,
                90,
                "TERM"
            ]
        ]
    },
    {
        "text": "Language use does not consist merely in the production of individual utterances; instead, utterances are interrelated with each other in what is called text or discourse (cf. 2.2.1).",
        "entities": [
            [
                152,
                156,
                "TERM"
            ]
        ]
    },
    {
        "text": "Let us look at one example from the case study chapter below, the collocation of alphabetical order.",
        "entities": [
            [
                66,
                77,
                "TERM"
            ]
        ]
    },
    {
        "text": "Apart from that, diachronic corpora differ widely in size, composition, scope, annotation, and the nature of their textual material.",
        "entities": [
            [
                17,
                27,
                "TERM"
            ],
            [
                79,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "Select one of the areas described in this section and briefly discuss how corpus methodology has changed the way that research is done in the area.",
        "entities": [
            [
                74,
                80,
                "TERM"
            ]
        ]
    },
    {
        "text": "A corpus of this kind allows us to seek answers to the question of how the situational constraints of lingua franca use shape the language.",
        "entities": [
            [
                2,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "In a keyword list comparing WH-Obama with the one-million-word spoken section of the BNC Sampler (a collection of diverse discourse types) the following items all appeared among the top 200 keywords: continue (as in continue our efforts, continue to work on . . .), forward (move the economy forward, as we go forward to create an America that . . .), action, progress, effort/efforts, measures, steps, commitment, decision/decisions.",
        "entities": [
            [
                5,
                12,
                "TERM"
            ]
        ]
    },
    {
        "text": "Furthermore, instead of revealing interesting combinations of content words, you'll often find more grammatical constructions or combinations of function + content words, especially if the corpus is not very homogeneous, as in our case.",
        "entities": [
            [
                189,
                195,
                "TERM"
            ]
        ]
    },
    {
        "text": "Creating a machine-readable corpus can be a very costly and timeconsuming exercise.",
        "entities": [
            [
                28,
                34,
                "TERM"
            ]
        ]
    },
    {
        "text": "Hyperlinks are preserved and rendered in angle brackets (<…>), italicised text surrounded by forward slashes (/…/), and underlined text surrounded by underscores (_…_).",
        "entities": [
            [
                74,
                78,
                "TERM"
            ],
            [
                131,
                135,
                "TERM"
            ]
        ]
    },
    {
        "text": "Yet beginning corpus users might benefit from learning about what may be regarded as tacit knowledge in corpus linguistics, and even the more advanced scholar may encounter issues new to them that have been addressed earlier.",
        "entities": [
            [
                104,
                122,
                "TERM"
            ],
            [
                14,
                20,
                "TERM"
            ]
        ]
    },
    {
        "text": "Added to this is a layer of inadvertent 'noise' created along the way as a corpus text travels from historical manuscript or print to digital edition.",
        "entities": [
            [
                75,
                81,
                "TERM"
            ],
            [
                82,
                86,
                "TERM"
            ]
        ]
    },
    {
        "text": "That means we want our frequency list to first give the frequency of the letter \"c\", then that of \"d\", then \"f\", then \"j\", then \"i\" (because \"c\" is already covered), etc.",
        "entities": [
            [
                23,
                37,
                "TERM"
            ]
        ]
    },
    {
        "text": "Let's consider relevant instances of our case example like in the Brown corpus, given in (7.6): Tagging of corpora is done with a clearly defined and confined inventory of tags (a controlled vocabulary) that is called a tagset.",
        "entities": [
            [
                72,
                78,
                "TERM"
            ]
        ]
    },
    {
        "text": "Yet the by far most serious issue is the presence of errors in the source material, which introduces errors to analyses, and, in the worst case, may compromise the representativeness of corpora.",
        "entities": [
            [
                164,
                182,
                "TERM"
            ]
        ]
    },
    {
        "text": "Then, we will present two types of descriptive statistics that, respectively, make it possible to measure lexical diversity (the type/token ratio), and to calculate lexical dispersion in a corpus.",
        "entities": [
            [
                189,
                195,
                "TERM"
            ],
            [
                134,
                139,
                "TERM"
            ],
            [
                129,
                133,
                "TERM"
            ]
        ]
    },
    {
        "text": "For Kindlemma, there are: Kindfreq (numeric, z-transformed), which encodes the lemma frequency; Kindgender (binary), which encodes the grammatical gender of the kind noun; Kindattraction (numeric, z-transformed), which encodes the influence of neighbouring constructions.",
        "entities": [
            [
                79,
                84,
                "TERM"
            ]
        ]
    },
    {
        "text": "Since the notion of word is a little ambiguous here, it is useful to introduce a common distinction between (word) type and (word) token.",
        "entities": [
            [
                131,
                136,
                "TERM"
            ],
            [
                115,
                119,
                "TERM"
            ]
        ]
    },
    {
        "text": "As the previous exercise has shown, finding individual larger units of/in text is quite difficult to achieve.",
        "entities": [
            [
                74,
                78,
                "TERM"
            ]
        ]
    },
    {
        "text": "Paste the data from the general subcorpus into the cells below rank_g, 'type', and freq_g, and the other data into the corresponding rows rank_n, type_n, and freq_n, for now leaving the cells in between the sets empty.",
        "entities": [
            [
                72,
                76,
                "TERM"
            ]
        ]
    },
    {
        "text": "Knowing all of this, you isolate this one pronoun type because you are interested in the use of first person pronouns (I, me, we, us).",
        "entities": [
            [
                50,
                54,
                "TERM"
            ]
        ]
    },
    {
        "text": "At least with respect to a register functional approach, any feature that is identified through a corpus-driven approach will still merit closer scrutiny and analysis in the corpus.",
        "entities": [
            [
                98,
                111,
                "TERM"
            ],
            [
                174,
                180,
                "TERM"
            ]
        ]
    },
    {
        "text": "The current corpus (at time of writing) contains texts from 22,388,141 web pages from 94,391 websites.",
        "entities": [
            [
                12,
                18,
                "TERM"
            ]
        ]
    },
    {
        "text": "The analysis of Trump Speak also contains a discussion of how to use concordancing programs to obtain, for instance, information on the frequency of specific constructions in a corpus as well as relevant examples that can be used in subsequent research conducted on a particular topic.",
        "entities": [
            [
                177,
                183,
                "TERM"
            ]
        ]
    },
    {
        "text": "An important set of tools influencing the choice of corpus architecture is NLP pipelines and APIs, which allow users to construct automatically tagged and parsed representations with complex data models (and these can be manually corrected if needed).",
        "entities": [
            [
                52,
                58,
                "TERM"
            ]
        ]
    },
    {
        "text": "The question marks at the beginning and end identify the tag on this line as a processing instruction, in our case specifically the one referred to as the XML declaration.",
        "entities": [
            [
                155,
                158,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is also argued that the observed tendencies -such as short-before-long and animate referents firstare in line with synchronic corpus-based and experimental findings about general cognitive principles underlying the framework of probabilistic grammar.",
        "entities": [
            [
                129,
                141,
                "TERM"
            ],
            [
                118,
                128,
                "TERM"
            ]
        ]
    },
    {
        "text": "Platforms which do limit posting length, such as Twitter (now X), usually limit the maximum length, confining all of their content into the range of short texts, which is mathematically difficult to work with in quantitative corpus linguistics.",
        "entities": [
            [
                225,
                243,
                "TERM"
            ]
        ]
    },
    {
        "text": "Finally, corpus architecture considerations also interact with the choice of search and visualization facilities that one intends to use.",
        "entities": [
            [
                9,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "Different XML tags are used for markup, metadata and annotation.",
        "entities": [
            [
                53,
                63,
                "TERM"
            ],
            [
                40,
                48,
                "TERM"
            ],
            [
                32,
                38,
                "TERM"
            ],
            [
                10,
                13,
                "TERM"
            ]
        ]
    },
    {
        "text": "Layout design for other (printed) media is also supposed to be enhanced through XML Formatting Objects (XSL-FO).",
        "entities": [
            [
                80,
                83,
                "TERM"
            ]
        ]
    },
    {
        "text": "When tri-grams occur with a particular frequency (e.g., 20 times) and in one specific register in a corpus, they are often called lexical bundles.",
        "entities": [
            [
                100,
                106,
                "TERM"
            ]
        ]
    },
    {
        "text": "If only one of these text types is included then the sample might not account for variation in the different types of news texts.",
        "entities": [
            [
                53,
                59,
                "TERM"
            ],
            [
                21,
                25,
                "TERM"
            ]
        ]
    },
    {
        "text": "Notable improvements could also be gained by the increased collaboration between traditional corpus linguists and scholars in the fields of computational linguistics and NLP, of which there are already encouraging examples.",
        "entities": [
            [
                93,
                99,
                "TERM"
            ]
        ]
    },
    {
        "text": "Of course, the issue of genre balance also affects smaller diachronic corpora like the Helsinki Corpus, because some genres may disappear and new ones appear over time, and compromises must be made in the compilation process in terms of representativeness vs. diachronic comparability.",
        "entities": [
            [
                237,
                255,
                "TERM"
            ],
            [
                271,
                284,
                "TERM"
            ],
            [
                59,
                69,
                "TERM"
            ],
            [
                260,
                270,
                "TERM"
            ],
            [
                30,
                37,
                "TERM"
            ]
        ]
    },
    {
        "text": "The motivation is to be able to contextualise the information in the corpus within the overall world of social media.",
        "entities": [
            [
                69,
                75,
                "TERM"
            ]
        ]
    },
    {
        "text": "What size a given corpus has will depend to a major extent on the kinds of texts included and the resources required to compile these into structured collections.",
        "entities": [
            [
                18,
                24,
                "TERM"
            ]
        ]
    },
    {
        "text": "Thus, an important aspect in constructing a query is to annotate a random sample of our corpus manual for the phenomenon we are interested in, and then to check our query against this manual annotation.",
        "entities": [
            [
                191,
                201,
                "TERM"
            ],
            [
                88,
                94,
                "TERM"
            ],
            [
                74,
                80,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is sometimes possible, moreover, with the benefit of corpus techniques, to observe how White House messages evolve, how the exact nature of the primings flooding into the discourse changes over time, which provides strong evidence of deliberate attempted linguistic engineering.",
        "entities": [
            [
                56,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "As mentioned above, Multi-CAST is available in various formats, and all data can be downloaded from the corpus website.",
        "entities": [
            [
                104,
                110,
                "TERM"
            ]
        ]
    },
    {
        "text": "These techniques include frequency profiling: listing all of the words (types) in the corpus and how frequently they occur, and concordancing: listing each occurrence of a word (token) in a corpus along with the surrounding context.",
        "entities": [
            [
                86,
                92,
                "TERM"
            ],
            [
                190,
                196,
                "TERM"
            ],
            [
                178,
                183,
                "TERM"
            ]
        ]
    },
    {
        "text": "A corpus website should also have all relevant information on the corpus and its various texts or in this case subcorpora.",
        "entities": [
            [
                2,
                8,
                "TERM"
            ],
            [
                66,
                72,
                "TERM"
            ]
        ]
    },
    {
        "text": "This can be done by calculating frequency, variance, vmr, and t fid f values for each column of the data matrix, sorting each set of values in descending order of magnitude, z-standardizing for comparability, and then co-plotting.",
        "entities": [
            [
                194,
                207,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the latter case, you won't need to re-run the concordance, but can simply click anywhere in the hits to remove all selections, although you'll still need to select the ones you want again.",
        "entities": [
            [
                49,
                60,
                "TERM"
            ]
        ]
    },
    {
        "text": "The importance of this information depends on the questions that the corpus is expected to answer.",
        "entities": [
            [
                69,
                75,
                "TERM"
            ]
        ]
    },
    {
        "text": "One means to represent different situationally defined text types evenly in a given corpus is to aim for balance.",
        "entities": [
            [
                105,
                112,
                "TERM"
            ],
            [
                84,
                90,
                "TERM"
            ],
            [
                55,
                59,
                "TERM"
            ]
        ]
    },
    {
        "text": "But including spoken language in a corpus is very important because spoken language is the essence of human language, particularly spontaneous conversation.",
        "entities": [
            [
                35,
                41,
                "TERM"
            ]
        ]
    },
    {
        "text": "One approach would seem to be to try and tweak hyperparameters of fitting trees, such as (1) the minimum decreases in deviance that define when trees stop splitting, (2) the minimum sample sizes per node, or (3) the tree depth.",
        "entities": [
            [
                182,
                188,
                "TERM"
            ],
            [
                199,
                203,
                "TERM"
            ]
        ]
    },
    {
        "text": "This is why corpus linguistics has grown as an area of linguistics and why many people now are using linguistic corpora for many different purposes.",
        "entities": [
            [
                12,
                30,
                "TERM"
            ]
        ]
    },
    {
        "text": "Even if an automatically annotated corpus is unlikely to meet all the expectations of a researcher in terms of its categories of annotation, it can still be an invaluable resource.",
        "entities": [
            [
                129,
                139,
                "TERM"
            ],
            [
                35,
                41,
                "TERM"
            ]
        ]
    },
    {
        "text": "We argue in this chapter that bootstrapping is underused in corpus linguistics, and that quantitative corpus linguists would do well to add this tool to their repertoire.",
        "entities": [
            [
                60,
                78,
                "TERM"
            ],
            [
                102,
                108,
                "TERM"
            ]
        ]
    },
    {
        "text": "We need to do this because these line breaks, as indeed anything that doesn't really form part of our text, may in fact interfere with the processing of the text later and even create a number of problems that could affect the meaningfulness of at least part of your data for linguistic analyses.",
        "entities": [
            [
                102,
                106,
                "TERM"
            ],
            [
                157,
                161,
                "TERM"
            ]
        ]
    },
    {
        "text": "This is useful, for example, when a classification has been made on a scale of very negative, negative, neutral, positive, and very positive type values, but one may want to combine the results to return the corpus grouped into positive, neutral, and negative.",
        "entities": [
            [
                208,
                214,
                "TERM"
            ],
            [
                141,
                145,
                "TERM"
            ]
        ]
    },
    {
        "text": "The practical implications of Zipf's law for corpus linguistics are as follows: we have to learn to critically evaluate the amount of evidence we have for our claims.",
        "entities": [
            [
                45,
                63,
                "TERM"
            ]
        ]
    },
    {
        "text": "In short, the goal of the second edition of English Corpus Linguistics is to provide a comprehensive description of all aspects of corpus linguistics, ranging from how to build a corpus to how to analyze a finished corpus.",
        "entities": [
            [
                131,
                149,
                "TERM"
            ],
            [
                179,
                185,
                "TERM"
            ],
            [
                215,
                221,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, given a rich historical record and a long tradition of high-quality text editions, it is perfectly possible to create a sizeable corpus of Old French, as shown by the FRANTEXT database.",
        "entities": [
            [
                142,
                148,
                "TERM"
            ],
            [
                81,
                85,
                "TERM"
            ]
        ]
    },
    {
        "text": "The graph-based data model allows for completely independent annotation layers, united only by joint reference to the same primary text.",
        "entities": [
            [
                61,
                71,
                "TERM"
            ],
            [
                131,
                135,
                "TERM"
            ]
        ]
    },
    {
        "text": "Further, a vast majority of the matrix clauses introducing both kinds of embedded inversions are declarative clauses (almost 90% in each corpus), which seems to indicate that it is the (\"interrogative-like\") matrix verb (not e.g. an interrogative clause) preceding the indirect question that has the strongest effect on the occurrence of the non-standard formulation for both speaker groups.",
        "entities": [
            [
                137,
                143,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this assignment and the next, we turn to a completely different corpus format.",
        "entities": [
            [
                67,
                73,
                "TERM"
            ]
        ]
    },
    {
        "text": "Also, we know how many different verbs occur after must (440) and what those are, so after creating a collector vector for the cells a + c, we do a second loop over the corpus files where now we determine the frequencies of these verbs after modals in general, not just after must) by looking into this loop's current.mpis.",
        "entities": [
            [
                169,
                175,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first thing you'll probably notice in terms of tagging errors is that most of the Roman numerals, apart from, strangely, \"IX\", have been tagged as proper nouns (NNP), apparently because the tagger has mistaken them for initials.",
        "entities": [
            [
                51,
                58,
                "TERM"
            ]
        ]
    },
    {
        "text": "Having an annotation tool which supports a complex data model may be of little use if the annotated data cannot be accessed and used in sensible ways later on.",
        "entities": [
            [
                10,
                20,
                "TERM"
            ]
        ]
    },
    {
        "text": "Thus, the importance resides in the comparability of the design of the two corpora from which the lists were culled rather than the \"quality \" or impact of the texts in the corpora themselves.",
        "entities": [
            [
                36,
                49,
                "TERM"
            ]
        ]
    },
    {
        "text": "Various types of metadata can be placed in a separate file or in a 'header', so that a computer script or web-based tool for example will be able to use the information in systematic ways when counting frequencies, searching for or displaying relevant data.",
        "entities": [
            [
                17,
                25,
                "TERM"
            ]
        ]
    },
    {
        "text": "As a matter of fact, issues related to multilingual annotation (e.g. whether it should be languagespecific or language-neutral, or, more generally, how cross-linguistic comparability can be achieved) have received relatively little attention in contrastive linguistics and translation studies (one notable exception is Neumann 2013).",
        "entities": [
            [
                169,
                182,
                "TERM"
            ],
            [
                52,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "This made accurate timing impossible, unless we were to take a more representative sample and check all the document images manually, which would have been a very labour-intensive task.",
        "entities": [
            [
                83,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "On the other hand, if you have a ready-made concordancer, you click a few buttons (and enter a search term) to get the job done.",
        "entities": [
            [
                44,
                56,
                "TERM"
            ]
        ]
    },
    {
        "text": "In addition to this, using such software also ties the average user unnecessarily into using often complex annotation tools that themselves represent a relatively steep learning curve, apart from further potential issues regarding platform availability and setup.",
        "entities": [
            [
                107,
                117,
                "TERM"
            ]
        ]
    },
    {
        "text": "Provided that you don't forget to change the option for creating a new corpus, there should be no issues in completing this exercise and creating a suitable frequency list.",
        "entities": [
            [
                157,
                171,
                "TERM"
            ],
            [
                71,
                77,
                "TERM"
            ]
        ]
    },
    {
        "text": "We can explore how frequently particular word combinations (n-grams) occur together in a corpus or how they are distributed across different registers.",
        "entities": [
            [
                89,
                95,
                "TERM"
            ]
        ]
    },
    {
        "text": "Each letter is also annotated for its date of publication, facilitating the analysis of temporal variation in this corpus.",
        "entities": [
            [
                115,
                121,
                "TERM"
            ]
        ]
    },
    {
        "text": "Below is a list of the nine most frequent words from the 2020 Matukar Panau corpus (150,740 words).",
        "entities": [
            [
                76,
                82,
                "TERM"
            ]
        ]
    },
    {
        "text": "Answering either of these questions involves answering the other, as the notion of a 'corpus' in contemporary linguistics is inextricably tied up with the ideas and techniques of corpus linguistics.",
        "entities": [
            [
                179,
                197,
                "TERM"
            ],
            [
                86,
                92,
                "TERM"
            ]
        ]
    },
    {
        "text": "The following is a brief sample paragraph that illustrates what a paragraph annotated in XML might look like in terms of elements and attributes.",
        "entities": [
            [
                25,
                31,
                "TERM"
            ],
            [
                89,
                92,
                "TERM"
            ]
        ]
    },
    {
        "text": "In Chapter 5, the question of the useability of databases for corpus linguistic purposes is addressed by Turo Hiltunen, who discusses issues relating to the use of materials in the British Library Newspapers database.",
        "entities": [
            [
                62,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "This combination across three dimensions will therefore allow a user to explore the corpus on many different interconnected levels and visualisations.",
        "entities": [
            [
                84,
                90,
                "TERM"
            ]
        ]
    },
    {
        "text": "Unfortunately, considering the plethora of textbooks in the field, it is the practical aspects of this process that are dealt with least out of the three key phases of corpus-linguistics methodology.",
        "entities": [
            [
                168,
                174,
                "TERM"
            ]
        ]
    },
    {
        "text": "Shorter plain-text files are therefore generally very small, sometimes even less than a kilobyte (kB; 1kB = 1024 bytes).",
        "entities": [
            [
                14,
                18,
                "TERM"
            ]
        ]
    },
    {
        "text": "Let us repeat the study with the BROWN corpus.",
        "entities": [
            [
                39,
                45,
                "TERM"
            ]
        ]
    },
    {
        "text": "The last point to discuss in this section is the appropriate test for statistical significance that can be used with cross-tabulation; a statistical significance test evaluates the amount of evidence against the null hypothesis (see Section 1.3).",
        "entities": [
            [
                70,
                94,
                "TERM"
            ],
            [
                137,
                161,
                "TERM"
            ]
        ]
    },
    {
        "text": "The detailed presentation of the study begins by describing the corpus used and explaining how data was retrieved from it, as well as their annotation.",
        "entities": [
            [
                140,
                150,
                "TERM"
            ],
            [
                64,
                70,
                "TERM"
            ]
        ]
    },
    {
        "text": "Since overcoming the fallibility of introspective data is one of the central motivations for using corpora in the first place, the analysis of a given phenomenon must not be based on a haphazard sample of instances that the researcher happened to notice while reading or, even worse, by searching the corpus for specific examples.",
        "entities": [
            [
                301,
                307,
                "TERM"
            ],
            [
                195,
                201,
                "TERM"
            ]
        ]
    },
    {
        "text": "The concept of Pattern Grammar (PG) came out of the COBUILD project, a large-scale lexicographical exercise that was unique when it began, in that the compilers relied on a very large (for its time) corpus of English to identify frequent usages and phraseologies of words.",
        "entities": [
            [
                199,
                205,
                "TERM"
            ]
        ]
    },
    {
        "text": "Other corpora have kept different information on individuals, relevant to the particular corpus being created.",
        "entities": [
            [
                89,
                95,
                "TERM"
            ]
        ]
    },
    {
        "text": "Parsed versions of part of this corpus are now available and included in the Penn-Helsinki Parsed Corpus of Middle English and the Penn-Helsinki Parsed Corpus of Early Modern English.",
        "entities": [
            [
                32,
                38,
                "TERM"
            ]
        ]
    },
    {
        "text": "Finally, when automatic processing tools have been used for preparing and (helping to) annotating the data in the corpus, they must be clearly indicated.",
        "entities": [
            [
                114,
                120,
                "TERM"
            ]
        ]
    },
    {
        "text": "Several points mentioned in the guidelines are directly relevant to whether or not the Microsoft Paraphrase Corpus (MPC) fits the definition of a corpus.",
        "entities": [
            [
                146,
                152,
                "TERM"
            ]
        ]
    },
    {
        "text": "Let us assume that the corpus targets a size of 200,000 words per genre, in order to have a representative sample of each of them.",
        "entities": [
            [
                23,
                29,
                "TERM"
            ],
            [
                107,
                113,
                "TERM"
            ]
        ]
    },
    {
        "text": "Perhaps most significantly, corpus approaches to academic writing provide insights into disciplinary practices which help explain the mechanisms by which knowledge is socially constructed through language.",
        "entities": [
            [
                28,
                34,
                "TERM"
            ]
        ]
    },
    {
        "text": "The actual frequencies of words in a real corpus will differ to a certain extent from those predicted by this model.",
        "entities": [
            [
                42,
                48,
                "TERM"
            ]
        ]
    },
    {
        "text": "A 'sample' is a subset of the population that we want to study.",
        "entities": [
            [
                3,
                9,
                "TERM"
            ]
        ]
    },
    {
        "text": "Taking into account important data related to each lemma's range (its frequency across academic disciplines) and dispersion, the researchers arrived at a new Academic Vocabulary List (AVL) of just over 3,000 words (the full list can be explored at www.wordandphrase.info/academic).",
        "entities": [
            [
                51,
                56,
                "TERM"
            ]
        ]
    },
    {
        "text": "Conversely, the type nonattachment illustrates the prefixation of a bipartite ment-type, resulting in a right-branching structure.",
        "entities": [
            [
                16,
                20,
                "TERM"
            ],
            [
                83,
                87,
                "TERM"
            ]
        ]
    },
    {
        "text": "Both types represent borrowed forms that encode means, type 2 (o = 10, e = 0.8) with transitive verbal stems, type 3 with nominal stems (o = 3, e < 0.1).",
        "entities": [
            [
                55,
                59,
                "TERM"
            ],
            [
                110,
                114,
                "TERM"
            ]
        ]
    },
    {
        "text": "All these reasons call for new visualisation techniques, or at least the adaptation of existing ones, in order to specifically address the particular needs of corpus linguistics in terms of scalability, and support for iterative exploration.",
        "entities": [
            [
                159,
                177,
                "TERM"
            ]
        ]
    },
    {
        "text": "In addition, the keyness method and the n-gram method can be combined in order to highlight key clusters, i.e. repeated sequences whose frequency differs significantly in one corpus compared to a reference corpus.",
        "entities": [
            [
                196,
                212,
                "TERM"
            ],
            [
                175,
                181,
                "TERM"
            ]
        ]
    },
    {
        "text": "Finally, we have seen that corpora can be very diverse in nature, depending on whether they are made up once and for all or incremental, general or specialized, annotated or not, monolingual or multilingual, synchronous or diachronic.",
        "entities": [
            [
                223,
                233,
                "TERM"
            ]
        ]
    },
    {
        "text": "In most cases, corpus designers should therefore consciously select texts on a range of topics.",
        "entities": [
            [
                15,
                21,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is a difficult question, and my aim in this chapter is to discuss it and review the state of historical pragmatic research using corpus-linguistic methods.",
        "entities": [
            [
                132,
                138,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, if the sampling frame for a corpus of French spoken in Switzerland includes different criteria such as gender, age, socio-economic level or place of residence, an equivalent number of samples should be chosen to match each selected criterion.",
        "entities": [
            [
                20,
                34,
                "TERM"
            ],
            [
                41,
                47,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first one basically gave you a means of designing your own ways of classifying your data in a sensible manner, which then enables you to use the corpus-based analysis methods we've discussed throughout the course in order to extract and count relevant information.",
        "entities": [
            [
                149,
                161,
                "TERM"
            ]
        ]
    },
    {
        "text": "At this age, her type/token ratio was 0.37.",
        "entities": [
            [
                22,
                27,
                "TERM"
            ],
            [
                17,
                21,
                "TERM"
            ]
        ]
    },
    {
        "text": "As we have discussed many times throughout the book, one can follow a) a corpus-driven or b) a corpus-based (often called corpus-informed) approach to linguistic analysis.",
        "entities": [
            [
                73,
                86,
                "TERM"
            ],
            [
                95,
                107,
                "TERM"
            ],
            [
                122,
                128,
                "TERM"
            ]
        ]
    },
    {
        "text": "Since neither informative nor imaginative writing is homogeneous, the corpus contained 2,000-word samples from a range of different articles, periodicals, books, and so forth to provide as broad a range as possible of different authors, books, periodicals, and so forth.",
        "entities": [
            [
                70,
                76,
                "TERM"
            ]
        ]
    },
    {
        "text": "The concordancer is an excellent way of locating examples of such prosodic clash.",
        "entities": [
            [
                4,
                16,
                "TERM"
            ]
        ]
    },
    {
        "text": "The development of corpus linguistics has led to the creation of new methods for collecting and analyzing linguistic data, which were made possible thanks to the development of computers and the arrival of the Internet.",
        "entities": [
            [
                19,
                37,
                "TERM"
            ]
        ]
    },
    {
        "text": "The second was to avoid a prior division of our corpus into registers.",
        "entities": [
            [
                48,
                54,
                "TERM"
            ]
        ]
    },
    {
        "text": "First, tools which provide Computer Assisted Qualitative Data Analysis (CAQDAS), such as ATLAS.ti, NVivo, QDA Miner, and Wordstat, incorporate some very similar methods to those described here but are not widely used in corpus linguistics.",
        "entities": [
            [
                220,
                238,
                "TERM"
            ]
        ]
    },
    {
        "text": "In some sense at least, this book is an introduction to corpus linguistics.",
        "entities": [
            [
                56,
                74,
                "TERM"
            ]
        ]
    },
    {
        "text": "It has been argued that an explanation for cooccurrence of lexeme and structure may sometimes be found in the more extensive co-text.",
        "entities": [
            [
                128,
                132,
                "TERM"
            ]
        ]
    },
    {
        "text": "Turning from the level of the texts that make up a corpus to the internal properties of those texts, perhaps the most fundamental question compilers and users of diachronic corpora must ask is to what extent they can rely on methods devised for the annotation and analysis of contemporary data in handling data from older periods. Older texts are in principle somewhat alien.",
        "entities": [
            [
                162,
                172,
                "TERM"
            ],
            [
                249,
                259,
                "TERM"
            ],
            [
                51,
                57,
                "TERM"
            ]
        ]
    },
    {
        "text": "As has been observed in instances such as the noun lifespan in the British National Corpus, and the adjectival phrase absolutely fabulous in British and Irish sections of the News on the Web corpus, sometimes the irrelevant tokens may far outnumber the relevant ones.",
        "entities": [
            [
                191,
                197,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, you could type this to load the package dplyr: library(dplyr) ¶.",
        "entities": [
            [
                24,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "The third component lists the percentage of corpus parts containing at least one match, which here amounts to 100 percent.",
        "entities": [
            [
                44,
                50,
                "TERM"
            ]
        ]
    },
    {
        "text": "These are the result of linguistic analysis and as such not appropriate for a corpus, but are good to be included in the apparatus once texts are collected.",
        "entities": [
            [
                78,
                84,
                "TERM"
            ]
        ]
    },
    {
        "text": "It means that we cannot reject the null hypothesis stating that there is no relationship between the article type and position.",
        "entities": [
            [
                109,
                113,
                "TERM"
            ]
        ]
    },
    {
        "text": "If the degree of expansion is low, then the original corpus was already nearly saturated and hence reasonably representative.",
        "entities": [
            [
                53,
                59,
                "TERM"
            ]
        ]
    },
    {
        "text": "These choices we have when using language can really only be investigated through finding ways of expressing this flexibility on the paradigmatic and syntagmatic axes in our corpus searches.",
        "entities": [
            [
                174,
                180,
                "TERM"
            ]
        ]
    },
    {
        "text": "Another solution, which keeps the structural and discourse units of a text together even more, is to divide the text into its paragraphs, or multi-paragraph chunks.",
        "entities": [
            [
                70,
                74,
                "TERM"
            ],
            [
                112,
                116,
                "TERM"
            ]
        ]
    },
    {
        "text": "Codeswitching is overall rare in our corpus, but its relative frequency is considerably higher in Montreal, as can be expected given the prevalence of bilingual speakers in the city.",
        "entities": [
            [
                37,
                43,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this case, devices are much more frequently referred to as electric and less frequently as electrical than expected, and, as in the LOB corpus, the nouns in the category industry are more frequently referred to as electrical and less frequently as electric than expected (although not significantly so).",
        "entities": [
            [
                139,
                145,
                "TERM"
            ]
        ]
    },
    {
        "text": "The text on the online page is editable, so you can also delete the u and see whether the quantification using the question mark still works, or even try some of your own words, if you know any other differences in spelling that relate to one character only.",
        "entities": [
            [
                4,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "A corpus should primarily contain simple plain texts.",
        "entities": [
            [
                2,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "These collocations make perfect sense in view of the search terms used for creating the corpus.",
        "entities": [
            [
                88,
                94,
                "TERM"
            ]
        ]
    },
    {
        "text": "But even in a lexical corpus, the numerous concordancing programs that are currently available can greatly facilitate searches.",
        "entities": [
            [
                22,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "In order to illustrate these types of data, let us turn to a linguistic phenomenon that is more complex than the distribution of words across varieties, and closer to the kind of phenomenon actually of interest to corpus linguists: that of the two English possessive constructions introduced in Section 4.2.3 of Chapter 4 above.",
        "entities": [
            [
                214,
                220,
                "TERM"
            ]
        ]
    },
    {
        "text": "Using the interface provided on the website of the Corpus français de l'université de Leipzig, what is the frequency order in this corpus of the words maison, chalet, immeuble, bungalow.",
        "entities": [
            [
                131,
                137,
                "TERM"
            ]
        ]
    },
    {
        "text": "Following the coding scheme described above, two independent raters coded a random sample of 100 concordance lines from a total of 1,053 containing the word religion in the corpus.",
        "entities": [
            [
                97,
                108,
                "TERM"
            ],
            [
                173,
                179,
                "TERM"
            ],
            [
                83,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "An early corpus of spoken British English, the London-Lund Corpus, contains 5,000-word samples.",
        "entities": [
            [
                9,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "This is because we have to understand the contents, design and structure of the corpora that we use, in order to select a corpus appropriate to what we want to research, in order to make appropriate use of that corpus, and in order to treat our results critically in terms of how far they may be extrapolated beyond the specific corpus at hand.",
        "entities": [
            [
                122,
                128,
                "TERM"
            ],
            [
                211,
                217,
                "TERM"
            ],
            [
                329,
                335,
                "TERM"
            ]
        ]
    },
    {
        "text": "Therefore, geographic region becomes a relevant trait for the sampling frame of a spoken French corpus.",
        "entities": [
            [
                62,
                76,
                "TERM"
            ],
            [
                96,
                102,
                "TERM"
            ]
        ]
    },
    {
        "text": "On average, 8 clusters per word (min = 3, max = 10) were retained for annotation.",
        "entities": [
            [
                70,
                80,
                "TERM"
            ]
        ]
    },
    {
        "text": "This measure takes into consideration how many different word types make up a token frequency.",
        "entities": [
            [
                78,
                83,
                "TERM"
            ]
        ]
    },
    {
        "text": "While grammatical and functional information can be found often even in a small corpus, it is not the only thing linguists want to study.",
        "entities": [
            [
                80,
                86,
                "TERM"
            ]
        ]
    },
    {
        "text": "Consider first the issue of accuracy in document-level metadata.",
        "entities": [
            [
                55,
                63,
                "TERM"
            ]
        ]
    },
    {
        "text": "This explains in part why documentarians' work differs from that of classical descriptive and typological linguists in its primary focus on data collection rather than analysis and comparison, and creating a corpus is part of a documentation project.",
        "entities": [
            [
                208,
                214,
                "TERM"
            ]
        ]
    },
    {
        "text": "COCA is not a published article; rather, it is a web-based corpus complied of a 520 million-word database from newspapers, magazines, fiction, and other academic text documents.",
        "entities": [
            [
                59,
                65,
                "TERM"
            ],
            [
                162,
                166,
                "TERM"
            ]
        ]
    },
    {
        "text": "If we categorize data in terms of such a nominal variable, the only way to quantify them is to count the number of observations of each category in a given sample and express the result in absolute frequencies (i.e., raw numbers) or relative frequencies (such as percentages).",
        "entities": [
            [
                156,
                162,
                "TERM"
            ]
        ]
    },
    {
        "text": "The corpus is, however, a reasonable model (or at least an operationalization) of this linguistic input.",
        "entities": [
            [
                4,
                10,
                "TERM"
            ]
        ]
    },
    {
        "text": "In any case, concordance lines serve to provide the lexical and/or grammatical context of a search term and thus can be needed at all stages of an analysis, from data coding to interpretation.",
        "entities": [
            [
                13,
                24,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the second case study, these same concepts are applied in the development of a more advanced program that can replicate and, in some ways, improve on the tools and functions found in ready-built corpus tools.",
        "entities": [
            [
                198,
                204,
                "TERM"
            ]
        ]
    },
    {
        "text": "That is why it is also known as part-of-speech (POS) annotation.",
        "entities": [
            [
                53,
                63,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, corpus researchers need an understanding of the issues that arise in corpus construction even if they never build any corpora of their own.",
        "entities": [
            [
                78,
                97,
                "TERM"
            ],
            [
                9,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "Also move the full copy of the Sherlock Holmes text here.",
        "entities": [
            [
                47,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "This section presents an overview of a recent study and findings on four syntactic features of spoken ELF carried out on a subset of the ELFA corpus.",
        "entities": [
            [
                142,
                148,
                "TERM"
            ]
        ]
    },
    {
        "text": "This annotation makes it possible to exclusively look for the noun occurrences of ferme, for example.",
        "entities": [
            [
                5,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "The main response to such criticism is that these areas are based on the use of quantitative methods (namely inferential statistics), which make it possible to draw conclusions from a sample and to extrapolate them to an entire population.",
        "entities": [
            [
                184,
                190,
                "TERM"
            ]
        ]
    },
    {
        "text": "Words used by different characters in classic literary works have been a very popular topic for keyword analyses.",
        "entities": [
            [
                96,
                103,
                "TERM"
            ]
        ]
    },
    {
        "text": "Then, in addition, to make this procedure robust, we repeat the process for every possible beginning-point in the corpus (think about the corpus as a circle rather than a line) and calculate the mean of all the reduced frequency values that we get via the procedure described above.",
        "entities": [
            [
                114,
                120,
                "TERM"
            ],
            [
                138,
                144,
                "TERM"
            ]
        ]
    },
    {
        "text": "These 15 columns correspond to the 15 text categories.",
        "entities": [
            [
                38,
                42,
                "TERM"
            ]
        ]
    },
    {
        "text": "Now, many computer programs designed to count words will split the input text on spaces and punctuation.",
        "entities": [
            [
                73,
                77,
                "TERM"
            ]
        ]
    },
    {
        "text": "One of the most common error messages you'll probably encounter will be something like \"XML Parsing Error: mismatched tag.",
        "entities": [
            [
                88,
                91,
                "TERM"
            ]
        ]
    },
    {
        "text": "Essentially, a concordance is a listing of individual word forms in a given specific context, where the exact nature of the context depends on the requirements of the analysis and which particular program one may be using.",
        "entities": [
            [
                15,
                26,
                "TERM"
            ]
        ]
    },
    {
        "text": "The corpus-based approach to dialectology contrasts with the standard approach, which is based on analyzing language elicited through interviews and questionnaires.",
        "entities": [
            [
                4,
                16,
                "TERM"
            ]
        ]
    },
    {
        "text": "Yet it is impossible to create a corpus of Old French that is comparable in any straightforward way to a corpus of Present-day French.",
        "entities": [
            [
                33,
                39,
                "TERM"
            ],
            [
                105,
                111,
                "TERM"
            ]
        ]
    },
    {
        "text": "The corpus needs to include different registers (e.g. informal speech, news reporting, academic writing, popular fiction etc.) and we need to identify relevant variables, that is, those that can distinguish between the registers in our corpus.",
        "entities": [
            [
                4,
                10,
                "TERM"
            ],
            [
                236,
                242,
                "TERM"
            ]
        ]
    },
    {
        "text": "While this is no problem with a one-dimensional frequency list, this is much harder with multidimensional frequency tables: Perl's arrays of arrays or hashes of arrays etc. are not for the faint-hearted, whereas R's table is easy to handle, and additional functions (table, xtabs, ftable, etc.) allow you to handle such tables very easily.",
        "entities": [
            [
                48,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the case of a special corpus, the identification of target users is important.",
        "entities": [
            [
                25,
                31,
                "TERM"
            ]
        ]
    },
    {
        "text": "Also, researchers can use ASCII files in parsers, concordance programs, and taggers.",
        "entities": [
            [
                50,
                61,
                "TERM"
            ]
        ]
    },
    {
        "text": "Studies with a focus on the development of grammar rely on grammatical annotations, especially lemmatization, parts of speech, and interlinear glossing (cf. below).",
        "entities": [
            [
                95,
                108,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, at 1.9 billion words in length, the Corpus of Global Web-Based English is so lengthy that it would be impossible to determine not just the content of the corpus but the distribution of such variables as the gender of contributors, their ages, and so forth.",
        "entities": [
            [
                168,
                174,
                "TERM"
            ]
        ]
    },
    {
        "text": "Click in the relative frequency cell for about in the general corpus.",
        "entities": [
            [
                62,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "While word-class annotation is very well established in corpus linguistics, there are other types of annotation as well.",
        "entities": [
            [
                56,
                74,
                "TERM"
            ],
            [
                17,
                27,
                "TERM"
            ],
            [
                101,
                111,
                "TERM"
            ]
        ]
    },
    {
        "text": "In 2008, Meunier and Gouverneur stated that publishers seem to acknowledge the importance of corpora in ELT but fail to give precise information on how exactly the corpus is used.",
        "entities": [
            [
                164,
                170,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first computer corpus, the Brown Corpus (a general-purpose corpus), contained various kinds of writing, such as press reportage, fiction, learned, and popular writing.",
        "entities": [
            [
                19,
                25,
                "TERM"
            ],
            [
                63,
                69,
                "TERM"
            ]
        ]
    },
    {
        "text": "This statistical noise explains why the models' coefficients do not correspond exactly to the number of milliseconds that we specified for each type of example.",
        "entities": [
            [
                144,
                148,
                "TERM"
            ]
        ]
    },
    {
        "text": "Unlike the corpus-driven approaches illustrated in this book, more advanced corpus-driven register studies have identified co-occurring linguistic features that have emerged through corpus analyses.",
        "entities": [
            [
                11,
                24,
                "TERM"
            ],
            [
                76,
                89,
                "TERM"
            ],
            [
                182,
                188,
                "TERM"
            ]
        ]
    },
    {
        "text": "These need to be collected in the form of audio files which are later transcribed to become analyzable with corpus searching tools.",
        "entities": [
            [
                108,
                114,
                "TERM"
            ]
        ]
    },
    {
        "text": "A node is a word that we want to search for and analyse.",
        "entities": [
            [
                2,
                6,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, this encoding does not correspond to text files containing French characters, because of accented characters.",
        "entities": [
            [
                14,
                22,
                "TERM"
            ],
            [
                46,
                50,
                "TERM"
            ]
        ]
    },
    {
        "text": "Even the same metaphors are used: Such similarities are of course not surprising, given that both Digital Humanities and corpus linguistics deal with textual data.",
        "entities": [
            [
                121,
                139,
                "TERM"
            ]
        ]
    },
    {
        "text": "Despite its modest size, this corpus makes it possible to study the specificities of the journalistic style when applied to a particular field.",
        "entities": [
            [
                30,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "I shall survey the applications of corpus-linguistic methods in historical pragmatics by a selection of articles that illuminate recent trends within the field, demonstrate the range, and indicate future avenues for research.",
        "entities": [
            [
                35,
                41,
                "TERM"
            ]
        ]
    },
    {
        "text": "Any corpus is fundamentally a sample.",
        "entities": [
            [
                4,
                10,
                "TERM"
            ],
            [
                30,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "Once a corpus is built and stored in an archive, we need robust schemes for regular maintenance, upgradation, and augmentation of the resource.",
        "entities": [
            [
                7,
                13,
                "TERM"
            ]
        ]
    },
    {
        "text": "In earlier corpora, most corpus compilers used analog recorders and cassette tapes, since such recorders were small and unobtrusive and cassette tapes were inexpensive.",
        "entities": [
            [
                25,
                31,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, this word will probably never be used in other texts in the corpus covering different fields, unlike words such as analysis, hypothesis or conclusion, which will probably be used in all scientific fields.",
        "entities": [
            [
                69,
                75,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, bus and ride co-occur in the corpus, as do ride and hour, and thirty and hour.",
        "entities": [
            [
                42,
                48,
                "TERM"
            ]
        ]
    },
    {
        "text": "To begin with, we will see that annotating a corpus is a way of adding value to it by widening the field of questions that it will make it possible to investigate.",
        "entities": [
            [
                45,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "Generally, when it comes to size it is obvious from a representativeness point of view that bigger is better.",
        "entities": [
            [
                54,
                72,
                "TERM"
            ]
        ]
    },
    {
        "text": "Additionally, verb and particle lemma grouping factors are annotated.",
        "entities": [
            [
                32,
                37,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the field of journalism, the most exhaustive resource is undoubtedly the Le Monde corpus, which contains the newspaper's archives for the period 1987-2012, representing a total of nearly 1,200,000 articles, corresponding to almost 20 million words per year.",
        "entities": [
            [
                85,
                91,
                "TERM"
            ]
        ]
    },
    {
        "text": "An outline of collocation and the measurements used to strengthen assumptions will be made from the collocations.",
        "entities": [
            [
                14,
                25,
                "TERM"
            ]
        ]
    },
    {
        "text": "In other words, the researcher will go through the concordance and assign every instance of the orthographic string in question to one word-sense category posited in the corresponding lexical entry.",
        "entities": [
            [
                51,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "You should definitely also delete all numbers, unless you want to change the token definition to include those, but, as I pointed out before, numbers may take many different forms and their meaning may be difficult to identify.",
        "entities": [
            [
                77,
                82,
                "TERM"
            ]
        ]
    },
    {
        "text": "As usual, we use rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory.",
        "entities": [
            [
                68,
                74,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first was to generate new dimensions from a corpus of articles from a single academic journal (Global Environmental Change).",
        "entities": [
            [
                48,
                54,
                "TERM"
            ]
        ]
    },
    {
        "text": "After you have selected a corpus, you will need to create an account to use the corpus.",
        "entities": [
            [
                26,
                32,
                "TERM"
            ],
            [
                80,
                86,
                "TERM"
            ]
        ]
    },
    {
        "text": "For other research questions, control of content is less of an issue: for instance, the relatively low lexicality of A-arguments should be observable in any text (but see below).",
        "entities": [
            [
                157,
                161,
                "TERM"
            ]
        ]
    },
    {
        "text": "When Professor Matti Rissanen wrote a short article entitled \"Three problems connected with the use of diachronic corpora\" for the ICAME Journal in 1989, the landscape of corpus linguistics looked quite different from today.",
        "entities": [
            [
                171,
                189,
                "TERM"
            ],
            [
                103,
                113,
                "TERM"
            ]
        ]
    },
    {
        "text": "Words marked with the color blue are among the top 500 most frequently occurring words in the corpus.",
        "entities": [
            [
                94,
                100,
                "TERM"
            ]
        ]
    },
    {
        "text": "By doing this, we can have a better view of the multilayered nature of the corpus.",
        "entities": [
            [
                75,
                81,
                "TERM"
            ]
        ]
    },
    {
        "text": "Provided that you type the formulae into the formula bar correctly and select the right cells, calculating all the relative frequencies accurately and efficiently should also not present any problems, although, if you haven't used spreadsheets extensively, filling cells by dragging may take some getting used to.",
        "entities": [
            [
                18,
                22,
                "TERM"
            ]
        ]
    },
    {
        "text": "These tags, of which there are many, are assigned as part of an \"idiom tagging\" step after the initial assignment of POS tags and involve matching the sequences of POS-tagged words in the corpus against templates of multiword sequences.",
        "entities": [
            [
                71,
                78,
                "TERM"
            ],
            [
                188,
                194,
                "TERM"
            ]
        ]
    },
    {
        "text": "This text provides a comprehensive, practical, and very accessible discussion of important issues in frequency list design and evaluation.",
        "entities": [
            [
                101,
                115,
                "TERM"
            ],
            [
                5,
                9,
                "TERM"
            ]
        ]
    },
    {
        "text": "Here, sadly, the designers of the architecture have introduced a serious flaw in the system that may well affect the overall calculations of the collocation statistics very strongly, which is to treat punctuation tokens (and their types) as equivalent to words.",
        "entities": [
            [
                145,
                156,
                "TERM"
            ]
        ]
    },
    {
        "text": "Specific information such as the length of each text (by number of words), the topic, or the type of each text (if you included different types within a specific genre) can also be included in individual text files.",
        "entities": [
            [
                93,
                97,
                "TERM"
            ],
            [
                48,
                52,
                "TERM"
            ],
            [
                106,
                110,
                "TERM"
            ],
            [
                204,
                208,
                "TERM"
            ]
        ]
    },
    {
        "text": "Some collocation measures such as MI highlight rare exclusivity of the collocational relationship, favouring collocates which occur almost exclusively in the company of the node, even though this may be only once or twice in the entire corpus.",
        "entities": [
            [
                5,
                16,
                "TERM"
            ],
            [
                236,
                242,
                "TERM"
            ],
            [
                173,
                177,
                "TERM"
            ]
        ]
    },
    {
        "text": "Node words tend to attract some words -such that these words occur close to the node word with greater than chance probability -while they repel others -such that these words occur close to the node word with less than chance probability -while they occur at chance-level frequency with yet others.",
        "entities": [
            [
                80,
                84,
                "TERM"
            ],
            [
                194,
                198,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this textbook, we attempt to counter-balance the traditional focus on written texts and refer to corpora of non-written language texts as much as possible.",
        "entities": [
            [
                40,
                47,
                "TERM"
            ]
        ]
    },
    {
        "text": "For each occurrence, the pronunciation should be noted, either by a phonetic analysis of the production of the vowel or by a manual annotation carried out by two native speakers.",
        "entities": [
            [
                132,
                142,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the current study, we analyzed co-cited documents published in corpus linguistics during the past twenty years.",
        "entities": [
            [
                66,
                84,
                "TERM"
            ]
        ]
    },
    {
        "text": "So, to begin with, it is necessary to convert the files included in the corpus to text format.",
        "entities": [
            [
                72,
                78,
                "TERM"
            ],
            [
                82,
                86,
                "TERM"
            ]
        ]
    },
    {
        "text": "The arrival of these tools has greatly accelerated research in corpus linguistics.",
        "entities": [
            [
                63,
                81,
                "TERM"
            ]
        ]
    },
    {
        "text": "In order to show the benefit of a qualitative analysis of concordance lines, we stick with the subject of constructions of refugees in the BNC.",
        "entities": [
            [
                58,
                69,
                "TERM"
            ]
        ]
    },
    {
        "text": "Many corpus linguists come from a tradition that has provided them with ample background in linguistic theory and the techniques of linguistic description but little experience of statistics.",
        "entities": [
            [
                5,
                11,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, by type frequency, bimorphemic and 3-morpheme word types are most frequent.",
        "entities": [
            [
                12,
                16,
                "TERM"
            ]
        ]
    },
    {
        "text": "I've chosen extracts from these three particular texts and period for a number of reasons: a) their authors all died more than 70 years ago so the texts are in the public domain; in other words, there are no copyright issues, even when quoting longer passages; b) they are included in corpus compilations; and c) they not only illustrate register/genre differences but also how the conventions for these may change over time, as can be seen, for example, in the spelling of to-day in the final extract.",
        "entities": [
            [
                285,
                291,
                "TERM"
            ]
        ]
    },
    {
        "text": "Most corpus tools avoid the capitalisation issue completely by forcing all characters to upper-or lowercase, but this can cause issues with different meanings, e.g. \"Polish\" versus \"polish\".",
        "entities": [
            [
                5,
                11,
                "TERM"
            ]
        ]
    },
    {
        "text": "Files saved in these formats can be easily transformed into text files thanks to specific options such as the \"save as\" command found in word processors.",
        "entities": [
            [
                60,
                64,
                "TERM"
            ]
        ]
    },
    {
        "text": "We generate a sixth we have now identified the optimal random-effects structure, which turns out to be much more complex than corpus-linguistic studies usually assume.",
        "entities": [
            [
                126,
                132,
                "TERM"
            ]
        ]
    },
    {
        "text": "For text analysis and similar contexts, the use of LL scores leads to considerably improved statistical results.",
        "entities": [
            [
                4,
                8,
                "TERM"
            ]
        ]
    },
    {
        "text": "In CFA, an intersection of variables whose observed frequency is significantly higher than expected is referred to as a type and one whose observed frequency is significantly lower is referred to as an antitype (but if we do not like this terminology, we do not have to use it and can keep talking about \"more or less frequent than expected\", as we do with bivariate 𝜒 2 tests).",
        "entities": [
            [
                120,
                124,
                "TERM"
            ]
        ]
    },
    {
        "text": "Currently, some corpora such as the Google Books corpus (see Chapter 5) and the FrenchWeb 2012 corpus (available on Sketch Engine), collected from the Internet, contain several billion words.",
        "entities": [
            [
                49,
                55,
                "TERM"
            ],
            [
                95,
                101,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, the most frequent equivalent in the translation corpus is increasingly, which is only mentioned in the LA.",
        "entities": [
            [
                57,
                63,
                "TERM"
            ]
        ]
    },
    {
        "text": "Compared to carefully constructed traditional corpora with often hand-picked text samples, there is obviously much less control over individual choices, and the fact that the exact 8.",
        "entities": [
            [
                77,
                81,
                "TERM"
            ]
        ]
    },
    {
        "text": "When we restricted our corpus queries to novels alone, which is possible if one uses the downloadable version of COHA rather than the online interface, the gender difference disappeared.",
        "entities": [
            [
                23,
                29,
                "TERM"
            ]
        ]
    },
    {
        "text": "This is the case of the EMA écrits scolaires corpus (Boré and Elalouf 2017), a longitudinal corpus bringing together the text productions of primary and middle school children.",
        "entities": [
            [
                45,
                51,
                "TERM"
            ],
            [
                92,
                98,
                "TERM"
            ],
            [
                121,
                125,
                "TERM"
            ]
        ]
    },
    {
        "text": "As pointed out above, spoken and signed texts need to be transcribed before they can be included in a corpus.",
        "entities": [
            [
                102,
                108,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, there are ways to increase the probability that that the speech included in a corpus will be natural and realistic.",
        "entities": [
            [
                87,
                93,
                "TERM"
            ]
        ]
    },
    {
        "text": "There is no alternative to knowing your corpora, this cannot be done more easily, and any concordance programs that come with more refined search options also require you to thoroughly consider the format of the corpus files even if their interface 'hides' such decisions behind clickable buttons with smiling corpus linguists on them, in settings, or in .ini files.",
        "entities": [
            [
                90,
                101,
                "TERM"
            ],
            [
                212,
                218,
                "TERM"
            ],
            [
                310,
                316,
                "TERM"
            ]
        ]
    },
    {
        "text": "When you are extracting n-grams from a corpus, you can imagine that your sequences could be endless.",
        "entities": [
            [
                39,
                45,
                "TERM"
            ]
        ]
    },
    {
        "text": "The final method in the corpus toolbox is collocation.",
        "entities": [
            [
                42,
                53,
                "TERM"
            ],
            [
                24,
                30,
                "TERM"
            ]
        ]
    },
    {
        "text": "Simple type/token ratio (TTR) was used to compare the texts because they were of the same length (2,000 tokens).",
        "entities": [
            [
                12,
                17,
                "TERM"
            ],
            [
                7,
                11,
                "TERM"
            ]
        ]
    },
    {
        "text": "Because you do not want this information counted as part of the linguistic characteristics of your text, you can put this or any other relevant information that you do not want to be a part of the linguistic analysis into angled brackets (< >).",
        "entities": [
            [
                99,
                103,
                "TERM"
            ]
        ]
    },
    {
        "text": "Such evaluation can also demonstrate that a list is indeed specialized if it provides higher coverage of a specialized corpus than of a general corpus.",
        "entities": [
            [
                119,
                125,
                "TERM"
            ],
            [
                144,
                150,
                "TERM"
            ]
        ]
    },
    {
        "text": "Any given type will be more frequent in a bigger corpus; for that reason, corpus frequencies are often given in relative terms, as frequencies per thousand words or per million words.",
        "entities": [
            [
                49,
                55,
                "TERM"
            ],
            [
                74,
                80,
                "TERM"
            ],
            [
                10,
                14,
                "TERM"
            ]
        ]
    },
    {
        "text": "In addition to charting the history and development of keyword research (in Section 1), our discussions have been designed to emphasize that key lexical items should be used as a guide for what to analyze qualitatively, and not considered the end product in themselves.",
        "entities": [
            [
                55,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are many considerations for formatting corpus material in ways that follow current standards and best practice.",
        "entities": [
            [
                45,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "While this approach is very versatile, it is also labor intensive: for each new type of information, a new script must be written which traverses the corpus in search of some information.",
        "entities": [
            [
                150,
                156,
                "TERM"
            ],
            [
                80,
                84,
                "TERM"
            ]
        ]
    },
    {
        "text": "Historical pragmatics has its roots in philology, and it is against this background that we can most clearly see what new corpus linguistics has brought to historical pragmatics and to historical studies in general.",
        "entities": [
            [
                122,
                140,
                "TERM"
            ]
        ]
    },
    {
        "text": "Then we use if and any (see the very simple definition at ?any ¶) to let R check whether the file has XML or SGML annotation; depending on that, search.expression.alph and search.expression.ord will be defined in the required way.",
        "entities": [
            [
                114,
                124,
                "TERM"
            ],
            [
                102,
                105,
                "TERM"
            ]
        ]
    },
    {
        "text": "In Section 2, the focus is on part-of-speech (POS) annotation, which can be regarded as one of the most useful kinds of linguistic annotation in contemporary research, but which, as we shall see, does not come without problems.",
        "entities": [
            [
                51,
                61,
                "TERM"
            ],
            [
                131,
                141,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, one of the occurrences of the word colline (hill) in the Le Monde corpus (year 2012) is \"Sur la colline, tout le monde est allé voter\" (On the hill, everyone went voting).",
        "entities": [
            [
                79,
                85,
                "TERM"
            ]
        ]
    },
    {
        "text": "The words were collected from all tweets and a frequency list created for each MP.",
        "entities": [
            [
                47,
                61,
                "TERM"
            ]
        ]
    },
    {
        "text": "If, say for various reasons related to copyright, it is not possible to make the complete set of corpus files available to others, the corpus could still be made searchable online and concordance lines from the corpus be shown.",
        "entities": [
            [
                184,
                195,
                "TERM"
            ],
            [
                97,
                103,
                "TERM"
            ],
            [
                135,
                141,
                "TERM"
            ],
            [
                211,
                217,
                "TERM"
            ]
        ]
    },
    {
        "text": "We have also seen that these tools often work in standardized formats like XML, which makes it easy to share annotations.",
        "entities": [
            [
                75,
                78,
                "TERM"
            ]
        ]
    },
    {
        "text": "A token is a single instance of any word at a particular place in the corpus; a type is an individual word-form, which can occur many times in the corpus: thus one type is usually instantiated by multiple tokens.",
        "entities": [
            [
                70,
                76,
                "TERM"
            ],
            [
                147,
                153,
                "TERM"
            ],
            [
                2,
                7,
                "TERM"
            ],
            [
                80,
                84,
                "TERM"
            ],
            [
                164,
                168,
                "TERM"
            ]
        ]
    },
    {
        "text": "Qualitative corpus analyses, as Hasko (2020: 952) comments, have a long tradition, dating back to the pre-electronic era.",
        "entities": [
            [
                12,
                18,
                "TERM"
            ]
        ]
    },
    {
        "text": "Meanwhile, publications by Mike Scott, Ken Hyland, and John Swales were first found in the middle time spans; their publications on corpus tools and discourse analysis in ESP or EAP were frequently cited.",
        "entities": [
            [
                132,
                138,
                "TERM"
            ]
        ]
    },
    {
        "text": "We sample to estimate parameters.",
        "entities": [
            [
                3,
                9,
                "TERM"
            ]
        ]
    },
    {
        "text": "The data sample was analyzed with a binary (continued) logistic regression (Chap. 21) in which the dependent variable was the choice of genitive (-s vs. of ) and the 14 variables were the predictor variables.",
        "entities": [
            [
                9,
                15,
                "TERM"
            ]
        ]
    },
    {
        "text": "As we move through the text word by word, initially all words are new types and hapaxes, so the type-and hapax-counts rise at the same rate as the token counts.",
        "entities": [
            [
                147,
                152,
                "TERM"
            ],
            [
                96,
                100,
                "TERM"
            ],
            [
                23,
                27,
                "TERM"
            ]
        ]
    },
    {
        "text": "The marriage of corpus linguistics and social science seems, initially, straightforward.",
        "entities": [
            [
                16,
                34,
                "TERM"
            ]
        ]
    },
    {
        "text": "Therefore, if a corpus is to be used only for non-profit academic research, this should be clearly stated in any letter of inquiry or email requesting permission to use copyrighted material and many publishers will sometimes waive fees.",
        "entities": [
            [
                16,
                22,
                "TERM"
            ]
        ]
    },
    {
        "text": "This book is one of a series entitled Quantitative Linguistics, whose remit is to cover 'all aspects of quantitative methods and models in linguistics, text analysis, and related research fields', and more specifically 'the whole spectrum of theoretical and empirical research, ultimately striving for an exact mathematical formulation and empirical testing of hypotheses: observation and description of linguistic data, application of methods and models, discussion of methodological and epistemological issues, modelling of language and text phenomena'.",
        "entities": [
            [
                152,
                156,
                "TERM"
            ],
            [
                539,
                543,
                "TERM"
            ]
        ]
    },
    {
        "text": "It should be immediately striking that there are many different behavioural profiles but also that there are not 851 distinct profiles, meaning there is consistency across the corpus in terms of not only the syntactic uses of run, but also the words it co-occurs with.",
        "entities": [
            [
                176,
                182,
                "TERM"
            ]
        ]
    },
    {
        "text": "For several decades now, corpus linguists have discussed dozens of association measures that are used to rank-order, for instance, collocations by the attraction of their constituent words.",
        "entities": [
            [
                25,
                31,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, this type of test can be used in studies seeking to find out whether advanced learners produce significantly more discourse connectives than intermediate learners, or whether speakers produce more fallacious arguments when speaking at political party meetings or at electoral campaigns.",
        "entities": [
            [
                18,
                22,
                "TERM"
            ]
        ]
    },
    {
        "text": "Frequency lists, usually of words, provide a list of all the items in the corpus and a count of how often they occur and in some cases how widely dispersed the items are across multiple sections of a corpus.",
        "entities": [
            [
                74,
                80,
                "TERM"
            ],
            [
                200,
                206,
                "TERM"
            ]
        ]
    },
    {
        "text": "On the one hand, corpus-linguistic observational data are typically much messier and unbalanced than psycholinguistic experimental data because many confounding and moderator variables that psycholinguists can control for (by randomising, blocking, etc.) plague corpus-linguistic analyses.",
        "entities": [
            [
                17,
                23,
                "TERM"
            ],
            [
                262,
                268,
                "TERM"
            ]
        ]
    },
    {
        "text": "Taken together, corpus investigations over the last few decades have greatly extended our understanding of phraseological lexical sequences in English discourse.",
        "entities": [
            [
                16,
                22,
                "TERM"
            ]
        ]
    },
    {
        "text": "Then, we saw that the important methodological trait to be respected when creating a corpus is datarepresentativeness.",
        "entities": [
            [
                85,
                91,
                "TERM"
            ]
        ]
    },
    {
        "text": "They are generally used in a span of French text within a codeswitched tweet where most tokens are in English; this explains why the tweets were tagged as written in English and retained in the corpus.",
        "entities": [
            [
                194,
                200,
                "TERM"
            ],
            [
                44,
                48,
                "TERM"
            ]
        ]
    },
    {
        "text": "Our framework splits along three orthogonal dimensions: linguistic (lexical, grammar/syntax, semantics), structural (to permit sub-corpora) and temporal (for diachronic corpora).",
        "entities": [
            [
                158,
                168,
                "TERM"
            ]
        ]
    },
    {
        "text": "The remainder of this paper is organized as follows: Section 2 describes our proposal, and Section 3 contains information regarding studies based on corpus compiled with this tool, as well as the description of future lines of action.",
        "entities": [
            [
                149,
                155,
                "TERM"
            ]
        ]
    },
    {
        "text": "When this level of technology is reached, corpus-based dialect studies will become the norm.",
        "entities": [
            [
                42,
                54,
                "TERM"
            ]
        ]
    },
    {
        "text": "CLAN offers the possibility of formulating queries on the CHILDES corpus for studying many aspects of children's language.",
        "entities": [
            [
                66,
                72,
                "TERM"
            ]
        ]
    },
    {
        "text": "The focus is on abstract data models and the ways in which they are realized in concrete formats for corpus representation, as well as consequences for the usability of the resulting corpora.",
        "entities": [
            [
                101,
                107,
                "TERM"
            ]
        ]
    },
    {
        "text": "Journal of English for Academic Purposes, along with English for Specific Purposes and Cognitive Linguistics, is another example of a specialized journal actively being cited by corpus linguistics papers.",
        "entities": [
            [
                178,
                196,
                "TERM"
            ]
        ]
    },
    {
        "text": "On a rainy Lancaster afternoon, I start searching the EEBO corpus.",
        "entities": [
            [
                59,
                65,
                "TERM"
            ]
        ]
    },
    {
        "text": "Between 2002 and 2006, although researchers still cited corpus-based grammar references for their studies (e.g., Cambridge Grammar of the English Language), one group of researchers made use of newly developed datasets, both large or small, such as The CHILDES Corpus, Wordnet, and A New Academic Word List.",
        "entities": [
            [
                56,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "Altering the span of the window around the node word where possible collocate words are considered can also significantly affect the results.",
        "entities": [
            [
                43,
                47,
                "TERM"
            ]
        ]
    },
    {
        "text": "Callies concludes his chapter by addressing potential bias in annotation methods within Learner Corpus Research (LCR) and related disciplines.",
        "entities": [
            [
                62,
                72,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is clear that many of the points made in the previous section apply equally well to corpus linguistics, where research is based on large amounts of language data which have been processed and turned into linguistic corpora.",
        "entities": [
            [
                87,
                105,
                "TERM"
            ]
        ]
    },
    {
        "text": "Likewise, a text produced by a learner is also natural, insofar as it is produced in its usual conditions, without there having been any particular manipulation.",
        "entities": [
            [
                12,
                16,
                "TERM"
            ]
        ]
    },
    {
        "text": "Both of these have their individual sections for restrictions further down the page, to allow you to narrow down your choices, based on domain information for the context-governed type, and age, social class, and sex of respondents -not the speakers, whose characteristics can be selected separately -for the demographically sampled data.",
        "entities": [
            [
                180,
                184,
                "TERM"
            ]
        ]
    },
    {
        "text": "The fundamentals of corpus architecture begin with conceptualizing corpora as collections of documents, possibly arranged in subcorpora, and often carrying metadata.",
        "entities": [
            [
                156,
                164,
                "TERM"
            ],
            [
                20,
                26,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are always reasons why it may be difficult to get data published, and of course, this step also requires some resources if the corpus is to be presented in a well-structured and well-designed way.",
        "entities": [
            [
                133,
                139,
                "TERM"
            ]
        ]
    },
    {
        "text": "If you set the argument characters.around to a number greater than zero, then the preceding and subsequent contexts will be as many characters (as opposed to corpus elements/lines).",
        "entities": [
            [
                158,
                164,
                "TERM"
            ]
        ]
    },
    {
        "text": "It also implies that the kind of text included as well as a combination of different text types is crucial in classifying corpora.",
        "entities": [
            [
                33,
                37,
                "TERM"
            ],
            [
                85,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "We use the usual for-loop with scan and grep to choose and process the files, and then we use exact.matches.2 to retrieve all the infinitives as well as all lemmas for the overall frequency list.",
        "entities": [
            [
                180,
                194,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the second part of the study, Granger and Lefer (2012) used the English part of the Label France translation corpus to identify frequent translation equivalents of two lexical bundles, i.e. de plus en plus (de) and sur le plan (de), and compare corpus-derived translation equivalents with those found in three French-English bilingual dictionaries: RC, HO, and the Larousse French-English Dictionary (LA).",
        "entities": [
            [
                112,
                118,
                "TERM"
            ],
            [
                248,
                254,
                "TERM"
            ]
        ]
    },
    {
        "text": "In fact, the definite article the is ranked #1 in the corpus with a frequency of 50,017,617.",
        "entities": [
            [
                54,
                60,
                "TERM"
            ]
        ]
    },
    {
        "text": "Therefore, we can furnish a general corpus with standard text samples of contemporary writings and a calculated and proportional representation of texts can suffice our requirement.",
        "entities": [
            [
                36,
                42,
                "TERM"
            ],
            [
                57,
                61,
                "TERM"
            ]
        ]
    },
    {
        "text": "Because of the difficulties in obtaining permission to use copyrighted materials, most corpus compilers have found themselves collecting far more written material than they are able to obtain permission to use: for ICE-USA, permission had been obtained to use only about 25 percent of the written texts initially considered for inclusion in the corpus.",
        "entities": [
            [
                87,
                93,
                "TERM"
            ],
            [
                345,
                351,
                "TERM"
            ]
        ]
    },
    {
        "text": "In summary, DP is clearly more effective than D at discriminating between uniform versus skewed distributions in a corpus, especially when it is computed based on a large number of corpus-parts.",
        "entities": [
            [
                115,
                121,
                "TERM"
            ],
            [
                181,
                187,
                "TERM"
            ]
        ]
    },
    {
        "text": "When compiling a new corpus, the task instructions and the input material should be neutral with regard to the elicitation of specific structure(s).",
        "entities": [
            [
                21,
                27,
                "TERM"
            ]
        ]
    },
    {
        "text": "Obviously, the more completely we can extract our object of research from the corpus, the better.",
        "entities": [
            [
                78,
                84,
                "TERM"
            ]
        ]
    },
    {
        "text": "Using an R script (R Core Team 2014), I extracted all instances of I and you (when tagged as PNP) from all 21 files of the British National Corpus World Edition (XML) whose names begin with 'KR'.",
        "entities": [
            [
                162,
                165,
                "TERM"
            ]
        ]
    },
    {
        "text": "Finally, thanks to the existence of a corpus of oral data, corpus linguistics also makes it possible to answer questions related to phonology and sociolinguistics.",
        "entities": [
            [
                59,
                77,
                "TERM"
            ],
            [
                38,
                44,
                "TERM"
            ]
        ]
    },
    {
        "text": "Other sections of the chapter focus on additional issues relevant to planning the construction of a corpus, such as how to determine the appropriate size of a corpus and the length of particular texts that the corpus will contain (complete texts versus shorter samples from each text, e.g. 2,000 words); how to select the particular genres to be included in a corpus (e.g. press reportage, technical writing, spontaneous conversations, scripted speech); and how to ensure that the writers or speakers analyzed are balanced for such issues as gender, ethnicity, and age.",
        "entities": [
            [
                100,
                106,
                "TERM"
            ],
            [
                159,
                165,
                "TERM"
            ],
            [
                210,
                216,
                "TERM"
            ],
            [
                360,
                366,
                "TERM"
            ],
            [
                279,
                283,
                "TERM"
            ]
        ]
    },
    {
        "text": "Analysis of corpus texts shows that apart from pure intralinguistic information, a corpus also carries several kinds of extralinguistic information.",
        "entities": [
            [
                12,
                18,
                "TERM"
            ],
            [
                83,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "This chapter has shown that corpus approaches to the study of literary style can take various forms.",
        "entities": [
            [
                28,
                34,
                "TERM"
            ]
        ]
    },
    {
        "text": "The type of English used 150 years ago in the United States is quite different from the type of English used in the United States today.",
        "entities": [
            [
                4,
                8,
                "TERM"
            ],
            [
                88,
                92,
                "TERM"
            ]
        ]
    },
    {
        "text": "Written texts, in contrast, are now widely available in digital formats and can easily be incorporated in a corpus after permission has been received to use a given text.",
        "entities": [
            [
                108,
                114,
                "TERM"
            ],
            [
                165,
                169,
                "TERM"
            ]
        ]
    },
    {
        "text": "The tagged version of the BROWN corpus does not contain quotation marks because they have intentionally been stripped from the text.",
        "entities": [
            [
                32,
                38,
                "TERM"
            ],
            [
                127,
                131,
                "TERM"
            ]
        ]
    },
    {
        "text": "Note that even if we manage to solve the problem of reliability (as systematic elicitation from a representative sample of speakers does to some extent), the epistemological status of intuitive data remains completely unclear.",
        "entities": [
            [
                113,
                119,
                "TERM"
            ]
        ]
    },
    {
        "text": "When building general reference corpora from the web we need to choose seed words that are likely to appear across a wide range of topics, but there has been some debate about exactly what constitute good seeds for a corpus of this type.",
        "entities": [
            [
                217,
                223,
                "TERM"
            ],
            [
                232,
                236,
                "TERM"
            ]
        ]
    },
    {
        "text": "For example, if we want to know whether learners of French as a foreign language at an advanced level are able to use collocations as native speakers do (collocations such as \"prendre une décision\" -to make a decision -or \"pleuvoir à verse\" -to pour with rain), we can search for occurrences of these expressions in text corpora produced by learners and compare the number of times these expressions appear -and their frequency -in a corpus of similar textual productions made by native speakers.",
        "entities": [
            [
                434,
                440,
                "TERM"
            ],
            [
                316,
                320,
                "TERM"
            ]
        ]
    },
    {
        "text": "The authors therefore coded every occurrence according to the type of process described: previous or past.",
        "entities": [
            [
                62,
                66,
                "TERM"
            ]
        ]
    },
    {
        "text": "In cross-linguistic research, it is of paramount importance to know, for instance, whether a given text was translated from Spanish into German or vice versa, because corpus studies have shown that translation direction influences translation choices, and hence the linguistic make-up of translated text (e.g. Dupont and Zufferey 2017).",
        "entities": [
            [
                167,
                173,
                "TERM"
            ],
            [
                99,
                103,
                "TERM"
            ],
            [
                299,
                303,
                "TERM"
            ]
        ]
    },
    {
        "text": "We can extend Script 3 in a different way to process an entire corpus.",
        "entities": [
            [
                63,
                69,
                "TERM"
            ]
        ]
    },
    {
        "text": "A small sample is more likely to be affected by chance and we may see spurious results.",
        "entities": [
            [
                8,
                14,
                "TERM"
            ]
        ]
    },
    {
        "text": "Take 100 instances of complement clauses and mark each for who uses them in terms of age and educational background (hopefully, this information will be available in the corpus).",
        "entities": [
            [
                170,
                176,
                "TERM"
            ]
        ]
    },
    {
        "text": "For the purposes of your study you want to select an unbiased sample of 40 texts that would represent the population.",
        "entities": [
            [
                62,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "Unfortunately, most of the best-known taggers that have been developed over the years, such as, for example, the CLAWS system, for which a number of tagsets, such as the C7 discussed above, have been developed, are generally not freely available to the public or may require the purchase of a commercial licence for tagging suitable quantities of text.",
        "entities": [
            [
                316,
                323,
                "TERM"
            ],
            [
                347,
                351,
                "TERM"
            ]
        ]
    },
    {
        "text": "Because of the complexity of parsing a corpus, there are relatively few corpora parsed in as much detail as ICE-GB and the DCPSE.",
        "entities": [
            [
                39,
                45,
                "TERM"
            ]
        ]
    },
    {
        "text": "Whatever seeds are chosen, this is only the first step in the process of building a corpus from the web.",
        "entities": [
            [
                84,
                90,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, we have seen that words have much more annotation and provide three attribute-value pairs: two with differently detailed POS tags and one with the word's lemma.",
        "entities": [
            [
                53,
                63,
                "TERM"
            ],
            [
                168,
                173,
                "TERM"
            ]
        ]
    },
    {
        "text": "Make sure that your analysis scripts are extensively commented (formats like R Markdown or Jupyter Notebooks invite extensive comments, but plain text scripts can also be used, of course).",
        "entities": [
            [
                146,
                150,
                "TERM"
            ]
        ]
    },
    {
        "text": "Of course, a corpus of the size of the BNC cannot be easily analyzed without the use of some kind of specialized software to be able to observe patterns using all the data contained in it.",
        "entities": [
            [
                13,
                19,
                "TERM"
            ]
        ]
    },
    {
        "text": "In order to be able to spot developments even when the corpus is not (yet) fully transcribed, it is useful to transcribe data according to a systematic \"zoom-in\" pattern, where one starts with the first and the last recording of a child, then transcribes the recording in the middle between the two, then the two recordings in the middle of the stretches thus defined, and so on.",
        "entities": [
            [
                55,
                61,
                "TERM"
            ]
        ]
    },
    {
        "text": "We will refer to this measure as the hapax-token ratio (or HTR) by analogy with the term type-token ratio.",
        "entities": [
            [
                89,
                105,
                "TERM"
            ],
            [
                43,
                48,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, corpus-based register studies could have far greater impact than they currently do.",
        "entities": [
            [
                9,
                21,
                "TERM"
            ]
        ]
    },
    {
        "text": "As a result, in the twenty-first century, corpus linguistics has become -in the words of Tony McEnery -a 'killer method' in linguistics, applied to a hugely diverse array of types of linguistic research.",
        "entities": [
            [
                42,
                60,
                "TERM"
            ]
        ]
    },
    {
        "text": "On the other hand, maybe keeping the amount of spoken data in the BNC relatively low was actually not too bad an idea, since transcribing spoken language is an expensive and time-consuming business, and one where corpus compilers often take too many 'shortcuts'.",
        "entities": [
            [
                213,
                219,
                "TERM"
            ]
        ]
    },
    {
        "text": "Likewise, the topics men and women talk about (identified from the keywords of the corpus) also differ.",
        "entities": [
            [
                83,
                89,
                "TERM"
            ]
        ]
    },
    {
        "text": "Likewise, we also consider how social science theory is exerting influence on studies in corpus linguistics.",
        "entities": [
            [
                89,
                107,
                "TERM"
            ]
        ]
    },
    {
        "text": "One of the earliest corpus-driven studies of this type was Salem's (1987) analysis of repeated lexical phrases in a corpus of French government resolutions.",
        "entities": [
            [
                20,
                33,
                "TERM"
            ],
            [
                116,
                122,
                "TERM"
            ],
            [
                50,
                54,
                "TERM"
            ]
        ]
    },
    {
        "text": "The annotation system has two basic facets: (1) it captures the identity of referents mentioned by different referring expressions, (2) it captures various aspects of the anaphoric relation.",
        "entities": [
            [
                4,
                14,
                "TERM"
            ]
        ]
    },
    {
        "text": "This type of information is nonetheless crucial for the quantitative comparison of linguistic productions between the two genders.",
        "entities": [
            [
                5,
                9,
                "TERM"
            ]
        ]
    },
    {
        "text": "An etymologically annotated text addresses all the questions and challenges related to the etymology of words.",
        "entities": [
            [
                28,
                32,
                "TERM"
            ]
        ]
    },
    {
        "text": "Second, most existing dispersion measures require a division of the corpus into parts and the decision of how to do this is not trivial.",
        "entities": [
            [
                68,
                74,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, many ready-made corpus tools can only offer the functionality they aim to provide for corpora with particular formats, and then can only provide a small number of kinds of output.",
        "entities": [
            [
                30,
                36,
                "TERM"
            ]
        ]
    },
    {
        "text": "This canonical form of the word is called its lemma.",
        "entities": [
            [
                46,
                51,
                "TERM"
            ]
        ]
    },
    {
        "text": "Understanding the choice between possible variants is one of the main foci of corpus linguistics, especially variationist corpus linguistics.",
        "entities": [
            [
                78,
                96,
                "TERM"
            ],
            [
                122,
                140,
                "TERM"
            ]
        ]
    },
    {
        "text": "While the reference corpus did offer sub-categorization of the fiction component of the corpus, aligning those sub-categories with Hemingway's oeuvre is not a straightforward task.",
        "entities": [
            [
                10,
                26,
                "TERM"
            ],
            [
                88,
                94,
                "TERM"
            ]
        ]
    },
    {
        "text": "Most of the analyses we will want to undertake with our corpus will be centred on a much smaller linguistic unit, such as the sentence, the clause or, perhaps most commonly, the word.",
        "entities": [
            [
                56,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "There are two additional measures that are important in other areas of empirical research but do not play a central role in corpus-linguistic data retrieval.",
        "entities": [
            [
                124,
                130,
                "TERM"
            ]
        ]
    },
    {
        "text": "The download and processing progress will be reported in the text window on the right, and a number of sub-folders will be created once the pages have been downloaded and processed successfully.",
        "entities": [
            [
                61,
                65,
                "TERM"
            ]
        ]
    },
    {
        "text": "As already mentioned, corpus linguistics has been criticized in relation to its suitability for the study of speech acts.",
        "entities": [
            [
                22,
                40,
                "TERM"
            ]
        ]
    },
    {
        "text": "Another approach, which usually looks at a larger span of potentially discontinuous words, is that of calculating the degree of relatedness of words that occur near a particular node word.",
        "entities": [
            [
                178,
                182,
                "TERM"
            ]
        ]
    },
    {
        "text": "The other corpora include children up to the age of 7, and only one of them (VionColas corpus) includes children up to 11 years old.",
        "entities": [
            [
                87,
                93,
                "TERM"
            ]
        ]
    },
    {
        "text": "For each type, determine whether there is a preference for the \"by phrase\".",
        "entities": [
            [
                9,
                13,
                "TERM"
            ]
        ]
    },
    {
        "text": "The third type usually either specifies formatting instructions, such as line breaks, etc., contains links to external resources, such as a style sheet that specifies the layout and formatting options, or can be used to include other information that doesn't require a containing element, such as, for example, a comment on a specific piece of data.",
        "entities": [
            [
                10,
                14,
                "TERM"
            ]
        ]
    },
    {
        "text": "For speech samples recorded from television or radio, additional information was written down, such as what station the recording was made from, where the station is located, and who should be contacted to obtain written permission to use the sample.",
        "entities": [
            [
                243,
                249,
                "TERM"
            ]
        ]
    },
    {
        "text": "Annotations of paralinguistic or linguistic features in a corpus impact its authenticity in complex ways.",
        "entities": [
            [
                58,
                64,
                "TERM"
            ]
        ]
    },
    {
        "text": "It was by means of this type of comparison that the annotations offered for the Google Books corpus in section 7.2 were considered as suitable tagging and parsing systems.",
        "entities": [
            [
                143,
                150,
                "TERM"
            ],
            [
                93,
                99,
                "TERM"
            ],
            [
                24,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "If I am interested in how local newspapers in Britain discussed the issue of crime in the years 2010 to 2013, I will almost certainly need to build a corpus myself for this specific purpose.",
        "entities": [
            [
                150,
                156,
                "TERM"
            ]
        ]
    },
    {
        "text": "In this sense, all research questions are framed from a particular perspective by reference to a specific methodology (corpus linguistics).",
        "entities": [
            [
                119,
                137,
                "TERM"
            ]
        ]
    },
    {
        "text": "The basis for this provocative question stemmed from research findings highlighting considerable cross-disciplinary variation in terms of lexical distribution, collocation, and even meaning.",
        "entities": [
            [
                160,
                171,
                "TERM"
            ]
        ]
    },
    {
        "text": "If the corpus is not suitable for the research questions, the corpus should be changed or retagged by the researcher.",
        "entities": [
            [
                7,
                13,
                "TERM"
            ],
            [
                62,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "Other textual markup is internal to the text, and in a spoken text would include such information as speaker IDs, and the beginnings and ends of overlapping speech.",
        "entities": [
            [
                14,
                20,
                "TERM"
            ],
            [
                40,
                44,
                "TERM"
            ],
            [
                62,
                66,
                "TERM"
            ]
        ]
    },
    {
        "text": "On the other hand, though, this makes it possible for the basic text to be linked to various types of data-enriching annotations, as well as to perform more complex search operations, or to store intermediate or final results of such searches for different users and for quicker access or export later.",
        "entities": [
            [
                64,
                68,
                "TERM"
            ]
        ]
    },
    {
        "text": "If we have chosen unrepresentative data or if we have extracted or annotated our data sloppily, the statistical significance of the results is meaningless.",
        "entities": [
            [
                100,
                124,
                "TERM"
            ]
        ]
    },
    {
        "text": "If text no. 10 were roughly three times longer than text no. 1, the two texts would be equivalent in terms of the proportion of passive sentences.",
        "entities": [
            [
                3,
                7,
                "TERM"
            ],
            [
                52,
                56,
                "TERM"
            ]
        ]
    },
    {
        "text": "Similar to taggers, once the parsers are trained, they automatically annotate the text for you.",
        "entities": [
            [
                82,
                86,
                "TERM"
            ]
        ]
    },
    {
        "text": "We have also described the different forms that variables in a corpus may adopt, and highlighted the fact that statistical tests must be adapted to the nature of the variables.",
        "entities": [
            [
                63,
                69,
                "TERM"
            ]
        ]
    },
    {
        "text": "Indeed, we wish to emphasize that resources like COHA are invaluable for diachronic linguistic research, and that these resources are even more useful when one knows exactly how they are constructed and annotated.",
        "entities": [
            [
                73,
                83,
                "TERM"
            ]
        ]
    },
    {
        "text": "Instead, the aim is to provide an overview of the main approaches to the problem and a discussion of the methods that are most often used and / or seem to the author to be most intuitively accessible and effective for corpus linguists.",
        "entities": [
            [
                218,
                224,
                "TERM"
            ]
        ]
    },
    {
        "text": "A second portion of the corpus focuses on advanced learners who have all lived in France, for a period ranging from 1-2 years to more than 30 years (see Chapter 3, section 3.3 for a study based on these data).",
        "entities": [
            [
                24,
                30,
                "TERM"
            ]
        ]
    },
    {
        "text": "This type of display may already be highly useful for identifying most of the potential for grammatical and semantic polysemy, but the maximum number may not allow you to extract enough samples in cases where a search term has a high degree of polysemy on both levels.",
        "entities": [
            [
                5,
                9,
                "TERM"
            ]
        ]
    },
    {
        "text": "Potentially ordinal data are actually frequently treated like nominal data in corpus linguistics (cf. Section 5.3.2), and with complex scales combining a range of different dimensions, this is probably a good idea; but ordinal data also have a useful place in quantitative corpus linguistics.",
        "entities": [
            [
                78,
                96,
                "TERM"
            ],
            [
                273,
                291,
                "TERM"
            ]
        ]
    },
    {
        "text": "Since most of the words in the non-specialized lexicon are polysemic, this annotation is essential.",
        "entities": [
            [
                75,
                85,
                "TERM"
            ]
        ]
    },
    {
        "text": "Adding new annotations to a spoken corpus, however, can still constitute a major challenge for the simple lack of suitable tools although great advances have been made in the last decade in terms of the interoperability of the major tools in use for spoken corpus construction: ELAN, Praat, EXMARaLDA and ANVIL (see Sect. 11.4 below) now all have import and export functions for their respective file formats so that it is possible to add new annotations with one of these tools to a spoken corpus that was compiled with another tool (for this Transformer by Oliver Ehmer can also be used).",
        "entities": [
            [
                257,
                276,
                "TERM"
            ],
            [
                35,
                41,
                "TERM"
            ],
            [
                491,
                497,
                "TERM"
            ]
        ]
    },
    {
        "text": "Again, as 'human consumers' of the text, this will not cause any processing problems, but for the computer, the, The, and THE are in fact three different 'words', or at least word forms.",
        "entities": [
            [
                35,
                39,
                "TERM"
            ]
        ]
    },
    {
        "text": "In all but the most basic examples, it is likely that the researcher will want to expand the corpus beyond the initial set of seeds.",
        "entities": [
            [
                93,
                99,
                "TERM"
            ]
        ]
    },
    {
        "text": "The genre features found in a piece of fiction can also be used to extend an argument regarding the intertextual properties of a text in such a way that the intellectual context of the writing can be connected to the written text.",
        "entities": [
            [
                129,
                133,
                "TERM"
            ],
            [
                225,
                229,
                "TERM"
            ]
        ]
    },
    {
        "text": "Similarly, it is possible to create a sizeable corpus of Latin, with a time-depth of over 2000 years, as shown by the 13-million-word LatinISE corpus, built by Barbara McGillivray.",
        "entities": [
            [
                47,
                53,
                "TERM"
            ],
            [
                143,
                149,
                "TERM"
            ]
        ]
    },
    {
        "text": "Our main argument was that the recent trends and advances in the field, such as the publication of big-data corpora, the increased reliance on statistical approaches to linguistic data, and the exploitation of various kinds of metadata, require that the linguist should have a detailed understanding of the structure of the corpus, its sampling procedure, and the principles followed in the construction of the metadata.",
        "entities": [
            [
                227,
                235,
                "TERM"
            ],
            [
                411,
                419,
                "TERM"
            ],
            [
                324,
                330,
                "TERM"
            ]
        ]
    },
    {
        "text": "To find out what the current working directory is, type getwd() in the code editor and run the code.",
        "entities": [
            [
                51,
                55,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is an interesting question to what extent such a citation database can be treated as a corpus (cf. the extensive discussion in Hoffmann 2004).",
        "entities": [
            [
                90,
                96,
                "TERM"
            ]
        ]
    },
    {
        "text": "COCA, on the other hand, currently has more than 180,000 texts and the 400-million-word COHA historical corpus has more than 100,000 texts.",
        "entities": [
            [
                104,
                110,
                "TERM"
            ]
        ]
    },
    {
        "text": "These are generally captured in corpus linguistics as part of the external contextual features.",
        "entities": [
            [
                32,
                50,
                "TERM"
            ]
        ]
    },
    {
        "text": "In a research context, in contrast, especially when researchers want to make a claim for exhaustive data retrieval and/or when the sequencing of attestations in the corpus matters for the research question at hand, one would only delete concordance lines that contain false hits.",
        "entities": [
            [
                237,
                248,
                "TERM"
            ],
            [
                165,
                171,
                "TERM"
            ]
        ]
    },
    {
        "text": "With mixed-effects models, it is important to report the type of mixed-effects model used as well as the details of the model.",
        "entities": [
            [
                57,
                61,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, even this relatively simple operation can raise issues regarding what is, and what is not, considered to be part of the same lemma.",
        "entities": [
            [
                134,
                139,
                "TERM"
            ]
        ]
    },
    {
        "text": "By providing a manually checked, dual POS tag which encodes both form and function (VOICE Project 2013), the XML corpus can be searched for all cases of an \"innovative\" form, in Cogo and Dewey's terms.",
        "entities": [
            [
                113,
                119,
                "TERM"
            ],
            [
                109,
                112,
                "TERM"
            ]
        ]
    },
    {
        "text": "It then offers some strong counter arguments for why an understanding of the basic concepts of programming is essential to any corpus researcher hoping to do cutting-edge work.",
        "entities": [
            [
                127,
                133,
                "TERM"
            ]
        ]
    },
    {
        "text": "The corpus created in this way can be directly analyzed using the tools provided by the platform, for example, for retrieving concordances, word lists or keywords.",
        "entities": [
            [
                4,
                10,
                "TERM"
            ]
        ]
    },
    {
        "text": "A historical or diachronic corpus is a collection of texts from different periods.",
        "entities": [
            [
                16,
                26,
                "TERM"
            ],
            [
                27,
                33,
                "TERM"
            ]
        ]
    },
    {
        "text": "We identify and mark various rhetorical devices used in a text.",
        "entities": [
            [
                58,
                62,
                "TERM"
            ]
        ]
    },
    {
        "text": "Spatial restrictions apply to a handbook article like this one (hence the 15line snippets as opposed to displaying the concordance in its entirety) as much as to the use of concordances for classroom use (not many students would want to inspect thousands of concordance lines).",
        "entities": [
            [
                119,
                130,
                "TERM"
            ],
            [
                258,
                269,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the search box, type in the word, round as indicated in the following graphic.",
        "entities": [
            [
                19,
                23,
                "TERM"
            ]
        ]
    },
    {
        "text": "There is a growing strand of research that explores the efficacy of so-called datadriven learning (DDL) approaches, which give students access to large amounts of authentic language data (Frankenberg-Garcia et al. 2011), and corpus-based materials naturally lend themselves to use in such an approach.",
        "entities": [
            [
                225,
                237,
                "TERM"
            ]
        ]
    },
    {
        "text": "If encoding does not match, you will potentially not find relevant text.",
        "entities": [
            [
                3,
                11,
                "TERM"
            ],
            [
                67,
                71,
                "TERM"
            ]
        ]
    },
    {
        "text": "The British press reporting is your target corpus and the American press reporting is your reference corpus.",
        "entities": [
            [
                91,
                107,
                "TERM"
            ],
            [
                43,
                49,
                "TERM"
            ]
        ]
    },
    {
        "text": "Hand-tagging an entire corpus, even a small one, is a monumental effort.",
        "entities": [
            [
                5,
                12,
                "TERM"
            ],
            [
                23,
                29,
                "TERM"
            ]
        ]
    },
    {
        "text": "Most relevant for general linguistic concerns is the register perspective on text varieties since this offers important insights into the functionality of linguistic structures in use.",
        "entities": [
            [
                77,
                81,
                "TERM"
            ]
        ]
    },
    {
        "text": "The first independent variable concerns the type of conversation from which the examples are drawn.",
        "entities": [
            [
                44,
                48,
                "TERM"
            ]
        ]
    },
    {
        "text": "The exploration can be done in different levels: all corpus or by user type (news agencies or individuals).",
        "entities": [
            [
                53,
                59,
                "TERM"
            ],
            [
                71,
                75,
                "TERM"
            ]
        ]
    },
    {
        "text": "As we have described in previous chapters, one of the main focuses of corpus linguistic research is variation.",
        "entities": [
            [
                70,
                76,
                "TERM"
            ]
        ]
    },
    {
        "text": "Some browsers will remove the HTML and only leave plain-text content, while others may retain some bits of HTML, such as, for example, email addresses contained in mailto links (i.e. those that allow you to fire up an email client when you click on them), etc.",
        "entities": [
            [
                56,
                60,
                "TERM"
            ]
        ]
    },
    {
        "text": "Second, extremely short texts cannot be easily compared with other texts using many typical quantitative corpus-linguistic methods, such as normalization, because the normalization results become meaningless as the length of the text becomes increasingly short.",
        "entities": [
            [
                105,
                111,
                "TERM"
            ],
            [
                229,
                233,
                "TERM"
            ]
        ]
    },
    {
        "text": "Such manual sorting tasks can actually be avoided if the words in the corpus have been previously annotated into grammatical categories.",
        "entities": [
            [
                70,
                76,
                "TERM"
            ]
        ]
    },
    {
        "text": "Thus, if we are dealing with a variable that is likely to be of general interest, we should consider the possibility of annotating the corpus itself, instead of first extracting the relevant data to a raw data table and annotating them afterwards.",
        "entities": [
            [
                135,
                141,
                "TERM"
            ]
        ]
    },
    {
        "text": "The other thing is that our expectations concerning the results may differ from the actual output of the concordancer, as you will hopefully have noticed while concordancing on the words this and in.",
        "entities": [
            [
                105,
                117,
                "TERM"
            ]
        ]
    },
    {
        "text": "Thus, if you're working on a relatively sizable corpus, be prepared to wait for a few minutes for n-gram calculations to finish.",
        "entities": [
            [
                48,
                54,
                "TERM"
            ]
        ]
    },
    {
        "text": "It is important for corpus users to be aware of such potential differences in the orthographic transcriptions as they may lead to the missing of tokens in a word-based corpus search: in a search for 'because', for instance, transcribed forms such as 'coz' and 'cos' will not be found.",
        "entities": [
            [
                20,
                26,
                "TERM"
            ],
            [
                168,
                174,
                "TERM"
            ]
        ]
    },
    {
        "text": "Spoken language does not generally have the same type of planning and opportunities for revision that we find in many types of written language.",
        "entities": [
            [
                49,
                53,
                "TERM"
            ]
        ]
    },
    {
        "text": "The main reason is that I have found, in my many years of teaching corpus linguistics, that most available textbooks are either too general or too specific.",
        "entities": [
            [
                67,
                85,
                "TERM"
            ]
        ]
    },
    {
        "text": "Yet, the building of an LD-based corpus faces particular challenges through the typically severer limitations of resources and the fact that potential academic users of the corpus have typically no prior knowledge.",
        "entities": [
            [
                33,
                39,
                "TERM"
            ],
            [
                173,
                179,
                "TERM"
            ]
        ]
    },
    {
        "text": "To get samples of roughly equal size for expository clarity, let us select every sixth case of the of -possessive, giving us 25 cases (note that in a real study, there would be no good reason to create such roughly equal sample sizes -we would simply use all the data we have).",
        "entities": [
            [
                221,
                227,
                "TERM"
            ]
        ]
    },
    {
        "text": "While a raw corpus is a highly useful resource, annotation provides an extra layer of information, which can be counted, sorted, and compared.",
        "entities": [
            [
                48,
                58,
                "TERM"
            ],
            [
                12,
                18,
                "TERM"
            ]
        ]
    },
    {
        "text": "The frequency list may be of word types, lemmas or any kind of tag -thus, we often talk about keywords and key tags.",
        "entities": [
            [
                4,
                18,
                "TERM"
            ]
        ]
    },
    {
        "text": "For instance, in the case of speech versus writing, we might propose a corpus that is 50 per cent spoken and 50 per cent written data (this is, in fact, the design of the International Corpus of English, ICE).",
        "entities": [
            [
                71,
                77,
                "TERM"
            ]
        ]
    },
    {
        "text": "But there is another dimension to extensibility: The reliability of a particular annotation can also \"vanish\" because it turns out to be misguided, for whatever reason.",
        "entities": [
            [
                81,
                91,
                "TERM"
            ]
        ]
    },
    {
        "text": "To obtain frequencies or examples, one must engage with the corpus or 'query' it.",
        "entities": [
            [
                60,
                66,
                "TERM"
            ]
        ]
    },
    {
        "text": "The word context here refers to something different from what we discussed above, that is, not the situational usage in a particular place and time, but instead the immediately surrounding text, something we can also refer to as co-text in case of ambiguity.",
        "entities": [
            [
                189,
                193,
                "TERM"
            ],
            [
                232,
                236,
                "TERM"
            ]
        ]
    },
    {
        "text": "This can be especially tricky, as there is often some variation in how these items appear across a corpus (or even within a text); for example, although the hyphenated form, well-being, is clearly the most common option found in the Corpus of Contemporary American English (COCA) (>8000 occurrences), the forms well being and wellbeing each occur several hundred times.",
        "entities": [
            [
                99,
                105,
                "TERM"
            ],
            [
                124,
                128,
                "TERM"
            ]
        ]
    },
    {
        "text": "Obligatory: Obtaining (relative) frequencies from the corpus of the linguistic variable of interest for each of the periods (e.g. years, decades etc.) covered by the analysis.",
        "entities": [
            [
                54,
                60,
                "TERM"
            ]
        ]
    },
    {
        "text": "In the book, it is noted that these two approaches are not necessarily mutually exclusive: the generative linguist, for instance, could very well work with examples that are corpus-based rather than examples that are made up.",
        "entities": [
            [
                174,
                186,
                "TERM"
            ]
        ]
    },
    {
        "text": "Note that this entire corpus -let us call it A Historical Corpus of American News Writing -would consist of three sub-corpora related to each of the three time periods.",
        "entities": [
            [
                22,
                28,
                "TERM"
            ]
        ]
    },
    {
        "text": "This corpus contains 400,000 words of spoken English from ICE-GB and an additional 400,000 spoken words from the London-Lund Corpus, a corpus that was based on texts recorded between 1960 and 1980.",
        "entities": [
            [
                5,
                11,
                "TERM"
            ],
            [
                135,
                141,
                "TERM"
            ]
        ]
    },
    {
        "text": "The York-Toronto-Helsinki Parsed Corpus of Old English Prose (YCOE) is a 1.5-million-word syntactically annotated corpus representing Old English prose, with texts dating from before 850 up to 1150.",
        "entities": [
            [
                114,
                120,
                "TERM"
            ]
        ]
    },
    {
        "text": "In summary, in addition to frequency indications, it is important to provide information about lexical dispersion in a corpus, either by simply calculating the percentage of texts in which a word is used, or by reporting a dispersion measurement, such as the deviation of proportions we have just described.",
        "entities": [
            [
                119,
                125,
                "TERM"
            ]
        ]
    },
    {
        "text": "As we will see throughout the chapter, creating a corpus is a challenging task and presents many difficulties.",
        "entities": [
            [
                50,
                56,
                "TERM"
            ]
        ]
    },
    {
        "text": "Throughout this section, we encountered various issues with tools and methods, again partly illustrating the effects of data where flaws in the basic compilation of the corpus may cause potential errors in the result, but partly also pointing out potential shortcomings in the particular tools at our disposal.",
        "entities": [
            [
                169,
                175,
                "TERM"
            ]
        ]
    },
    {
        "text": "This illustrates the first problem with the n-gram method, since even with a small text such as this, a large number of results is generated.",
        "entities": [
            [
                83,
                87,
                "TERM"
            ]
        ]
    },
    {
        "text": "Such a corpus is sometimes referred to as a balanced corpus.",
        "entities": [
            [
                7,
                13,
                "TERM"
            ],
            [
                53,
                59,
                "TERM"
            ]
        ]
    },
    {
        "text": "It throws open the notion of 'aboutness' and uses a data-driven way of organising the content of a large corpus.",
        "entities": [
            [
                105,
                111,
                "TERM"
            ]
        ]
    },
    {
        "text": "The best way to achieve this is to draw a complete sample of the phenomenon in question, i.e. to retrieve all instances of it from the corpus (issues of retrieval are discussed in detail in Chapter 4).",
        "entities": [
            [
                135,
                141,
                "TERM"
            ],
            [
                51,
                57,
                "TERM"
            ]
        ]
    },
    {
        "text": "For annotations of a specific linguistic phenomenon (in the situation presented at the end of section 7.2), one solution would be to retrieve the relevant data from the corpus, and then to annotate them separately.",
        "entities": [
            [
                169,
                175,
                "TERM"
            ]
        ]
    },
    {
        "text": "For a fully automatic large-scale annotation of the syntax, speech acts, etc., you can also try my Dialogue Annotation and Research Tool (DART), which not only allows you to annotate hundreds of dialogues in this way within minutes, but also to post-edit/correct the annotations, as well as to carry out similar analysis operations to those we learnt how to perform in AntConc, including concordancing, n-gram analysis, etc.",
        "entities": [
            [
                34,
                44,
                "TERM"
            ]
        ]
    },
    {
        "text": "The complexity of the corpus architecture results from its annotations: as the data is collected, student annotators iteratively apply a large number of annotation schemes to their data using different formats and tools, including document structure in TEI XML, POS tagging, syntactic parsing, entity and coreference annotations and discourse parses in Rhetorical Structure Theory.",
        "entities": [
            [
                153,
                163,
                "TERM"
            ],
            [
                266,
                273,
                "TERM"
            ],
            [
                22,
                28,
                "TERM"
            ],
            [
                257,
                260,
                "TERM"
            ]
        ]
    },
    {
        "text": "While it is true that a sample is expected to bring together a rather restricted version of the overall population, it should also reproduce its main features so that the results obtained on this sample can be extended to the entire population.",
        "entities": [
            [
                24,
                30,
                "TERM"
            ],
            [
                196,
                202,
                "TERM"
            ]
        ]
    },
    {
        "text": "Like semantic tagging, automated parsing has a much higher error rate than POS tagging, because the task is inherently more difficult.",
        "entities": [
            [
                14,
                21,
                "TERM"
            ],
            [
                79,
                86,
                "TERM"
            ]
        ]
    },
    {
        "text": "This process was repeated 10 times with a different 2,000 word random sample each time.",
        "entities": [
            [
                70,
                76,
                "TERM"
            ]
        ]
    },
    {
        "text": "Complications may also arise if the character set is not directly supported by the computer the corpus is viewed on.",
        "entities": [
            [
                96,
                102,
                "TERM"
            ]
        ]
    },
    {
        "text": "For the former, our table contains the frequencies of \"perl\" in each corpus part (in the column for TRUEs), which we can divide by the overall frequency of \"perl\" in the file to get percentages (to be stored in a vector called obs.percs), and we can use the function rowSums to compute the corpus part sizes in percentage in a vector exp.percs (which should all be really close to 10 percent, given how we split the corpus up into ten parts above), from which we can compute DP.",
        "entities": [
            [
                69,
                75,
                "TERM"
            ],
            [
                290,
                296,
                "TERM"
            ],
            [
                416,
                422,
                "TERM"
            ]
        ]
    },
    {
        "text": "All of the corpora in Sketch Engine that are publicly accessible and that are more than a billion words in size are based on web pages, and there are currently three corpora of English that contain more than a billion words of text.",
        "entities": [
            [
                227,
                231,
                "TERM"
            ]
        ]
    },
    {
        "text": "Even if the corpus compilers are deeply familiar with the material, it is still the case that memory is both short and fallible, so if they want to use the corpus in a few years' time, important details of the specific context of the data may well have been forgotten.",
        "entities": [
            [
                12,
                18,
                "TERM"
            ],
            [
                156,
                162,
                "TERM"
            ]
        ]
    },
    {
        "text": "Another limitation to the use of online corpora available for consultation is that they only offer limited access to their metadata (see Chapter 6).",
        "entities": [
            [
                123,
                131,
                "TERM"
            ]
        ]
    },
    {
        "text": "However, a true conversion to percentage can be achieved by considering the maximum possible variation in a given corpus.",
        "entities": [
            [
                114,
                120,
                "TERM"
            ]
        ]
    },
    {
        "text": "In Chapter 7, we will see that errors can be systematically annotated in a corpus, in the same way as syntactic or semantic information is provided.",
        "entities": [
            [
                75,
                81,
                "TERM"
            ]
        ]
    },
    {
        "text": "The linguist Randolph Quirk created what is now known as the Quirk Corpus, a corpus of spoken and written British English collected between 1955 and 1985.",
        "entities": [
            [
                77,
                83,
                "TERM"
            ]
        ]
    },
    {
        "text": "It could even be argued that clear outliers do not even represent the varieties of interest in the corpus particularly well, and that therefore it would even be beneficial to exclude them.",
        "entities": [
            [
                99,
                105,
                "TERM"
            ]
        ]
    },
    {
        "text": "At the end of the corpus, Anne produced 230 word types against 182 for Max, which tends to confirm that her language was more advanced.",
        "entities": [
            [
                18,
                24,
                "TERM"
            ]
        ]
    },
    {
        "text": "Examples include translations from EU Parliament debates into the 23 languages of the European Union, or the Canadian Hansard corpus, containing Canadian Parliament debates in English and French.",
        "entities": [
            [
                126,
                132,
                "TERM"
            ]
        ]
    },
    {
        "text": "So, by checking the frequency of definite, indefinite and \"zero\" articles in a corpus of bios and then looking at concordance lines for each, we find that professors are far more likely to use naming terms that collocate with definiteness (she is professor of, he is the author of) which serve to uniquely identify them.",
        "entities": [
            [
                114,
                125,
                "TERM"
            ],
            [
                79,
                85,
                "TERM"
            ]
        ]
    }
]