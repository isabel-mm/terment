In Part I, the first three chapters focus on corpus design and address issues related to corpus compilation, corpus annotation, and corpus architecture.
Part II deals with corpus methods: Chapters 4-9 provide an overview of the most commonly used methods to extract linguistic and frequency information from corpora (frequency lists, keywords lists, dispersion measures, co-occurrence frequencies and concordances) as well as an introduction on the added value of programming skills in corpus linguistics.
Chapters 10-16 in Part III review different corpus types (diachronic corpora, spoken corpora, parallel corpora, learner corpora, child language corpora, web corpora, and multimodal corpora), with each chapter focusing on the specific methodological challenges associated with the analysis of each type of corpora.
Chapters 18 and 19 focus on exploratory techniques, i.e., cluster analysis and the multidimensional exploratory approaches of correspondence analysis, multiple correspondence analysis, principal component analysis, and exploratory factor analysis.
In many of the well-known corpora of English, the ambition has been to cover a general and very common type of discourse (such as 'conversation in a variety of English') or a very large population (such as 'second-language learners of English').
However, if the available types of discourse are not already classified in some reliable way, as in the case of spoken language, it means that the corpus builder will have to dedicate a great deal of time to researching the characteristics of the target discourse in order to develop valid and acceptable selection criteria.
While they often make it possible to search A. Ädel the archive, they may not make the text files downloadable other than one by one by clicking a hyperlink.
There is no rule of thumb for corpus size, except for the general principle 'the more, the better'.
There are grey areas in copyright law and copyright infringement is looked at in different ways in different parts of the world, so it is difficult to find universally valid advice on the topic, but generally speaking copyright may prove quite a hindrance for corpus compilation.
There are several different ways in which information about the corpus design can be disseminated.
Corpus descriptions are sometimes published in peer-reviewed journals, especially if the corpus is breaking new ground (as is the case in Representative Study 2 below), so that the research community can benefit from discussions on corpus design.
Corpora often come with "read me" files where the corpus design is accounted for.
Through the inclusion of 'metadata'-data about the data-about the type of discourse represented in the corpus, the corpus user can keep track of or investigate different factors that may influence language use, which may explain differences observed in different types of data.
For example, the corpus compiler may include information based on interviews with participants or participant observation.
A common way of collecting metadata is by asking corpus participants to fill out a questionnaire which has been carefully designed by the corpus compiler so as to include information likely to be relevant with respect to the specific context of the discourse included and the people represented.
Based on metadata from the questionnaire, it is possible to select a subset of the ICLE corpus, for example to study systematically potential differences in language use between learners who have and who have not spent any time abroad in a country where the target language is spoken natively-and thus test a hypothesis from Second Language Acquisition research.
A plain text format (such as .txt) is often used for corpus files.
When naming files for the corpus, it is useful to have the file name in some way reflect what is in the file.
If, say for various reasons related to copyright, it is not possible to make the complete set of corpus files available to others, the corpus could still be made searchable online and concordance lines from the corpus be shown.
Some corpora are intentionally constructed for comparative studies (this includes parallel corpora, covered in Chap.
The corpora were compared qualitatively as well, by identifying patterns in the concordance lines and analysing the context ("collocational profiles") of the references to hosts, specifically of people and locals, which occurred in both corpora.
However, given that the intended audience of this handbook is expected to have limited resources for corpus compilation, it seems useful to provide an example of a study where it was possible to use part of an already existing corpus.
If we consider the future of corpus building from the perspective of the loss of complex information, it is interesting to note that few existing corpora reflect a feature which many present-day types of discourses exhibit: that of multimodality.
Some of the possibilities of corpus annotation are presented in the next chapter.
In order to promote and make better use of corpus enrichment, there is a need for collaborative work between linguists with a deep knowledge of the needs to different areas such as Second Language Acquisition or Historical Linguistics and experts in Computational Linguistics or Natural Language Processing.
Consequently, it is automated linguistic annotation that we will be concerned with in this chapter (see Part III of this volume for discussion of manual annotation in certain kinds of corpora, e.g., annotation of errors in a learner corpus).
The CLAWS tagger, for example, assigns a hyphenated POS tag, referred to as an "ambiguity tag", when its tagging algorithm is unable to unambiguously assign a single POS to a word.
Hyphenated tags also have a useful role to play in the tagging of diachronic corpora where a word may come to be associated with different parts of speech or different functions through time (cf.
POS tags may also be attached to sequences of words.
These tags, of which there are many, are assigned as part of an "idiom tagging" step after the initial assignment of POS tags and involve matching the sequences of POS-tagged words in the corpus against templates of multiword sequences.
Like the lemmas found in corpora, dictionary headwords often aim to represent a base word form (e.g., toaster, shine, be), rather than provide separate entries for each distinct inflected word form (e.g., toasters, shines / shone / shining, is / am / are / were / was / been / being).
As when using other corpus annotations that have been produced by automatic or semi-automatic procedures, understanding the limitations of automatic lemmatization and treating its outputs accordingly with a degree of circumspection is often a necessary part of the corpus annotation and analysis process.
Identifying particular multiword expressions, as mentioned in the preceding section, is an example of this.
In the case of relatively well-resourced languages like English, for which many corpus annotation standards and tools exist, many common annotation tasks can be accomplished with 'off-the-shelf' software tools and minimal customization of annotation standards or procedures.
In the most straightforward case, it may be possible to use existing annotation software to automatically apply a given set of POS tagging conventions to corpus source materials.
Creating a new POS tagging model is often an iterative process: small samples of text are manually annotated and used as training data to create a provisional POS tagger, which then automatically annotates more texts.
These texts are then reviewed and their POS tags corrected, creating more data for the next round of training and annotation (cf.
Natural language processing pipelines provide one means of applying separate annotation tasks to texts in user-defined sequences (e.g., applying tokenization, then part-of-speech tagging, then syntactic parsing to unannotated materials), which can greatly assist in constructing annotated corpora of all sizes.
Tools for Corpus Linguistics provides an up-to-date list of software packages for corpus annotation and analysis, and includes information about their pricing and the operating systems that they support.
For example, although spoken language is considered 'primary' in many senses, corpus architectures usually treat aligned audio/video information (A/V for short) as a type of annotation, and this can have consequences for corpus architecture.
In stand-off formats, different layers of information are stored in separate files using a referencing mechanism which allows us, for example, to leave an original text file unchanged.
POS annotations in a separate file specifying the character offsets in the text file at which the annotations apply (e.g.
The complexity of the corpus architecture results from its annotations: as the data is collected, student annotators iteratively apply a large number of annotation schemes to their data using different formats and tools, including document structure in TEI XML, POS tagging, syntactic parsing, entity and coreference annotations and discourse parses in Rhetorical Structure Theory.
The PTB format is the most popular way of representing projective constituent trees (no crossing edges) with single node annotations (part of speech or syntactic category).
Some corpus practitioners use scripts, often in Python or R, to evaluate their data, without using a dedicated search engine (see Chap.
It is therefore often desirable to have a search engine that is capable of extracting data based on a simple query.
In public projects, a proprietary search engine tailored explicitly for a specific corpus is often programmed, which cannot easily be used for other corpora.
A fundamental statistic in assessing the saliency of any linguistic feature is frequency of occurrence, or, simply, the number of times a feature of interest occurs in a data set.
Measures of dispersion provide a more comprehensive picture of frequency distributions, allowing researchers to determine whether features are generally representative of a target discourse domain or, alternatively, limited to certain contexts or idiosyncrasies of certain language users.
The simplest measure of dispersion is range, and it is typically operationalized in terms of the number of texts and/or sections in which a feature occurs.
Gardner and Davies' study exemplifies a contemporary approach to constructing and validating a frequency list of academic vocabulary-one based on lemmas rather than word families-by taking advantage of technological advances (e.g., part of speech tagging; the ability to compile substantially larger corpora) and the application of statistics to building frequency lists.
Gardner and Davies argue that using the lemma as the unit of analysis will allow list users to more accurately target the most frequently occurring forms and meaning senses of academic vocabulary.
Discipline measure: Selected words could not occur at more than 3 times the expected frequency in any one discipline, in order to avoid including discipline-specific vocabulary Gardner and Davies arrived at each cut-off rate via experimentation, as no guidance was available in previous frequency list research.
Results of this analysis demonstrate that the list performs considerably better coverage-wise in academic writing than in other written genres, in both in the academic portion of the corpus from which the list was constructed (13.8%), and a different academic sub-corpus from the BNC (13.7%) (compared to 8.0% for newspaper articles and 3.4% for fiction).
Of particular note in their study is an additional step that they employ for addressing reliability: comparing frequency list items across different corpora.
Such disambiguation remains both a theoretical and practical challenge for corpus-based frequency list research.
The most frequently used measure in contemporary frequency list research is Juilland's D (e.g.
That is, to what degree are corpus-based frequency lists generalizable to target discourse domains?
At present, evidence of frequency list generalizability tends to come in one or two forms, both of which are indirect.
The first comprises primarily corpus-external evidence, focused on corpus design (i.e., Do samples represent the diversity of text types, topics, etc.
Manuals on lexical frequency list research, however, do recommend such comparisons.
Future research should continue to investigate how different variables such as frequency profiles of target features, selection criteria used, or corpus design may affect the reliability of lists.
This text provides a comprehensive, practical, and very accessible discussion of important issues in frequency list design and evaluation.
Nation provides useful recommendations for researchers through all steps in frequency list development, from designing (or choosing) a corpus to choosing an appropriate unit of analysis (including dealing with homoforms and multi-word units), to determining criteria for word selection and ordering.
Imagine further that corpus linguist is looking at that list to identify verbs and adjectives within a certain frequency range -maybe because he needs to (i) create stimuli for a psycholinguistic experiment that control for word frequency, (ii) identify words from a certain 100 S. Th.
Imagine, finally, the frequency range he is currently interested in is between 35 and 40 words per million words and, as he browses the frequency list for good words to use, he comes across an adjective and a verbenormous and staining -that he thinks he can use because they both occur 37 times in the Brown corpus (and are even equally long) so he notes them down for later use and goes on.
However, just like trying to summarize the distribution of any numeric variable using only a mean can be treacherous (especially when the numeric variable is not normally distributed), so is trying to summarize the overall 'behavior' (or the co-occurrence preferences or the keyness) of a word x on the basis of just its frequency/frequencies because, as exemplified above, words with identical frequencies can exhibit very different distributional behaviors.
Some of these measures are effect sizes in the sense that they do not change if the co-occurrence tables from which they are computed are increased by some factor (e.g., the odds ratio), others are based on significance tests, which means they conflate both sample size/actual 112 S. Th.
Gries observed frequencies and effect size (e.g., the probably most widely-used measure, the log-likelihood ratio).
This is relevant in the present context of dispersion measures because we are now facing a similar issue in dispersion research, namely when researchers and lexicographers also take two dimensions of information -frequency and the effect size of dispersion -and conflate them into one value such as an adjusted frequency (e.g., by multiplication, see above Juilland's U).
Gries Also, even just in the sixth frequency band, the extreme range values that are observed are 85 / 905 = 9.4% vs. 733 / 905 = 81% of the corpus files, i.e.
Frequency lists have also been allied with other kinds of grammatical information (such as major word class) and translational glosses (both for the words themselves and example sentences) and turned directly into frequency dictionaries for learners and teachers in a number of languages (e.g.
However, a word may have a high frequency count in the BNC not because it is widely used in all sections of the corpus, but because it has a very high frequency in only certain parts of the corpus and not in others, e.g.
As a first step, two frequency sorted word lists are prepared, one from the corpus being studied (the 'target') and one from a reference corpus.
The reference dataset in corpus linguistics studies is usually a general corpus, representative of some language or variety of language.
Next, the frequency of each word in the target corpus is compared to its frequency in the reference dataset in order to calculate a keyness value.
The resulting sorted list contains two kinds of keyword: positive (those which are unusually frequent in the target corpus relative to the reference corpus) and negative (words which are unusually infrequent in the target corpus).
It is also common to describe these two groups of words as overused (for positive) and underused (for negative), particularly in the learner corpus literature (cf.
The log-likelihood score in this case is LL = 2*((a*ln(a/E1)) + (b*ln(b/E2))).
Often, stylistic, grammatical, or syntactical features of the target corpus are highlighted through keyness comparison with a general reference corpus.
However, the comparative keyword analysis reported here did not involve a general reference corpus (which may conflate or obscure gender differences in the specific data sets.
However, with all p-values nearing zero and no thresholds of log-likelihood included, it is unclear whether the most salient semantic categories were, in fact, included and populated.
Keyword analysis can be illuminating, but can also be misleading, because semantic similarity is not explicitly taken into account during analysis.
While many (or even most) researchers who use keyword analysis end up grouping or discussing words in semantic categories, this can be a somewhat flawed method, as these categories are subjective, and words which are too infrequent to appear as key on their own will be discounted.
In this study, semantic domains were annotated and analysed using UCREL's (University Centre for Computer Corpus Research on Language, based at Lancaster University) Semantic Analysis System (USAS) in Wmatrix.
If the resulting metric (log-likelihood in our case) exceeds a certain critical value, then the null hypothesis can be rejected.
The USAS semantic tagger can be used to identify semantically meaningful multiword expressions (MWEs) since these chunks need to be analysed as belonging to one semantic category or are syntactic units e.g.
Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specific research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identification of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results.
As a corpus community, we need to agree on better guidelines and expectations for filtering results in terms of minimum frequencies and significance and effect size values rather than relying on ad hoc solutions without proper justifications.
Power calculations can be used alongside significance testing and effect size calculations and are increasingly employed in other disciplines, e.g.
This might mean setting the effect size in advance and then calculating (a-priori) how big our corpora need to be, or at least being able to (post-hoc) calculate and compare the power of our corpus comparison experiments.
Selection of a reference corpus will impact the results, and some care should be taken to select an appropriate 'benchmark' to highlight differences aligned with a given research question.
While obtaining sorted frequency lists that reveal which collocates or constructions occur most often or are most likely around an element E is straightforward, much corpus-linguistic research has gone a different route and used more complex measures to separate the wheat (linguistically revealing co-occurrence data) from the chaff (the fact that certain function words such as the, of, or in occur with everything a lot, qua their overall high frequency.
In this case, and this is not atypical, the difference between the conditional probabilities and the corresponding P-values is quite small and may even seem to be negligible.
For these foreign language learners, it seems that the number of times a collocation occurs is a far more important factor than the strength of association between components.
Finally, the dispersion of a collocation across a reference corpus had only a weak relationship with knowledge (more widely spread collocations were better recognized), and this relationship was significant only in the BNC.
For three of the verbsencourage, support, and fear -use in one of the searched constructions is rare (less than 1% of occurrences of the verb) (continued) 7 Analyzing Co-occurrence Data 151 and not listed as a possible form in the corpus-based Collins Cobuild English Language Dictionary.
What these analyses do provide, however, is a clearer picture of the language use for which any linguistic model would need to account.
Space does not permit a detailed discussion and exemplification here in prose; for detailed code, computations, and results in R, see the companion code file.
Suffice it to say here, that • G 2 is the difference between a regression model that predicts the use of E (any verb vs. regard) given X (any construction vs. the as-predicative) and a null model that predicts the use of E (any verb vs. regard) given no other information; • the odds ratio is the exponentiated coefficient in the regression model; • MI is log 2 of the predicted probability of E being regard happening when X is the as-predicative divided by the probability of E being regard in general; etc.
Second, these tools only provide AMs for what they 'think' are words, which means that colligations/collostructions and many other co-occurrence applications cannot readily be handled by them.
In a section devoted to qualitative analysis, we detail how a discourse-analytical approach, either on the basis of unannotated concordance lines or on the basis of output generated by a prior quantitative examination of the data, can help describe and, crucially, explain the observable patterns, for instance by recourse to concepts such as semantic prosody.
In a section devoted to quantitative analysis, we discuss how concordance lines can be scrutinized for various properties of the search term and annotated accordingly.
In this chapter, to avoid confusion, we refer to a concordance as the list of citations, distinguishing it from a concordance line, which is a single citation.
By pruning, we here mean one or more of the following: deleting certain concordance lines and keeping others; narrowing down the context window; or blanking out the search term and/or collocates.
Most typically, we delete concordance lines and/or clip the context window in the interest of saving space.
Spatial restrictions apply to a handbook article like this one (hence the 15line snippets as opposed to displaying the concordance in its entirety) as much as to the use of concordances for classroom use (not many students would want to inspect thousands of concordance lines).
In a research context, in contrast, especially when researchers want to make a claim for exhaustive data retrieval and/or when the sequencing of attestations in the corpus matters for the research question at hand, one would only delete concordance lines that contain false hits.
Another typical reason for deleting certain concordance lines is that depending on your search term (as well as the functionality of the software you are using and the make-up of the corpus data), the resulting concordance may contain a sizeable share of false hits.
In any case, concordance lines serve to provide the lexical and/or grammatical context of a search term and thus can be needed at all stages of an analysis, from data coding to interpretation.
For example, the first concordance line in Fig.
Let us consider another, even more important example of why a qualitative concordance analysis is important.
It is often wise to look at expanded concordance lines before making a strong claim, in order to consider more context.
The use of quotes at the start and end of this line perhaps indicates an intertextual use of language (where a text references another text in some way), and it is worth expanding the line to see whether this is occurring, and if so why.
However, due to the nature of a concordance analysis -the number of lines the analyst is presented with, along with the fact that they only contain a few words of context either side of a search term, it is possible that these more nuanced cases may be overlooked.
Before making claims, it is important to consider a sample of expanded concordance lines and to maintain vigilance in terms of spotting lines that potentially may be functioning differently to a first glance.
The result was a spreadsheet with one row for each concordance line and one column for each variable considered for analysis.
The conversion of the concordance lines into a spreadsheet like in Fig.
There is a growing strand of research that explores the efficacy of so-called datadriven learning (DDL) approaches, which give students access to large amounts of authentic language data (Frankenberg-Garcia et al.
The concordance analysis also noted several contradictory (continued) representations, including the view that foreign doctors were desperate to work in the UK and were 'flooding' into the country (similar to the water metaphor used to describe refugees), appearing alongside other articles which claimed that foreign doctors 'ignore' vacancies in the UK.
Representations of foreigners were largely concerned with political institutions like the foreign office, although a sample of 21 out of 100 concordance lines taken at random (using an online random number generator) showed negative constructions of foreigners involving stereotyping, implying they were taking up British resources or jobs, or controlling British interests.
They annotated 2,986 attestations captured as concordance lines for 14 variables that were previously shown to impact native speakers' choices, including the semantic relation encoded by the noun phrases, the morphological number marking on the noun phrases, their animacy, specificity, complexity, and, crucially, the L1 background of the learners, among others.
The inspection of dozens of alphabetically sorted concordance lines enables patterns to emerge from a corpus that an analyst would be less likely to find from simply reading whole texts or scanning word lists.
By bridging quantitative and qualitative perspectives on language data, concordance analysis is and will remain a centerpiece of corpus-linguistic methodology.
There are also multilingual concordancers specifically designed to query parallel corpora, i.e.
An early introduction to corpus linguistics written for students in language education.
The chapter opens with a history of programming in the field of corpus linguistics and presents various reasons why corpus linguists have tended to avoid programming.
In the first case study, basic programming concepts are applied in the development of simple programs that can load, clean, and process large batches of corpus files.
However, some languages are more suited to the tasks commonly carried out by corpus linguists, i.e., reading large text files into memory, processing text strings, counting tokens, calculating statistical relationships, formatting data outputs, and storing results for use with other tools or for later use.
To achieve this goal, they compare various keyness measures that are primarily based on the frequency of words in a corpus together with their own "text dispersion keyness" measure, which is based on the number of texts in which a word appears.
In order to carry out the analysis, the authors use Python scripts to calculate a traditional keyness measure based on log-likelihood, as well as two variations, each with different minimum dispersion cut-off values.
Interestingly, the importance of Python in corpus linguistics work might possibly grow as it is one of the most popular languages used to develop artificial intelligence (AI) and deep learning applications due to its rich number of natural language processing and machine learning libraries.
The adapted script is shown as Script 4, with the statements in lines 9 and 10 serving to download and read the contents of a webpage into memory for processing.
Line 19 is then used to call the new function with the specific URL address of the desired webpage.
To do this, we create a new function ("show corpus") that finds the paths of all the corpus files in a folder (e.g.
Each corpus file contains a single, short sentence and the whole corpus is stored in a folder called "target_corpus" in the project folder.
The "Counter" object is a very fast and memory efficient "key-value" data structure that is used to store the word types as keys and their growing frequencies as values as each corpus file is processed.
Finally, it performs the main action of the function, i.e., locating search term hits in the corpus files (via "finditer") and generating KWIC results for each of them (lines 35-51).
This is very much faster than most desktop corpus analysis tools, which have to deal with color highlighting and other display issues.
Or the sorting could be carried out later in a spreadsheet software tool, such as Excel.
There is also an aspect of creativity and beauty in computer language use that mimics that of human languages.
However, they are limited in terms of scope (e.g., only two core functions of corpus analysis toolkits are presented) and functionality (e.g., the KWIC tool does not include a sorting function).
Despite justified enthusiasm about the recent advances in diachronic corpus linguistics, such concerns must be taken into account in corpus design as well as in the development of heuristic techniques to crack the code of past language systems and to explain variation and change.
As diachronic corpora are typically used to study language change, and language change is generally understood to arise from and give rise to language variation, it is something of a bitter irony that one of the greatest difficulties diachronic corpora face lies precisely in capturing historical variation.
Arguably, all major diachronic reference corpora, though often striving to produce stratified samples of language use across time, suffer from this problem.
The implication is again that responsibility for interpreting the structure of a corpus moves from the corpus compiler to the researcher.
The development of new techniques of corpus analysis is often spearheaded by research on contemporary performance data.
The syntactic annotation scheme is a simplified and in some ways deliberately agnostic version of generative X-bar theory.
Spoken language that was produced without any involvement of the corpus compiler is often referred to as 'authentic' or 'natural' language, as it avoids the observer's paradox, i.e.
While in uncontrolled data social, situational and genre-related variation in language use is typically present, it is increasingly restricted in the different types of elicited language data.
As the selection of the corpus data is determined by the intended uses of the corpus, the spoken corpora that are collected in the various linguistic subdisciplines differ sharply in terms of the raw data they contain.
Spoken corpora that comprise 'authentic' raw data are typically compiled for the study of spoken language morphosyntax, pragmatics, discourse, conversations and sociolinguistics as they contain a breadth of different types of language (registers) and a sufficient amount of language variation.
This is true for all reference corpora, which aim to constitute representative samples of a language or language variety.
The one type of annotation that all spoken corpora share is an orthographic transcription (see also Chap.
If a potential spoken corpus user interested in the grammatical variation between older and younger speakers cannot find information on the age of the speakers represented in the corpus, and if a researcher interested in the interplay between prosody and syntax in a language cannot interpret the transcription symbols used for prosody, re-use of corpora is impossible.
It is therefore essential for corpus compilers to make available to future corpus users ample metadata and a corpus manual detailing the corpus compilation and annotation process.
Thus, the syntactic or prosodic annotation of a corpus might be based on a different theoretical tradition than the one preferred by the researcher or one type of annotation that is necessary for the current study might be missing altogether.
For corpus data stored in nondigital form such as analogue tapes (there is still a lot of historical data that has not been digitised yet) every access means loss of quality.
I have no doubts, however, that they will help to provide many more important insights into human language use.
It contains chapters on innovative approaches to phonological corpus compilation, corpus annotation, corpus searching and archiving and exemplifies the use of phonological corpora in various linguistic fields ranging from phonology to dialectology and language acquisition.
Furthermore, it contains descriptions of existing phonological corpora and presents a wide range of popular tools for spoken corpus compilation, annotation, searches and archiving.
The individual contributions discuss these issues and illustrate current practices in corpus design, data collection and annotation, as well as strategies for corpus dissemination and for increasing the interoperability between tools.
Parallel corpora (also called translation corpora) contain source texts in a given language (the source language, henceforth SL), aligned with their translations in another language (the target language, henceforth TL).
Lefer belong to comparable genres or text types and deal with similar topics (e.g.
The compilation of parallel corpora started in the 1990s.
Progress has been rather slow, compared with monolingual corpus collection initiatives, but in recent years we have witnessed a boom in the collection of parallel corpora, which are increasingly larger and multilingual.
Parallel corpora are highly valuable resources to investigate cross-linguistic contrasts (differences between linguistic systems) and translation-related phenomena, such as translation properties (features of translated language).
They can also be used for a wide range of applications, such as bilingual lexicography, foreign language teaching, translator training, terminology extraction, computer-aided translation, machine translation and other natural language processing tasks (e.g.
This chapter is mainly concerned with the design and analysis of parallel corpora in the two fields of corpus-based contrastive linguistics and corpus-based translation studies.
Lefer rather limited number of text types or genres.
It is also possible, on the basis of parallel corpora, to work out what Altenberg has termed mutual correspondence (or mutual translatability), i.e.
As mentioned above, parallel corpora, especially bidirectional ones, tend to be modest in size and are often restricted to a small number of text types.
This imbalance can take several forms: either there are simply fewer texts translated in one direction than in the other (especially when the language pair involves a less "central", or more "peripheral", language), or certain text types are only (or more frequently) translated in one of the two directions.
In this respect, parallel corpora are clearly lagging behind compared with other corpus types, such as learner corpora, which are more richly documented (Chap.
If they are provided with some kind of language-neutral annotation (for parts of speech, syntax, etc.
The languagespecific and language-neutral approaches are both used in parallel corpora, the former being more common.
Low-frequency linguistic phenomena may be hard to analyze on the basis of parallel corpora, for sheer lack of sufficient data that would allow reliable generalizations.
Researchers in contrastive linguistics and translation studies are therefore often forced to combine several parallel corpora to extract a reasonable amount of data, but this approach raises a number of problems.
One final point to be made in this section is that parallel corpora (even those whose texts have all been translated by highly-skilled professionals) contain infelicities and even translation errors (to err is human, after all).
Researchers may therefore feel uncomfortable with some of the data extracted from parallel corpora.
Moreover, looking on the bright side, these infelicities and errors can prove to be highly valuable in applied fields such as bilingual lexicography, foreign language teaching or translator training.
The authors apply multivariate statistics (profile-based correspondence analysis and logistic regression analysis) to measure the effect of the three factors investigated on the variability of English loanword use.
Relying on existing parallel corpus resources poses its own challenges as well, as present-day parallel corpora tend to be quite small and/or poorly meta-documented and typically cover relatively few text types.
Notwithstanding these issues and challenges, parallel corpus research to date has yielded invaluable empirical insights into cross-linguistic contrasts and translation.
There are many hopes and expectations for tomorrow's parallel corpora.
Lefer two are related to the design of new parallel corpora, while the third is concerned with a rapprochement between natural language processing and cross-linguistic studies.
Second, whenever possible, we should go beyond the inclusion of translated novels, news, and international organizations' legal and administrative texts, and strive for the inclusion of more genres and text types, especially those that are dominant in today's translation market, to which corpus compilers have had limited access to date, for obvious reasons of confidentiality and/or copyright clearance.
Finally, we need to cross-fertilize insights from natural language processing and corpus-based cross-linguistic studies.
On the one hand, cross-linguistic research should take full stock of recent advances in natural language processing, for tasks such as automatic alignment and multilingual annotation.
At present, for instance, very few parallel corpora are syntactically parsed or semantically annotated.
On the other hand, natural language processing researchers involved in parallel corpus compilation projects could try to document, whenever possible, meta-information that is of paramount importance to contrastive linguists and translation scholars, such as translation direction (from L X to L Y , or vice versa) and directness (use of a pivot language or not).
In turn, taking this meta-information into account may very well help significantly improve the overall performance of data-driven machine translation systems and other tools relying on data extracted from parallel corpora.
As a matter of fact, there is as yet no up-to-date digital database documenting all existing parallel corpora (be they bilingual or multilingual, directional or non-directional, developed for cross-linguistic research and/or natural language processing).
It describes their main characteristics, with particular emphasis on those that are distinctive of learner corpora.
Special types of corpora are introduced, such as longitudinal learner corpora or local learner corpora.
The issues of the metadata accompanying learner corpora and the annotation of learner corpora are also discussed, and the challenges they involve are highlighted.
Several methods of analysis designed to deal with learner corpora are presented, including Contrastive Interlanguage Analysis, Computeraided Error Analysis and the Integrated Contrastive Model.
The development of the field of learner corpus research is sketched, and possible future directions are examined, in terms of the size of learner corpora, their diversity, or the techniques of compilation and analysis.
The chapter also features representative corpus-based studies of learner language, representative learner corpora, tools and resources related to learner corpora, and annotated references for further reading.
While the first corpora were compiled in the 1960s, it took some 30 years before the first learner corpora started to be collected, both in the academic world (International Corpus of Learner English (ICLE)) and in the publishing world (Longman Learners' Corpus).
Gradually, however, learner corpora representing other languages as well as spoken learner corpora made their appearance, while written learner corpora were increasingly compiled directly from electronic sources, which facilitated the compilation process.
The nature of learner language made it necessary to rethink and adapt some of the general principles of corpus data collection and analysis.
This led, among other things, to the creation of new types of corpora, like longitudinal corpora representing different stages in the language learning process, to the collection of new types of metadata, such as information about the learner's mother tongue and exposure to the target language, and to the use of new methods to annotate or query the corpus, for example to deal with the errors found in learner corpora.
The 'Learner Corpora around the World' resource (see Sect.
Other types of corpora, however, including spoken learner corpora and corpora representing other target languages, are becoming more widely available.
As for size, many of the learner corpora listed in the 'Learner Corpora around the World' resource are under one million words, with some of them not even reaching 100,000 words and a couple just containing some 10,000 words.
It is likely that among those learner corpora that are not listed but exist 'out there', most can be counted in tens of thousands rather than in millions of words.
This concept of authenticity, however, tends to be problematic in the case of learner corpora.
One issue to bear in mind, however, is that, with very few exceptions, the tools that one has to rely on to annotate learner corpora automatically are tools that have been designed to deal with native data.
For POS tagging, for example, the many spelling errors found in written learner corpora have been shown to lower the accuracy of POS taggers (de Haan 2000; Van Rooy and Schäfer 2002).
The model aims to predict possible cases of negative transfer (when the CA shows the target language and the mother tongue to differ in a certain respect) and seeks to explain problematic uses -misuse, overuse, underuse -in the learner corpus (by checking whether they could be due to discrepancies between the target language and the mother tongue).
The last few years have witnessed a general refinement of the methods of analysis in learner corpus research.
While statistical significance testing has almost always been part of learner corpus studies, through the notions of over-and underuse, criticism has recently been voiced against this type of monofactorial statistics.
Looking at the treatment of morphological productivity in different acquisition models, including generative and usage-based models, the authors put forward a number of hypotheses, which are then tested against a learner corpus.
The paper illustrates some of the latest developments in learner corpus research, such as a solid grounding in theories and a combined aggregate and individual approach.
It also makes the interesting methodological point that, through corpus annotation, categorization of the data can be made explicit and available to other researchers.
This study is based on one of the large learner corpora coming out of the testing/assessment world (see Sect.
Written learner corpora are already quite numerous and large.
Such target language data are likely to be included in the mega databases of the future.
At the same time as we should witness an exponential growth in the size of learner corpora/databases, we should also observe the creation of new types of learner corpora, some of which have already started to be collected.
Starting with metadata, although learner corpora have included a large variety of them from the very beginning, there is G. Gilquin also a growing recognition that these may not be enough to reflect the complexity of the second language acquisition process.
How to use foreign and second language learner corpora.
In Research methods in second language acquisition: A practical guide, eds.
After briefly introducing learner corpora, this paper clearly presents the different stages that can be involved in a learner corpus study: choice of a methodological approach, selection and/or compilation of a learner corpus, data annotation, data extraction, data analysis, data interpretation and pedagogical implementation.
This handbook provides a comprehensive overview of the different facets of learner corpus research, including the design of learner corpora, the methods that can be applied to study them, their use to investigate various aspects of language, and the link between learner corpus research and second language acquisition, language teaching and natural language processing.
To model the child's proficiency at a specific point in time or over a period of time, a session by session comparison of the child's and surrounding adults' constructions is key.
In this type of corpus, development is inferred via between-group comparisons, i.e.
In this design, several children are observed over a predetermined time span but recordings start and end at different ages for different groups of children.
In the following we present the main steps and layers in corpus design and their respective challenges.
Studies with a focus on the development of grammar rely on grammatical annotations, especially lemmatization, parts of speech, and interlinear glossing (cf.
While glosses are highly useful even without POS tags, the reverse is hardly true because POS tags alone do not make it possible to search for specific functions or morphemes.
Thus, POS tagging should be done simultaneously with or after glossing.
The combination of the two layers has many applications in developmental research, thus making POS tags another recommendable tier.
Ethical considerations should therefore be an integral part of language acquisition research.
The described high demands are often directly opposed to long-standing claims for more exchange and interoperability of language acquisition data, which are recently being refueled by the open access and open data movements.
Publishing language acquisition data without taking any measures for protecting subjects is ethically highly problematic.
Two issues are key here: Corpus size is relevant because we need reliable frequency distributions of a large range of structures and constructions to estimate developmental growth curves.
The key benefits of the web over such corpora are its size and the fact that it is constantly updated with new texts and, thus, examples of the latest language use.
When considering the term 'web as corpus', the first question we must ask is whether the web can actually be classed as a corpus according to the criteria set out in Chap.
Sinclair's first objection relates to corpus size.
Commercial search engines are geared towards information retrieval rather than the extraction of linguistic data.
WebCorp Live originally used a process known as 'web scraping': the extraction of useful information from the HTML code of a web page, in this case the 'hit' URLs from the Google results page and examples Few researchers would now claim that the web is a corpus in any meaningful sense, but the web as corpus approach can still be fruitful for certain kinds of research and it is particularly useful for introducing newcomers to the field.
However, the 'web for corpus' approach utilises search engines in a different way, using them at the initial stage of corpus building only.
The type of corpus required will determine what seeds should be chosen.
The assumption is that it is possible to build a corpus covering a particular domain (in this case dogs) by using a commercial search engine to find web pages containing words likely to occur in that domain.
As an initial step, BootCaT fetches 10 hits from Bing for each tuple then downloads and processes the corresponding web pages to build a corpus in the form of a text file.
In theory this process could run indefinitely but crawls run for corpus-building purposes tend to be restricted to a fixed time period.
The most popular crawler in web for corpus research is the open-source Heritrix system (used by Kehoe and Gee 2007 amongst others).
Like boilerplate, such documents can skew word frequency counts so it is desirable to remove them from the corpus.
These factors may have an impact on the accuracy of corpus annotation since the linguistic models underpinning off-the-shelf annotation tools are usually derived from standard written language.
It is important to stress that the decisions made at each stage of the web corpus building process will have a significant impact on the resulting corpus, in terms of size but also in terms of composition.
In recent years most compilers of web corpora have worked on the assumption that maximising corpus size is likely to result in a more representative corpus, even if we have no reliable way of measuring this (see Representative Study 2 for a discussion of the related topic of balance in web corpora).
As can be seen from the above discussion, the 'web for corpus' approach is rather more complex than the previously described 'web as corpus surrogate' approach.
One specific area where the web has had a transformational impact is in the building of parallel corpora for use in multilingual natural language processing.
Parallel corpora are pairs of corpora in two different languages where the texts in one are translations of those in the other (also called bitexts; cf.
For example, San Vicente and Manterola (2012) used anchor text, URL matching and HTML structure to build Basque-English, Spanish-English and Portuguese-English parallel corpora.
It is intended as a monitor corpus yet it does not contain examples of the latest trends in language use, which tend to be found in blogs and other less formal text types.
Of particular interest is the sub-corpus from WordPress and Blogger, which includes 95 million words of blog posts and a further 86 million words of reader comments on those posts.
The crawling for the WordPress and Blogger sub-corpus of the BBC took place without relying on commercial search engines.
These developments have proven to be particularly beneficial to the emergent field of multimodal corpus linguistic enquiry.
A corpus is a principled collection of language data taken from real-life contexts.
The key difference between multimodal corpus linguistics and the multimodal analysis of corpora using MDA is that, as with traditional text-based corpus enquiry, multimodal corpus analysis includes not only detailed qualitative analyses, but also quantitative analyses of emerging patterns of language-in-use.
Many of the most pertinent issues and methodological challenges faced in multimodal corpus research are tied to the construction and availability of resources for further research.
As already noted, multimodal corpus analysis is essentially a mixed methods approach, one which seeks to combine quantitative techniques with qualitative textual analyses, as utilised in conventional corpus enquiry.
To support detailed quantitative analysis of phenomena in traditional corpus research, data is often marked-up and annotated first, to make specific features searchable using concordancing tools.
It has highlighted that while multimodal corpus research is gaining some momentum, there are still some areas where further development is required.
A final challenge for current multimodal corpus research relates to more recent discussions and developments in this field.
Multimodal interaction includes a range of different semiotic resources, and multimodal corpora, as already noted, have the potential for enabling the researcher to study the use of language along a continuum of dynamically changing contexts.
As outlined in this chapter, multimodal corpus research is somewhat still in its infancy and as such we can expect a step-change in our description and understanding of language based on this research.
The most widely used tools, and the specific areas of multimodal corpus research that is supported by these tools, focusing on corpus compilation, annotation and analysis, are presented below.
In a similar way to Knight, Gu concentrates on providing some guidelines for an approach to multimodal corpus analysis.
Gu focuses on different modalities within the analysis, beyond what the majority of multimodal corpus studies typically afford, making this work of particular relevance to the final section of this chapter: projections for the future directions of this field.
The main focus is here placed on visualization techniques that may be used to explore a dataset before any statistical test is applied; visualization of statistical results is described in the relevant following chapters.
The chapter comes with a supplementary R code file that exemplifies all functions used and provides some more information about additional useful functions.
Thus, the following two lines of code can be used interchangeably to load the tab-delimited 17_clauseorders.csv file and assign its values to the dataframe cl.order, with the file.choose() function opening up a file selection menu from where the appropriate dataset can be selected: More settings can be adjusted, including which character is used in the file for decimal points (.
A useful way of obtaining an overview of the dataset is to use the summary() function: As can be seen in Fig.
For example, we may want to determine whether the two types of subordinate clause differ with regard to their mean word length.
Without a measure of dispersion, it is not possible to know how good the measure of central tendency is at summarizing the data.
Pearson's product-moment coefficient r is probably the most frequently used correlation coefficient but its use is probably best restricted to interval-scaled (or ratio-scaled) variables that are both approximately normally distributed.
This correlation coefficient is based only on the ranks of the variable values and is therefore more suited to ordinal variables; it is also less sensitive to outliers.
The code file that accompanies this chapter also includes code for making other common graphs, such as scatterplots.
To graphically present the frequency differences between the two levels of ORDER, namely main clause followed by subordinate clause (mc-sc) and subordinate clause followed by main clause (sc-mc), type plot(ORDER) in the code editor and run the code.
The first line of code plots the barplot and assigns the values of the horizontal middles of the bars to the variable "graph1"; the second line of code prints the frequency of each level of ORDER on top of the corresponding bar (i.e.
In addition to the plot itself, this line of code specifies the colors of the bars, gives the graph a heading and labels the x-axis (Fig.
We can also note that the vast majority of subordinate clauses contain fewer  To check whether data resembles a normal distribution, histograms and Q-Q plots can be used (see functions in accompanying R code).
The second line of code adds a grid in the background to facilitate reading.
The second line of code plots the mean for each box using the character "x"; finally, the size of this character (here: 60% of its original size) is specified.
The following line of code thus enables placement of two graphs next to one another: par(mfrow = c(1, 2)).
Descriptive statistics should also be reported with care in research papers, making sure at least the following values are reported: sample sizes, number of cases by main groups (i.e.
The aim of this chapter is to outline how cluster analysis can be used for interpretation of such data.
Cluster analysis is primarily a tool for hypothesis generation.
The method has been used in historical, grammatical, geographical, and social variation research 402 H. Moisl as well as in language processing technologies such as information extraction, question answering, machine translation, and text type identification.
A representative corpus of speech is collected, a set of phonetic variables is defined, and the number of times each speaker uses each of these variables is recorded, thereby building up a body of data.
It is, therefore, crucial to ensure that, prior to applying any form of cluster analysis, the various data preparation issues have been addressed.
Textbook discussions of cluster analysis uniformly agree, however, that no one has thus far succeeded in formulating such a definition.
In principle, this lack deprives cluster analysis of a secure theoretical foundation.
In practice, the consensus is that there are intuitions which, when implemented in clustering methods, give conceptually useful results, and it is on these intuitions that contemporary cluster analysis is built.
The fundamental intuition underlying cluster analysis is that data distributions contain clusters when the data objects can be partitioned into groups on the basis of their relative similarity such that the objects in any group are more similar to one another than they are to objects in other groups, given some definition of similarity.
The most commonly used similarity definition is based on the concept of proximity in vector space.
The following account first describes the standard k-means algorithm and then identifies issues associated with it.
K-means is based on the idea that, for a given set of vectors V, each cluster is represented by a prototype vector, and a cluster is defined as the subset of vectors in V which are in distance terms closer to the prototype than they are to the prototype of any other cluster.
For k-means to have optimised this function, the prototypes have to be placed in the data space so that the Euclidean distances between them and their associated data points is globally minimised across all clusters.
It is easy to see that this is what k-means does: the procedure converges on stable cluster centroids, and a centroid is by definition the minimum distance from all the points on which it is based.
The k-means algorithm is easy to understand and its results easy to interpret, it is theoretically well founded in linear algebra, and its effectiveness has repeatedly been empirically demonstrated in research applications.
K-means requires the user to specify the number of clusters k to identify in the data.
If the value chosen for k is incompatible with the number of clusters the data actually contains, however, the result will be misleading because k-means will deliver k clusters whatever the actual number of clusters intrinsic to the data, including none.
It runs the risk of reinforcing preconceptions and discounting the unexpected and potentially productive insights which are the prime motivation for use of cluster analysis in hypothesis generation: given a range of different analyses, one might subconsciously look for what one wants to see.
Cluster analysis of MDECTE therefore empirically supports the hypotheses that there is systematic phonetic variation in the Tyneside speech community, and that this variation correlates systematically with social factors.
It was also noted that, because different selections of initial prototypes affect the success with which k-means identifies cluster structure, multiple trials with different random initialisations are conducted for each value of k, and the one with the smallest SSE is selected.
The R code for this follows.
As such, there are no statistical measures such as standard deviations or p-value to report when preparing results for publication.
I present four multivariate exploratory techniques: correspondence analysis (henceforth CA), multiple correspondence analysis (henceforth MCA), principal component analysis (henceforth PCA), and exploratory factor analysis (henceforth EFA).
These techniques rely on dimensionality reduction, i.e.
Entries in each cell are integers, namely the number of times that observations (in the rows) are seen in the context of the variables (in the columns).
Because the χ 2 score varies greatly depending on the sample size, it cannot be used to assess the magnitude of the dependence.
Individuals and variables can be declared as active or supplementary/illustrative, as is the case with multiple correspondence analysis and principal component analysis (see below).
The data were submitted to correspondence analysis.
To further explore the detail of the sociolinguistic variation at work with run, Glynn resorts to multiple correspondence analysis.
Each adjective and noun appearing in A as NP was assigned a range of mean scores based on the following measures: an asymmetric association measure (ΔP ), a symmetric association measure (collostruction strength indexed on the log-likelihood statistic), type frequency (V ), the frequency of hapax legomena (V 1), potential productivity (P), and global productivity (P * ).
Then, factor analysis is used to identify the dimensions, where each dimension captures a pattern of underlying co-occurrence patterns among linguistic features.
It displays the number of times each preposition type is found in a certain context.
The nineteenth column specifies the word length of the prepositions.
The first lines of the ouput give the χ 2 score and the associated p-value.
The χ 2 score is very high (10,053.43) and it is associated with the smallest possible p-value (0).
While this should be kept in mind, it does not preclude the fact that the choice of a preposition and the variety of English are globally interdependent, given the importance of the score.
The chi square of independence between the two variables is equal to 10053.43 (p-value = 0 ).
When the dimensionality of a dataset is high, the representation quality of a variable on a given plane is bound to be poor.
This textbook focuses on CA and its variants (joint correspondence analysis, canonical correspondence analysis, co-inertia analysis, co-correspondence analysis) as well as multiple correspondence analysis.
This chapter focuses on monofactorial statistical tests, statistical procedures that produce p-values, and their interpretation.
The choice of the statistical test depends on the research design and the shape of the data.
All statistical tests produce a p-value.
The reporting of the test might look complicated because different values get reported, such as the test statistics (t = 0.67; U = 70) or the degrees of freedom (16.77).
With applying multiple tests on the same dataset, each with its own type I error, the probability of rejecting a true null hypothesis (i.e.
In addition to looking at p-values, we should also consider the size of the observed effect in the sample and the estimation of the size of the effect in the population, which can be expressed as a confidence interval (see Sect.
The chi-squared test statistic, as every test statistics, has a known distribution under the null hypothesis.
However, the reader does not need to worry about these technical details because the p-value is automatically provided by R (see below).
The p-value in this case is very low, p < .001.
This p-value, as we have already seen in Sect.
As with the previous tests, R also outputs the appropriate p-value for the given F-statistic (with particular degrees of freedom).
The simplest option recommended for corpus data (cf.
The appropriate p-value for this test statistic and sample size is 0.128 (R provides it automatically), hence the result is not statistically significant.
In addition to the correlation value (r in Pearson's correlation), a p-value is therefore also reported.
A p-value smaller than 0.05 is conventionally considered to indicate statistical significance.
What about p-values of 0.06, 0.07, 0.08 etc.?
What if two replication studies reach a p-value close to significance-does this strengthen or weaken the case?
In addition to any p values, we measure and report the magnitude of the effect we observe in the data or the effect size.
We can also report a range of values the effect size is likely to have in the population.
We call this range a confidence interval (CI), usually a 95% CI.
A 95% confidence interval is an interval that is constructed around a statistical measure (here an effect size) based on the sample in such a way that the true value of this measure lies within this interval for 95% of the samples taken from the same population.
In 95% of the samples, the measure (effect size) will lie within the confidence interval.
A popular standardised effect size measure is Cohen's d. It is used to express the difference between two groups.
If we, however, take a corpus with 100 male and 100 female speakers, where the difference between the groups is exactly the same (16 and 14), we get more evidence for the effect (the dataset is available from the companion website): d = 0.3 95% CI [0.03, 0.59].
Reporting For the t-test, three pieces of information need to be reported: (i) the test statistic (t), (ii) degrees of freedom (df ) and the p-value.
In such applications, the dependent variable represents a variable linguistic phenomenon that can be observed in corpus data.
The idea behind using simulated data is that we know exactly what information is contained in a given dataset and what effects should be discovered with a regression analysis.
Below, we will use it to analyze a corpus based measure, namely the type-token ratio (TTR) of different texts that have the same overall length.
In order to understand this kind of output a little better, it is useful to see how it differs once the second independent variable, the type of newspaper, is entered into a more complex regression model.
For all TTR values in our dataset, the regression model computes predictions that can differ more or less from the actual measurements.
A binary logistic regression analysis requires a dataset that contains tokens of the dependent variable in their two different realizations.
With a dataset that is annotated for dependent and independent variables and their values, a binary logistic regression can determine whether the independent variables have a measurable effect on the choice in the dependent variable, and how the independent variables differ in relative impact.
Different strategies can be employed in model selection.
Each observation should be independent, that is, it should not be influenced by other observations that are contained in the same dataset.
The t-values drop according to the diminished sizes of the estimates, but the p-values remain significant.
These interdependencies are called interactions, and in this section, we will modify the dataset in such a way as to create an interaction effect.
The R code below first creates a data frame with our three independent variables.
The R code below specifies a regression model that includes the new independent variable, alongside the two other ones, as a main effect.
In order to bring this to light, we need to specify what is known as an interaction term in the regression model.
This is done in the R code below.
In the formula that specifies the regression model, the asterisk between the variable of high frequency verb and the variable of adverb indicates that we are testing for the main effects of these variables as well as for a possible interaction between them.
Below, we create a fictitious dataset of 1000 examples.
The dataset features four independent variables.
The R code below specifies different biases for these variables.
The coefficients indicate that three of the four effects that we manufactured into the data were picked up by the logistic regression model.
The R code below installs and loads a package that allows the creation of effect plots for binary logistic regression models.
The R code below creates a dataset that emulates this phenomenon, increasing the effect of formality by 5% for each additional year of age.
The code also runs a regression model that includes the relevant interaction term, and visualizes the interaction effect.
The table should be accompanied by further information about the regression model.
For the results of a linear regression, the write-up needs to include the R 2 -value, the F-statistic, the degrees of freedom, and a global p-value of the regression model.
A similar situation would arise in a study of learner corpus data (even of the same alternation phenomenon) with a learner grouping factor if we also knew that the number of years learners have learned a language influences their performance with regard to a specific phenomenon.
Logistic regression examples are used throughout this section, and we begin with the fictional corpus study of the dative alternation introduced in Sects.
This is obvious in mathematical notation corresponding to the above R code as shown in (22.2).
We add a numeric second-level fixed effect which specifies the token frequency for each level of LEMMA in the following R code.
The variance estimate for mode is close to 0 from the beginning of the model selection procedure.
It is also shown that the classification accuracy is improved over that of a GLM without random effects, but differently for different lexical groups and subregisters.
For the first command, the output (95% confidence interval) is 0.887 and 1.414.
Beyond this, practitioners should not do model selection for random effects.
The sample size was n=5,063 with 1,134 cases with the genitive and 3,929 cases with case identity.
Reaction times in lexical decision task, for instance, tend to decrease in a non-linear way as a function of words' frequency of occurrence in corpora.
For Gaussian models, for instance, the errors may show heteroskedasticity, and when this happens, the validity of significances reported by the linear model is no longer assured and p-values listed in model summaries will be unreliable.
The proportion of the original weight of a basis function that is retained after penalization is referred to as the effective degree of freedom (edf) of that basis function.
In this way, parameters are shrunk towards zero, just as in the linear mixed effect model.
For each of these 27062 diphones, we considered the following variables: DictDiphoneAbsent, with values TRUE or FALSE, depending on whether the dictionary diphone was realized by the speaker; this is the response variable for our analyses; DictDiphonePosition, an integer indicating the position of the dictionary diphone in the word; DictDiphoneCount, an integer with the number of dictionary diphones in the word; PhraseInitial, with values TRUE or FALSE, indicating whether the word carrying the diphone is phraseinitial; PhraseFinal, with values TRUE or FALSE, indicating whether the word carrying the diphone is phrase-final; PhraseLength, an integer indicating the length in words of the phrase; PhraseRate, the speech rate (number of syllables per second); LogDuration, the logarithm of the duration of the word (in seconds); DictDiphoneActDiversity, a measure, based on discriminative learning, gauging the lexical uncertainty caused by the diphone; WordActDiversity, a measure gauging the lexical uncertainty of the carrier word in a semantic vector space model derived with discriminative learning; SemanticTypicality, the extent to which the semantic vector of the carrier word is similar to the average semantic vector; and CorpusTime, the position of the diphone in the corpus (ranging from 1 to 27062) but scaled and centered to 575 make this variable commensurable with the other numeric predictors.
For corpus data, a random effect factor such as Word can cause serious problems for the analyst.
Recall that in the present dataset predictors at the word level are repeated in the dataset for each of a word's diphones.
However, to date bootstrapping techniques have seldom been used with corpus data.
We include an overview of two representative studies that have successfully used bootstrapping techniques with corpus data.
Finally, we demonstrate how to perform bootstrapping on corpus data using R, and how to visualize and interpret the results.
We then include two study boxes that provide an overview of two representative studies that have successfully used bootstrapping techniques with corpus data.
Next, we demonstrate how to perform bootstrapping on corpus data using R, and how to visualize and write about the results for publication.
Moreover, we are often constrained in what we can learn about a population from a sample because of several limitations: small sample size, unknown or nonnormal distribution, the presence of statistical outliers, and the effects of potential model overfitting (see Sect.
Bootstrapping does not automatically shrink the confidence interval for a sample.
The width of a confidence interval is based on a sample's n size and the standard deviation for the parameter of interest, neither of which are changed during the process of bootstrapping.
From the list of percentages (one per file), Gries drew 50,000 random samples with replacement for each sub-register, matching the sample size (number of files) in the original sub-register.
The use of the bootstrap in this study provided a sampling distribution on which to base the cluster analysis and related interpretations.
The results suggest that variety of English is one of the strongest predictors of variation in the use of all three features.
Each tree is based on a random sample of n observations from the original dataset, usually with replacement, and on a random sample of k predictors from all predictors in the model.
By default, the algorithm returns the asymptotic p-values because the test statistics used in the algorithm are shown to tend to well-known distributions.
If there are fewer cases than required, no split will be made; • the minimum number of cases in a node after a split.
Importantly, one can obtain the values predicted by the model for the observations in the original dataset or for new data.
A multiple regression model would be a much better choice in such cases (see Chap.
The dataset contains 489 tokens from 83 individuals.
The directions of the effects of the contextual variables are stable across the varieties in all three alternations, although there are quantitative differences with regard to the effect size.
This is why the R code provided in the supplementary materials is based only on the functions from party.
If necessary, one can use teststat = "max"; • use of p-values with the Bonferroni correction testtype = "Bonferroni".
Alternatively, one can use testtype = "MonteCarlo", which performs actual reshuffling of the data the number of times specified by nresample (9999 by default).
Note, however, that the permutation may take a while if the dataset is large and the number of replications is high.
In principle, it is also possible (but not advisable, unless the user knows well what she or he is doing) to take the p-values without the Bonferroni correction (testtype = "Univariate") or to use the test statistics themselves instead of the p-values (testtype = "Teststatistic"); • 0.95 as the minimal 1p value needed to implement a split, defined by mincriterion = 0.95.
If no p-values are computed, then this hyperparameter specifies the minimum score of the test statistic.
As one can see from the bar plot in that node, the proportion of ty is very high in those contexts.
The R code is provided, as well.
The 'Methods' section is concerned with which corpora are used, why and especially how variables, or factors or predictors of interest, are operationalized in the corpus data, how the relevant data points are extracted from the corpus and annotated as required by the questions/hypotheses outlined in the intro, and how they were statistically (or otherwise) analyzed.
This might begin with the number of hits from a corpus query using regular expressions to find matches of the phenomenon in question, how these were winnowed down by disregarding false positives, the result of sampling procedures or data transformation procedures, etc.
The method used to select the final dataset should also be carefully described.
A word of caution is warranted here: many off-the-shelf tools offer a 'random selection' option that makes it possible to retrieve randomly x instances of the searched item out of the total number of occurrences.
If a programming language was used, exact search expression (in particular more complex regular expressions) should always reported; depending on the complexity of all analytical procedures, even providing pseudocode can help readers comprehend the research reported on better.
This also means that care has to be taken to make sure the right kinds of statistics are reported because even descriptive statistics sometimes come with some assumptions that need to be borne in mind: For instance, (i) it does not make much sense to report one overall mean for a Zipfian or a bimodal distribution of a numeric variable and (ii) it does not make sense to report any measure of central tendency without a measure of dispersion.
If a regression model with multiple predictors was computed, how was collinearity diagnosed (and addressed)?
That means, one does not need a bar plot of two percentages: this is a statistical result simple enough to not require visualization.
On the other hand, the numerical results of a multifactorial multinomial regression model are likely to be virtually incomprehensible without any visual aids.
Note that, since this is an effects plot, the effect shownthe interaction of complement subject length and register/mode -is represented while every other effect in the regression model is controlled for, which is important because the frequently used plots of observed means/correlations do not do that.
Some areas within corpus linguistics that may be ripe for meta-analysis include research on register variation (see, e.g.
Whereas most studies collect data from individual participants or texts, a meta-analysis does so from an exhaustively collected sample of studies, extracting substantive and methodological information from each report as well as statistical results (i.e., an effect size).
Narrative reviews are often primarily concerned with whether the results of primary studies are statistically significant (e.g., Is there a difference in the use of feature X between text type A and B?).
Statistical significance, usually expressed using p values, is problematic on a number of levels; for starters, the information it provides is generally unstable (i.e., fluctuates as a function of sample size) and binary (i.e., resulting in a dichotomous outcome and, therefore, not particularly informative; see Norris 2015; Plonsky 2015b).
Although frequency counts are not generally perceived as an effect size, they are quantitative and standardized and would therefore appear to meet our definition.
A meta-analyst could also then apply moderator analysis, described below, to examine whether the use of the formulaic sequences in question varies across different study features such as text type (percentage of text in written vs. oral mode).
More commonly, however, a weighting function based on the sample size or some other measure of precision is included in the calculation of the mean.
In addition, because by entering ztor = TRUE, we are converting Fisher's z back into rs for ease of interpretation, if you change it to ztor = FALSE, you can view the correct Fisher's z and its p-value.
The estimate column shows the mean rs obtained from the subgroup analysis under a mixed-effects (i.e., a combination of fixed and random effects) model, the "ci.l" column shows the lower limit of the 95% confidence interval, and the "ci.u" column shows the upper limit of the 95% confidence interval.
Looking at Q-between (Qb), degrees of freedom for Q-between (Qb.df ), and Q-between p-value (Qb.p), the Qb.p is p < .001, which also allows us to say that the mean rs of the two subgroups are significantly different (Q = 26.849, df = 1, p < .001).
Meta-analyses provide strong evidence of validity generalization (e.g., across text types, languages, linguistic features, registers).
However, because meta-analysis necessarily relies on primary studies (previous research) for its data collection, before you find and interpret the overall effect size, it is critical to determine whether these studies are representative of the presumed larger (if hypothetical) universe of studies and not a biased sample of that universe.
Although meta-analysis has yet to take root in the field of corpus linguistics, we look forward to future applications of meta-analysis in this domain.
There are other such distributions, notably the chi-square (or χ 2 ) distribution used to model the population under study in chi-square tests (nominal data).
However, it still assumes that i) the data are a random sample from the population ii) the chi-square distribution is a fair model of how the phenomenon under study is distributed in the population iii) expected observations in each cell larger than five iv) you have actual observed frequencies -never do a chi-square test on percentages!
The Pearson chi-square can be used in two ways, as a test of independence / correlation or as a test of goodness of fit.
The goodness of fit test is used to check whether a set of observations are adequately represented by the chi-square distribution, and it will not be discussed further here.
The issue is somewhat complicated, but there are good reasons not to use the Yates corrected chi-square.
How are the results of a Pearson chi-square test to be interpreted?
In other words, the p-value indicates whether the null-hypothesis (the set of observations is a random selection from a single, chi-square distributed population) should be rejected (low p, in linguistics and the social sciences often somewhat arbitrarily set to p < 0.05) or whether we should choose to not reject the null hypothesis (p > 0.05).
In the example above, the result of the uncorrected Pearson chi-square was p = 0.0847.
But note that the obtained p-value is also quite close to the conventional 5% threshold.
As pointed out above, the Pearson chi-square assumes that we have a random sample from the entire population we want to generalize to.
In this case, we need to interpret the results with more care, and take into consideration the size of the sample in relation to the entire population as well as the effect size (see below), instead of blindly trusting in the chi-square p-value.
Note that the p-value does not say anything about the association between the observed values, it refers to the whole set of observations in relation to a larger population (for between-observation association, see the section on effect size below).
Among its advantages are that it is less conservative than the Pearson chi-square, that is, it can more easily detect a real relationship in the data.
Furthermore, the Fisher exact test p-value can be interpreted as a reasonable measure of the size of the observed effect , i.e., the strength of association between the variables for purposes of comparison, cf.
The Fisher Exact p-value can be interpreted as the likelihood of obtaining Interpretation of Fisher's exact test the observed table, or a table with 'more extreme' (essentially larger differences) observations.
Additionally, the p-value gives a relative effect size adjusted for the observed frequencies in the table.
Generally, the p-value of a statistical test says nothing about the size of the observed effect in the data, that is, the association between variables in the data.
Rather, the p-value tests the hypothesis that the distribution in the data is a random sample from a population which has the properties of some mathematical distribution (e.g.
That is, the p-value indicates how likely we would be to observe the data -the full set of data -in this table if we assume that the population follows a chi-square distribution and if the data in our matrix is a random sample from some population.
Recall that the chi-square p-value is very sensitive to the sample size (n).
That is, in this case information about time period is only moderately helpful in explaining the variation (conversely, if the test is done on the rows instead of the columns, the result is 0.5, indicating that other factors have more explanatory value here).
For a particular research project, there might be useful tests and measures to be found among the parametric tests, as well as in the Bayesian and correspondence analysis traditions.
These lists of words that are particularly salient (i.e., frequently encountered) in a particular discourse domain have several practical applications, but perhaps the most common relate to language teaching and learning purposes.
To this end, many lists have been developed for discourse domains of varied breadth, with "general " lists being those intended to capture words that are highly frequent and widely distributed across a language generally, i.e., across a variety of language use domains, and "specialized " lists intended to represent words that are highly salient to discourse domains of more limited scope, e.g., academic discourse.
Constructing a word list usually involves four key steps: 1) compiling a corpus representing a target discourse domain; 2) deciding on word selection criteria; 3) applying selection criteria & extracting the list from the corpus; 4) evaluating the list.
Following is a discussion of each of these steps, with a particular focus on steps #1 and 4, as the evaluation of resulting lists (step #4) can provide insights into the extent to which corpora that have been designed (step #1) represent the vocabulary in a target discourse domain.
One way to view "linguistic/distributional " representativeness, especially with regard to corpora designed to represent lexical distributions in a given discourse domain, might be through the lens of replicability.
Such a case would suggest a need to revise the original corpus design.
This type of analysis has been suggested previously for word list development.
Corpora of 100 applied linguistics articles produced lists with 80.8% overlap, whereas the same corpus design applied to environmental science produced lists with only 75.8% overlap.
For these words, then, this would be evidence that their discourse domain distributions may have been captured with 200-article corpora.
With other words, however, such as those discussed above, the lack of stable agreement between lists suggests greater variation in their distribution and thus the need for larger corpora to capture their natural distribution across the target discourse domains.
We have found that offering students the opportunity to build and analyze their own corpora gives them valuable experience in corpus building and sometimes even encourages them to build other corpora for projects outside of the class.
Each corpus and software program has its own idiosyncrasies and we have found that these different corpora and software programs are sometimes confusing to students who do not have access to the corpora and/ or struggle to learn one program or search interface in a corpus and then have to learn another.
Register analysis has strongly influenced our work and we believe that this approach to understanding language use is broad enough to encompass the various types of projects that students choose to do.
In Chapter 2, we outline the basics of a register approach to language analysis and then ask students to refer to this same framework when building their corpus and doing their corpus study.
In the final chapter (Chapter 9), we ask students to consider more advanced types of corpus research with the hope that this book will serve as an introduction to the field and encourage students to pursue these ideas at a more advanced level and perhaps even impact the field in significant ways.
One defining component of the scientific study of language (i.e., linguistics) includes a description of how language works.
Although both descriptive and prescriptive perspectives refer to language rules, prescriptive rules attempt to dictate language use while descriptive rules provide judgment-free statements about language patterns.
In some respects, this may be the case, but there is another -perhaps somewhat misunderstoodissue related to language that deserves some attention and serves as a basis for this book: the role of language variation.
The study of language variation seeks to understand how language changes and varies for different reasons and in different contexts.
There are different perspectives on how to investigate and understand language variation.
While understanding variation and contextual differences is a goal shared by researchers in other areas of linguistic research, corpus linguistics describes language variation and use by looking at large amounts of texts that have been produced in similar circumstances.
Although text messaging and academic writing are both written, the purpose of text messaging is quite different from the purpose of academic writing and we would likely expect some degree of language variation in these different written contexts.
We will consider how different circumstances (or situational variables) can affect language use in the following chapter.
A corpus is a representative collection of language that can be used to make statements about language use.
The result of this analysis is a collection of language patterns that are recurrent in the corpus and either provide an explanation of language use or serve as the basis for further language analysis.
One common method used in corpus research is to look at the environment of a particular word or phrase to see what other words are found (i.e., "collocate") with the reference word.
This evidence suggests that a strong prescriptive statement such as "don't ever split an infinitive" runs into serious problems when looking at actual language use.
Despite the large number of texts and the relative ease of obtaining numerous examples, a corpus analysis does not only involve counting things (quantitative analysis); it also depends on finding reasons or explanations for the quantitative findings.
In Part I, we introduce the concept of a corpus and locate corpus linguistics as an approach to language study that is concerned with the analysis of authentic language, and a focus on language variation, using large amounts of texts (Chapter 1).
Once these basics of corpus analysis and an analytical framework have been addressed, readers will be ready to build their own corpora and conduct their own research study.
Because this introductory book contains some of the basics of how to conduct a corpus research project, we do not cover many of the relevant issues that corpus linguistics is presently addressing in its research.
In Chapter 9, we discuss some of these issues with the hope that this book has taught you enough about corpus research to pursue a more advanced study of the field.
As we have discussed in Chapter 1, language variation is a prevalent characteristic of human language.
We could also take a different perspective and examine variation in language use by reference to the different contexts in which language is used.
This approach can be done on both large and small scale depending on a specific research goal; however, at the foundation of this approach to language analysis is the assumption that language variation is functionally motivated by reference to clear descriptions of context.
This perspective on language variation is referred to as register analysis which uses a series of steps to describe and interpret linguistic differences across relatively general contexts such as written versus spoken language or face-to-face conversation versus academic lectures.
Large-scale investigations require a representative (and usually quite large) sample of language and a method to determine linguistic features that are frequent in a given context.
The research goal of this broad approach to register analysis seeks to identify the linguistic characteristics of language used in general contexts such as face-to-face conversation or academic writing.
More recently, similar methods used in the analysis of registers have been used to identify and interpret language variations that are not concerned with identifying and describing registers but are instead concerned with describing and interpreting language variation.
As noted above, linguists have taken different approaches to investigate language variation.
Researchers in this field seek to understand how language variation is related to factors such as geographic region, identity, ethnicity, age, and socio-economic status.
Viewing language variation in this way essentially "predicts" that contextual differences will result in the variation of linguistic features.
In basic terms, a register is a variety of language that is characterized by both a specific context and the language used in the context.
Variables in register analysis are not restricted to linguistic characteristics that are not meaning-changing; register analysis considers the context as a variable and looks at the different linguistic features that are found in specific situations.
Both sociolinguistic variation and register variation studies are interested in how social or situational characteristics relate to language use; however, register analysis considers a wider range of factors that are not only due to what are traditionally viewed as "social" factors (e.g., age, identity, socio-economic status).
For example, when looking at potential differences between speaking and writing, the communicative purpose and topic are likely not as socially conditioned as are other components accounted for in register variation such as the relationship between participants.
Seen from this perspective, register analysis takes into consideration a wider range of factors that may include social factors but may also include other factors, for example, topic, purpose of communication, and mode of communication.
Another difference between sociolinguistics and register analysis relates to the linguistic features under investigation.
Register analysis takes a different view of language variation by using corpora to identify and interpret linguistic features.
A register approach also uses a different type of analysis to investigate the extent to which linguistic features co-occur in given situations of use.
From this perspective, the focus can either be on specific linguistic features or on the co-occurrence of multiple features found in particular situations of language use.
Because register variation considers how linguistic features co-occur in a given context, a corpus linguistic approach is well-suited to register analysis because corpora provide large amounts of authentic texts for analysis.
In fact, it would be hard to see how a register analysis could be achieved without the use of corpora.
Looking at a smaller number of texts would likely not provide a representative sample of language use to allow for a characterization of a given register.
However, as discussed above, the tools used in register analysis are also well-suited to identifying and interpreting variation in texts.
The relevance of both applications of register analysis relates closely to the definition of corpus linguistics discussed in Chapter 1.
If we see register as a variety of language, then we can describe register analysis as a framework to understand language variation.
Register analysis is most readily associated with the work of Douglas Biber and his colleagues and students.
On the other hand, scholars such as Biber and his colleagues are interested in describing language variation from another point of view.
These studies report on how these multiple linguistic features work together (i.e., how they cooccur) in texts, and then examine how their co-occurrence patterns vary in the different registers/contexts.
Following the three components of register analysis described above, we focus on describing situational variables in this chapter.
In a register analysis of potential differences between different types of news writing, many situational variables associated with participants, mode, channel, and production circumstances may not differ although the communicative purpose may.
To understand how to apply the situational variables to different text types, we provide three examples of situational analyses below.
Because register analysis seeks to describe the relationship between situational variables and linguistic variables, the occurrence of linguistic features requires a description of the context.
Therefore, the presenter is expected to talk continuously for a period of time, after which the questions from the audience may be asked.
In terms of how the information is conveyed, we see differences in the type-token ratio.
Lexical bundles are the most frequently occurring word combinations in a register; that is, in situational language use.
These approaches to studying language use in registers provide detailed analyses of these individual features and their individual patterns.
While these studies are interesting and very informative for these features separately, as Csomay indicates, "comprehensive descriptions of variation in language use cannot be based on investigating a single linguistic form or a single linguistic feature in isolation" (2015: 5).
To do this, we need to norm the feature count with a simple statistic: (raw feature count / actual text length) * desired text length.
For example, in the following two sentences, there are ten tokens (i.e., number of words) and eight types (because "the" and "cat" are repeated): He saw the cat.
In this chapter, we will use the Corpus of Contemporary American English (COCA) to illustrate the most commonly identified units of language that researchers use for their analyses: words, collocations, n-grams/lexical bundles for lexical patterns, and part of speech (POS) tags for grammatical patterns.
We will also recommend a software tool, AntConc, to carry out a keyword analysis (further details on the software is in Chapter 5).
This chapter is divided into four main sections: 1) Words with two subsections: KWIC (keyword in context) and keyword analysis (based on word frequency); 2) Collocations; 3) Ngrams; 4) POS tags.
As we described in the previous chapter, after logging into the latest version of COCA (currently 2023), you have different choices, namely, choose "list", which will provide you with concordance lines, "chart" which will provide a chart of the frequency of your selected keyword in different registers, or "word" which will give you a plethora of information about the word you are searching (e.g., collocations, synonyms, clusters, related words, and much more).
If you click on "chart", you get a summary of the frequency distribution for this word across registers.
The most common form to display a keyword in context (KWIC) is through concordance lines.
As mentioned above, concordance lines highlight the word you pick and provide additional text around it.
You can set the size of the text by selecting the number of characters (see our discussion on AntConc in Chapter 5) in the window around the keyword, and the output lists the examples selected from the text.
Because you can see the word in context now, you will be able to see patterns surrounding the When you access COCA, the different colors denote different part of speech categories (here everything is black and white only).
Project 3.1: "Say" Followed by What Part of Speech?
Given the 100 randomly selected examples that you see, answer the research question "What is the distribution of the different part of speech categories following the keyword 'say' when it is a verb?".
In this case, since you are more interested in how your keyword is used in different registers, you would want 100 sample concordance lines randomly selected from spoken discourse, and 100 concordance lines randomly selected from academic prose.
To get those concordance lines for your sample, go to the main page of COCA, click on "chart" and type in "sa*_v" in the search box.
Type in "say_spoken" under "Create new list" above the sample concordance lines and save the list.
In other words, a keyword analysis helps you identify unique words in a text (keywords) when you compare that text with another text.
In this case, you compare words in one corpus, called target corpus, with words in another, called reference corpus.
Following the online tutorial on YouTube referenced above, identify the press reports sub-corpus in both the American and the British collections (AmE06_PressRep and BE-06_PressRep) and feed them into the tool.
The British press reporting is your target corpus and the American press reporting is your reference corpus.
Once you have downloaded the appropriate sub-corpora, run the analysis by clicking on the "start" button.
The higher the keyness value for the words, the more likely that they appear in the target versus the reference corpus.
Read some of the concordance lines and try to answer the question: Why do you think it is used with a capital "L"?
The one thing to keep in mind when you compare corpora for a keyword analysis is to choose corpora that are approximately the same size in terms of the number of words in them.
With more words, the frequency of each word increases and since the keyword analysis is based on frequency in one corpus over another (see tutorial), this may be problematic if you have different-sized corpora.
We will use two areas because they have about the same number of words in the files.
Run the keyword analysis and then determine what sort of groupings you can identify for the types of words one session is using over the other.
If the basic unit of analysis is a word, then we can call that a uni-gram (1-gram).
That is, if you want to identify bi-grams, you will capture each two-word sequence in the corpus.
If you are looking for tri-grams, you will capture each three-word sequence, and so on.
Each time the same word sequence is found, the program counts the frequencies of that sequence.
If you know ahead of time what sequences you are looking for, you can just type the sequence in the search engine.
At the same time, the four-word sequence sleep like a baby only appears 25 times in the same corpus.
For example, the third person pronoun he is ranked as the 15th most frequently occurring word in the corpus with a frequency of 6,468,335.
Make a frequency table as we did before and report on their distributional patterns.
That is, every four-word sequence in a corpus is a 4-gram.
Reset the sample size to 200, run it again, and see whether your results are similar.
As a final note to this section, the longer the n-gram, or the word sequence, the less frequently it will occur simply because n-grams are embedded in one another.
For example, in the four-word sequence on the other hand, the three-word sequences of on the other and the other hand are both present.
On the one hand, POS tags can help you be more specific about the words you are searching if you are going through an already existing search engine and if you are searching a corpus that has been tagged for part of speech.
On the other hand, POS tags can also give you more options and more flexibility in your search.
There are many other part of speech categories that could be potentially interesting for any linguistic study, but before going into some analyses we can do when we work with tags, let's clarify some basic grammar.
Each part of speech belongs to one of two basic classes: open or closed.
Those part of speech categories that belong to the open class contain an unlimited number of members in them.
Examples of POS belonging to the open category are the four main parts of speech: nouns, adjectives, verbs, and adverbs.
As we have also seen in that chapter, the co-occurring patterns of these categories are the most interesting types of studies from the perspective of register variation because they are able to provide us with more comprehensive and detailed analyses of texts.
Now, some corpora (e.g., COCA) have POS tags attached to each word, and some corpora (e.g., Michigan Corpus of Academic Spoken English [MICASE], and Michigan Corpus of Undergraduate Student Papers [MICUSP]) do not have that feature in addition to the actual words in a corpus.
Some scholars find it more difficult to do a search on POS tags, and others write their own computer programs to process and count the different grammatical patterns through those tags.
Select all four part of speech categories.
Find five words that are recent (have a higher frequency in the most recent time period) and five examples of words that are more common in the earliest time period.
For the more recent words, is there a steady growth curve or does the word gain popularity fairly rapidly (i.e., in a single time period)?
This provides information such as register frequency, meanings, topics, collocates by word class, frequent clusters containing literally, concordance lines, and references to entire texts that contain the word.
Comment: A search using the wildcard * + gate will yield a list of words containing -gate.
Using the patterns you found for both American and British English, try to find a language variety that patterns like American English and a language variety that patterns like British English for each of the five modifiers.
This option allows you to control the number of words to the left and right.
These corpora can be used to explore language variation by reference to different situations of use, such as newspaper writing, fiction, and spoken language from news talk shows.
These corpora are not, however, designed to understand language variation in other contexts that may also be of interest.
If you were interested in looking at gender or age differences in language use, these corpora would not be of much use.
Corpus building not only allows you to answer a specific research question, but it also gives you experience in corpus construction.
There is likely no better way to learn about the issues in corpus design and to appreciate the larger corpora built by other researchers than to build one on your own.
Before covering the steps in corpus building, we should acknowledge potential copyright issues.
Concordance lines and short language samples (e.g., fewer than 25 words) are preferable over larger stretches of text.
For those interested in more information on corpus building and copyright laws, there are some sources to consult at the end of this chapter.
Each of the topics and subtopics described above address issues that are not specific to the field of corpus linguistics but lend themselves to corpus research quite well.
Because this book uses register analysis as a framework for interpreting your research, the research questions in your projects all share the similarity of investigating the extent to which situational variables result in different linguistic features for some functional reason.
A final consideration relates to the type of corpus that you will build to conduct your project.
Giving careful thought and consideration to the importance and relevance of your research topic (including a strong justification for your selection of a research topic, i.e., the motivation for your study) is more likely to result in a project that you are proud of and that contributes to an understanding of language variation.
This is not to say that specialized corpora are never used to answer different research questions, but they generally are designed to investigate a restricted set of questions, and therefore, are less likely a representative of language use in general terms.
As you will see further in the next chapters, with smaller, specialized corpora, you are only able to draw conclusions in your dataset rather than generalize the results to larger contexts.
Additionally, a corpus addressing this issue would need to have sub-corpora of relatively equal size.
Thus, corpus "balance" is a key aspect of reliable corpus building.
Note that the balance should consider not only the number of texts in each sub-corpus but should also consider the word count of the sub-corpora.
Frequency comparisons are done on the basis of the number of words, not by the number of texts.
If your specialized corpus also contains subcorpora, then you should ensure the sub-corpora is of fairly equal sizes.
Another issue related to corpus balance in your corpus relates to text types.
If only one of these text types is included then the sample might not account for variation in the different types of news texts.
A balanced news writing corpus would either include texts of sports, lifestyle, and general news texts or would select only one of these text types for analysis.
Note that this entire corpus -let us call it A Historical Corpus of American News Writing -would consist of three sub-corpora related to each of the three time periods.
All of the files in a single time period would be available in a single folder so that each sub-corpus could be loaded separately.
Note that if the files followed a consistent labeling practice, you would be able to determine the time periods by reference to the file name easily.
Depending on your research goal and the design of your corpus, you may also want to include specific information on the situational characteristics of each text in individual text files in your corpus.
Specific information such as the length of each text (by number of words), the topic, or the type of each text (if you included different types within a specific genre) can also be included in individual text files.
To be able to identify these different types of song lyrics, you could either come up with a system of naming each file (as described above) or you could include some of this information in each text file.
Thus, each individual text file can include a relevant header and other extra-textual information as well as the text itself.
If relevant to your research goal, is the corpus constructed so that specific sub-corpora are readily retrievable?
While it is worth finding out about each one of the programs on Anthony's webpage as they are very useful, we will mainly focus on two here, AntWordProfiler for lexical analyses and AntConc for lexical as well as grammatical analyses, as the most pertinent for your use when analyzing your corpus.
Take one of the papers that you have written for another class and save it as a text file.
Then search for the same topical area on Wikipedia, and copy the text, saving it into a text file.
Using your own corpus, you should be able to do KWIC searches through the concordance lines, and other types of lexical as much as grammatical analyses in your own texts.
Read in (i.e., upload) your corpus through the "File" menu ("Open files") and type any search word in the search box that you would like to find out about in your text(s) and hit the "start" button to get a KWIC concordance line.
It is important to keep in mind that the colors in AntConc do not denote part of speech categories as they do in COCA; they simply show first and second and third place after the search term.
When doing register analyses, researchers look for patterns of language use and their associations with the texts' situational characteristics.
We need empirical measures to see what these associations are, and we need quantitative measures (e.g., the frequency of a particular language feature) to see how commonly these patterns occur.
However, this only gives us an impressionistic view of the difference for our dataset.
Generalizability means that the results in our sample can be predicted to be true, with a high level of certainty, to samples outside of our own dataset as well.
Second, we introduce measures of central tendency ("typicality" in a dataset) and measures of variability or dispersion ("spread").
They provide the unit of analysis that will make up your data.
A boxplot is also able to show outliers in the dataset.
However, the mean becomes vastly different depending on the actual scores in the dataset.
That one student changes the mean score dramatically.
There are no outliers or extreme scores in the dataset.
However, what we want is a measure that takes the distribution and deviation of all scores in the dataset into account.
To compute variance, take the deviation of the individual scores from the mean, square each deviation, add them up (oftentimes called the "sum of squares") and average them for the dataset dividing it by the number of observations minus one.
Standard deviation tells us the variability of the scores -i.e., the spread of the scores from the central point -and is most often used as a measure of dispersion in studies of a variety of fields, including corpus linguistic studies.
Non-parametric test results can only be interpreted in relation to the dataset in question.
That is, no projections or predictions could be made about the population it was drawn from, and the interpretation can only relate to the dataset investigated.
A nonparametric test, for example, is Chi-square (see details on this in Chapter 7).
If you are using corpus data from COCA, for example, you may not want to use the frequency data but the normed score (frequency per million words) to make sure your values are interval.
A small sample size will make it problematic to do this -a minimum of 30 observations for each variable is needed.
Before we explain each view a bit more in detail, let's review one more time the dependent versus independent variables and what the basic unit of analysis is (observations) in the example we use.
When we characterize registers based on one or more linguistic features, the unit of analysis is a text.
When we characterize individual speakers' way of using certain language features, the unit of analysis is the text produced by those speakers.
The unit of analysis is still the text (because the language was produced and transcribed), but it may not be obviously understood in the same sense as the text above because each text is more associated with individual speakers who would have certain characteristics.
Finally, when we look at characteristics of individual linguistic features (e.g., article type in subject and object positions), our unit of analysis is each instance of that feature.
The filenames will be portrayed as a string variable called "text_number" (we really are not including this as a variable in any calculations; it is more like a reference for us to know which text file the data is coming from).
This way, your descriptive statistics will be calculated for each level (i.e., for each of your disciplines) versus giving just one mean score of the entire dataset you have.
She tagged the texts with a grammatical tagger, counted the appropriate part of speech tags (see Chapter 9 about tagging), and normed the feature counts to 1,000 words each.
Is there a difference in first person pronoun use across disciplines?
The two variables are: First person pronoun use in each text normed to 1,000 words (interval variable) and Discipline (nominal with three levels: 1 = Business; 2 = Humanities; 3 = Natural Sciences).
Conceptually, we are looking for the mean score for each group and then the variation as to how the scores are dispersed or spread (i.e., how far away each score is from the mean).
Step 1: Calculate the mean score for each group and for the entire dataset.
We will work with the following terminology: within sum of squares (SS W ) (the sum of squares within each group), between sum of squares (SS B ) (the sum of squares across groups), total sum of squares (SS T ) (the sum of squares for the entire dataset), degree of freedom within (Df W ) (degree of freedom within each group) and degree of freedom between (Df B ) (degree of freedom across groups).
As an intermediary step between the distance calculations and the degree of freedom, we need an average of the squares.
The mean square within the group is the within sum of squares divided by within degree of freedom.
In our example, R 2 = 192/288 =.666 R 2 =.666 means that 66% of the variance in the first person pronoun use can be accounted for by the discipline.
We are using Scheffe for the current question and dataset to illustrate how this works.
As with other parametric tests, the dependent variable has to be an interval score (as the mean has to be the best measure of central tendency and the standard deviation has to be the best measure of dispersion), and the independent variables have to be nominal.
All in all, you do not know whether the language change is attributed to only one of the variables (discipline/level) or the two together (discipline and level).
As for the teacher "talking differently", you continue to believe that, based on your previous readings, first person pronoun use is what makes the difference.
Is there a difference in first person pronoun use across disciplines or across levels of instruction?
Is there an interaction between discipline and level of instruction in terms of first person pronoun use?
H 1 : There is an effect on first person pronoun use for discipline.
H 2 : There is an effect on first person pronoun use for level of instruction.
H 3 : There is an interaction effect on first person pronoun use.
Instead, the two variables together cause the change in the dataset.
The results of non-parametric tests, like Chi-square, cannot be generalized to the population the sample was drawn from but we can ask questions related to the given dataset.
Chi-square compares the actual observed frequencies of some phenomenon with the frequencies we would expect if there were no relationship at all between the two variables in the sampled dataset.
That is, Chi-square tests our actual results against the null hypothesis (i.e., no relationship) and assesses whether the actual results are different enough to overcome a certain probability that they are due to sampling error.
The further apart the observed and expected values are, the more likely it is to be a significant Chi-square.
We calculate what we would expect if there were no relationship and compare that with the existing dataset.
Conceptually, we are looking for relationships between two or more variables in the dataset.
Again, as with Chi-square, we do not look at how one variable affects the other but how they relate to each other.
You have a small corpus of presentations comprising two sub-corpora: teacher presentations and student presentations.
Let's say, the mean score for I mean used for teachers is 39.1, and for students, it is 42.5.
In contrast, the mean score for ok use for teachers is 150, and for students, it is 328.
The correlation coefficient (r) is between 0 and 1 (whether positive or negative depending on the direction of the correlation explained above), where zero means no correlation (i.e., absolutely no relationship), and +1 means perfect correlation with a 100% overlap.
Let's have a visual about a potential dataset.
Conceptually, effect size measures point to how strong an association there is between the dependent and the independent variable.
The larger the effect size, the stronger the relationship; that is, the more important the connection is between the two variables.
In this section, we will focus on Cohen's d only as an effect size measure as this measure has been used more prominently in recent years.
In the following section, we will provide some guiding principles for how to go about answering your research question(s) using a register analysis framework and corpus methods.
The essays were then typed and saved as text files with headers and codes to show the authors and topics of the essays.
The corpus was also divided into two sub-corpora: one consisting of collaboratively written essays (N = 51) and another consisting of individual essays (N = 102).
If, for example, a word list shows that one particular word is more frequent in one sub-corpus than in another (a corpus-driven method), then the researcher will still need to look at the distribution and use of this feature more closely in the corpus by investigating its use in some more detail.
We see the merits of both approaches in trying to understand language use and would encourage the use of both methods, especially in the smaller corpora that serve as the basis for your projects.
In Chapter 5, we mentioned the importance of building sub-corpora of fairly equal sizes (see Section 5.3).
If your project involves comparing the frequency findings of sub-corpora in your corpus (or to another existing corpus), you will need to employ a simple statistical procedure known as "normalization" to ensure that your results are comparable (see a brief mention of this in Chapter 6).
Normalization allows frequency counts taken from corpora of different sizes to be compared by providing a count of the frequency of the feature in a similar number of words.
If the latter, you may need to be aware that that language feature is probably used in an idiosyncratic way; that is, it is used only by one or two participants or in only a few of the texts (depending on your unit of analysis).
We have also illustrated the notion of a corpus-driven study, as we extracted lexical items (n-grams) from a small corpus and showed what kinds of questions a keyword analysis can answer.
Should you choose to expand your corpus linguistic skills, we present below some influential register studies that a corpus-driven approach can offer with the goal of providing comprehensive linguistic characterizations of texts and alternative ways to do keyword analysis in different registers.
Very few publications discuss specific requirements for extending corpus retrieval software (c.f.
Build: corpus design and compilation 3.
Corpus retrieval software, our focus here, is intended to facilitate exploration of the annotated corpus data using a variety of quantitative techniques.
A typical corpus investigation would proceed with a large number of retrieval operations conducted through the corpus retrieval software (e.g.
The common thread between these two approaches is map-based visualisations of language data.
More generally, static and full text representations do not sit well with the iterative and data-driven nature of the corpus linguistics methodology.
Firstly, the choice of statistical measure (and the significance or effect size threshold chosen) dramatically affects the resulting graph.
Either these things have to be built-in to support richer interaction, or there must be an interchange format to communicate with other corpus tools (a corpus data connector of some kind).
The words were collected from all tweets and a frequency list created for each MP.
We removed URLs and user mentions from the list of words as URLs were very rarely repeated and were mostly auto-created short URLs for Twitter, and user mentions were removed to avoid overlap with follow relationships.
Thus, our third case study shows that the existing visualisation technique previously used for exploring the network of relationships in an online social network can also be used to explore the linguistic similarity of specific subcorpora at the word level.
Our framework splits along three orthogonal dimensions: linguistic (lexical, grammar/syntax, semantics), structural (to permit sub-corpora) and temporal (for diachronic corpora).
In a context where modern computer technology continues to penetrate life and living of common people, we need more clarity to visualize the importance of digital corpora in developing 'knowledge-based societies' where language data, linguistic information, and technology developed with language data and information play a beneficial role for the betterment of societies.
In recent years, we have noted a positive change in approach to the use of language corpora as a reliable resource in many domains of language technology and linguistics.
This phenomenon of trust in corpora is triggered through several factors such as easy and customized accessibility of corpus data, objective analysis and description of a language based on actual empirical evidence, utilization of language data and information in various domains of language application and utilization of language-based technology in development of new societies empowered with linguistic support systems.
The use of computer technology in linguistics opens many new methods of collecting, storing, and processing language data, interpreting and analyzing them, and fruitfully utilizing them in different domains of humanities, social sciences, natural sciences, and technology.
In the earlier years, we had to be happy with a limited amount of language data for linguistic works because we had no automated system under our disposal by which we could assemble a large amount of language data from various domains of language use, analyze them, interpret them, and utilize them for our purposes.
Now we use a computer to collect language data of any size, type, and variety; classify them, process them, analyze them, and utilize them in various works.
The importance of a corpus is acknowledged because it contributes to making new findings, helps in utilizing data in applications, provides scopes for modifying old observations, generates opportunities for formulating new theories, and prepares new fields for applying language data in direct service to society.
It is primarily concerned with interactions between language data and computers.
It is now treated as a sub-branch of Artificial Intelligence (AI) because language processing is a highly complex method of human-computer interaction.
The question of whether natural language processing is different from or identical to language technology is a matter of perspective.
On one side, we may define language technology by way of focusing on the theoretical aspects of language processing; on the other side, we may look at it as a way of analyzing and devising systems for language application on digital platforms.
All language technology systems are, therefore, primarily grounded on machine learning and a major load of information that is needed in machine learning comes from language data.
Essentially, modern approaches to language technology are grounded on various machine learning strategies although the paradigms of machine learning are significantly different from those that were applied at the early stages of language processing.
The most common trait of these algorithms is that they take a large set of 'features' that are generated from the analysis of input language data as inputs.
In the following sections, I shall discuss digital font generation, corpus generation, corpus processing, corpus annotation, and application of corpus in different areas of human knowledge.
We have to keep in mind that, even at this advanced stage of language processing, there are many non-advanced and minority languages, which have not yet been successful in producing digital language texts or linguistic resources due to the non-availability of digital fonts that could be used to produced digital texts.
It argues for studying a language through empirical analysis of language data produced in machine-readable form with a large collection of texts.
This approach, over the years, has been successful in bringing in new perspectives towards language study in several domains, namely, language description, language education, language experiments, and language computation.
All such goals have inspired computer scientists and linguists to work together to develop language corpora to be processed and utilized in designing intelligent systems like machine translation, speech recognition, information extraction, question answering, sense disambiguation, sentiment recognition, language education, machine-aided instruction, etc.
What we have understood from our involvement with activities like corpus generation, processing, annotation, analysis, and applications over the years, is that both language and technology receive huge benefits from insights and information gathered from corpora.
It typically contains a collection of representative samples that are obtained from texts of different varieties of language use in various domains.
The external criteria refer to a text type that is linked with participants, occasion, social setting, and function of a text.
The internal criteria, on the other hand, refer to the use of language properties within a piece of text.
It also implies that the kind of text included as well as a combination of different text types is crucial in classifying corpora.
These issues vary based on the type of text and the purpose of its use.
For instance, issues relating to speech corpus generation differ from issues relating to text corpus development.
On the other hand, the development of a text corpus addresses issues like the size of a corpus, representation of text types, question of nativity of language users, determination of target users, selection of time-span of production of texts, coverage of disciplines, selection of text documents, collection of source text materials, methods of data sampling, manners of data collection, manners of text normalization, management of corpus files, types of text annotation, and issues of copyright, etc.
That means, based on the type of text, one has to address various issues of corpus generation.
Keeping this view, I address below some of the common issues of corpus generation.
This is related to the size of a corpus as size is an important issue in corpus generation.
In the early 1960s, when computer technology was not conducive to collecting a large amount of language data, a corpus of one million words was considered good enough (e.g., Brown Corpus, Lancaster-Oslo-Bergen (LOB) Corpus, KCIE (Kolhapur Corpus of Indian English)).
That means language data should come from the texts of all possible domains of language use.
The Brown, LOB corpus, and Survey of English Usage (SEU), for instance, contain a wide variety of text types, and therefore, they are considered much better representatives of English.
However, the BNC and the ANC, which are not only very large but also far more diversified in structure and text types, empirically settle the issues relating to size and representation.
The question of the nativity of text producers is another crucial issue in corpus generation.
This method is widely used in many corpus generation projects across languages.
Alternatively, we may apply selective sampling methods that are used in the Brown Corpus and the LOB corpus or consider more suitable methods of text representation keeping in mind the goal and purpose of a corpus.
Processing is necessary for utilizing corpus data in linguistic research and technology development.
Without knowledge of statistics about various properties of a language, we make mistakes in the analysis of language data and inference deduction.
In quantitative statistical analysis, we classify linguistic properties or features in a corpus, count their frequency of occurrence, and construct statistical models to explain what we observe.
To perform comparative studies, we apply multivariate statistical techniques (e.g., Factor Analysis, Multidimensional Scaling, Cluster Analysis, Log-linear Models, and Pearson Correlation) to extract hidden patterns of language use from frequency data obtained from a corpus.
Analysis of corpus texts shows that apart from pure intralinguistic information, a corpus also carries several kinds of extralinguistic information.
An expression that is composed of two or more words and is not predictable by any of the words which are used to construct it is considered a multiword unit.
We must agree that quantitative data retrieved from a corpus is necessary not only in language technology but also in many areas of linguistics (e.g., speech analysis, lexicography, discourse analysis, language teaching, stylometrics, translation, and language planning).
In language teaching, information about the use of phonemes, morphemes, words, and sentences in corpora is used by teachers while they teach a language scientifically.
Information about the frequency of use of language properties is not available from introspection; it is to be collected from language corpora only.
In language acquisition, observation of actual evidence is a source for verification and validation, since no intuitive judgment can justify a phenomenon observed in language use by infants.
Even generative linguists acknowledge the value of a speech corpus as a source of evidence in language acquisition studies.
There are undoubtedly many types of persistent problems, common frustrations, and messiness in corpus data that seasoned scholars have encountered and know about, but which are seldom specifically addressed.
Reporting on a number of instances where initial findings from corpora turned out to be misleading or inconclusive upon closer inspection of the data, the authors advocate for methodological improvements, user feedback channels, and balancing subgenres.
The impact of insufficient metadata is also the topic of Chapter 3 by Mark Kaunisto, who examines the problems seen in the annotation of named entities in corpora, delving into the problems that arise in interpreting corpus data.
Despite the long-recognized importance of considering proper nouns and names in corpus annotation schemes, many contemporary linguistic corpora lack the capability to exclude these items effectively when setting up queries.
Chapter 4 by Marcus Callies discusses challenges related to the special characteristics of learner corpus data, emphasizing the importance of valid data representation for studying L2 production and development.
Learner corpora, particularly those containing academic texts, often include instances of multilingual practices, code-switching, and borrowed content, presenting difficulties for annotation and analysis.
Specific attention is called for in tagging elements like expert terminology, metalinguistic language use, and quoted passages to ensure the accuracy of word counts and concordance analyses.
Compilers and users of learner corpora also grapple with the issue of unwanted lexical bias, stemming from task topics, writing prompts, or other input materials.
Methods to tackle lexical bias involve treating biased words as stopwords or excluding L2 structures likely induced by bias.
Callies concludes his chapter by addressing potential bias in annotation methods within Learner Corpus Research (LCR) and related disciplines.
Callies also raises an important point about the challenges that the use of AI or other writing tools may pose in the compilation of learner corpora in the future -a point that will also be relevant to the compilation of many different types of corpora -as one needs to make sure that the samples compiled truly reflect the writers' own language choices.
The subject of accessibility of data and its repercussions in corpus study is also covered by Stefan Hartmann (Chapter 6), who brings up the problem of replicability of corpus studies, often resulting from the limited accessibility of the corpora studied, as some corpora are only available behind a paywall.
Aatu Liimatta's chapter (Chapter 7) focusses on the challenges posed by variation in text length and the specific issue of short texts, an issue which so far has not received much attention from quantitative corpus linguists.
Largely the reason for this is how previously short texts have not been regarded as being a major problem, but with the advent of new forms in contemporary digital communication, the 'problem of text length' , as Liimatta calls it, needs to be addressed.
In Chapter 8, Daniel Ocic Ihrmark explores the challenges of categorizing fiction genres in corpus compilation, especially when catering to both linguistic and literary research fields.
General-purpose linguistic corpora have traditionally aimed to include works of fiction; however, as Ihrmark notes, the practices of categorizing (and subcategorizing) literary genres in the disciplines tend to differ, which may result in difficulties in trying to make use of corpora in literary studies.
Ihrmark examines various methods employed in corpus stylistics, such as keyword analysis and n-gram searches, and their reliance on genre categorization for comparative studies.
As observed in other chapters in the volume, corpus linguistics stands to benefit from the innovative ideas, methods, and expanded possibilities developed in related fields such as natural language processing and computational linguistics, enriching its analytical toolkit and enhancing its capacity for nuanced linguistic analysis.
Having each word in the corpus tagged according to its part of speech (i.e., word class) permits, for instance, the study of partiallyfilled constructions (e.g., N-PROP of N-PROP; "the Einstein of Italy") and of lexical items of a specific part of speech (e.g., love_V; "I love you").
To summarize, the pitfall related to POS annotation in the case of category change is that even though word classes are treated as static entities in corpus annotation, they are in fact dynamic; word classes exhibit both category-internal and category-external gradience, and this gradience may be a result of ongoing language change.
While software like CLAWS also provide probabilistic information about word class, in practice this information is typically not available to the end users of the corpus.
A possible solution to these kinds of problems is to make use of queries that not only combine lexical items with POS tags (e.g., key_j, fun_j) but also target the surrounding context of the item under study.
These corpora are intended to cover all stages of the history of English, and as such, the annotation scheme has been designed to be backwards compatible all the way to Old English.
Consequently, big-data corpora are likely to contain errors pertaining to the dating of some of the texts, their genre, and even the language variety, which the researcher must be aware of.
On the one hand, the linguist must accept that while sociocultural contextualization remains as important as ever, it is not always possible to check every corpus text or concordance line manually.
This knowledge is more abstract in nature, and it is partly a reflection of the more abstract kinds of research questions explored in corpus-based research today, as well as of a shifting focus towards an increasingly statistical orientation in research design.
Even when the coding schemes are intended to be theoretically as neutral as possible, the POS tags always add an analytical layer to the corpus, which reflects a particular theoretical stance towards word class categorization.
While the precision of queries can always be increased by going through the hits manually or semi-automatically (taking a smaller sample of the full dataset if needed), striving for perfect recall in data afflicted by OCR errors may prove to be a doomed endeavour.
As noted in Section 4.2.1, however, if the OCR errors are distributed relatively equally across the corpus data, settling for lower recall may be justified, as there is so much data that missing some of it does not significantly impact the results.
Not only have new corpora become available that are massive in size compared to those compiled in the 1980s and 1990s, but search engines or online interfaces have also become technically more and more sophisticated, giving users the opportunity to observe statistically organised search results that go far beyond the presentation of mere concordance lines.
In concrete terms, the changes are noticeable on the level of educating students about the basics of corpus linguistics: with the evolving nature of the field, there is a constant need to update course materials to provide novices with an overview of both the possibilities and challenges relevant to corpus study.
The identification of multi-word units functioning as proper names is a task that has received a great deal of attention from scholars to solve problems relating to different purposes -for example, data mining, automatic translation, named entity recognition -generally in the field of Natural Language Processing (NLP).
Proper names and multi-word proper names in particular pose challenges to the study of language use, and many currently available linguistic corpora are lacking in this kind of annotation.
In this chapter, I will examine through different types of examples how the frequencies of English proper name uses can distort studies focussing on word frequency and collocational behaviour, and how the occurrences of proper name use may show different degrees of prominence of words in different genres and regional varieties.
Overall, the main argument is that due caution must be given to such items and sufficient manual inspection of concordance lines is needed to avoid the possibility of misinterpreting initial findings from corpus data.
There have been different ways of annotating named entities in corpus data, and the strategies tend to reflect both the various interests (e.g., translation and information retrieval, alongside general language study) as well as the practical possibilities of doing so.
What is the extent of potential noise, and why is the only option often to manually inspect the concordance lines in order to exclude irrelevant items from the analysis?
The order of the two tags in a portmanteau tag is significant: in this case, the automatic tagger has found insufficient evidence to determine the accurate part of speech, but that it was more likely an adjective -the first element of the tag -than a past participial form of a lexical verb -identified by the tag _VVN.
When we examine the characteristics of near-synonyms with corpus data and attempt to analyze the differences in the uses of the words, we typically examine their collocational behaviour.
Many corpus interfaces and tools allow for the analysis of the strongest collocates of words based on different methods of assessment: Mutual information, log-likelihood, T-scores, raw frequency ranking, and so on.
If the taggings of the corpus data are not helpful in this regard, one can try to perform case-sensitive searches; in the BNCweb, the searches could be restricted to all-lowercase spellings of royal and regal, for example.
Nevertheless, based on these observations, it would be highly advisable to inspect concordance lines of the highest-ranking collocations to check for possible skewing effects by names.
Because learner corpus and SLA researchers use their data to study L2 production and development, it is of utmost importance that the data are valid, that is, they represent "authentic" L2 production, which means that the data must stem from the studied learners' own language production.
Thus, such cases have to be treated separately so that they can be excluded from search results and word counts to not distort the data in learner corpus studies.
From a practical point of view, the annotation of instances of multilingual practices in learner corpora facilitates their automatic search and identification through corpus software such as concordancers.
Despite the pervasiveness and importance of instances of multilingual language use in SLA, learner corpora are not commonly annotated for such features.
In some corpora, however, specific types of multilingual language use have explicitly been annotated.
A prompt may elicit a high number of occurrences of a particular structure (e.g., temporal clauses, pronouns) as a natural consequence of language required to meet functional communicative requirements of the task (e.g., past tense narrative) -A task is neutral with regard to the elicitation of a specific structure.
Since texts compiled in learner corpora have been produced by multilingual individuals, learner data are rich in phenomena induced by multilingualism and language con-5.
Learner corpora of academic writing, on the other hand, contain various kinds of metalinguistic language use and language taken over or copied from secondary sources.
I have suggested that multilingual practices, metalinguistic language use and instances of intertextual reference should be identified and annotated so that they can be dealt with in or be excluded from corpus analysis.
In fact, the field of corpus linguistics is often conceptualised as being part of the Digital Humanities (e.g., Roth 2018; Zottola 2020; Mehl 2021) despite the fact that it has a longer history.
With Primary Sources this is only possible if the documents are individually downloaded and analysed using some other corpus tool.
For example, the part-of-speech tool (using spaCy) only offers visualisations of POS-frequencies but does not give direct access to the tagged texts or include them in the downloadable content sets, which effectively means that the morpho-syntactic annotation provided by Digital Scholar Lab is of no use for corpus research.
Of these, the latter indicates whether the text belongs to section A, B, etc., and as such is of limited use to register analysis.
Especially in the case of child language data, the recordings can contain sensitive information such as the child's or the parents' name or the place where they live.
Make use of the possibility to assign a DOI to the dataset(s).
I would like to share my dataset and analysis scripts with reviewers.
At the very basic level, the confounding effect of variation in text length is obvious.
This is a problem particularly for text-analytic corpuslinguistic studies, which are interested in comparing how many of these items appear in different types of texts: if two texts have a different number of occurrences of the feature of interest simply because they are of different lengths, it can be very difficult to compare texts of different lengths with each other.
However, even variationist analyses may be affected by the distribution of text lengths in their dataset.
However, while it is a working solution to a huge potential problem in a large number of cases, normalization is not without problems itself.
For instance, consider a short text of only five words (such as a tweet, a postcard, or a sticky note) which contains one instance of a feature, for example, a single first-person pronoun.
While not every five-word text contains a first-person pronoun, it is also not that unusual if one does.
While both the five-word text and the 1,000-word text have the same rate of occurrence of first-person pronouns, surely the 1,000-word text with 200 first-person pronouns is much more unusual than the five-word text with one first-person pronoun.
In other words, there are two related problems caused by text length.
I call this the problem of text length.
In this chapter, my goal is to bring more attention to the problems of text length and short texts, and to encourage the development and application of new and improved approaches to the problem.
In Section 2, I describe how the problem of text length was historically less of an issue but is coming to the forefront with the rise of research into the language of social media.
In Section 3, I cover various methods which have been used to either solve or work around the issues caused by the problem of short texts, the problem of text length, and related problems, and discuss their upsides and downsides, as well as suggest best practices and propose potential improvements to these methods.
Furthermore, I will briefly discuss the related topic of the effect of text length on measures of lexical diversity, which has been studied in more detail.
Finally, in Section 4, I will conclude this chapter with some final thoughts on the problems caused by text length and their solutions.
Because of the comparatively long text length in such genres, the normalization method works reasonably well with them.
At the same time, the rise of web and CMC texts has brought many of the issues with text length to the forefront.
While many online genres have a highly variable text length and a large proportion of shorter texts, such as blog posts or Wikipedia articles, this is particularly true for computer-mediated communication and social language use on the internet, such as postings on various social media platforms.
In other words, the free nature of internet writing has brought texts with a wide variety of lengths into the corpora of many linguists, and consequently made the problem of text length and, particularly, the problem of short texts more central than ever.
But do we actually have to care about text length?
Reddit in particular is arguably a very fruitful source of material for quantitative linguistic analyses overall, and especially for the analysis of the effects of variation in text length.
Some of these approaches help solve or work around the problems caused by variation in text length, some the problem of short texts, and some can help alleviate the effects of both.
While the solutions to the problems with lexical diversity measures are not directly applicable to the problem of text length, they may still provide inspiration and starting points for new ways of approaching the problem of text length.
I have divided the solutions and workarounds to the problem of text length and short texts into two main categories.
For instance, we might simply choose to remove all texts shorter than, say, 400 words, 500 words, or 1,000 words from our dataset.
Particularly when typical texts from the shorter end of the length range start to be excluded from the analysis in addition to obvious outliers, it is clear that the dataset starts to lose some of the information potentially available within the data.
Consequently, different solutions and workarounds to the problems of short texts and variation in text length need to be used when dealing with such data.
On the other hand, it is also possible that texts may end up combined in such a way that the resulting dataset overstates the importance of some feature which is actually quite rare overall, for instance, if a feature is highly frequent in a small number of texts.
What texts exactly are considered "similar" is however a question which depends on the dataset and research questions.
When using this method, the fact that all texts included in the analysis are of (roughly) the same length facilitates their comparison using feature counts or rates of occurrence, since the confounding effects of variation in text length have been diminished.
A straightforward approach is to simply split a text into chunks of a certain number of words.
It would be difficult to meaningfully divide the longer texts into chunks of equivalent length if the shortest texts in the dataset are very short, such as on social media.
The main downside to both lengthwise rarity scaling and lengthwise quantile scaling is that they require a very large dataset, so that there are enough texts of every individual length to make it possible to compare texts of the same length.
However, if the dataset mostly contains longer texts, even a slightly smaller dataset will do if texts of adjacent lengths are binned together.
If the dataset is even smaller still, and/or includes shorter texts as well, the two methods may not work too well.
In situations where the dataset is relatively small and includes a large number of shorter texts, other kinds of implementations of the lengthwise scaling method family may work better.
For instance, factor analysis methods, such as those used in the multidimensional method of register analysis, rely on feature frequencies, and as such the methodology is difficult to apply to genres which include a large proportion of short texts.
Due to this, the co-occurrence patterns end up saturated when analyzing longer texts, rendering the method unusable with such texts.
While the type-token ratio differs as a measure from the typical calculated normalized frequencies, its relationship to text length still bears discussing in this context.
Like its name suggests, the type-token ratio is the ratio of the number of different words in a text (types) to the number of all words in the text (tokens).
However, since all texts in a normal-sized corpus are rarely close enough to each other in length, as a typical workaround, the ratio is calculated for a set number of words (such as 400 words) taken from the beginning of each text.
The solution is a lot less optimal still for datasets with a lot of variation in text length, since the 400-word sample covers a different fraction of each text, which means that every text is represented differently by the sampling.
Due to these problems, and the fact that being able to measure lexical diversity in a meaningful way would be very desirable for many linguistic questions, the question of whether a method which is less sensitive to text length could be devised has received a decent amount of attention from corpus linguists and others.
The problem of lexical diversity measures is closely related to the problem of text length and short texts in focus in the present study.
While the efforts to develop a measure of lexical diversity which is less affected by text length do not directly target the problem of text length and short texts, the implication of these efforts is clear: methods which lessen the confounding effects of variation in text length can be developed.
Maybe some method created for the purpose of measuring lexical diversity could even be adapted to help with the problem of text length in feature frequencies.
While these problems have not received as much attention than they could have from quantitative corpus linguists (as evidenced by, e.g., the body of research on measures of lexical diversity), the difficulties caused by the confounding effects of text length are only going to become more central to many studies, as more and more research is being done on social media and web data.
However, there still is no one-size-fits-all solution to the problems caused by text length and short texts in quantitative text-analytic corpus-linguistic studies.
There certainly exist many other approaches not mentioned here, particularly various more advanced statistical and computational methods, which are less affected by variation in text length.
The reference corpus was further limited to include only materials produced between 1900 and 1960 to correctly match the time period.
My line of thought had been that the texts belonged within the same category as they were all fiction, and that they were produced during the same time period.
While the reference corpus did offer sub-categorization of the fiction component of the corpus, aligning those sub-categories with Hemingway's oeuvre is not a straightforward task.
The occasional overlap between the idea of a genre and that of a text type serves to further muddy the waters in linguistic usage of the genre term.
In the case of a resource created for the linguistics community it would make sense to include both the text type and the broader genre, as both have a theoretical background through which results could be connected to previous and future research, as well as inform about the nature of the item in an objective way.
In determining the cut-off point, our aim was to find a reasonable tradeoff between a reduction in time span and the remaining amount of data.
Sample clusters of tweets are presented in the keyword-in-context format, for ease of reading as well as to illustrate the effect of this approach on manual perusal of corpus data, as observed during the manual annotation.
These patterns may be indicative of different degrees and factors of diffusion of semantic shifts within the local speech community.
We first checked the relationship between the three scores by calculating Spearman's rank correlation coefficient.
This may appear to be a relatively high value, but it is in fact just above the 10th percentile for all users in the corpus (0.73); at least within this dataset, this is suggestive of a comparatively and the bilingualism score (y-axis).
The challenges that we encountered are related to several distinct issues: (i) a strong assumption on regional variation underpinning the methodological design -while some language use specific to Montreal is related to language contact, not all is; (ii) inherent limitations of the methods we used, with BERT occasionally capturing phenomena unrelated to lexical semantics; (iii) inherent limitations of the data we used, with a carefully filtered Twitter corpus representing an improvement on highly generic datasets, but still suffering from the 280-character limit and the limited ability to validate user descriptions, among other issues; (iv) the complexity of the phenomenon under study, which often involves very subtle -but nevertheless perceptible and socially meaningful -differences in language use.
Specifically, I shall discuss potentially problematic choices or omissions in the area of general corpus statistics, in particular the choice of association measures for co-occurrence data, that is, measures with which corpus linguists quantify the degree of association between two linguistic expressions (e.g.
More specialized areas are currently booming, it seems: diachronic corpus linguistics, which needs to deal with the problem of how temporally-ordered corpus data are grouped into temporal stages for subsequent analysis; and learner corpus research, which needs to move on from decontextualized studies of over-and underuse to more comprehensive models of learner language and its differences to native language.
The simplest possible way to do this would be by raw co-occurrence frequency or, more likely, conditional probabilities such as p(function|E) or p(contextual element(s)|E).
More precisely, it shows how one can use G to identify n-grams, and how a G-based cluster analysis of spoken and written data from four different varieties (British, Hong Kong, Indian, and Singaporean English) perfectly distinguishes speaking from writing.
While it is usually freely admitted that corpus data are much more messy/noisy than (often carefully) controlled psycholinguistic experimental data, the massive interrelatedness of corpus data along the above three lines is typically ignored.
Let us assume that one decided to begin with a first maximal model that tries to predict MATCH, that is, the choice of I and you on the basis of all fixed-effects predictors-SEX, SENTENCE, and DISTANCE-as well as their pairwise interactions, and that one used a backwards model selection process in which the least significant predictor is deleted till only significant predictors are left.
While it is still not as good as one would theoretically want it to be, it is much higher than the previous one: marginal R 2 = 0.044 and conditional R 2 = 0.24, C = 0.717, and the classification accuracy is now at 65.7%, which is now highly significantly better than chance.
The most obvious was already mentioned: the GLMEM achieves a much higher and highly significant classification accuracy.
First, the GLM assigns to this interaction a p-value that is 24 orders of magnitude smaller (i.e.
It should have become clear, however, that much of what happens in corpus data is a result of word-/speaker-/file-/register-specific random effects rather than of the fixed effects we as corpus linguists are usually interested in.
In addition, ignoring the repeated-measurements nature as well as the hierarchical structure of the corpus data not only violates the fundamental assumptions of most statistical methods-the independence of data points-but also distorts our results in unpredictable ways.
Thus, most of the approaches above are relatively easy ways in which we can try to make our co-occurrence-based studies more robust; there is no reason not to pursue those strategies if corpus linguistics as a whole wants to evolve in tandem with what happens in other disciplines.
Likewise, the lack of bidirectionality and of type frequencies and their distributions in the computation of AMs is a threat to virtually all studies based on co-occurrence data.
In this section, I shall discuss one example each from two areas in which corpus research is currently booming.
In §3.1, I shall discuss the issue of studying temporally-ordered corpus data in a way that is both bottom-up/exploratory and principled/objective; in §3.2, I shall turn to the field of learner corpus research and the question of how to make the best use of what native and non-native learner corpora have to offer.
On the one hand, there is the area of first language acquisition.
In that area, corpus data are both longitudinal and cross-sectional and in order: (i) to discern longitudinal trends in the data for one or more children, (ii) to identify children at comparable levels of development for cross-sectional analysis, or (iii) to increase sample sizes and/or filter out outliers, it is often useful to be able to group the temporal data for children into different stages.
On the other hand, there is the area of diachronic historical corpus linguistics, in which corpus data are-given the relevant time spans-usually cross-sectional, covering, for instance, several centuries of the history of a language.
Given that historical data are not collected in the carefully controlled ways in which psycholinguists (try to) collect language acquisition corpus data, such historical data are often quite heterogeneous so that here, too, it is useful to be able to group temporal data and at the same time clean the data of outliers in a principled fashion.
A frequent exploratory method to answer the first question, namely to discern sub-structure(s) in corpus data, is hierarchical cluster analysis, a statistical tool that groups data points into clusters on the basis of the points' pairwise similarity (such as the differences between MLU values or differences between percentages of (e)s).
However, a cluster analysis should not group such distant data points together given that, in historical data, grouping data points that might be 150 or more years apart makes little sense linguistically just as, in language acquisition data, grouping data points that might be 2 or more years apart makes little sense cognitively.
In all of the above, VNC was used on data in which the measured data could be univariate (just one frequency as in the case of just because) or multivariate (several frequencies (of grammatical patterns) as in the language acquisition data), but where the dimension along which the clustering happened and along which VNC restricted it to neighboring elements was onedimensional: time.
This field has become increasingly vibrant over the last 15 years or so, given the increasing availability of learner corpora.
Much of this work is contrastive in the sense that NNS language is compared to the target of the learner as well as his L1(s), and an increasing amount of work approaches learner corpus data from a cognitively-informed perspective.
It is this regression approach that precisely answers the core question of learner corpus research-in this linguistically and maybe contextually complex situation where the NNS had to make a choice, did he make a nativelike choice, 'Yes or no?'.
So, again, it remains to be hoped that analytical strategies like this one will gain more ground in learner corpus research, the research on varieties, and any other domain where one part of the corpus data can be considered a standard or target with which the others can be meaningfully compared.
Corpus is the main data that a corpus linguist (or a researcher aiming to explore the use of language) needs to investigate a specific area of a particular language(s).
For this reason, sampling is an essential issue that a corpus compiler needs to consider.
The purpose of the study determines the corpus type the researcher will come up with at the end of the compilation of the texts.
For example, suppose a researcher wonders about the use of language among teenagers or children.
At the end of the data collection process, a corpus such as CHILDES, which aims to explore the language acquisition process of the children, will be created.
Even socioeconomic status may play an essential role in language variation.
Furthermore, in the description and explanation of the use of language among specific people, such as teenagers, corpora help researchers a lot during the course of detecting language varieties.
Similarly, researchers can use data collected from children or adults acquiring their second language to reveal the stages and difficulties they face during the second language acquisition process.
CHILDES (Child Language Data Exchange System) can be a good corpus for researchers focusing on acquisition.
For this reason, some corpora such as ICLE and Longman's Learner Corpus have been created to understand the systems of language learners and reveal their interlanguage systems.
The results received from a study based on learner corpus can be used for educational purposes.
As researchers can understand the needs of students thanks to such studies, they can review the curriculum, materials, and even teaching methods thanks to learner corpus-based studies.
Even though it is thought that corpus is quite helpful in language teaching, scholars still do not fully come to an agreement on this issue.
A specialized corpus is a corpus type represented by a collection of texts compiled from a particular genre (newspaper articles, agreement letters, academic articles, lectures, essays, etc.).
Since it can be used to produce reference materials, it is sometimes called a reference corpus.
Comparable corpora consist of two or more sub-corpora complied from different languages or varieties of a particular language.
Parallel corpora should consist of at least two sub-corpora compiled from different languages, including source and target texts or texts produced simultaneously in two or more languages (e.g., EU texts).
Parallel corpora can be used by translators and learners to find potential equivalents in each language and to investigate differences between languages.
A learner corpus is a collection of texts from learners of a particular language.
Researchers can use learner corpora to focus on various aspects of learner language, such as differences among learners, frequency and type of errors, etc.
This corpus type might be helpful in foreign language learning studies and language pedagogy.
The proportion of text types has to remain constant so that each year is comparable with every other.
Balanced or representative corpus consists of texts selected in predefined proportions to mirror a particular language or language variety.
While annotating the data, a label is attached to each linguistics item indicating its grammatical class or part of speech.
Parsing can also be used while annotating a corpus, done through grammatical markup inserted by a software program called a parser that automatically assigns labels to forms beyond word level (phrase, clause, etc.).
To use the corpus for analysis, the researcher needs to frame a research question that will help the researcher set the parameters of the process of the corpus analysis.
The corpus studies are crucial since they either test the validity of a language theory or hypothesis or help researchers create a language theory based on corpus analysis.
Even though many people assert that corpus-based studies are essential in explaining the actual use of language, it should be kept in mind that corpus studies are challenging and require a lot of time and energy.
He specializes in corpus linguistics, statistics and applied linguistics, and has designed a number of different tools for corpus analysis.
The emphasis of the book on the practical aspects of statistical analysis of language is also reflected in its focus on research design and the implications of different 'shapes' of data for statistical analysisfor this reason, the companion website offers complete datasets used in this book for easy replication of the analyses.
Corpus linguistics is an extremely versatile methodology of language analysis applicable in a wide range of contexts, in linguistics, social science, digital humanities and elsewherethe book thus aims to facilitate meaningful use of corpora for as wide a range of users as possible.
It is also good practice in corpus linguistics to make corpora available to other researchers who can explore the same dataset further and thus advance knowledge in the field.
It includes key terms with examples from corpus research and is ordered from basic concepts to more complex ones which rely on the understanding of the previous terminology.
A corpus usually represents a sample of language, i.e.
Dataset is a series of corpus-based findings that can be statistically analysed.
Figure 1.3 provides an example of a dataset with five variables and multiple cases, each case representing one speaker.
Much corpus research can be characterized as searching for variables in corpora and analysing the relationship between them.
A nominal variable has values that represent different categories into which the cases in a dataset can be grouped; there is no order or hierarchy between the categories.
The frequency distribution of a variable provides information about the values a variable takes and their frequencies.
Mean (M or x̄), as we have already seen, is the sum of all values divided by the number of cases (see Section 1.2).
Dispersion is the spread of values of a variable in a dataset.
We usually get two important values from a statistical test: (a) the test statistic and (b) the p-value.
If the p-value is small enough, usually smaller than 0.05, i.e.
This means that the difference observed in the corpus (sample) is likely to be a true difference in the population (all language use).
If the p-value is equal to or is larger than 0.05 (or 5%) we conclude that there is not enough evidence in the corpus to reject the null hypothesis.
Note that 0.05 or 5% is the conventional cut-off point which can be imagined as the risk we are willing to take when inferring from the sample to the population (see p-value below).
If we are willing to take only a smaller risk than 5%, we can decide on the p-value cut-off point 0.01 (1%) or even 0.001 (0.01%).
A p-value is often the most visible sign of a statistical test (see above).
However, it would be misleading to reduce all statistics to p-values.
A p-value is a probability value (p stands for probability) and is one of the outcomes of a statistical test.
P-value can be defined as the probability that the data would be at least as extreme as that observed if the null hypothesis were true.
In the example of a sociolinguistic research looking at the use of swearwords by men and women we would get a p-value that would give us the probability of seeing the observed or even more extreme difference between the two groups if the null hypothesis were true, that is, if there really was no difference in the population and the difference observed in the corpus (sample) was merely due to chance as the result of a sampling error.
This assumption presupposes that the frequency distribution of the linguistic variable does not deviate considerably from the normal distribution.
The confidence interval (CI) in inferential statistics is an attempt to move away from the dichotomous thinking that is often connected with NHST, statistical tests and p-values.
Rather than a yes/no decision about statistical significance, the confidence interval provides an estimation of the true value of a statistical measure (such as the mean) or of a difference between two statistical 1.3 Basic Statistical Terminology measures (such as the difference between two means) in the population.
To help us with this judgement, effect size measures such as r, odds ratio or Cohen's d can be used.
In this traditional corpus design, the aim of the corpus creators is to achieve an unbiased sample of texts in the categories from the sampling frame.
When selecting the sampling frame we are often predisposed to see as salient the text types that have traditionally been included in language corpora such as those in the Brown family sampling frame (see • Legal considerations bias: corpus designers often face the problem of copy- right; this is especially the case when corpus creators want to share their corpora with other researchers.
So far, the traditional approach to corpus design has been considered.
Having discussed different aspects of corpus building, one basic question still remains to be answered: how large should a corpus be?
There is no universal answer to this query because corpus size depends on the research question and the kind of linguistic features we want to investigate.
As corpus users we therefore need to think critically about the nature of the evidence that corpora provide in terms of their quality (representativeness and balance) as well as their quantity (corpus size).
Inferential statistics produces p-values or confidence intervals and we use words such as 'statistically significant' or 'nonoverlapping 95% confidence intervals' to describe the inferences.
Currently, there is a debate in a number of disciplines such as psychology, sociology and applied linguistics about the place that inferential statistics, especially p-values, should have in the research process.
This is a so-called research design; research design is important because it has considerable implications for the specific statistical procedures that we can use with the data.
In general, three main types of research design can be distinguished: (1) whole corpus design, (2) individual text/speaker design and (3) linguistic feature design.
What types of errors can we encounter in a dataset?
This largely depends on the research question and the type of study (research design) we are dealing with.
Then, 95% confidence intervals were calculated for the two subcorpora and r was used as a standard effect size measure.
What is a 95% confidence interval?
A type is a unique word form in the corpus.
On the other hand, if we want to use lemma as the unit of analysis, we need to automatically process the corpus to assign each form its part-of-speech and lump together all inflectional forms related to the same basethis involves a certain percentage of errors.
In fact, in all corpora of written English you can expect the definite article to be at the top of the wordlist, with an absolute frequency roughly equivalent to 6% of the overall number of tokens in the corpus.
The reason for this is that relative frequency is used not only to compare frequencies of a particular type in two (or more) corpora, but it is also used to present evidence about the frequency of words and phrases in a form that is easier for a reader to grasp than the absolute frequency would be.
If we choose a basis for normalization that is too large relative to the actual corpus size, this can 'blow up' our numbers artificially and thus effectively misrepresent the (limited) evidence we have.
It is crucial to always remember that a corpus is a sample of language (see Section 1.4).
Because of this, range 2 can be used for a first (simple) exploration of the corpus data; but for further analyses more sensitive dispersion measures are preferable.
Standard deviation is a classic measure of dispersion, which is used very often also outside corpus linguistics.
Juilland's D is a measure of dispersion that builds on the coefficient of variation.
It can be used to rank-order words in a frequency list to highlight the most frequent and evenly dispersed items.
For TTR the text length needs to be reported; for STTR and MATTR the standard segment size and the window size respectively need to be reported.
The frequency list based on lemmas confirms our initial assumption (based on Zipf's law) about the number of words with the frequency of 30 and over: there are 3,196 such lemmas in the corpus.
In fact, the actual number of lemmas is higher than expected, which shows that Zipf's law provides only a rough estimate of word frequency distributions.
Note that in each case, the frequency of co-occurrence was provided in the brackets; we call this value the observed frequency of collocation.
Random co-occurrence baseline ('shake the box' model): we compare the observed frequencies with frequencies expected by chance alone and evaluate the strength of collocation using a mathematical equation which puts emphasis on a particular aspect of the collocational relationship.
Word competition baseline: we use a different type of baseline from random co-occurrence; this baseline is incorporated in the equation, which again highlights a particular aspect of the collocational relationship.
We merely produce a rank-ordered list of words co-occurring with the node based on their frequency, such as, in our example, my (3), is (2), thee (2), will (2) .
The words towards the top of the list are the strongest collocates by the frequency count.
The second option involves a comparison with a random co-occurrence baseline.
We ask whether it is possible that the combination of words in question (e.g.
We call this process establishing the random co-occurrence baseline and the resulting value is called the expected frequency of collocation.
The baseline needs to be understood on the caseby-case basis derived from the specific formula of the association measure.
These are best displayed in the form of contingency tables (showing all possible combinationscontingenciesof word co-occurrence).
Frequency refers to the number of instances in which a node and collocate occur together in a corpus.
For effective building of collocation networks specialized software is necessary that is able to run multiple comparisons of word co-occurrence and display the data in a visual form.
How to Choose a Reference Corpus?
Typically, a reference corpus is larger than or similar in size to the corpus of interest so as to provide a large enough amount of evidence about word frequencies (see question 2 below).
Generally speaking, the larger and the more similar the reference corpus is to the corpus of interest the more reliable and focused the comparison is.
The crucial question to ask in this context therefore is: what kind of language do the corpus of interest (C) and the reference corpus (R) represent and how is the composition of each corpus reflected in the comparisonthe keyword procedure?
We should also consider which words would get highlighted as keywords had we chosen a different reference corpus.
We know that unless a corpus represents the population (all language use), absence of evidence is not evidence of absence (see Section 1.
Let's also assume that AmE06 is our corpus of interest (C) and BE06 is the reference corpus (R).
However, as we have seen, the keyword list is a result of multiple decisions in the process starting with the selection of the reference corpus and finishing with the choice of the particular statistic.
Another example is the categorization from the 'Think about' task where you were asked to decide which concordance lines show the use of the word religion in a positive context and which in a negative context.
We then ask the second rater to independently code the same dataset or (especially if the dataset is large) a random sample taken from the dataset.
Let's assume that we have asked two raters, a religious person and an atheist, to code the concordance lines from the 'Think about' task.
When extracting keywords for one of the newspapers, the comments of the readers from the other newspaper acted as a reference corpus in order to highlight words specific to the Guardian or Daily Mail readership.
Which association measure would you choose?
Compare your coding in Exercise 9 with the coding of the same dataset by a different rater (e.g.
Imagine you need to produce a research report based on the dataset discussed in Exercises 9 and 10.
There is no one best association measure.
What types of research design can be used?
The approach that would arguably be the most fruitful for answering the question in the 'Think about' task is the exploration of the linguistic contexts in which articles in English are used (following the Linguistic feature research design).
In our case, the p-value associated with the test value 85.25 is very small p < 0.0000000000000001, which is usually reported as < 0.0001.
Because this effect size is a ratio of two values, it is suitable only for simple 2 × 2 cross-tab tables where there are only two categories (levels) of each variable.
Finally, it should be noted that in addition to the effect size measure we should also compute the confidence intervals (95% CIs) for effect size to be able to estimate the range within which the effect is likely to occur in the population (language use in general).
For the chi-squared test the following should be reported: (i) degrees of freedom (see note 9), (ii) test value, (iii) p-value, (iv) effect size (probability ratio or Cramer's V or both) and (v) 95% confidence interval for the effect size.
First, we need to make sure that the dataset is organised according to the principles of the Linguistic feature research design (see Section 1.4).
As a general principle, the more explanatory variables we use, the more data (cases or lines in the dataset) we need to have.
In addition to statistical significance, which is measured by the log-likelihood test, we also use AIC (Akaike information criterion) to establish which model is the most efficient by reaching significance with as few variables as possible.
The effect size for each parameter is the odds ratio discussed above, which is supplemented with 95% confidence intervals, showing us where the odds ratio is likely to lie in the population.
This observation is confirmed by the confidence interval which actually includes 1; this is a sign of a statistically non-significant result, because in the population the effect can well be null (odds ratio 1).
Third, we need to provide details about the overall statistics of the model (LL, p-value, C-index) as well as a table of individual coefficients, including statistical significance information, the odds ratios and 95% confidence interval for the odds ratios.
The p-value is a result of a test that evaluates the null hypothesis which states that the correlation in the population is 0 (i.e.
However, the p-value is larger than 0.05 (p =.083), i.e.
The difference between the p-value associated with Pearson's and the p-value associated with Spearman's correlation can be explained by the fact that by converting the actual values to ranks we lost some information and hence also the power to reject the null hypothesis.
First, Pearson's correlation coefficient r can be used to account for the amount of variability in one variable shared by the other variable.
Traditionally, the correlation coefficients (r, r s ) are reported together with the related p-values.
Often, however, reporting confidence intervals (CIs) instead of p-values is preferable because CIs provide a more precise estimate about the actual value of the correlation in the population.
It is also important to interpret the size of the correlation (effect size)the observed correlation is best compared with similar correlations in the data or those reported in the literature.
The most economical way of reporting correlation (used especially when reporting multiple correlations in a table) is to add a single (*) or double (**) asterisk next to the correlation coefficient.
We can also type out the p-values (although with large corpora with many files these are always very low) or specify the CIs.
Here, we used a simple example of colour terms characterized by the frequency of use and word length.
For an example of reporting and discussing results of the cluster analysis see Section 5.5.
The checking can be done by looking at the Pearson's correlations between pairs of variables; the correlations are usually displayed in a form of a correlation matrix where each variable is correlated with the rest of the variables in the dataset.
These are marked by a gradually decreasing number of contractions and increasing mean word length.
Dimension 1 is thus a more powerful predictor of register variation than Dimension 2.
Group the individual text types into larger categories based on their functional similarity.
They are of comparable age and gender with Speakers 1 and 2, but differ in the historical variety of English they speak, social class (Romeo and Juliet come from wealthy families in Shakespeare's imaginary Verona) and, most importantly, the fact that Romeo and Juliet are products of Shakespeare's imagination, not real persons.
In practice, it is the number of cases (texts/speakers) or groups (when looking at group variance) minus one.
We subtract one from the number of cases (groups) because the last case is always predictable from the previous cases.
In the sociolinguistic context, the independence of observations means that each observation (text or speech sample) comes from a different (randomly sampled) speaker 4  and that the use of language by one speaker in the sample is not affected by the use of language by another speaker.
As can be seen from the equation, there are three factors that have an effect on whether the test will be significant: (i) size of the mean difference, (ii) variance in each of the two groups and (iii) sample size (number of cases, i.e.
The t-test value is large (and the test is significant) if there is a large difference between the means, small variance in the groups and a large number of cases; these factors combined show that there is enough evidence in the data that the two groups are different with respect to the use of the linguistic variable in question.
With the t-test, we have several options of effect size measures that include Cohen's d and r as two typically used effect size measures.
As with any other effect size measure, we also need to look at the 95% confidence interval for Cohen's d, which, in our example, is 0.18 to 1.21, as calculated automatically by statistical packages such as Lancaster Stats Tools online.
Because this 95% CI is extremely wide ranging from a minimum to a large effect, we cannot be sure about the actual size of the effect in the population; this is due to a relatively small sample size (64 speakers).
The degrees of freedom is the number of cases minus number of groups.
This is because with each test that uses a p-value we are willing to accept that in a small number of cases (5%) the result will be statistically significant, even if the null hypothesis is true (there is no effect of the explanatory variable).
Finally, when reporting ANOVA, in addition to statistical significance, the effect size needs to be reported.
The overall (omnibus) effect size that is sometimes reported is eta squared (η 2 ).
As discussed above, an effect size that specifically quantifies the difference between two groups (rather than an omnibus effect size measure) is probably most useful to report.
Another option for effect size measure is to use probability of superiority (PS) discussed in Section 8.4.
Exact p-values are reported unless they are smaller than 0.001; after this point p<.001 is reported.
Correspondence analysis is a summary technique which outputs a correspondence plot.
In forensic linguistics (my quest closely resembled the detective work of a forensic linguist), there are two basic approaches, which depend on the amount of evidence available: if the amount of evidence is small (a few sentences or paragraphs), close reading for signs of idiosyncratic language use (or shibboleths) is appropriate.
We To answer this question, correspondence analysis was used with all the speech samples in the corpus plus transcript X (34 + 1).
We have already seen three figures (7.1, 7.2 and 7.3) that visualize language change in the form of a line graph.
Obligatory: Fitting a non-linear regression model (displayed as a curve in the graph), computing 95% and 99% confidence intervals (displayed as shaded areas around the curve) and identification of significant outliersdata points outside the confidence interval area.
Although formal meta-analysis is now fairly common in a number of disciplines such as psychology, second language acquisition, medical science etc., its application in corpus linguistics has been problematic due to the general lack of reporting of effect size measures.
This chapter argues in favour of standardized reporting of effect sizes in corpus research and shows how meta-analysis can be carried out.
Finally, the chapter reviews common effect size measures and provides a guide for their interpretation.
While the focus in statistical textbooks and in the field in general is on statistical techniques, interpretations of p-values etc., low-level operations such as getting data from a corpus tool into a spreadsheet and then into a statistical package often remain in the background.
This often involves careful reading through the corpus manual to familiarize oneself with the corpus composition, inspecting the concordance lines to see the actual examples of language use behind the numbers we have obtained and producing overviews and simple graphs that reveal the main tendencies in the dataset.
Without a reliable description of the data, generalizing from the sample (corpus) to the population (language as such) using sophisticated statistical tests and p-values lacks grounding.
DATA: pay special attention to the quality of the corpus data and search procedures.
EFFECT SIZE: calculate, report and interpret the size of the effect observed in the data.
To help us express this aspect of the findings, effect size measures should be used.
When comparing two groups such as two subcorpora, the effect size measure Cohen's d is often used (see Section 6.3).
Which of these is worse in corpus research: linguistics without statistics or statistics without linguistics?
Linguistics without statistics lacks effective tools for analysing large quantities of language data, while statistics without linguistics can easily turn into a mindless exercise in number crunching without a connection to linguistic and social reality.
The process of repeating a study with the same research question but a different dataset is called replication.
For example, a very broad question would be: Is there an effect of gender on the use of language?
In practical terms, it is important to explicitly specify inclusion criteria for the studies such as what linguistic and what explanatory variables we are looking for, requirements for research design and relevant time frame for the studies, e.g.
This section focuses on the use of effect size (ES) measures in corpus linguistics.
The concept of effect size was introduced in Chapter 1 and different effect size measures have been discussed throughout this book.
The robust evidence found in these electronic collections of language offers countless possibilities for both linguistic and social research providing a unique insight into patterns of language use.
Statistics, if applied appropriately, can facilitate the process of analysis by serving as a zoom lens through which we can observe the linguistic reality: the details of individual examples of language use as well as the larger picture of grammar, vocabulary and discourse.
There will then be a description of some of the methodologies behind corpus research, with an emphasis placed on the word-based approach.
It involves the collection of data; spoken, written, or both, and collating it into one or more text files.
These text files are then searchable and the resulting data can be further studied for the purpose of linguistic research.
It will also teach you how to use corpus data in more applied contexts, such as e.g.
In general, we can probably see all different types of language manifestation as language data that we may want/need to investigate, but unfortunately, it's not always possible to easily capture all such 'available' material for analysis.
Beyond this distinction based on medium, there are of course other classification systems that can be applied to data, such as according to genre , register , text type , etc., although these distinctions are not always very clearly formalised and distinguished from one another, so that different scholars may sometimes be using distinct, but frequently also overlapping, terminology to represent similar things.
Or, if, for example, we're interested in the average number of words uttered by each character, how do we deal with hesitation markers?
Unfortunately, though, this would leave us with some very strange 'words' (that superficially look like hyphenated compounds ), them-their and honey-moon-over, in any resulting word-frequency list.
The final section then takes the notion of adding linguistic information to your data further, and illustrates how to enrich corpus data using basic forms of XML in order to cyclically improve your analyses or publish/visualise analysis results effectively.
As corpus linguistics is a methodology that allows us to develop insights into how language works by 'consulting' real-life data, it should be fairly obvious that we cannot learn how to do corpus research on a purely theoretical basis.
At the time these first corpora were created, one million words still seemed like a huge amount of data, partly because computers in those days had a hard time handling even this amount, and partly because no-one had ever had such easy access to so much language data in electronic form before.
Such corpora exist for example for Polish (PELCRA), Czech (CNC), Chinese (Modern Chinese Language Corpus: MCLC), and Korean (Sejong/Korean National Corpus: KNC), to list but a few.
Among these, we'll only discuss two specific types here, academic and learner corpora.
On the other hand, though, if a corpus is too small, it may not be very useful for general purpose research because the amount of data needed to conduct research into, for example, collocations (the habitual co-occurrence of words; see Section 10.5) apparently increases exponentially (c.f.
Thus researching or making use of meta-information for instructional purposes normally doesn't make much sense because it represents language data (in the widest sense) from highly limited/restricted domains.
The reason for this is that it's often stored in a particular proprietary format that's only 'understood' by programs designed for dealing with this particular type of file.
In our discussion of file types, and in Section 2.3 when we discussed issues of encoding for diachronic/historical corpora briefly, we've already seen that not all forms of textual representation are equally useful for corpus analysis.
For instance, we saw that if we want to treat a Word or PDF document as a corpus file, we first have to extract its textual contents to a plain-text file in order to be left with any amount of usable text.
Shorter plain-text files are therefore generally very small, sometimes even less than a kilobyte (kB; 1kB = 1024 bytes).
In order to collect this much -or rather, little -text, we'd probably need the equivalent of about 2.4 to 3 pages of scanned text, as the number of words in texts roughly varies between 250 per page for double-spaced texts of average font size (i.e.
Perhaps one more thing is worth mentioning before we move on, though, which is that sometimes, if you use special programs or corpus data that other people have collected, you may occasionally encounter files (or file types) with uncommon extensions that are not recognised by any other programs.
In most cases, this should not be a problem, though, because, as pointed out earlier, most corpus data is stored in some form of plain text, so you can always try opening these files in your editor first.
As almost no programs for corpus analysis can deal with documents in 'graphical' formats, such as PDF, or proprietary formats, such as MS Word, the only logical choice for working with corpora is to use either plain text or other types of documents that contain minimal or easily recognisable annotations, such as HTML or XML documents.
Once you start compiling your own data later, you'll obviously need to make your own decisions regarding which data exactly suits your research purposes, and also how much to collect in order to get a representative sample that may reflect all or a specific sub-part/genre/text type of the language you're trying to describe.
Therefore, this preparatory process should not be taken lightly, especially because corpus compilation and preparation, if done well, is a very time-consuming effort.
From the context menu that will open, select 'Save Link As …' and save the file to your computer or memory stick, possibly changing the name to something more telling than the original file name suggested.
Open either one of the text files and scroll through it to see whether you may be able to recognise anything special about the formatting, layout, etc.
As most operating systems recognise the extension .txt and will automatically open an appropriate built-in editor when a plain-text file with this extension is clicked, I'd strongly recommend you to use this for your own corpus data, at least for data that doesn't contain any special annotations, even if some operating systems, such as Linux or Mac OS X, may not require it, and default installations of Windows will unfortunately also hide known extensions from the user.
Under 'Save as type:' (or whichever entry is equivalent in the dialogue box on your operating system), select 'Text Files ( * .txt ; * .text)' in Firefox, 'Text Files ( * .txt)' in IE for the type.
Specify a file name or accept the one provided by the browser.
Under 'Save as type:', select 'Web Page, HTML only' or 'Webpage, HTML only" for the type.
Specify a file name or accept the one provided by the browser.
For example, you might be tempted to remove all single quotation marks in a text altogether because they 'interfere with' the creation of word frequency lists (see Chapter 9), etc., but of course if this is done carelessly and improperly, you may end up taking out all apostrophes, too, in which case you might, for example, end up with a single neuter possessive pronoun form its instead of the contraction it's which actually represents two separate word forms!
This should ideally be done in the form of a text file that lists all the separate editing steps, and can later provide the basis for part of a manual of information to be distributed with your corpus if you ever plan to release or distribute it.
Such information will then help other users of your data to understand better what to expect from your corpus, or allow yourself to refresh your memory if you should use the corpus data once more after an extended period of time of not working with it.
Open the resulting text files and see whether you can identify any other clean-up operations you may need to carry out.
Therefore, in addition to the file describing the editing process, you'll probably want to keep at least one extra file that lists the contents of the corpus file-by-file if your data contains materials from different genres, text types or domains, or, as with our web page data, that lists information about where the file was retrieved from, when, what the original file name was if you've changed it), etc.
Finder will automatically create a zip archive for you, which, by default, is simply called 'Archive.zip', and which you can then rename to something more appropriate by first selecting the archive and then clicking on the file name once again (avoiding a double-click, which will extract from the archive instead).
Now that we've discussed most of the preliminary issues in corpus design, and seen how we can actually collect and prepare our own data for analysis, we can soon move on to learning how to analyse linguistic data in various forms.
A longer paragraph will be broken into a number of lines, each ending in a line break, where the maximum line length seems to be about 80 characters.
The table here is presented in a different way from the paragraphs, with no extra line breaks following it, but with each cell inside a row separated from the next by a tab and the rows themselves separated by a single line break.
Learners can thus achieve a more realistic learning experience that is at least a little closer to language acquisition, rather than simply learning specific structures and rules.
The context (or co-text), for a concordance, as in traditional, printed concordances, may be a whole sentence, a paragraph, or simply a given number of characters to the left and/or right of the search term.
The results of these searches can usually also be saved to disk, together with additional information, such as the line number or file name where the occurrence was found.
In contrast, a line-based concordancer, such as the one built into my Simple Corpus Tool (downloadable from martinweisser.org), will only extract and display the immediate context found on the same line, plus a number of surrounding lines specified.
The first type is accessible through the 'File→Save Output to Text File…' menu option, which saves the hit number, the concordance line, plus the file name for each hit.
I'd suggest you save all such files to a folder called 'results' inside your 'AntConc' folder, and always give each file a suitably descriptive file name that will later allow you to identify what its contents are.
In case you're now worried that this may destroy your corpus data, there's no need, because whatever AntConc displays as a list of results in its KWIC window is in fact copied from the original file, rather than showing the original data itself.
Don't worry, you'll always be able to re-run the concordance to get the original results back Save the results of the search that you've just pruned under a suitable file name.
For now, we only want to practise adding simple word class codes to our results file.
First of all, that -just as in our previous exercise -the word form mind is again grammatically polysemous, i.e.
Secondly, that sorting the output in this way makes it far easier to see which word forms may precede the hit most frequently, and last, but not least, also which word classes/parts of speech may occur most frequently/typically with a given word form.
William did not seem to mind it himself, he was so pleased to think" -illustrate clearly, disambiguating the word class simply based on the preceding word may not be straightforward, either, as the first of them has the word form to occurring as a position marker, and the second as an infinitive marker.
Such issues may also cause problems for approaches to the automated processing of language where such disambiguation is of course also important, but naïve algorithms based on probability-based assumptions regarding the word class of only a single word preceding a grammatically polysemous item would potentially fail, as such probabilities would, in our case, clearly favour the more frequent use of to as an infinitive marker.
In the past, having long file names wasn't even possible, but these days, having a maximally explicit file name that is up to maybe 20 characters long is no longer an issue, although I have occasionally experienced some issues with exceedingly long folder or file names when trying to back up files on even more recent versions of Windows.
Regular Expressions (or regexes, for short) are an important and very powerful means of specifying such complex search terms for concordances or computer programs for language processing.
Each character class, the way we're defining and using them at the moment, essentially represents options that act as placeholders for one single character only.
So, for instance, if we simply exclude a capital <T> and expect now only to find instances of words that start with a <t>, just because the word form we had in mind may have been <the> or <this>, this is clearly wrong because we're only excluding one single character option, rather than the whole set of potential options we may have wanted to exclude.
The specific (basic) options for quantification in regexes are: r a * following a character (class)/group means it may occur from 0 to an unlimited number of times; expressed in natural language, this would be from none to infinitely many.
For the example of the whitespace-bounded words, also experiment with the curly-bracket type to practise more exact quantification for the number of characters allowed inside a word.
Thus, for example, (\sapples\s) * would look for the exact group/sequence of characters representing the (white-space-bounded) word apples, as well as specify that the word may occur zero or any number of times in a row.
However, in some rare cases, apart from the question just cited, it may indeed be grammatically correct to repeat the same word form twice in a row, albeit with different grammatical functions and meanings.
To some extent, we've already done this when we used word boundaries above, because whenever we inserted a boundary marker in our regex constructs, we effectively said "don't allow another word character to occur here", thereby constraining the options for a match.
To return to our example of and from Section 5.4.1, where we wrote \band\b, we stated that there should be no word character preceding the <a>, and no word character following the <d>.
As you'll hopefully already be able to guess, this finds all words that follow a major punctuation mark and a space, but unfortunately misses all first words in paragraph-initial sentences, as these are not preceded by punctuation.
However, what you'll hopefully have learnt from the online demo display is that a character class on its own simply represents alternative options for finding a single character, which is why I've chosen to surround all instances by a little extra space to make the individual characters stand out more clearly.
However, although, for instance, negating the original character class [A-Z] (i.e.
Thus, one thing you may need to do when defining/using a negative character class, apart from thinking about it carefully anyway, is to not think in terms of binary oppositions.
Examples of type b) make it possible to identify phonotactic features, such as the presence or absence of reduplication and its effect on pronunciation, while type c) may be useful for selecting or extracting words of different length in order to establish potential correlations between word length and complexity, or, if we assume that shorter words are indeed less complex, to extract simpler vocabulary from texts in order to teach it at less advanced learner levels.
Of course, now knowing that this sequence may occur inside another word, we could make it easier to highlight such examples simply by adding a \w in front of the character sequence <and> in our search, thereby specifying that it has to be preceded by at least another word character.
The first one, make, is relatively easy to fix because we only need to add a character class that specifies the consonant options, i.e.
However, after sorting the list on the search term and scrolling down to the end of the hits, we can see that our attempt to cover all inflectional verb endings has inadvertently led to our also finding a number of occurrences of the plural of the noun thought, which is definitely something we'd like to avoid if possible.
Thus, the following sections will try to provide you with a rough overview of what exactly PoS tagging is and how it can be carried out, where its strengths and weaknesses lie, and how you may be able to use it with your own data.
For the moment, you don't need to understand what the tags mean, as we'll soon explore which different bits of information may be contained in a PoS tag.
As we've seen in some of our previous exercises and discussions, frequently one and the same word form may have different meanings or functions, that is, be semantically or grammatically polysemous.
Very frequently, this polysemy is in fact of a morpho-syntactic nature because these differences in meaning depend on the word class associated with the word form in a particular context, as well as potentially its inflection.
Remember the example of the duplicated word form that in Section 6.4.2, where the first occurrence was a relative pronoun and the second a demonstrative determiner?
This type of grammatical polysemy is actually far more likely than you may assume, although it rarely occurs in such reduplicated word form contexts, but more frequently in the shape of words that are commonly assumed to be the products of zero-derivation or conversion.
Usually, and whenever possible, the designers of tagsets try to choose a mnemonic for this initial letter, based on the first letter of the word class itself (e.g.
In this case, an alternative letter that doesn't constitute the beginning of any word class will be used, for example, commonly J for ad j ectives, R for adve r bs, where at least the letter used tends to be part of the word class name.
Run the tagging operation, and copy and paste the results into another text file in your editor.
What you may, perhaps, naïvely have assumed, without any prior knowledge of how modern taggers work, is that most of them use a lexicon to look up words and then apply some linguistic -in technical terms referred to as symbolic -morphological or syntactic rules in order to assign a PoS tag.
This, however, is not true for many modern PoS taggers because they often primarily rely on lexica in combination with statistical rules of co-occurrence for fixed, and highly limited, sequences of words extracted from existing pre-annotated data.
However, even for written language, the accuracy of a tagger across different text types/genres may vary strongly.
We'll discuss issues like this further in Chapter 9, but, for the moment, suffice it to say that the PoS tagging of such languages normally requires programs to artificially introduce spaces during the so-called tokenisation process.
Overall, we've already seen that the tagger seems to have difficulties in making the right decisions when it encounters a number of words with initial capitals or that are completely capitalised, as may frequently occur in headings.
Furthermore, even if they are obtainable, there may be a number of issues that make it difficult to handle them for the average corpus user.
In doing so, we'll also make use of the knowledge you gained in the previous chapters for specifying linguistic patterns and working with PoS tags, in order to fine-tune our searches.
This can be done by following the hyperlink behind the file name on the left, which will switch to a full, static, display of all the file details, rather than just a tooltip when you hover over the link.
The mechanisms for this in BNCweb are sometimes misleadingly similar to the use of regular expressions we've learnt in Chapter 6, but the most basic forms employ a different system referred to as wildcards, whereas genuine regular expressions are in fact a feature of the CQP (Corpus Query Processor) syntax that BNCweb uses internally for its queries (without you necessarily noticing it), or that you can use to write more advanced and complex queries yourself.
However, while, as we've seen, in regexes lowercase refers to a 'regular' character class and using uppercase indicates the 'negation' of this character class, in BNCweb lowercase indicates the occurrence of single characters and uppercase of multiple (potentially unlimited) ones.
For instance, \w finds a single word character, and \W multiple word characters, so that our 'colour' examples from above could also have been written colo\Wr instead of colo * r, or rather colo+r, to be more precise, and colo\wr instead of colo?r.
To be able to complete the above task, we need to employ phrase alternation, which is meant to allow us to specify searches for a number of words at a time, and looks rather like the kind of alternation we know from regular expressions.
As constructing the BNC was a major exercise involving the digitisation of very large amounts of text, sorting out meta-information as much as possible, and PoS tagging and annotating the data in a number of ways, the care taken in checking and correcting the final result of the tagging has, at least to some extent, been sub-optimal.
After this brief excursion, let's return to investigating how we can make use of PoS tagging in BNCweb.
The general notation that allows us to look for a combination of word form and tag here involves specifying a word form, followed by an underscore (_), followed by a PoS tag.
Run a search on the word form mind, initially without specifying a PoS tag.
This will show you a breakdown of how often the word form occurs with a particular PoS tag, at the same time allowing you to see which tags it may occur with in the first place.
Investigate the different tag options first, then start a new query where you use a combination of word form, underscore, and a suitable wildcard for extracting all verb forms at the same time.
Here, the distinction between the two is essentially that the headword encompasses all the occurrences of a base form, regardless of PoS, while the lemma always represents a combination of base form + PoS tag (forms).
Essentially, the two options produce the same results, but I wanted to introduce the second option to you here because enclosing the simplified tag in curly brackets in this way also allows us to use it when we're not looking for lemmas, but for sequences of words where we may only want to specify the word class, rather than a word form + tag, and use a wildcard to find any word that occurs with this particular word class.
Plus symbols used below do not appear in queries, but simply indicate combinatorial options in a more abstract form: r word form(s): finds exact words or phrases only r word form(s) including wild cards: finds variable words or phrases r [base form] finds lemma: e.g.
Choosing this option will initially provide you with a selection of 100 random samples (adjustable to a maximum of 1,000) that are distinctively colour-coded for the main content word PoS categories, but unfortunately not for function word categories, which are marked with a single colour.
Hovering over the file name on the left displays a different tooltip, this time providing fairly detailed information about the text the hit was found in.
Keep the option 'Write information about order…' set to 'no' as well because it only describes the order of the basic output fields, that is, number of hit, file name, unit number where the hit was found, possibly speaker ID (for spoken language), left context, hit, and right context.
Instead, the query will have returned any word that starts with the grapheme combination/character sequence <colo>, followed by any number of characters, and ends in an <r>, because the wildcard asterisk ( * ) means 'zero or more characters'.
Apart from the fact that you can even find 115 examples (1% of the total forms) of the American spelling in the British corpus data at all, the results should not be surprising now.
You may also notice that, occasionally, as I've pointed out before, CLAWS has assigned two tags to a word form (e.g.
When you display the 'Frequency breakdown', the first result will not be very exciting because, as we've specified only one word form, it only displays that, together with the 'No.
What may be a little puzzling, though, is that the word form displayed is actually the one with an initial capital, which is perhaps not the most representative way of displaying it because this exact form is bound to be much rarer than the non-capitalised one.
Once you've specified the option for viewing according to 'Frequency breakdown of word and tag combinations', your display will show you the exact word forms in combinations with all the tags -or 'ambiguous' tag combinationsaccording to their frequency of occurrence.
The methods described in this chapter can be considered a starting point for providing us with some quick hints as to which particular type of language, or perhaps even genre, we're dealing with in our corpus analysis by investigating how frequently certain words occur in a corpus.
This type of analysis will then be continued in the next chapter, where we'll discuss fixed or variable combinations of words that may be equally, or sometimes even more so, relevant to a particular type of text or genre.
Returning to English, further, but similar, problems are caused by other multiword units (often abbreviated MWUs), such as phrasal (prepositional) verbs, for example, give up/in or get along with.
In this context, problems with the semantic content of frequency lists also already become apparent to some extent, due to the polysemy of the little function word clitics indicated above.
And, of course, a simple listing of the single word form without context in a frequency list would (normally) not allow us to disambiguate our examples, as would for example be possible through sorting a concordance of the word by its left or right context.
All of the above are issues that are often largely neglected in the analysis of corpus data, especially in more automated and quantitatively oriented types of corpus analysis, where the norm still seems to be to assume that the traditional 'definition' of a word is relatively unproblematic, and that synonymous expressions generally consist of single words only.
Such a representative instance/word form in a frequency list is referred to as a type, and each individual occurrence of a particular type as a token, which is why splitting a text into individual word-level units is also referred to as tokenisation.
Basically, these lists can be created in two different ways by a program (or a person), either by producing a token list first, then sorting it and counting how many times a given word form occurs in a row, or, more efficiently, by keeping track of each word form encountered and increasing its count as it recurs in the text.
Creating a word frequency list in AntConc is a very simple task.
Scroll through the list and try to understand what type of texts we may be dealing with here, both in terms of text type/category and domain/topic.
Exercise 53 was designed quite deliberately using this type of data in order to demonstrate the potential usefulness of word frequency lists for identifying distinctions between spoken and written language, as well as finding indicators for possible domains.
Thus it's generally advisable to first check any output of a frequency list produced by some program to see whether it may exhibit any unusual features that could influence the analysis negatively.
As will hopefully have become clear from the discussion of Exercise 53, the default frequency list in AntConc treats clitics, such as 's (but without the apostrophe) as separate words, which is often what we want because they're in fact abbreviated words that have been fused with a preceding word.
Now, simply click again to re-create the frequency list including the two extra characters, and observe the changes in the list by scrolling through it.
Unlike the sorting options we had for concordance lines, where we were able to sort according to n number of words to the left or the right quite freely, in this case, we have a more limited set of options, based on the options for combinations of output for types and frequencies, as already mentioned above.
The remaining option under 'Sort by', 'Sort by Word End', is useful if you're carrying out morphological analyses on corpus data, as it groups together words with the same endings, so that for instance plural forms of nouns or third person singular and other forms of verbs will end up being grouped together.
Once you're happy with the results of your frequency list, no matter which output format you've chosen, you can save the list to a text file again, just like you were able to do with the concordance output.
Almost all texts, apart from maybe certain text types including telegrams and recipes, tend to have a rather high occurrence of high frequency function words, something we've just seen during our first explorations of frequency lists.
Once you're happy with the results, copy your list into a new text file and save it as stop_words_trains.txt.
From all the problems we've seen above, it may seem as if single word frequency lists are actually best avoided, but nevertheless, they may provide us with at least some superficial information as to lexical density or makeup/type of a text/corpus.
In information retrieval, a frequency list, if properly constructed and filtered, may also provide the basis for accessing indexes of search engines by ranking pages according to the frequencies of occurrence of individual or combined search terms.
And, of course, word frequency lists also provide the basis for many probabilistic approaches to language processing, such as establishing collocation measures or conducting probabilistic PoS tagging, some of which we've already discussed before, and others we'll turn to soon.
In language teaching and learning, they can for instance be used by teachers to analyse materials and select appropriate or required vocabulary items, or by students to identify vocabulary they may need in order to cope with specific types of language materials, for instance in English for Academic or Specific Purposes (EAP/ESP).
From the 'Range of texts' dropdown list, select the subcorpus we just added and create a frequency list.
In Excel, the option should read 'Text Files ( * .prn; * .txt; * .csv)' and in Calc 'Text CSV ( * .csv; * .txt)'.
Select the frequency list you just saved and click .
Once you're happy with the results, save the file, ideally using the same file name you used for the text file, apart from the extension.
We now have our frequency list stored in a very convenient format, as spreadsheets not only allow us to re-sort our data easily (and repeatedly, if necessary), but also because this makes it possible to investigate and enrich the data in various ways.
If you want to, you can also cut less useful entries, such as maybe those pertaining to numbers or stopwords, from the list and paste them to another if you're not sure whether you might need them again later.
Create a frequency list based on the new subcorpus, import it into a spreadsheet, and sort it as we just did in the previous exercise.
The latter has definite advantages in that you don't need to select and load a number of different files each time, but only a single one, which is also much easier to exchange with colleagues who may not have access to the original data you used, or, in our case, to use a frequency list based on part of the BNC.
As Exercise 63 will have shown you, the keyword list, at least in our case and for positive keywords, may not necessarily provide you with more information than a frequency list that has been filtered well through the use of stopwords.
In addition, the ability to highlight negative keywords in AntConc may also allow you to investigate under-use of specific vocabulary relatively easily, for instance when comparing learner data with that produced by native speakers, etc., an option that, obviously, a pure frequency list of only the source corpus is unable to provide.
As the previous exercises have hopefully shown, keyword analysis does have at least some potential in identifying domain-specific content, although it doesn't necessarily always perform better than a well-executed single-word analysis.
It's thus well worth bearing the above-mentioned factors in mind when conducting any kind of statistics-based keyword analysis, and especially when reporting on the presumed importance of particular keywords for a given text/corpus.
Next, get the token count from the 'Make/edit subcorpora' page and paste it into the spreadsheet, ideally at the top and to the right of the second frequency list, as we may need to shift some items in the list down later to align the data.
If you've studied the list fairly closely, you should have recognised a number of things: In terms of text type/category, the fairly high number of tokens for words, such as okay, so, uh, um, etc., clearly indicates that we're here dealing with a corpus of spoken texts.
Again, clicking on the item and investigating the concordance lines will soon tell us that s isn't only used to mark a particular speaker, but of course also represents the clitic (contraction) forms of is (as in that's) and us (as in let's), although there are no possessive markers in the corpus.
Section 4.3), that are such typical indicators of spoken interaction, the number of function word tokens is also fairly high.
In terms of regexes, a true hyphen would be defined as \w-(\w|$), that is, a 'dash' that has to be preceded by a word character and either followed by another word character or the end of a line (and one on the next as well), while our definition here essentially only covers the former and a 'dash' that appears to occurs independently, which is how it appears in the list.
This goes to show that, by observing items in a frequency list, we may often be able to see things we might have overlooked or ignored while concordancing, simply because the results would have been easier to understand.
And even if this number represents only 0.01% of all the words in the written parts of the BNC, the number of potential errors, which appear mainly due to tokenisation errors, is staggering, particularly when considering that this affects only one of the parts of speech represented in the corpus.
For verbs, perhaps the most obvious thing to investigate would again be the most or least frequent types, but, as these still exhibit far more inflectional options than other parts of speech, it may also be interesting to investigate inflectional suffix patterns by selecting the 'ending in' option for 'Word pattern', or possibly verbs that have potential negative prefixes by choosing the 'starting with' option.
Provided that you don't forget to change the option for creating a new corpus, there should be no issues in completing this exercise and creating a suitable frequency list.
Of course, you could fix this manually, but, since we've now separated the data from our ability to concordance on it, we'd have to go back into BNCweb to look at the original frequency list.
Once you've created the keyword list, you should immediately be able to see that some of the words we'd previously only been able to identify through the basic frequency list after repeatedly manipulating the stop word list should now more or less automatically have 'jumped' to the top.
If there are still some words in the list that seem odd, perhaps because they are part of the meta-information of the dialogues or indicate paralinguistic features, such as breathing, etc., you can easily eliminate them from the list in order to clean it up further by manipulating the stopword list, then re-creating the Trains frequency list, and re-running the keyword analysis.
The smaller the corpora compared are, the easier it'll of course become to narrow down such selections, but essentially, the technique itself is similar to creating basic stopword lists, only that, in this case, a word list from a whole corpus is used as a stopword list.
This is similar to the + and -symbols display in the keyword analysis in BNCweb, and represents just one of the ways in which we can visualise differences in the data easily for a quick overview.
These two cases clearly represent issues related to automatic PoS tagging and the theoretical assumptions and rules behind it.
Even if (sorted) concordance lines can already represent an extremely valuable asset in a teaching context because learners -as well as teachers -can investigate words as they're really used in authentic materials, such an analysis may be rather time-consuming.
Justify your choice each time, thinking about why you've chosen to break the unit at this point, and also why you've decided to use this particular punctuation mark.
This is generally done in order to be able to group words with or without initial capitals together, just as we've seen for sorting above, and represents one of the 'shortcuts' in language processing that could potentially lead to errors of analysis if not borne in mind.
The frequency of occurrence of the n-grams you'll have observed doing the above exercise can also be counted, just as in basic frequency lists.
Additionally, we can also do contextual or so-called proximity searches, where it's possible to search for words that occur within certain textual units or within a certain number of words of one another, etc.
To identify such affinities at the most basic level, one can start by looking at the sequential co-occurrence of words in context.
You can also use the simplified PoS tags after the underscore, and don't even need to add any word or wildcard before it.
The option to produce contractions in the strict grammatical sense, which is what we want to investigate here, rather than just any abbreviated word form, is limited to a closed set of words, belonging to few word classes, and co-occurring with an equally limited set of word classes.
In addition to straightforward concordancing, though, also explore other ways of investigating the results, such as those that the sorting options/restrictions for the concordance lines offer.
The above exercise should have given you an insight into how particular clitics in contractions can combine with other word classes on the syntagmatic axis, although the options for each one of the parts of speech that can occur to the left of the clitic obviously represent items that are exchangeable on the paradigmatic axis.
This obviously isn't as useful because there's no way to directly investigate the context by clicking a link whenever you find an interesting example, but of course better than nothing -or looking through hundreds of concordance lines generated for single prepositions in the hope of finding suitable combinations, which you could still do by using the KWIC display option and sorting according to first word on the right of the node.
A much simpler way to search the data, though, is to use a line-based concordancer, such as my own Simple Corpus Tool, where you can both look at the whole file easily once it's been loaded and also devise a suitable regex that will match two occurrences of the tag used for prepositions occurring in a row at the end of the line.
The previous exercise was mainly designed to show you that line-based concordancers can give you an alternative view of some types of corpus data, as well as obviously to help you get sensitised to the issue of multi-sequence prepositions a little further.
To potentially distinguish between the three options if they should cooccur in the same KWIC window, you can also make use of the columns labelled A, B, or C for each concordance line.
It's therefore less sensitive to corpus size, and hence probably more reliable for smaller corpora.
The best way to do this is to save both sets of result to text files as we did earlier for our concordancing results.
Open the text files in your editor Next, either simply separate the lines that contain scores above the cutoff points by spaces or some other marking from the rest of the results, or even delete all results below the cut-off points.
To get a concordance for the collocate, this time you need to follow the hyperlinked frequency in the column for 'Observed collocate frequency', while the link in the 'Word' column provides a breakdown of the statistics for the word form, including scores for the other statistical measures available and distribution of collocates within the given span.
If you want to compare results produced on a corpus analysis in AntConc with those drawn from a subcorpus of BNCweb, you should definitely select either the 'Mutual Information' or 'T-score' options there, depending on the size of your samples.
Finding a suitable punctuation mark may be easier for unit 2, which simply consists of a single noun phrase, and clearly is a kind of statement that just provides the name of a company.
In turn 3 by speaker A, unit 7 is a discourse marker that indicates that a new stage in the dialogue is beginning.
A more general wildcard pattern that may only be looking for '+ (or ' * ) will fail here because it can only identify word tokens that actually consist of the apostrophe followed by any number of characters, but will not include anything preceding the apostrophe, due to restrictions of the wildcard syntax.
If we start by excluding pronouns, the first word class we find when we start ignoring everything 'nouny' is adverbs (although some of them may have been mis-tagged).
Having realised that some adverbs may in fact be colligates of this clitic, we can now go the other way and, instead of excluding a particular word class, restrict our sorting to display only adverbs (using 'any adverb') to the left of the clitic.
In initial position, the meaning and function is that of a discourse marker (DM), and either tends to indicate the beginning of a new sequence in the spoken interaction or the beginning of a response on the part of one speaker, where that particular speaker wants to preface this response by indicating that they don't agree fully with what has been said before.
Of course, you may also encounter atypical combinations, such as consider about in "PRACTICAL POINTS TO CONSIDER ABOUT_PRP-AVP A HOME BURIAL" (ACM 711), as is in the nature of corpus data.
From about page 24 onwards, I managed to find examples involving above this way, but remember, as we've thinned the examples randomly, your frequency distribution may be somewhat different.
As you may have noticed, this editor is really not optimised for handling large files, so to just view the frequency list without the ability to concordance on it, a dedicated editor, such as Notepad++, would be far better.
As we saw earlier, the original literary selection was (deliberately) very heterogeneous, which did allow us to identify features related to language change nicely.
In a number of instances, though, and co-ordinates adjectives in predicative structures that, again, help to characterise people, for example in fair and virtuous, which actually occurs twice in the data, each time characterising a different woman, Katherina or Bianca, or fair and fresh and sweet, which once more represents an epithet of the former.
Changing the minimum frequency options for either the co-occurrence of node and collocate, or the collocate only, theoretically makes it possible to thin down the results further, but in practice probably only works for relatively high values, and may thus impose artificial restrictions.
The 'Filter results by' options allow you to either quickly select a word form from the list of collocates produced, or even calculate a statistic for a form that hasn't been identified during the analysis.
In addition, you can also restrict the output to a given PoS tag category to either disambiguate a grammatically polysemous word form provided on the left, or to filter the list of collocates by PoS.
While LL and most of the other statistics tend to emphasise the content word collocations in our example, two of the measures, 'T-Score' and 'MI3' sometimes rank function words a little higher, while a raw frequency ranking places predominantly function words at the top of the list, as is to be expected.
In order to refer to the speech act attribute-value combination, you need to use a similar syntax to the one we used for our definitions of the different turns, only without an element name in front of the square brackets.
Finally, we also want to override the punctuation mark that occurs after all syntactic units that may contain requests for information, which may also include declarative questions, apart from the fragments we just handled.
For analysis purposes, both techniques, if used sensibly, will often lead to a cyclical refinement of the categories identified and their respective annotation forms, enabling the researcher to 'fine-tune' the analysis results and thereby also the conclusions that can ultimately be drawn from the corpus data.
Adding further levels of annotation to corpus data almost always leads to added complexity in the data, although, for instance, striking the right balance between using an appropriate number of container elements, empty elements, and suitable attributes can already help us to go a long way, as hopefully Exercise 84l has shown you.
Therefore, for instance, a very good, and still valid, example of the use of different versions of one and the same set of corpus data is the SEC (see Section 2.3.1.2), which actually exists in five different forms.
If your editor is encoding-sensitive, like Notepad++, it'll normally have retained the original encoding of the text file, which was already the correct one.
In some editors, the encoding can be set in the 'Save' dialogue, together with the file name.
To do all this, you need to define one character class enclosed in round brackets to match the speakers, either A or B, at the beginning of the line, ideally escape the dot that follows, then capture one or more digits for the turn number, again enclosed in round brackets, match a colon, one or more spaces (just to be on the safe side), then as many characters as possible, again in round brackets, until you reach the end of the line.
Thus, a 'reqInfo' speech act by one speaker will often trigger an 'answer' by the other, but this answer is generally a statement ('state') in our above taxonomy.
Thus, to be more explicit, we could in fact also allow more than one speech act to occur in our annotation, which would result in 'answer-state' for this particular example.
As an alternative to some of the steps we modelled as regexes above, and for slightly more convenient manual annotation of the remaining XML structure, you could also use an annotation tool, such as my Simple Corpus Tool, which actually includes an editor that'll allow you to add these tags and attributes through the click of a button.
In the first two main sections of the book (Chapters 2-4), we started out by investigating the different forms language data may come in, especially in the shape of existing corpora, and then moved on to developing an understanding of how you can complement such data by collecting your own, including which problems and pitfalls you may encounter in this endeavour, focussing on the nature and sometimes 'messiness' of electronic data.
In the next major section (Chapters 5-10), we then investigated various techniques for analysing language data using established methods of corpus linguistics.
Another issue we've encountered in this context is that often tools designed for analysis almost force us into accepting the 'orthographic word' as the correct unit of analysis, which is e.g.
Here, I've deliberately advocated a form of annotation I call 'Simple XML', which still makes it possible to read and edit the corpus data, or annotate it further, but without the need for complex interfaces many less computer-literate corpus linguists may have a hard time to even install, let alone use without accepting a steep learning curve.
In translation studies and teaching, parallel corpora, that is, corpora that cover similar textual matter or even represent aligned translations of texts, continue to have a very strong influence.
KWIC) (see Section 5.1) concordancer: a software package that generates concordances, as well as possibly displays of other linguistic features, for corpora (see Section 5.1) corpus (pl.
Not all approaches to using corpus data stray towards positivism to the extent that the critique expressed by the likes of Zoldan holds.
Also, within any subject within the social sciences, the orientation towards the study of language may vary.
Some of these subjects engage with the study of language to a degree (psychology and sociology, for example), some might conceivably have research questions to which linguists could contribute (social work and education, for example), while others may be focused so far away from language that the likely interaction with linguistics is marginal at best (social statistics, for example).
While it might be conceivable that such information could be of importance to linguistsfor example, those looking for cohort effects in language change might conceivably be interested in varying patterns of birth and deathmost linguists would have no use for such data.
For example, in CQPweb, it is possible to categorise examples according to an annotation scheme and then to use the scheme to explore the data.
Packages in the social sciences such as NVivo and Altas.ti are, arguably, very helpful packages for corpus linguists to use, especially where they are introducing manual annotations to corpus data, as this process is analogous to some of the typical uses of these packages (e.g.
Notably, NVivo is not a good source of frequency data, as the types of frequency lists that are common in corpus analysis packages are absent.
Likewise, a host of techniques that many corpus linguists would want to use are absent, including collocation analysis and keyword analysis.
By showing positive results and new, productive theories, users of corpus data in linguistics have been able to tilt the scales in favour of corpus use.
While this may, in light of what has been outlined in this paper, prove to be an opportunity for corpus linguists, allowing them to appeal to social science researchers who wish to engage with corpus data but not to shift towards positivism, it will only be so if two conditions are met.
If they are not, it is very easy to bracket corpus linguistics together with approaches to language data which, very often, are free of any serious reflection upon the nature of language in the social world.
Through years, corpus mechanisms have moved out the domain of verifiable research in relevance to linguistic studies and language education to the use of corpora to serve certain functions in conducting research in specific areas of study.
Each selected newly published journal and outstanding study is checked out to select studies in relevance to how language corpus has been employed by those who conduct studies to treat a different sequence of themes in their recent scholarly efforts, concentrating particularly on how their essential studies have been fulfilled in Applied Linguistics.
Corpora have been at the front of two of the most important variations in language education in latest years.
On one aspect they have supplied teachers and materials preparers with more firm depictions of how language is designed and employed, disclosing the prevalent appearance of phrasal units as the foundation of idiomatic language use.
In these projects, they try to set up how diverse types of corpora have shared the progress of direct and indirect programs in language teaching.
On one hand, it normally supplies a preface to the domain of corpus linguistics and its high relevance to language teaching and learning.
However, as the range of research questions that linguists aim to address by means of corpus data has expanded both in diversity and complexity, and as researchers have started to resort to more complex (often multivariate) statistical analysis to address these questions, it may no longer be practically feasible to continue working this way.
This paper contributes to tackling this challenge by exploring how corpus data annotation can be made (semi-)automatic by means of machine learning.
In natural language processing (NLP), contextualized word embeddings generated with LLMs are often shown to perform impressively at "downstream tasks", like part-of-speech tagging, dependency parsing, or named-entity recognition (e.g.
In particular, we resorted to the k-nearest neighbour (KNN) and the support vector machine (SVM) algorithms, as implemented by the scikit-learn software package.
We divide the available data into ten non-overlapping sections or "splits", and test the performance of each classification approach on each of these splits.
Yet, as with the previous case study, the most accurate (and most stable) classification approach is metric fine-tuning.
These include that Bayesian model comparison helps overcome a problem that occurs with frequentist methods, where the estimated effect size of the observed differences between models is entangled with the underlying sample size.
That is, as input we use the classification accuracythat is, the proportion of correctly annotated itemsof each classification approach (i.e.
The output of the comparison method consists of the estimated probability that a particular classification approach performs differently from or similarly to one of the others.
Furthermore, the procedure we presented has the added benefit that models used to annotated the data can be shared, and used to replicate the data annotation scheme in corroboration and follow-up studies.
Parallel corpora contain two languages with one language translated into another language, and the two corpora aligned at the level of the sentence.
There are many learner corpora, which contain samples of English written by speakers for whom English is a second or foreign language.
Learner corpora can be used to study language acquisition, and to develop pedagogical tools and strategies for teaching English as a second or foreign language.
Many new corpora have been created in the area of language change.
At Lancaster University, one of the first major part-of-speech tagging programs, the CLAWS Tagger, automated the insertion of part-of-speech tags (e.g.
The chapter also contains discussions of corpus tools, such as concordancing programs, that can facilitate the analysis of corpora by retrieving relevant examples for a given corpus analysis, indicating their overall frequency, and specifying (depending upon the program) the particular genres in which they occur.
In addition to illustrating how a primarily qualitative corpus analysis is conducted, the analysis of Trump's speech provides a basis for describing the various steps involved in conducting a corpus analysis, such as how to frame a research question, find suitable corpora from which empirical evidence can be obtained to address the research question, and so forth.
The remainder of the chapter focuses on more quantitatively based corpus research, such as Douglas Biber's work on multi-dimensional analysis and the specific statistical analyses he used to determine the register distributions of a range of different linguistic constructions, for instance, the higher frequency of pronouns such as I and you in spontaneous conversations than in scientific writing.
However, as the example corpus analysis in the final section of the chapter demonstrates, conducting "interesting" linguistic research based on "real" samples of language are not necessarily mutually exclusive activities: a corpus can serve as a test bed for theoretical claims about language; in turn, theoretical claims can help explain patterns and structures found in a corpus.
The paraphrase of each sentence makes the MPC similar to a parallel corpusa type of corpus that typically contains sentences from a whole text, with each individual sentence translated into a different language.
The major difference is that parallel corpora typically contain samples from larger texts (rather than single sentences), with each sentence in the text translated into a particular language.
According to this guideline, a corpus is a type of text that is regarded "as composite, that is, consisting of several components which are in some important sense independent of each other" (www.tei-c.org/release/doc/tei-p5-doc/en/html/DS.html).
This corpus was created so that linguists with more computationally based interests could conduct research in natural language processing (NLP), an area of study that involves the computational analysis of corpora often (though not exclusively) for purposes of modeling human behavior and cognition.
For this reason, many have argued that corpus data should be supplemented with other means of gathering data.
These corpora range from multipurpose corpora, which can be studied to carry out many differing types of corpus-based analyses, to learner corpora, which have been designed to study the types of English used by individuals (from many differing first language backgrounds) learning English as a second or foreign language.
All of these corpora contain numerous registers of speech or writing (such as fiction, press reportage, casual conversations, and spoken monologues or dialogues) representative of a particular variety of English.
Learner corpora enable researchers to study such matters as whether one's native language affects their mastery of a foreign language, in this case English.
That is, each word has been assigned a lexical tag identifying its part of speech (such as verb, noun, preposition).
Although FLOB (The Freiburg LOB Corpus of British English) and FROWN (The Freiburg Brown Corpus of American English) are not historical corpora in the sense that the Helsinki and ARCHER Corpora are, they do permit the study of changes in British and American English between the periods 1961-1991.
While genre or register variation focuses on the particular linguistic constructions associated with differing text types, sociolinguistic variation is more concerned with how various sociolinguistic variables, such as age, gender, and social class, affect the way that individuals use language.
To study press reportage, for instance, it is only necessary to take from a given corpus all samples of press reportage, and to study within this sub-corpus whatever one wishes to focus on.
Included within the spoken part of the BNC is a sub-corpus known as the Corpus of London Teenage English (COLT).
A software program, SARA (SGML-Aware Retrieval Application), was designed to read the headers and do various analyses of the corpus based on a prespecified selection of sociolinguistic variables.
In calculating how frequently males and females use lovely, both programs can only count the number of times a male or female speaker uses this expression; neither program can produce figures that, for instance, could help determine whether females use the word more commonly when speaking with other females than males.
These frequencies illustrate a simple fact about English vocabulary (or, for that matter, vocabulary patterns in any language): a relatively small number of words (particularly function words) will occur with great frequency; a relatively large number of words (content words) will occur far less frequently.
In a matter of seconds, a concordancing program can count the frequency of words in a corpus and rank them from most frequent to least frequent.
Chapter 4 ("Analyzing a Corpus") describes in detail how to conduct a corpus analysis, covering such topics as how to frame a research question for a particular corpus analysis and select the appropriate corpora for conducting the analysis.
Finally, a case study was presented to demonstrate that corpus analyses and various linguistic theories go hand in hand, and that such studies can do more than simply provide examples of constructions and document their frequency of occurrence.
If this is the only information that a corpus analysis could provide, then corpus linguistics would have at best a marginal role in the field of linguistics.
But while written texts can easily be digitized, technology has not progressed to the point where it can greatly expedite the collection and transcription of speech, especially spontaneous conversations: There is much work involved in recording this type of speech and manually transcribing it.
However, in some instances, the number of occurrences is so high that even though the BNC may contain lower frequencies, the numbers are still high enough to permit valid studies.
For instance, while COCA contains 2,900,000 be passives, the BNC contains 890,000 examples (Davies 2015: 15), a number of occurrences that is certainly sufficient enough to study this type of passive.
The reason that a carefully selected range of genres is not important in this corpus is that the corpus is not intended to permit genre studies but to, for instance, "train" taggers and parsers to analyze English: to present them with a sufficient amount of data so that they can "learn" the structure of numerous constructions and thus produce a more accurate analysis of the parts of speech and syntactic structures present in English.
For instance, many texts, such as books, are quite lengthy, and to include a complete text in a corpus would not only take up a large part of the corpus but require the corpus compiler to obtain permission to use not just a text excerpt, a common practice, but an entire text, a very uncommon practice.
In addition to studying the distribution of word categories, such as nouns or prepositions, Biber (1993: 250) calculated the frequency with which new words are added to a sample as the number of words in the sample increases.
If the number of samples included in the various genres of the BNC and ICE Corpora are surveyed, it is immediately obvious that both of these corpora place a high value on spontaneous dialogues, and thus contain more samples of this type of speech than, say, scripted broadcast news reports.
However, linguists disagree about whether purely synchronic studies are even possible: New words, for instance, come into the language every day, indicating that language change is a constant process.
Moreover, even grammatical constructions can change subtly in a rather short period of time.
The best that the corpus compiler can do is to be aware of the variables, confront them head on, and deal with them as much as is possible during the construction of a corpus.
The Child Language Data Exchange System, or CHILDES Corpus, includes transcriptions of children engaging in spontaneous conversations in English and other languages.
However, because it is virtually impossible for the creators of corpora to anticipate what their corpora will ultimately be used for, it is also the responsibility of the corpus user to make sure that the corpus he or she plans to conduct a linguistic analysis of is a valid corpus for the particular analysis being conducted.
Once these determinations are made, the corpus compiler can begin to collect the actual speech and writing to be included in the corpus.
However, it is important not to become too rigidly invested in the initial corpus design, since obstacles and complications may be encountered while collecting data that may require changes in the initial corpus design: It might not be possible, for instance, to obtain recordings for all the genres originally planned for inclusion in the corpus, or copyright restrictions might make it difficult to obtain certain kinds of writing.
The International Corpus of English (ICE) Project provides a number of examples of logistical realities that forced changes in the initial corpus design for some of the components.
A second way to enhance naturalness is to record as lengthy a conversation as possible so that when the conversation is transcribed, the transcriber can select the most natural segment of speech from a much lengthier speech sample, for instance, 30 minutes or longer.
Moreover, a lengthier segment allows the corpus compiler to select a more coherent and unified segment of speech to ultimately include in the corpus.
How much speech needs to be recorded is also determined by the type of speech being recorded.
To find information on specific recorders and microphones, it is useful to consult what researchers who specialize in the study of spoken language use to make voice recordings.
This type of speech is best recorded not with a microphone but directly from a radio or television by running a cord from the audio output plug on the radio or television to the audio input plug on the tape recorder.
In gathering written texts for inclusion in a corpus, the corpus compiler will undoubtedly have a predetermined number of texts to collect within a range of given genres: twenty 2,000-word samples of fiction, for instance, or ten 2,000-word samples of learned humanistic writing.
A numbering system of this type (described in detail in Greenbaum 1996b: 601-14) allows the corpus compiler to keep easy record of where a text belongs in a corpus and how many samples have been collected for that part of the corpus.
The names supplied to a text sample are short and mnemonic and give the corpus compiler (and future users of the corpus) an idea of the type of text that the sample contains.
The remaining information recorded about texts depended very much on the type of text that was being collected.
Although the process of transcription has been automated, current voice recognition technology has not reached the level of sophistication to be able to accurately transcribe the most common type of speech: spontaneous conversations.
But organizing a corpus into a series of directories and sub-directories makes working with the corpus much easier, and allows the corpus compiler to keep track of the progress being made on corpus as it is being created.
Each text included in a given ICE component is assigned an identification letter and number indicating the type of speech or writing that it represented.
As corpora have become larger and larger, oftentimes containing millions of words of text, the feasibility of, for instance, proofreading a corpus or placing individual samples into neatly delineated sections becomes less viable: Such corpora are simply too large for any kind of proofreading to be done, or for a single text type (such as a press editorial from a particular newspaper) to be placed into a single directory.
However, the accuracy of such transcriptions depends upon the type of speech that is being transcribed.
All of this extra-linguistic information is very difficult to encode in a written transcription without the corpus compiler developing an elaborate system of markup identifying this information and the transcriber spending hours both interpreting what is going on in a conversation and inserting the relevant markup.
Likewise, because written corpora were encoded in text files, there was no way to indicate certain features of orthography, such as italicization or boldface fonts.
While the CLAWS tagsets were developed to facilitate the study of the linguistic structure of various kinds of spoken and written texts, other tagsets were created to enable research in the area of natural language processing (NLP), an area of language inquiry that is more focused on the computational aspects of designing taggers (and also parsers) to annotate and study corpora.
While this seems like a very high accuracy rate, in a large corpus, the remaining 3 percent of words with multiple tags can constitute a very sizable number of words.
In contrast, the Penn Treebank tagset is much smaller (36 tags), mainly because this tagset was developed not necessarily to enable detailed linguistic analyses but to advance research in the area of natural language processing (a point described in detail earlier in this chapter).
But other text types can prove more difficult to parse, resulting in lower accuracy rates.
In corpus linguistics, parsed corpora serve two purposes: to enable the analysis of larger syntactic structures, such as phrases and clauses, by individuals conducting linguistic analyses, and to provide testbeds for those in the area of natural language processing interested in the development of parsers.
Within each of these constituents, every word is assigned a part of speech tag: a, for instance, is tagged "AT", indicating it is an article; move is tagged "NN", indicating it is a singular common noun; and so forth.
Like the corpus compiler, the corpus analyst needs to consider such factors as whether the corpus to be analyzed is lengthy enough for the particular linguistic study being undertaken and whether the samples in the corpus are balanced and representative.
In contrast, other corpus studies are more quantitative, subjecting the results of a corpus analysis to, for instance, statistical analyses to determine whether the particular linguistic differences in corpora under study are significant or not.
In his work on register variation, Biber develops a series of what he calls dimensions: general parameters that describe a particular style of communication.
For instance, Xueliang Chen, Yuanle Yan, and Jie Hu (2019) conducted a corpus analysis of the use of language by Hillary Clinton and Donald Trump when they were running against each other for president.
But as was discussed in Chapter 1 (Section 1.5), there is a range of other types of corpora that can be used for analysis, including multi-purpose corpora, learner corpora, historical corpora, and parallel corpora.
Categories are represented at both the phrase and word level: A son is a "noun phrase" (NP), as is Alistair.
Most of the examples documented above are insults, a type of speech act that violates many norms of politeness, particularly because Trump's insults of individuals occur in very public forums, such as Twitter, debates, and campaign rallies.
The frequent co-occurrence of negative descriptors with particular individuals can also result in what is referred to as a negative semantic prosody.
One final characteristic of Trump Speak has less to do with his repetition of particular vocabulary items and more with his general use of language, particularly his propensity to frequently lie.
But the first computer corpus for the computational analysis of language, as Hasko (2020: 952) comments, was the Brown Corpus, a corpus that paved the way for not only quantitative corpus research but qualitative studies of language as well, as demonstrated by the particular registers included in the Brown Corpus.
But because this kind of discussion is subjective and impressionistic, it is better to devote the bulk of a corpus study to supporting qualitative judgements about a corpus with quantitative information.
To perform this comparison, the Chi square test compares "observed" frequencies in a given dataset with "expected" frequencies (i.e.
While the Chi square test is one of the more common tests used with linguistic data, as Oakes' (1998: 1-51) survey of statistical tests for corpus data demonstrates, there are a range of other tests as well.
For instance, in the very simple sentence I like pizza, the pronoun I would be tagged as a first person pronoun; the verb like would be as a present tense lexical verb; and the noun pizza as a singular common noun.
Lexical tagging is crucial because it will enable the factor analysis program to determine where the various parts of speech occur: e.g.
While it is certainly not the case that any particular linguistic item can be automatically retrieved instantlymany linguistic constructions are simply too complex for this type of "instant" retrievalnevertheless, the process of corpus analysis has been greatly automated.
For instance, it can be used to study language variation or to help in the creation of dictionaries.
Is any one type of analysis better than another?
Conducting a corpus analysis is a multi-stage process, involving framing a research question, finding a suitable corpus or corpora for the analysis, extracting relevant information from the corpus or corpora chosen for the analysis, and integrating information from relevant articles or books into the analysis.
On the one hand, there are textbooks that provide excellent discussions of the history of corpus linguistics or the history of corpus design, or that discuss the epistemological status of corpus data in a field that has been dominated far too long by generative linguistic ideas about what does and does not constitute linguistic evidence.
What I would have wanted and needed when I took my first steps into corpus linguistic research as a student is an introductory textbook that focuses on methodological issues -on how to approach the study of language based on usage data and what problems to expect and circumvent.
The first part of the book begins with an almost obligatory chapter on the need for corpus data (a left-over from a time when corpus linguistics was still somewhat of a fringe discipline).
Berlin, 8th March 2020 viii 1 The need for corpus data Broadly speaking, science is the study of some aspect of the (physical, natural or social) world by means of systematic observation and experimentation, and linguistics is the scientific study of those aspects of the world that we summarize under the label language.
Among these researchers, the role of corpus data, and the observation of linguistic behavior more generally is highly controversial.
While there are formalists who have discovered (or are beginning to discover) the potential of corpus data for their research, much of the formalist literature has been, and continues to be, at best dismissive of corpus data, at worst openly hostile.
Corpus data are attacked as being inherently flawed in ways and to an extent that leaves them with no conceivable use at all in linguistic inquiry.
To put it mildly, inventing one's own data is a rather subjective procedure, so, again, anyone unfamiliar with the last sixty years of linguistic theorizing might wonder why such a procedure was proposed in the first place and why anyone would consider it superior to the use of corpus data.
Readers familiar with this discussion or readers already convinced of the need for corpus data may skip this chapter, as it will not be referenced extensively in the remainder of this book.
For all others, a discussion of both issues -the alleged uselessness of corpus data and the alleged superiority of intuited data -seems indispensable, if only to put them to rest in order to concentrate, throughout the rest of this book, on the vast potential of corpus linguistics and the exciting avenues of research that it opens up.
Section 1.1 will discuss four major points of criticisms leveled at corpus data.
As arguments against corpus data, they are easily defused, but they do point to aspects of corpora and corpus linguistic methods that must be kept in mind when designing linguistic research projects.
Section 1.2 will discuss intuited data in more detail and show that it does not solve any of the problems associated (rightly or wrongly) with corpus data.
This is a problem not only for studies that are interested in linguistic variation but also for studies in core areas such as lexis and grammar: many linguistic patterns are limited to certain varieties, and a corpus that does not contain a particular language variety cannot contain examples of a pattern limited to that variety.
To do cognitive linguistics with corpus data, you need to interpret the data -to give it meaning.
The meaning doesn't occur in the corpus data.
I argued in Section 1.1.1 above that this claim makes sense only in the context of rather implausible assumptions concerning linguistic knowledge and linguistic usage, but even if we accept these assumptions, the question remains whether intuited judgments are different from corpus data in this respect.
There is little to add to this statement, other than to emphasize that if it is possible to construct a model of linguistic competence on the basis of intuited judgments that involve factors other than competence, it should also be possible to do so on the basis of corpus data that involve factors other than competence, and the competence/performance argument against corpus data collapses.
However, it seems implausible to assume that such judgments are more complete than corpus data.
First, just like a corpus, the linguistic experience of a speaker is finite and any mental generalizations based on this experience will be partial in the same way that generalizations based on corpus data must be partial (although it must be admitted that the linguistic experience a native speaker gathers over a lifetime exceeds even a large corpus like the BNC in terms of quantity).
While I may rightly consider myself the final authority on the intended meaning of a sentence that I myself have produced, my interpretation ceases to be privileged in this way once the issue is no longer my intention, but the interpretation that my constructed sentence would conventionally receive in a particular speech community.
The fact that corpus data do not allow us to maintain this illusion does not make them inferior to intuition, it makes them superior.
To put it bluntly, then, intuition "data" are less reliable and less valid than corpus data, and they are just as incomplete and in need of interpretation.
In language acquisition or in historical linguistics, for example, researchers could not use their intuition even if they wanted to, since not even the most fervent defendants of intuited judgments would want to argue that speakers have meaningful intuitions about earlier stages of their own linguistic competence or their native language as a whole.
For language acquisition research, corpus data and, to a certain extent, psycholinguistic experiments are the only sources of data available, and historical linguists must rely completely on textual evidence.
But whatever the case 6 Perhaps Speech Act Theory could be seen as an attempt at discourse analysis on the basis of intuition data: its claims are often based on short snippets of invented conversations.
The problem is that in observational studies no disruption is ever minimalas soon as the investigator is present in person or in the minds of the observed, we get what is known as the "observer's paradox": we want to observe people (or other animate beings) behaving as they would if they were not observed -in the case of gathering spoken language data, we want to observe speakers interacting linguistically as they would if no linguist was in sight.
There is one famous exception to the observer's paradox in spoken language data: the so-called Nixon Tapes -illegal surreptitious recordings of conversation in the executive offices of the White House and the headquarters of the opposing Democratic Party produced at the request of the Republican President Richard Nixon between February 1971 and July 1973.
Researchers may sometimes deliberately choose to depart from authenticity in the corpus-linguistic sense if their research design or the phenomenon under investigation requires it.
A researcher may be interested in a phenomenon that is so rare in most situations that even the largest available corpora do not contain a sufficient number of cases.
The way that corpus creators typically aim to achieve this is by including in the corpus different manifestations of the language it is meant to represent in proportions that reflect their incidence in the speech community in question.
You may have noted that in the preceding discussion I have repeatedly used terms like language variety, genre, register and style for different manifestations of language.
In this book, I use language variety to refer to any form of language delineable from other forms along cultural, linguistic or demographic criteria.
Second, even if we did know, it is not clear that all manifestations of language use shape and/or represent the linguistic system in the same way, simply because we do not know how widely they are received.
For example, emails may be responsible for a larger share of written language produced in a given time span than news sites, but each email is typically read by a handful of people at the most, while some news texts may be read by millions of people (and others not at all).
Third, in a related point, speech communities are not homogeneous, so defining balance based on the proportion of language varieties in the speech community may not yield a realistic representation of the language even if it were possible: every member of the speech community takes part in different communicative situations involving different language varieties.
This, in turn, requires a definition of what constitutes a typical speaker in a given speech community.
It is made up exclusively of edited prose published in the year 1961, so it clearly does not attempt to be representative of American English in general, but only of a particular kind of written American English in a narrow time span.
There is no reason to believe that this corresponds proportionally to the total number of words produced in these language varieties in the USA in 1961.
The creators of the BROWN corpus are quite open about the fact that their corpus design is not a representative sample of (written) American English.
Basing the estimation of the proportion of language varieties on a different source would, again, have yielded a very different corpus design.
While corpora will always be skewed relative to the overall population of texts and language varieties in a speech community, the undesirable effects of this skew can be alleviated by including in the corpus as broad a range of varieties as is realistic, either in general or in the context of a given research project.
Thus, they could quickly be lost in their entirety when the sample size drops substantially below the size of the population as a whole.
And when a genre (or a language variety in general) goes missing from our sample, at least some linguistic phenomena will disappear along with it -such as the expression [bring NP LIQUID [ PP to the/a boil]], which, as discussed in Chapter 1, is exclusive to cookbooks.
On the one hand, corpus size correlates with representativeness only to the extent that we take corpus diversity into account.
Most commonly, they contain information about the word class of each word, represented in the form of a so-called "part-of-speech (or POS) tags".
In contrast, it is highly controversial how many parts of speech there are and how they should be identified, or how the structure even of simple sentences is best described and represented.
Accepting (or working around) the corpus creators' assumptions and decisions concerning POS tags and annotations of syntactic structure may seriously limit or distort researcher's use of corpora.
Also, while it is clear that speakers are at some level aware of intonation, pauses, indentation, roman vs. italic fonts, etc., it is much less clear that they are aware of parts of speech and grammatical structures.
These may be recorded in a manual, a separate computerreadable document or directly in the corpus files to which they pertain.
Typical metadata are language variety (in terms of genre, medium topic area, etc., as described in Section 2.1.2 above), the origin of the text (for example, speaker/writer, year of production and or publication), and demographic information about the speaker/writer (sex, age, social class, geographical origin, sometimes also level of education, profession, religious affiliation, etc.).
Metadata are also crucial in recontextualizing corpus data and in designing certain kinds of research projects, but they, too, depend on assumptions and choices made by corpus creators and should not be uncritically accepted by researchers using a given corpus.
This definition is more specific with respect to the data used in corpus linguistics and will exclude certain variants of discourse analysis, text linguistics, and other fields working with authentic language data (whether such a strict exclusion is a good thing is a question we will briefly return to at the end of this chapter).
If anything, we must thus replace the reference to corpus analysis software by a reference to what that software typically does.
Software packages for corpus analysis vary in capability, but they all allow us to search a corpus for a particular (set of) linguistic expression(s) (typically word forms), by formulating a query using query languages of various degrees of abstractness and complexity, and they all display the results (or hits) of that query.
Instead, the definition just given captures an important aspect of a discipline referred to as statistical or stochastic natural language processing (Manning & Schütze 1999 is a good, if somewhat dense introduction to this field).
In corpus linguistics, the object of research will usually involve one or more aspects of language structure or language use, but it may also involve aspects of our psychological, social or cultural reality that are merely reflected in language (a point we will return to in some of the case studies presented in Part II of this book).
In the example above, the dependent variable is Word for the Forward-Facing Window of a Car with the values windshield and windscreen; the independent variable is Variety of English with the values british and american (from now on, variables will be typographically represented by small caps with capitalization, their values will be represented by all small caps).
In the context of corpus linguistics, this means annotating them according to an annotation scheme containing the operational definitions.
This is highly desirable given that these methods are fundamentally based on the same assumptions as to how language can and should be studied (namely on the basis of authentic instances of language use), and that they are likely to face similar methodological problems.
In the case of windscreen and windshield, we actually find counterexamples once we increase the sample size sufficiently, but there is still an overwhelming number of cases that follow our predictions.
But if we claim the existence of these constructs, we must define them; what is more, we must define them in a way that enables us (and others) to find them in the real world (in our case, in samples of language use).
Similarly, there may be situations where we simply accept the part-of-speech tagging or the syntactic annotation in a corpus, but given that there is no agreedupon theory of word classes, let alone of syntactic structures, this can be problematic in some situations.
These manuals and other literature provided by corpus creators must be read and cited like all other literature, and we must clarify in the description of our research design why and to what extent we rely on the operationalizations described in these materials.
More drastically, LOB and FLOB treat some sequences of orthographic words as multi-word tokens belonging to a single word class: in front of is treated as a preposition in LOB and FLOB, indicated by labeling all three words IN (LOB) and II (FLOB), with an additional indication that they are part of a sequence: LOB attaches straight double quotes to the second and third word, FLOB adds a 3 to indicate that they are part of a three word sequence and then a number indicating their position in the sequence.
Such tag sequences, called ditto tags make sense only if you believe that the individual parts in a multiword expression lose their independent word-class membership.
It is clear, then, that tokenization and part-of-speech tagging are not inherent in the text itself, but are the result of decisions by the corpus makers.
However, POS tagging is not usually done by skilled, experienced annotators, bringing us to the second, completely different way in which POS tags are based on operational definitions.
It has to "learn" them from a corpus that has been annotated by hand by skilled, experienced annotators based on a reliable, valid annotation scheme.
This is likely to work better in some situations than in others, which means that incorrectly assigned tags will not be distributed randomly across parts of speech.
For example, in the BNC, the word form regard is systematically tagged incorrectly as a verb in the complex prepositions with regard to and in regard to, but is correctly tagged as a noun in most instances of the phrase in high regard.
Which one to use depends on a number of factors, including first, what aspect of word length is relevant in the context of a particular research project (this is the question of validity), and second, to what extent are they practical to apply (this is the question of reliability).
The question of reliability is a simple one: "number of letters" is the most reliably measurable factor assuming that we are dealing with written language or with spoken language transcribed using standard orthography; "number of phonemes" can be measured less reliably, as it requires that data be transcribed phonemically (which leaves more room for interpretation than orthography) or, in the case of written data, converted from an orthographic to a phonemic representation (which requires assumptions about which the language variety and level of formality the writer in question would have used if they had been speaking the text); "number of syllables" also requires such assumptions.
When we want to measure the length of linguistic units above word level, e.g.
While Word Length and Discourse Status are not the only such phenomena, they are not typical either.
Let us look at two such phenomena, Word Sense and Animacy.
In either case, we must make the set of senses and the criteria for applying them transparent, and in either case we are dealing with an operational definition that does not correspond directly with reality (if only because word senses tend to form a continuum rather than a set of discrete categories in actual language use).
This is at least in part due to the fact that corpora consist of text that is represented as a sequence of word forms, and that, consequently, word forms are easy to retrieve.
As we saw when discussing the case of pavement in Chapter 3, a corpus query for a string of characters like ⟨ pavement ⟩ may give us more than we want -it will return not only hits corresponding to the word sense 'pedestrian footpath', which we could contrast with the synonym sidewalk, but also those corresponding to the word sense 'hard surface' (which we could contrast with the synonym paving).
The query in (3) makes use of the annotation in the corpus (in this case, the part-of-speech tagging), but it does so in a somewhat cumbersome way by treating word forms and the tags attached to them as strings.
For example, (6a) will find all instances of the word form walk tagged as a verb, while (6b) will find all instances tagged as a noun.
When talking about a query in a particular corpus, I will use the annotation (e.g., the part-of-speech tags) used in that corpus, when talking about queries in general, I will use generic values like noun or prep., shown in lower case to indicate that they do not correspond to a particular corpus annotation.
However, as we saw in the preceding section and in Chapter 3, it is not always possible to define a corpus query in a way that will retrieve all and only the occurrences of a particular phenomenon.
Such scores are useful in information retrieval or machine learning, but less so in corpus-linguistic research projects, where precision and recall must typically be assessed independently of, and weighed against, each other.
In addition, some of the strings in (9b) above are ambiguous, i.e., they can represent parts of speech other than determiner; for example, that can be a conjunction, as in (9c), which otherwise fits the description of a ditransitive, and in (11d), which does not.
Obviously, a corpus tagged for parts of speech could improve the precision of our search results somewhat, by excluding cases like (9c-d), but others, like (9a), 4.1 Retrieval could never be excluded, since they are identical to the ditransitive as far as the sequence of parts-of-speech is concerned.
We could construct a query that would retrieve all instances of the lemmas ANGER, RAGE, FURY, and other synonyms of anger, and then select those results that also contain (within the same clause or within a window of a given number of words) vocabulary from domains like 'liquids', 'heat', 'containers', etc.
If we assemble our initial list of expressions systematically, perhaps from a larger number of native speakers that are representative of the speech community in question in terms of regional origin, sex, age group, educational background, etc., we should end up with a representative sample of expressions to base our query on.
In some cases, the variables and their values will be provided externally; they may, for example, follow from the structure of the corpus itself, as in the case of british english vs. american english defined as "occurring in the LOB corpus" and "occurring in the BROWN corpus" respectively.
Whatever the case, we need an annotation scheme -an explicit statement of the operational definitions applied.
Of course, such an annotation scheme is especially important in cases where interpretative judgments are involved in categorizing the data.
In this case, the annotation scheme should contain not just operational definitions, but also explicit guidelines as to how these definitions should be applied to the corpus data.
Take the example of british english and american english used in Chapters 3 and 2: If we accept the idea that the LOB corpus contains "British English" we are accepting an interpretation of language varieties that is based on geographical criteria: British English means "the English spoken by people who live (perhaps also: who were born and grew up) in the British Isles".
However, we may not have thought of all potentially unclear cases before we start annotating our data, which means that we may have to amend our annotation scheme as we go along.
No matter how explicit our annotation scheme, we will come across cases that are not covered and will require individual decisions; and even the clear cases are always based on an interpretative judgment.
For example, the distinctions between different degrees of Animacy need to be defined in a way that allows us to identify them in corpus data (this is the annotation scheme, cf.
However, the field of corpus linguistics is not well-established and methodologically mature enough yet to have yielded uncontroversial and widely applicable annotation schemes for most linguistic phenomena.
The first step in creating an annotation scheme for a particular variable consists in deciding on a set of values that this variable may take.
As an example, consider the so-called Silverstein Hierarchy used to categorize nouns for (inherent) Topicality (after Deane 1987: 67): Note, first, that there is a lot of overlap in this annotation scheme.
For example, a first or second person pronoun will always refer to a human or animate NP and a third person pronoun will frequently do so, as will a proper name or a kin term.
After defining a variable (or set of variables) and deciding on the type and number of values, the second step in creating an annotation scheme consists in defining what belongs into each category.
In particular, it needs to be specified what distinguishes an organization from other groups of human beings (that are to be categorized as human according to the annotation scheme).
The annotation scheme defines an organization as a referent involving "more than one human" with "some degree of group identity".
By listing properties that a group must have to count as an organization in the sense of the annotation scheme, the decision is simplified considerably, and by providing a decision procedure, the number of unclear cases is reduced.
One advantage is that examples may help the annotators understand the annotation scheme.
The third step, discussed in detail in the next section, consists in testing the reliability of our annotation scheme.
For example, if we define Word Length in terms of "number of letters", we could write a simple computer program to go through our corpus, count the letters in each word and attach the value as a tag.
One approach would be to have the entire data set annotated by two annotators independently on the basis of the same annotation scheme.
Obvious possibilities include cases that are not covered by the annotation scheme at all, cases where the definitions in the annotation scheme are too vague to apply or too ambiguous to make a principled decision, and cases where one of the annotators has misunderstood the corpus example or made a mistake due to inattention.
Where the annotation scheme is to blame, it could be revised accordingly and re-applied to all unclear cases.
Both problems can be solved (or at least alleviated) by testing the annotation scheme on a smaller dataset using two annotators and calculating its reliability across annotators.
If this so-called interrater reliability is sufficiently high, the annotation scheme can safely be applied to the actual data set by a single annotator.
The minimal requirement of an incremental and collaborative research cycle is what we might call retraceability: our description of the research design and the associated procedures must be explicit and detailed enough for another researcher to retrace and check for correctness each step of our analysis and when provided with all our research materials (i.e., the corpora, the raw extracted data and the annotated data) and all other resources used (such as our annotation scheme and the software used in the extraction and statistical analysis of the data) and all our research notes, intermediate calculations, etc.
This is broadly what replicability refers to, but the term may also be used in an extended sense to situations where a researcher attempts to replicate our results and conclusions rather than our entire research design.
This is not a purely theoretical possibility even with such a simple research design -you will often find that even word frequencies reported for a given corpus in the literature will not correspond to what your own query of the same corpus yields.
And if the data had to be annotated in any way more complex than "number of letters", that annotation will be difficult to reconstruct even if an annotation scheme is provided, and impossible to reconstruct if this is not the case.
The first option is routinely chosen in the case of automatically annotated variables like Part of Speech, as in the passage from the BROWN corpus cited in Here, the annotation (i.e., the part-of-speech tags) are attached to the data they refer to (i.e., words) by an underscore (recall that alternatively, a vertical format with words in the first and tags in the second column is frequently used, as are various kinds of XML notation).
Frequently, spreadsheet applications are used to store the corpus data and annotation decisions, as in the example in The first line contains labels that tell us what information is found in each column respectively.
While the direct annotation of corpus files is rare in corpus 4.2 Annotating linguistics, it has become the preferred strategy in various fields concerned with qualitative analysis of textual data.
They typically allow the user to define a set of annotation categories with appropriate codes, import a text file, and then assign the codes to a word or larger textual unit by selecting it with the mouse and then clicking a button for the appropriate code that is then added (often in XML format) to the imported text.
Specifically, we will discuss three types of data (or levels of measurement) that we might encounter in the process of quantifying the (annotated) results of a corpus query (Section 5.1): nominal data (discussed in more detail in Section 5.2), ordinal (or rank) data (discussed in more detail in Section 5.3, and cardinal data (discussed in more detail in Section 5.4.
In other words, we are not really ranking french and german as values of Language at all; instead, we are ranking values of the variables Size of Native Speech Community and Number of Countries with Official Language X respectively.
Again, what we would be calculating a mean of is the values of the variable Size of Native Speech Community, and while it makes a sort of sense to say that the mean of the values number of french native speakers and number of german native speakers was 83.5 in 2005, it does not make sense to refer to this mean as number of javanese speakers.
Thus, this kind of research design should already be somewhat familiar.
This is done in order to show that they are values of a variable in a particular research design, based on a particular theoretical construct.
To turn these definitions into operational definitions, we need to provide the specific queries used to extract the data, including a description of those aspects of corpus annotation used in formulating these queries.
The annotation of the results for proper name or common noun status can be done in various ways -in some corpora (but not in the BROWN corpus), the POS tags may help, in others, we might use capitalization as a hint, etc.
This correlation is not perfect, as common nouns can also encode old information, so using Part of Speech of Nominal Expression as an operational definition for Givenness is somewhat crude in terms of validity, but the advantage is that it yields a highly reliable, easy-to-annotate definition: we can use the part-of-speech tagging to annotate our sample automatically.
In order to convert an absolute frequency n into a relative one, we simply divide it by the total number of cases N of which it is a part.
Using appropriate descriptive statistics (percentages, observed and expected frequencies, modes, medians and means), we were able to determine that the data conform to these predictions -i.e., that the quantitative distribution of the values of the variables Givenness (measured by Part of Speech, Animacy and Length across the conditions s-possessive and of-possessive fits the predictions formulated.
This is called the probability of error (or simply p-value) in statistics.
Even the majority of researchers who are unwilling to report such results would take them as an indicator that additional research might be in order (especially if there is a reasonable effect size, see further below).
Clearly, what probability of error one is willing to accept for any given study also depends on the nature of the study, the nature of the research design, and a general disposition to take or avoid risk.
However, it is theoretically possible to generalize the coin-flipping logic to any research design, i.e., calculate the probabilities of all possible outcomes and add up the probabilities of the observed outcome and all outcomes that deviate from the expected outcome even further in the same direction.
In practice, we just have to look up the test statistic in a chart that will give us the corresponding probability of error (or p-value, as we will call it from now on).
In the following three sections, I will introduce three widely-used tests involving test statistics for the three types of data discussed in the previous section: the chi-square (𝜒 2 ) test for nominal data, the Wilcoxon-Mann-Whitney test (also known as Mann-Whitney U test or Wilcoxon rank sum test) for ordinal data, and Welch's 𝑡-test for cardinal data.
In these cases, correlation tests are used, such as Pearson's product-moment correlations (if are dealing with two cardinal variables) and Spearman's rank correlation 6.3 Nominal data: The chi-square test coefficient or the Kendall tau rank correlation coefficient (if one or both of our variables are ordinal).
I will therefore treat them in slightly more detail than the other two types, introducing different versions and (in the next chapter) extensions of the most widely used statistical test for nominal data, the chi-square (𝜒 2 ) test.
Thus, a two-by-two table has one degree of freedom.
Turning to the table of 𝜒 2 values in Section 14.1, we first find the row for one degree of freedom (this is the first row); we then check whether our 𝜒 2 -value is larger than that required for the level of significance that we are after.
In our case, the value of 272.16 is much higher than the 𝜒 2 value required for a significance level of 0.001 at one degree of freedom, which is 10.83.
First, there are generally agreed-upon verbal descriptions for different ranges that the value of a correlation coefficient may have (similarly to the verbal descriptions of p-values discussed above.
A one-by-two table has one degree of freedom (if we vary one cell, we have to adjust the other one automatically to keep the marginal sum constant).
However, there are 41 data points in our sample, so the rank positions will range from 1 to 41, and there are only 10 rank values in our annotation scheme for Animacy.
Prediction: The mean Length (in "number of words") of modifiers of the s-possessive should be smaller than that of the modifiers of the of-possessive.
Finally, note that, again, the significance level does not tell us anything about the size of the effect, so we should calculate an effect size separately.
The most widely-used effect size for data analyzed with a t-test is Cohen's d, also referred to as the standardized mean difference.
The dependent variables were more complex: the cardinal variable Length obviously has a potentially infinite number of values and the ordinal variable Animacy was treated as having ten values in our annotation scheme.
In the preceding chapter, 𝜙 was introduced as an effect size for two-by-two tables (see 7).
For example, we may find that the independent variable under investigation has a statistically significant influence on our dependent variable, but that the effect size is very small, suggesting that the distribution of the phenomenon in our sample is conditioned by more than one influencing factor.
Even if we are pursuing a well-motivated bivariate research design and find a significant influence with a strong effect size, it may be useful to take additional potential influencing factors into account: since corpus data are typically unbalanced, there may be hidden correlations between the variable under investigation and other variables, that distort the distribution of the phenomenon in a way that suggests a significant influence where no such influence actually exists.
The difference is highly significant, although the effect size is rather weak (𝜒 2 = 773.55, df = 1, 𝑝 < 0.001, 𝜙 = 0.1061).
This overrepresentation of young women and old men is not limited to our sample, but characterizes the spoken part of the BNC in general, which should intrigue feminists and psychoanalysts; for us, it suffices to know that the asymmetries in our sample are highly significant, with an effect size larger than that of that in the preceding two tables (𝜒 2 = 2142.72, df = 1, 𝑝 < 0.001, 𝜙 = 0.1765).
More generally, the danger of bivariate designs is that a variable we have chosen for investigation is correlated with one or more variables ignored in our research design, whose influence thus remains hidden.
There is no logical limit to the number of dimensions, but if we insist on calculating this statistic manually (rather than, more realistically, letting a specialized software package do it for us), then a three-dimensional table is already quite complex to deal with.
In this case, as before, each cell has one degree of freedom and the significance levels have to be adjusted for multiple testing.
However, a closer look will show that studying the co-occurrence of words and/ or word forms is simply a special case of precisely this kind of research program.
First, the co-occurrence of words in a sequence is restricted by grammatical considerations.
Second, the co-occurrence of words is restricted by semantic considerations.
Finally, and related to the issue of world knowledge, the co-occurrence of words is restricted by topical considerations.
Words will occur in sequences that correspond to the contents we are attempting to express, so it is probable that co-occurring content words will come from the same discourse domain.
Some treat co-occurrence as a purely sequential phenomenon defining collocates as words that co-occur more frequently than expected within a given span.
Other researchers treat co-occurrence as a structural phenomenon, i.e., they define collocates as words that co-occur more frequently than expected in two related positions in a particular grammatical structure, for example, the adjective and noun positions in noun phrases of the form [Det Adj N] or the verb and noun position in transitive verb phrases of the form [V [ NP (Det) (Adj) N]].
In principle, there is nothing wrong with exploratory research -on the contrary, it would be unreasonable not to make use of the large amounts of language data and the vast computing power that has become available and accessible over 7.1 Collocates the last thirty years.
Instead, researchers frequently need a way of assessing the strength of the association between two (or more) words, or, put differently, the effect size of their co-occurrence (recall from Chapter 6 that significance and effect size are not the same).
Let us use the adjective-noun sequence good example from the LOB corpus (but horse lovers need not fear, we will return to equine animals and their properties below).
I am not aware of any research using phi as an association measure, and in fact the chi-square statistic itself is not used widely either.
One reason for this is that it dramatically overestimates the effect size and significance of such events, and of rare events in general.
Since collocations are often relatively rare events, this makes the chi-square statistic a bad choice as an association measure.
For example, there are more than 100 collocates in the LOB corpus with a Fisher's exact 𝑝-value that is smaller than the smallest value that a standard-issue computer chip is capable of calculating, and more than 5000 collocates that have 𝑝-values that are smaller than what the standard implementation of Fisher's exact test in the statistical software package R will deliver.
For the time being, this is the p value of Fisher's exact test (if we have the means to calculate it), or G (if we don't, or if we prefer using a widely-accepted association measure).
It would thus have been better to categorize the corpus data according to these properties -in other words, a more strictly deductive approach would have been more promising given the small data set.
This seems like an unnecessarily complicated way of representing the kind of co-occurrence design used in the examples above, but I have chosen it to show that in this case sentences containing a particular word are used as the condition under which the occurrence of another word is 7.2 Case studies investigated -a straightforward application of the general research design that defines quantitative corpus linguistics.
Thus, there is no way of telling whether co-occurrence within the same sentence is something that is typical specifically of antonyms, or whether it is something that characterizes word pairs in other lexical relations, too.
As we saw above, this is easy to remedy by simply determining the exact number of times that the phenomenon in question occurs in a given sample.
Such studies are left as an exercise to the reader -this case study was mainly meant to demonstrate how informal analyses based on the inspection of concordances can be integrated into a more rigorous research design involving quantification and comparison to a set of control data.
Again, instead of Stubbs' original data (which he identifies on the basis of raw frequency of occurrence and only cites selectively), I use data from the BNC and the G test statistic.
In the preceding chapter we saw that while collocation research often takes a sequential approach to co-occurrence, where any word within a given span around a node word is counted as a potential collocate, it is not uncommon to see a structure-sensitive approach that considers only those potential collocates that occur in a particular grammatical position relative to each other -for example, adjectives relative to the nouns they modify or vice versa.
His focus is on the second hypothesis, which he tests on the LOB corpus by, first, identifying all occurrences of both verbs with both complementation patterns and, second, categorizing them according to whether the verb in the complement clause refers to an activity, a process or a state.
The individual cells (i.e., intersections of variables) have one degree of freedom, which means that our critical 𝜒 2 value is 8.20.
To come up with a reliable annotation scheme for this categorization task would be quite a feat.
It also demonstrated that semantic categories can be included in a corpus linguistic design in the form of categorization decisions on the basis of an annotation scheme (in which case, of course, the annotation scheme must be documented in sufficient detail for the study to be replicable), or in the form of lexical items signaling a particular meaning explicitly, such as adverbs of gradualness (in which case we need a corpus large enough to contain a sufficient number of hits including these items).
In contrast, if we go by number of cases, then cases with very little difference in frequency count just as much as cases with a vast difference in frequency.
The case study also shows that word frequency may have effects on grammatical variation, which is interesting from a methodological perspective because not only is corpus linguistics the only way to test for such effects, but corpora are also the only source from which the relevant values for the independent variable can be extracted.
Let us also exclude cases where one or both nouns are in the plural, as we are interested in the influence of the final consonant, and it is unclear whether this refers to the final consonant of the stem or of the word form.
In terms of content, it was meant to demonstrate how phonology can interact with grammatical variation (or, in this case, the absence of variation) and how this can be studied on the basis of corpus data; cf.
Both queries will only find those cases that occur before a punctuation mark signaling a clause boundary (what to include here will depend on the transcription conventions of the corpus, if it is a spoken one).
We can cross-check this by counting the total number of words uttered by male and female speakers in the spoken part of the BNC: there are 5 654 348 words produced by men and 3 825 804 words produced by women, which means that men produce 59.64 percent of the words, which fits our estimate very well.
Note that these corpora are rather small and 30 years is not a long period of time, so we would not necessarily expect results even if the hypothesis were correct that American English reintroduced postpositional notwithstanding in the 20th century (it is likely to be correct, as Berlage shows on a much larger data sample from different sources).
However, like words, they can also be used as representatives of some aspect of the speech community's culture, specifically, a particular culturally defined scenario.
Crucially, the verb slot offers an opportunity to introduce additional information (such as the manner of speaking, as in the examples of manner verbs verbs just mentioned (that often contain evaluations), but also the type of speech act being performed (ask, order, etc.).
It is also easy to find even in an untagged corpus, since it includes (by definition) a passage of direct speech surrounded by quotation marks, a subject that is, in an overwhelming number of cases, a pronoun, and a verb (or verb group) -typically in that order.
In order to study differences in the representation of men and women, we can query the pronouns he and she separately to obtain representative samples of male and female speech act events without any annotation.
When we are interested in the frequency of occurrence of a particular word, it seems obvious that every occurrence of the word counts as an instance.
For example, in order to determine the number of instances of the definite article in the BNC, we construct a query that will retrieve the string the in all combinations of upper and lower case letters, i.e.
Second, we could exclude repetitions and count only the number of instances that are different from each other, for example, we would count King's Cross only the first time we encounter it, disregarding the other 321 occurrences.
Which of the two levels is relevant in the context of a particular research design depends both on the kind of phenomenon we are counting and on our research question.
Again, whether type or token frequency is the more relevant or useful measure depends on the research design, but the issue is more complicated than in the case of words and grammatical structures.
For example, the token frequency of the suffix -icle is higher in the BROWN corpus (269 tokens) than in the LOB corpus (225 tokens).
For mini-, the type-token ratio is much higher: it occurs in 382 different words, so its TTR is 382 /1702 = 0.2244.
In fact, an affix can have a high TTR even if it was never productively used, for example, because speakers 9.1 Quantifying morphological phenomena at some point borrowed a large number of words containing it; this is the case for a number of Romance affixes in English, occurring in words borrowed from Norman French but never (or very rarely) used to coin new words.
The reason for this is that, as mentioned above, type-token ratios and hapax-token ratios are dependent on sample size.
The same is true for HTRs, with the added problem that, under certain circumstances, it will decrease at some point as we keep increasing the sample size: at some point, all possible words will have been used, so unless new words are added to the language, the number of hapaxes will shrink again and finally drop to zero when all existing types have been used at least twice.
As we can see, the TTR and HTR of both affixes behave roughly like that of Jane Austen's vocabulary as a whole as we increase sample size: both of them 9.1 Quantifying morphological phenomena However, note that -ify has a token frequency that is less than half of that of -ise/-ize, so the sample is much smaller: as in the example of lexical richness in Pride and Prejudice, this means that the TTR and the HTR of this smaller sample are exaggerated and our comparisons in Tables 9.4 and 9.5 as well as the accompanying statistics are, in fact, completely meaningless.
The TTR of -ise/-ize based on the random sub-sample is 78 /356 = 0.2191, that of -ify is still 49 /356 = 0.1376; the difference between the two suffixes is much clearer now, and a 𝜒 2 test shows that it is very significant, although the effect size is weak (cf.
The proportion of hapax legomena actually resulting from productive rule application becomes smaller as sample size decreases.
In the case of affixes with low productivity, this will typically add little insight over studies based on dictionaries, but for productive affixes, a corpus analysis will yield more detailed and comprehensive results since corpora will contain spontaneously produced or at least recently created items not (yet) found in dictionaries.
Let us also determine the recall of neologisms from the OED (using the definition "first documented in the 20th century according to the OED"): the OED lists 31 of the 45 neologisms, so the recall is 31 /45 = 0.6889; this is much better than the recall of the corpus-based hapax definition, but it also shows that if we combine 9 Morphology commodify, desertify, extensify, geriatrify corpus data and dictionary data, we can increase coverage substantially even for moderately productive affixes.
In this case, devices are much more frequently referred to as electric and less frequently as electrical than expected, and, as in the LOB corpus, the nouns in the category industry are more frequently referred to as electrical and less frequently as electric than expected (although not significantly so).
As mentioned in Chapter 6, word length (however we measure it) rarely follows a normal distribution, so the U test would probably be the better choice in this case, but let us use the 𝑡-test for the sake of practice (the data are there in full, if you want to calculate a U test).
There are 373 stem types occurring with -ic in the LOB corpus, with a mean length of 7.32 and a sample variance of 5.72; there are 153 stem types occurring with -ical, with a mean length of 6.60 and a sample variance of 4.57.
Instead, we need to look at the type-token ratio and the hapax-token ratio.
However, the notion "hapax" is only an operational definition for neologisms, based on the hope that the number of hapaxes in a corpus (or sub-corpus) is somehow indicative of the number of productive coinages.
However, the type-based differences do not have a very impressive effect size in our design and they are unstable across conditions in Säily's, so perhaps they are simply not very substantial.
There is a substantial body of corpus-linguistic research based on designs that combine the two inherently represented variables Word (Form) and Text; such designs may be concerned with the occurrence of words in individual texts, or, more typically, with the occurrence of words in clusters of texts belonging to the same language variety (defined by topic, genre, function, etc.).
In other words, the corpus-linguistic identification of keywords is analogous to the identification of differential collocates, except that it analyses the association of a word W to a particular text (or collection of texts) T in comparison to the language as a whole (as represented by the reference corpus, which is typically a large, balanced corpus).
When applied to text categories, the aim is typically to identify general lexical and/or grammatical properties of the language variety represented by the text categories.
Note that the occurrence of some tokens (such as the dates and the parentheses) may be characteristic of a language variety rather than an individual text, a point we will return to below.
But keyword analysis reveals its true potential when we apply it to clusters of texts, as in the case studies in the next section.
Even more interestingly, keyword analysis can reveal function words that are characteristic for a particular language variety and thus give us potential insights into grammatical structures that may be typical for it; for example, is, the and of are among the most significant keywords of Scientific English.
This (and other observations made on the basis of keyword analysis) would of course have to be followed up by more detailed analyses of the function these words serve -but keyword analysis tells us what words are likely to be interesting to investigate.
Not all of them will be relevant to a particular research design, and some of them are fundamental problems for any research design and must be dealt with before we can proceed.
The with an uppercase T does not occur in the tagged LOB corpus, because case is normalized such that only proper names are capitalized.
But the remainder of the keywords is now representative of the kinds of differences a dialectal keyword analysis will typically uncover.
There are also personal names that differ across corpora; for example, the name Macmillan occurs 63 times in the LOB corpus but only once in BROWN; this is because in 1961, Harold Macmillan was the British Prime Minister and thus Brits had more reason to mention the name.
They then investigate the importance of different areas of life for the cultures involved (where the importance of an area is operationalized as "having a large number of words from the corresponding semantic field among the differential keywords").
For example, they note that there are obvious differences between the types of sports whose vocabulary differentiates between the two corpora (baseball is associated with the BROWN corpus, cricket and rugby with the LOB corpus), reflecting the importance of these sports in the two cultures, but also that general sports vocabulary (athletic, ball, playing, victory) is more often associated with the BROWN corpus, suggesting a greater overall importance of sports in 1961 US-American culture.
Note that such variables may be nominal (sex), or ordinal (age, income, education); however, even potentially ordinal variables are treated as nominal in keyword-based studies, since keyword analysis cannot straightforwardly deal with ordinal data (although it could, in principle, be adapted to do so).
This should be no surprise, of course, since keyword analysis was originally invented to uncover precisely such differences in content.
Of course, the fact that our prediction is borne out does not mean that the hypothesis about 10.2 Case studies This case study has demonstrated that keyword analysis can be used to investigate ideological differences through linguistic differences.
One issue that needs consideration is whether in the context of a specific research design it is more appropriate to compare two texts potentially representing different ideologies directly to each other, as Rayson does, or whether it is more appropriate to compare each of the two texts to a large reference corpus, as the usual procedure in keyword analysis would be.
In the first case, the focus will necessarily be on differences, as similarities are removed from the analysis by virtue of the fact that they will not be statistically significant -we could call 10 Text this procedure differential keyword analysis.
As we saw in Chapter 8, the difficulty of accessing corpora at levels of linguistic representation other than the word form is problematic where our aim is to investigate grammar in its own right, but since grammatical structures tend to be associated with particular words and/or morphemes, these difficulties can be overcome to some extent.
If metaphor were indeed located at the word level, it should be straightforwardly amenable to corpus-linguistic analysis.
Deignan does not explicitly present an annotation scheme, but she presents dictionary-like definitions of her categories and extensive examples of her categorization decisions that, taken together, serve the same function.
Her categories differ in number (between four and ten) and semantic granularity across the four words, let us design a stricter annotation scheme with a minimal number of categories.
This case study demonstrates the use of corpus data to evaluate claims about conceptual structure.
Deignan's design thus has two nominal variables: Word Form of Flame (with the variables singular and plural) and Connotation of Metaphor (with the values positive and negative.
She does not provide an annotation scheme for categorizing the metaphorical expressions, but she provides a set of examples that are intuitively quite plausible.
Let us compare British English (the LOB corpus) and Indian English (the Kolhapur corpus constructed along the same categories) instead.
In other cases, it depends on our judgment (which we have to defend within a given research design) whether a hit constitutes a metaphorical pattern.
You might want to think about these and other cases in the concordance, to get a sense of the kind of annotation scheme you would need to make such decisions on a principled, replicable basis.
Instead of searching for co-occurrence in a span, let us construct a set of structured queries that would find metaphorical patterns instantiating a given metaphor.
This case study demonstrates that central metaphors for a given target domain can be identified by applying a keyword analysis to a specialized corpus of texts from that domain.
If it is language use, as it usually is in historically or sociolinguistically oriented studies, the distance is relatively short, requiring the researcher to discover the systematicity behind the usage patterns observed in the data.
This view is explicitly taken in language acquisition research conducted within the Usage-Based Model (e.g.
Finally, in usage-based models as well as in models of language in general, corpora can be treated as models (or operationalizations) of the typical linguistic output of the members of a speech community, i.e.
Even under this view, corpus data remain one of the best sources of linguistic data we haveone that can only keep growing, providing us with ever deeper insights into the leaky, intricate, ever-changing signature activity of our species.
Furthermore, corpus linguistics has played a crucial role in the advancement of natural language processing (NLP) technologies.
This has led to significant progress in areas such as machine translation, information retrieval, and other NLP applications.
To summarize, the text-corpus method is an immensely powerful tool in linguistic research that enables researchers to gain valuable insights into the rules and patterns of a language by analyzing a vast collection of texts.
Its applications range from studying language variation and change to the development of NLP technologies.
One notable milestone in the field of corpus linguistics was the publication of "Computational Analysis of Present-Day American English" in 1967 by Kučera and Francis.
Kučera and Francis utilized computational analyses along with elements from linguistics, language teaching, psychology, statistics, and sociology to create a comprehensive and diverse body of work.
This is a valid question because, given the upsurge of studies using corpus data in linguistics, there are also already quite a few very good introductions available.
For example, if you want to generate a frequency list of a corpus or a concordance of a word in a corpus with R, you must write a small script or a little bit of code in a programming language, which is the technical way of saying you write lines of text that are instructions to R. If you do not need pretty output, this script may consist of just a few lines, but it will often also be longer than that.
This way, the actual effort of generating a frequency list, a collocate display, a dispersion plot, etc.
In fact, R may even be faster than competing applications: For example, some concordance programs read in the corpus files once before they are processed and then again for performing the actual task -R requires only one pass and may, therefore, outperform some competitors in terms of processing time.
Consider a very simple example: R has a function called table that generates a frequency table.
While this is no problem with a one-dimensional frequency list, this is much harder with multidimensional frequency tables: Perl's arrays of arrays or hashes of arrays etc.
I believe learning one environment can be sufficiently hard for beginners, and therefore recommend using the more comprehensive environment with the greater number of simpler functions, which to me clearly is R. And, once you have mastered the fundamentals of R and face situations in which you need maximal computational power, switching to Perl or Python in a limited number of cases will be easier for you anyway, especially since much of the programming languages' syntaxes is similar and the regular expressions used in this book are all Perl compatible.
Somewhat polemically speaking, being able to enter a URL and type in a search word shouldn't make you a corpus linguist.
For instance, with R you can readily use Chapter 3 introduces the fundamentals of R, covering a variety of functions from different domains, but the area which receives most consideration is that of text processing.
The main chapter of this edition, Chapter 5, is brand new and, in a sense, brings it all together: More than 30 case studies in 27 sections illustrate various aspects of how the methods introduced in Chapters 3 and 4 can be applied to corpus data.
Using a variety of different kinds of corpora, corpus-derived data, and other data, you will learn in detail how to write your own programs in R for corpus-linguistic analyses, text processing, and some statistical analysis and visualization in detailed step-by-step instructions.
This is particularly important because the code file from the companion website contains more than 6,500 lines of code and a huge amount of extra commentary to help you understand the code much better than you can understand it from just reading the book; this is particularly relevant for Chapter 5!
This does not mean, however, that corpus linguists only deal with raw text files -quite the contrary: some corpora are shipped with sophisticated retrieval software that makes it possible to look for precisely defined lexical, syntactic, or other patterns.
It does mean, however, that you would have a hard time finding corpora on paper, in the form of punch cards or digitally in HTML or Microsoft Word document formats; the probably most widely used format consists of text files with a Unicode UTF-8 encoding and XML annotation.
Those would be databases of texts which • may not have been produced in a natural setting; • have often not been compiled for the purposes of linguistic analysis; and • have often not been intended to be representative and/or balanced with respect to a particular linguistic variety or speech community.
On the other hand, the perception of errors is contingent on the acuity of the researcher while, with corpus research, the corpus compilation would not be contingent on a particular person's perceptual skills.
Yet another distinction is that between monolingual corpora and parallel corpora.
Static corpora have a fixed size (e.g., the Brown corpus, the LOB corpus, the British National Corpus), whereas dynamic corpora do not since they may be constantly extended with new material (e.g., the Bank of English).
The final distinction I would like to mention at least briefly involves the encoding of the corpus files.
However, the number of corpora for many more languages has been increasing steadily, and given the large number of characters that writing systems such as Chinese have, this is not a practical approach.
However, in the interest of overcoming compatibility problems that arose due to how different languages used different character encodings, the field of corpus linguistics has been moving towards using only one unified (i.e., not language-specific) multilingual character encoding in the form of Unicode (most notably UTF-8).
This development is in tandem with the move toward XML corpus annotation and, more generally, UTF-8 becoming the most widely used character encoding on the internet.
It is up to the researcher to interpret these frequencies of occurrence and co-occurrence in meaningful or functional terms.
On a very general level, the frequency information a corpus offers is exploited in four different ways, which will be the subject of this chapter: frequency lists (Section 2.2), dispersion (Section 2.3), lexical co-occurrence lists/collocations (Section 2.4), and concordances (Section 2.5).
You generate a frequency list when you want to know how often something -usually words -occur in a corpus.
Thus, a frequency list of a corpus is usually a two-column table with all words occurring in the corpus in one column and the frequency with which they occur in the corpus in the other column.
In this parlance, a frequency list lists the types in one column and their token frequencies in the other; often you will find the expression type frequency referring to the number of different types attested in a corpus (or in a 'slot' such as a syntactically defined slot in a grammatical construction).
First, a frequency list may provide the frequencies of all words together with the words with their letters reversed.
Another area of interest is, for example, spelling error correction, where frequency lists can be useful in two ways: First, for the computer to recognize that a string is perhaps a typo because it neither occurs in a gold-standard frequency list of words of the input language nor in, say, a list of named entities; second, for the computer to rank suggestions for correction such that the computer first determines a set of words that are sufficiently similar to the user's input and then ranks them according to their similarity to the input and their frequency.
From a methodological perspective, frequency lists are useful for computing many co-occurrence statistics.
In this section we will restrict ourselves to collocations because they are a natural extension of frequency lists; later in the book, we will of course deal with other kinds of co-occurrence, too.
On the one hand, it is obvious that collocate displays usually provide information on lexical co-occurrence, but the number of grammatical features that is amenable to an investigation by means of collocates alone is limited.
On the other hand, even the investigation of lexical co-occurrence by means of collocate displays can be problematic.
These observations are a subset of Zipf's laws, the most famous of which states that the frequency of any type is approximately proportional to its rank in a frequency list, and such a distribution is often referred to as a Zipfian distribution.
Some programs output only a user-specified number of occurrences, usually either the first n occurrences in the corpus or n randomly chosen occurrences from the corpus.
As mentioned above, I very strongly recommend that you read this book while sitting at your computer with the relevant code file open in RStudio -in fact, you should go to <_qclwr2/_scripts> and double-click on <03-04_allcode.r> right now, which will also make <_qclwr2/_scripts> your current working directory in RStudio, which nearly all of the code below presupposes -so that you can follow along more easily.
Ideally, you would read the book and run the code I am discussing (by pressing CTRL + ENTER whenever I discuss a line of code so you see how it is executed in RStudio and what it returns); also, you can of course add your own notes to the R code files directly (ideally always preceded by the pound/hash sign # -see below for why).
Thus, this book is not a general introduction to R, and while I aim at enabling you to perform a multitude of tasks with R, I advise you to also consult the additional references mentioned below and the comprehensive documentation that comes with R and RStudio; there are also many instructional videos out there, but as far as I can tell they are very heavily biased in the direction of statistical analysis rather than text processing.
Input to R is usually given in blocks of code as below, where "•" means a space character and " ¶" denotes a line break (i.e., an instruction for you to press ENTER).
Data structures can be entered into R at the prompt, but the more complex a data structure becomes, the more likely it becomes that you read it from a file, and this is in fact what you will do most often in this book: reading in text files or tables, processing text files, performing computations on tables, and saving texts or tabular outputs into text files.
Third, as you could see when you executed the lines of the code file in RStudio and as you now see again, R ignores everything after a #, so you can use this to comment your lines when you write small scripts and want to tell/remind yourself what a particular line is doing.
When you want to assign some content to some data structure for later use, you must use the assignment operator <-(a less-than sign and a minus, which together look like an arrow).
As a result of assignment, the content resulting from the function is available in the data structure just defined.
For example, this is how you store the square root of 5 into the kind of data structure you will get to know as a vector, which is here named aa.
If that data structure x is a vector of one number only, then R outputs a random ordering of the numbers from 1 to x; if x is a vector of two or more elements, then R outputs a random ordering of the elements of that vector.
While this may ultimately boil down to a matter of personal preferences, I recommend using more explicit code at the beginning in order to be maximally aware of the options your R code uses.
Instead of providing a path by entering it there directly, you can also use file.choose() as the first argument, in which case R will prompt you with an Explorer/File Manager window so you can click your way to the desired file; once you choose a file, R will return the path to that file as a character string and, thus, to scan.
Thus, if you want to read a text file into a vector such that each line is one element of the vector, you write sep="\n", which is what we will do nearly all of the time.
This is an important issue since corpus files these days will typically be in Unicode encoding, specifically UTF-8.
The simplest ways to get a glimpse of what any data structure looks like are the functions head and tail.
These take the name of a data structure as one argument and return the six first (for head) or last (for tail) elements of a data structure; if you want a number of elements other than six, you can provide that as a second argument to head and tail.
The first argument of %in% is a data structure (usually a vector) with the elements to be matched; the second is a data structure (also usually a vector) with the elements to be matched against.
The output is a logical vector of the length of the first argument with TRUEs and FALSEs for the elements of the first data structure that are found and that are not found in the second data structure respectively.
This data structure, which basically corresponds to a two-dimensional matrix, will be illustrated in this section.
The first step is to save that file as a raw text file.
Then, you enter a file name, confirm "Automatic file name extension" and confirm you want to save the data into a text CSV format file, if prompted to do so.
Lists are a much more versatile data structure which can in turn contain various different data structures within them.
This way, you get each element as the kind of data structure you entered into the list, namely as a vector, a data frame, and a vector respectively.
For example, we have used the data frame a.dataframe in this section, and above we introduced subset to access parts of a data frame that share a set of variable values or levels.
While R offers several different ways to use loops, I will only introduce one of them here in a bit more detail, namely for-loops, and leave while and repeatloops for you to explore on your own (I do provide one example for each in the code file, though).
Since you specify a sequence of iterations, for-loops are most useful when you must perform a particular operation a known number of times.
However, sometimes you do not know the number of times an operation has to be repeated.
For example, sometimes the number of times something has to be done may depend on a particular criterion, e.g., when your corpus has 200 files but you only want to search those files that contain written data, but you want R to find that out for you from the loaded corpus file's header, meaning you know what will be in the header of a file with written data, but you don't even know which and how many files those are.
The first of the two aspects is concerned with capitalizing on the fact that R's most fundamental data structure is the vector, and that R can apply functions to all elements of a vector at once without having to explicitly loop over all elements of a vector individually.
We have seen above that lists are a very versatile data structure.
The function sapply (and its sister function lapply) apply to the data structure given in the first argument the function called in the second argument; okay, so far nothing new.
For example, what if you wanted to generate a frequency list of a reasonably large corpus such as the 100 millionword BNC, where you do not know the exact number of word tokens and types in advance?
Two possibilities: First and as briefly mentioned above, you can loop over each file, identify all word tokens in it, use table to generate a frequency table of it, and then save that frequency list into a separate file, which means you end up with 4,049 very small frequency list files, none of which will take up a lot of memory on its own.
Then, in a second step, you do a second loop in which you load and amalgamate all 4,049 frequency list files.
This strategy will cost much less time and memory than trying to do it all in one loop, and it is a strategy we will employ a number of times in Section 5.2.8.
It is absolutely imperative that you know exactly what your corpus files look like -both in terms of the corpus data themselves and their annotation -which may require going over corpus documentation or, if you use unannotated texts, over parts of the corpus files themselves, in order to get to know spelling, formatting, annotation, etc.
There is no alternative to knowing your corpora, this cannot be done more easily, and any concordance programs that come with more refined search options also require you to thoroughly consider the format of the corpus files even if their interface 'hides' such decisions behind clickable buttons with smiling corpus linguists on them, in settings, or in .ini files.
If you want to access a part of a character string, you can use substr, which we will always use with three arguments.
First, the character(s) to be replaced; second, the character(s) that are substituted (this needs to have as many characters as the first argument); third, the character vector to which the operation is applied.
If it is executed without invoking its regular expression capabilities, its most basic form requires only two arguments: a one-element character vector to search for and a character vector (or something that can be coerced into a character vector, such as a factor) in which the first argument is to be found.
This list has as many vectors as the data structure that was searched has vectors (recall that behavior from strsplit?
The painful way uses gregexpr's output to retrieve the starting points of all matches in all elements of txt and then also the lengths of all matches to compute their end points, because then, once we have starting points and end points, we can use substr to extract the relevant matches from the original input vector txt.
However, the pain doesn't end here: When we do that -use substr and get the starting and end points from gregexpr, we also have to make sure that we get the first argument of substr right, the input vector: we cannot give substr just txt as its first argument because in the present case we need the first element of txt five times (for the five matches in it and, therefore, the five starting and end points we will get from gregexpr), but we also need the second element of txt six times (for the six matches in it).
First, there is a function in R called regmatches, which can take three arguments: first, a character vector with the input data; second, an object created by gregexpr with match data (usually from the same input data of course); third, invert=FALSE (the default) or TRUE.
The second component is a numeric vector which states in which parts of the input vector there were matches (and how many): Above we can see that both the first and the second part of txt contain two instances if "is".
The third argument is the character vector you want to search and modify.
Let us apply this function to our small character vector txt.
The caret "^" and the "$" specify the beginning and the end of a character string.
Another domain of application of such quantifying expressions would be to clean up text files by, for example, changing all sequences of more than one space to just one space.
If you want to include the minus sign in your character class, you can put it at the first position within the square brackets.
For cases in which you would like to match every character that does not belong to a character class, you can use the caret "^" as the first character within square brackets.
Note how the caret within the definition of a character class in square brackets means something else (namely, "not") than the caret outside of a character class (where it means "at the beginning of a string"); this is one of two regular expression characters that has a different meaning in a certain environment (here, in square brackets) than in another (at the beginning of a string).
As before, changing "\\s" into "\\S" gives you the opposite character class.
The expression "\\b" refers to a word boundary, which is the position between a word character and a non-word character (as defined above and in either order); again "\\B" is the opposite.
The first one applies to a complete search expression, setting all quantifiers in the regular expression to non-greedy matching.
Because the (?u) makes every quantifier lazy, i.e., also the "+" after "\\w" so R stops after one word character.
That is to say, while you have used simple matches as well as more complex matches in terms of what you were searching for, your replacement has so far always been a specific element (a character or a character string).
Imagine, for example, you would like to tag the vector txt such that every word is followed by "<w>" and every punctuation mark is followed by "<p>".
Imagine, for example, your data contain dates in the American-English format, with the month preceding the day (i.e., Christmas would be written as "12/25/2016") and you want to reverse the order of the month and the day so that your data could be merged with corpus files that already use this ordering.
Of course, if you are sure that the only characters in the data used to separate the parts of the date are a period and slash, a character class such as "[./]" would have worked just as well as "\\D".
First, this expression also matches the end of the string if there is no period because it does not require a separate non-word character to match after the rhyming character(s).
The first character after the opening angular bracket of the tag is the tag's name "w" (for "word"), which is mostly followed by a space and a three character POS tag.
Apart from POS tags, other information is also included.
Let us exemplify this by means of a character string that may be found like this in the BNC with SGML annotation: Every word is preceded by a POS tag in angular brackets, just as in the above examples.
Think how this seemingly small decision has potentially big implications: If you generate a frequency list of the BNC using the first approach, then you lose the counts of all multi-word units but every because of, in spite of, out of, etc.
A frequency list generated with the second approach doesn't have that problem, which is why we will go with the second approach here.
The second one is not followed by a space because its characters were the last ones in the vector tag text and, more often, they might not be followed by a space because they are immediately followed by a punctuation mark.
I will treat three ways of creating case-insensitive frequency lists of the words in this file, all of which involve the strategy of splitting up the corpus file using strsplit, but they differ in how we define how you are splitting.
To make it a little more interesting, however, we will first determine which characters are in the file and which of these we might want to use for strsplit, so, how do you find out all characters that are used in the corpus file?
As mentioned above, one difference to the above treatment of ASCII data is that in an American-English locale, you cannot use characters such as "\\b" or "\\W" with perl=TRUE here because what is a word character depends on the locale, and the Cyrillic characters simply are not part of the ASCII character range [a-zA-Z].
However, there is a nice alternative worth knowing, which brings us to the second way to split up the corpus file into words, which is using character points and, more usefully, character ranges.
The alternative involves negative lookaround: You look for the search word, but only if there is no character from the Cyrillic character range in front of it and no character from the Cyrillic character range after it.
One is simply what we did in the previous sections of this chapter: We treat the XML file as a regular flat text file (essentially not paying much attention to the hierarchical structure of the file) and use regular expressions to find exactly what we want.
Many of them, for instance those just targeting words and their tags, could be done with regular expressions, but the kinds of searches exemplified here at the end show how useful some knowledge of how to use R for XML with XPath searches really is because they allow you to combine information from different levels of annotation relatively easily -all of the above is possible even when you treat an XML file as a text file -but trust me, it's painful.
In what follows, I provide a very brief overview of a few additional ways in which the XML package can help you search XML corpus data in sophisticated ways.
Thus, the following line retrieves from H00.file the utterances by the speaker with the id "PS2AD", specifically sentence 162, looks for words whose POS tag is "VERB" and whose lemma is "work", and then recovers the data values of the preceding words (which are siblings of work by virtue of being in the same sentence): See the code file for another similar example.
This is of course useful if your corpus files come in a particular directory structure.
Sometimes, you need to use a data structure, such as a data frame or a list, again and again; as such, you want to save it into a file.
While you can always save data frames as tab-delimited text files with write.table, which are easy to use with other software, sometimes your files may be too large to be opened with other software (in particular spreadsheet software such as Microsoft Excel or LibreOffice Calc).
The option of using raw text files may also not be easily available with lists.
In its simplest syntax, which is the only one we will deal with here (see the documentation for more detailed coverage), you can save any object into a binary compressed file, which will usually be much smaller than the corresponding text file and which, if you gave it the extension ".RData", makes your data available to a new R/RStudio session upon a simple double-click.
Also, you can just save your whole R workspace, not just one data structure, into a user-definable file with save.image, which is useful to store intermediate results for later processing into an.
Let's imagine a scenario in which we want to generate a frequency table of a vector, which we would normally do with table, but we do not want the table sorted alphabetically (table's default) or by frequency (which we would do with sort(table(...)) -we want our table to be sorted by the order of occurrence in the input vector.
It turns out the only data structure required is the input vector, which above was called qwe, and this is because there is only one other data structure we use, which is asd, but asd can be derived from qwe.
Thus, we now can turn to step 3: We take all the code we wrote, but wrap it into a function definition (let's assume we want to call our function table.1stocc, for table sorted by first occurrence), ensure that the user is forced to provide an input vector, and use more revealing names than qwe, asd, and the like.
Thus, check the code file for how sleek a definition of this table.1stocc this operator allows you to write.
For example, the syllabic length of an NP is a numeric variable; other examples are pitch frequencies in hertz, word frequencies in a corpus, number of clauses between two successive occurrences of two ditransitives in a corpus file, and the reaction time toward a stimulus in milliseconds.
On the other hand, if you operationalize Length as number of words, the subject and the object get values of 3 and 4 respectively.
H 0 : The number of cases where the subject is longer (in morphemes) than the direct object is different from the number of cases where the subject is not longer (in morphemes) than the direct object.
Technically speaking, you are introducing a second independent variable, ClauseType, with two levels, main clause and subordinate clause.
For example, your analysis might begin with you writing a script to retrieve corpus data.
Second, every column but the first represents either the data in question (e.g., (parts of) a concordance line to be investigated) or one and only one variable with respect to which the data are coded.
In corpus studies, it is often useful to also have columns for variables that you might call "source of the data"; these would contain information regarding the corpus file and the line of the file where the match was obtained etc.
It is called the probability of error or the p-value -this p is computed on the basis of your data, i.e., computed after the analysis.
Now that the groundwork has been laid, we will look at a few statistical techniques that allow you to compute p-values to determine whether you are allowed to reject the H 0 and, therefore, accept your H 1 .
To that end, one could decide to look at corpus data for the two constructions.
The test that is used to investigate whether an observed frequency distribution deviates from what might be expected on the basis of chance is called the chi-squared test for goodness of fit (because it tests how good the fit of the observed data is to some expected distribution).
Since R already knows what the observed data look like (from the vector Gries.2003), we can immediately use the function chisq.test to compute the chi-squared value and the p-value at the same time by providing chisq.test with three arguments: a vector with the observed data, a vector with the probabilities resulting from H 0 (i.e., two times 0.5, because according to H 0 we have two equally likely constructions), and correct=TRUE or correct=FALSE: If the size of the data set n is small (15 ≤ n ≤ 60), it is sometimes recommended to perform a so-called continuity correction; by calling correct=TRUE you can perform this correction.
One is how strong the effect is: It is important to note that one cannot use the chi-squared value as a measure of effect size, i.e., as an indication of how strong the correlation between the two investigated variables is.
This is due to the fact that the chisquared value is dependent on the effect size, but also on the sample size.
This is of course a disadvantage: While the sample size of this increased table is ten times as large, the relations of the values in the table have of course not changed.
In order to obtain a measure of effect size that is not influenced by the sample size, one can transform the chi-squared value into a measure of correlation.
First, the chi-squared test is actually not the best test to be applied here given the small expected co-occurrence frequency, which is much smaller than five.
Here, both p-values are much smaller than 0.05, which is why we conclude that the difference in the median lengths is most likely not due to chance.
The absolute size of the correlation coefficient, on the other hand, indicates the strength of the correlation.
The correlation between two numeric variables is referred to as Pearson's product-moment correlation r (for regression), but its significance test, too, comes with an assumption that corpus data usually do not meet, namely that the population from which your samples were taken is bivariately normally distributed, which in practice is often approximated by testing whether each variable is distributed normally.
That means you may think that every corpus file contains matches of an expression, but your script still needs to be able to handle cases when a corpus file does not contain a match.
This section explains in plain English which steps the relevant task involves; it uses hardly any R code but already introduces the kinds and names of a few data structures that the script will contain.
This section provides a skeleton of the R code we will use in what is called pseudocode: a description of the algorithm and structure of a program that performs a particular task.
It is imperative that you read this part with the relevant script open in RStudio because in the pseudocode I will provide the line numbers of the relevant R script from the companion website so you can see exactly which lines of code in the script do which part of the pseudocode, or how the pseudocode is 'translated' into actual R code.
Then, DP is computed like this: Simplifying a bit, DP ranges from 0 (a word is perfectly evenly distributed in the corpus, i.e., in accordance with the sizes of the corpus files) to 1 (a word is completely unevenly distributed in the corpus).
Recall from Section 3.6.3, this is a script in which we know the dimensions of the output in advance: If we have three words and 4,049 corpus files, we know, for instance, that the vector corresponding to sizes.of.files.in.words above will need to have 4,049 slots, and we know that the list that collects the three words' frequencies will need three components each with 4,049 slots.
Obviously, we need to be able to define the corpus files we want to search, which means it will be useful to use rchoose.dir to define the directory containing the corpus files; also, we will need to be able to retrieve all the file names from that directory using dir.
We will use scan to load each corpus file in the outer loop, use tolower to make everything case-insensitive, and we will use grep to make sure we only search for words in the part of the corpus file that's not the header.
Once we're done with the loops, we will need to compute the file sizes in percent, which means we need sum to compute the overall corpus size from sizes.of.files.in.words so we can compute each file's size as a fraction of that, and we will need lapply to access each element of freqs.of.words to do the same.
We use lapply to access each element of the list and then apply an anonymous/inline function to it that takes the list element -which you know is a numeric vector -temporarily refers to it as x, and divides each number of that vector x (i.e., each frequency of a word in a corpus file) by the sum of all numbers in x (i.e., the overall frequency of a word in the whole corpus).
We need scan and tolower to load and prepare the corpus file, and we will need strsplit to retrieve the word tokens as well as unlist to turn the list returned by strsplit into a vector word.tokens.
The two final steps are computing DP and plotting the bar plot.
We need scan and tolower to load and prepare the corpus file, and we will need gsub to clean up characters we do not want to consider, and we will need paste and gsub again to create a clean version of a character vector with one element that contains the whole text.
Finally, we will use sort and table to create our frequency list of the three-grams, quantile to explore their distribution, and plot with type="h" again to visualize the results.
We need scan and tolower to load and prepare the corpus file, and we will need strsplit and unlist to split up the corpus file into words (and nzchar to eliminate empty strings) and put them into a vector called textfile.words.
Finally, we will use sort and table to create our frequency list of the three-grams, quantile to explore their distribution, and plot with type="h" again to visualize the results.
However, do note that lines 75-90 provide you with a function word.ngrams that uses that apply(mapply(...)) approach and that you can use whenever you need to create n-grams from a character vector each element of which is a word.
Obviously, we need to be able to define the corpus files we want to search, which means we use rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory.
We will need to define empty character vectors to collect results using character, and we will need to use a for-loop to load each corpus file; during each iteration, we will use grep to find all corpus sentences and then use exact.matches.2 to find all matches of run, runs, walk, and walks.
The solution to this little exercise is in the code file with free-spacing in lines 157-165.
As usual, we use rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory.
We then use the usual for-loop to load each corpus file during each iteration and use grep to find all corpus sentences.
Obviously, we need rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory.
We will need to define three vectors to collect numeric results using integer, and we will need to use a for-loop to load each corpus file.
Finally, we use an alternative to par, layout, to divide up the plotting window into five regions and then plot five scatterplots into them with plot, log, lines(lowess(...)), and abline; we also do a correlation test to see how strongly Fichtner's C is related to the number of verbs/sentence and the number of words/ sentence.
This script does the exact same thing as the one just discussed and most of the code is in fact identical, but it does it not by essentially treating the corpus file as a simple flat sequence of character strings as we did above, but by utilizing the hierarchical XML annotation in ways discussed in Section 3.8.2.
We need read.table and rchoose.files to load the frequency list file, and we need grepl, subsetting, and droplevels to extract the sub-data frames with -ic and with -ical adjectives from it; using droplevels is important: It makes sure that all the adjectives that are not -ic and -ical adjectives don't "stick around" as factor levels and in frequency tables!
To create a downsized version of the frequency table, use apply and min with subsetting.
The next script does something seemingly elementary -we will create a frequency list of word-tag combinations (so as to be able to distinguish run as a noun from run as a verb) -but we will do it on a relatively large data set, the complete BNC World Edition with XML annotation, and we will use the annotation well by utilizing information provided by the BNC's multi-word tags, i.e., tags that mark multi-word units such as because of, in spite of, on behalf of, etc.
Therefore, we will create a subdirectory into which we will save interim output after each corpus file, and after having gone through all 4,049 corpus files we will then merge the 4049 output files into the one data structure we want, results.df.
Once we're done with all 4,049 frequency list files, we use nzchar and subsetting to delete all empty character strings that might remain in our long collector vectors and use tapply to sum up all frequencies of each word-tag combination and save the results as a .csv file with cat and as a data frame (created by strsplitting the words and tags) into an .RData file with save.
As an additional practice assignment, now that this is all done, why don't you try to revise the script so that it uses the corpus files' XML annotation, i.e., try to use the packages XML and/or xml2.
We use a for-loop with scan to load each file and if as well as grepl to check for whether the corpus file contains spoken data.
Also, we know how many different verbs occur after must (440) and what those are, so after creating a collector vector for the cells a + c, we do a second loop over the corpus files where now we determine the frequencies of these verbs after modals in general, not just after must) by looking into this loop's current.mpis.
A for-loop will load each corpus file with scan, we use tolower and grep to get lower-case sentences, and we use just.matches to find our modal verbs plus infinitives as well as grep and gsub to extract the infinitives after must.
We then tabulate the infinitives after must, create a vector freqs.overall (using rep) with a 0 for each infinitive, and enter into a second loop that does everything as before but now has another for-loop in it in which we look for each of the infinitives using a search expression created with paste0; we add the frequency of each infinitive to the relevant slot of freqs.overall and then, after the loop, use data.frame to save the results in the relevant format.
Lines 85 to 88 are then just a compact representation where a table of nouns and adjectives is created, but never stored or used other than as immediate input to chisq.test, and even the result of that main significance test is never used but we immediately jump to the residuals part of that test's output, which we essentially use as an association measure -this may strike you as strange, but recall (1) from Section 5.3.1 that, for instance, an association measure such as MI is based on the ratio of observed and expected frequencies, and (2) from Section 4.2.2 that the residuals of a chi-squared test are, too -a simulation with 1,000 random 2 × 2 tables shows that MI values and the residuals are in fact very highly correlated with each other (adj.
In order to handle such cases, it is useful to define a function that I will here call ranger and that is provided in the code file, which you should explore (in general and for its use of lapply with an anonymous function).
Check out the small example in the code file to see how ranger gives you all possible collocate positions for a corpus vector of a certain length (and pads the remaining positions with NAs so all output is reliably equally long).
We use the function ranger, which uses lapply to apply seq to all numbers in its first argument positions, with seq's starting point being the maximum of desired.min and the smallest starting point of collocates and its end point the minimum of desired.max and the end point of the input vector.
From that, everything till the first underscore is deleted with sub, which makes the remainder of the string begin with the four-digit year of the file name.
We use the usual for-loop with scan and grep to choose and process the files, and then we use exact.matches.2 to retrieve all the infinitives as well as all lemmas for the overall frequency list.
The last step consists of merging the frequency list files: We generate an empty table first, and then use another for-loop to load each frequency list file and merge them into one long table (with c).
We use rchoose.dir and dir to define the corpus files and dir.create to create an output directory.
We use source to load exact.matches.2 and scan to load the text file <_qclwr2/_inputfiles/corp_indexing-1.txt> into R. We use paste to merge all elements into one long character vector and unlist plus strsplit to break it up into a character vector of pages.
We then for- loop over the list of function names and use grep to find them in the character vector with the book pages and store them in a list -note how it helps here that grep only returns one position even if a function name is attested on one page multiple times (because an index entry lists each page on which a word occurs just once, too).
To make this section worth your time, though, most code in this particular code file, <_qclwr2/_ scripts/05_20_celex.r>, will not use the traditional R syntax with nesting of functions, but the %>% operator from the package dplyr (which we then obviously need to load).
Here in the book, I will discuss just a few of them, but study the code file to see what else there is for you to explore.
This is because this exercise is really only one (complicated) regular expression: The goal is to match all kinds of formats of numbers, which is something that can easily come up when you generate frequency lists of (large) corpora and want to avoid having potentially tens of thousands of frequency list entries that are really just different numbers -in such a situation, you would probably want just one entry "_NUM_" or something similar; thus, this is a realistic situation.
After the loop, we'll pick the most frequent n adjectives (something like n = 2,000 for the learner data case study and n = 5,000 for the Brown corpus case study) occurring in it and tag all occurrences of these forms in the untagged corpus files, and then we will retrieve sequences of two adjective tags and whatever they tag from these corpus files; with the ICLE corpus, we will actually save the tagged corpus files before we search them, with the Brown corpus example, we'll tag the files and immediately search them while they are still in memory.
One comment here: We could just use Adam Kilgarriff's frequency list file or the results from the word-tag combination exercise in Section 5.2.8 to get adjectives to tag, and one could just use a tagged version of the ICLE or Brown corpus, but we will pretend we don't have access to any such resources and write a script from scratch just so you get some more practice that'll help you do these things when those additional resources are really not available; unlike several scripts above, we will write this one again such that it uses the BNC's XML annotation and, thus, the packages XML and xml2.
Alternatively, the y-coordinates of all ten points are the number of unique types you have seen at each position if you've read all tokens from slot 1 to that position, i.e., what is here called type.freqs: For instance, note how the rightmost value is 7, i.e., the type-token ratio of the whole vector (0.7) times the length of the vector (10).
We use a for-loop to scan and tolower the corpus files, and grep to get the sentences.
We use exact.matches.2 to find hyphenated forms and sub to clean away the tags and spaces; then we apply table to create a frequency list and then save it.
In the second part, we use character and numeric to create empty collector vectors to merge the hyphenated forms and their frequencies from all over the corpus, a for-loop to load each frequency list file, and, if there are hyphenated forms in the file, we merge them with subsetting, incrementing the vector counter on each iteration (as in Section 5.2.8).
Then we do a second for-loop over the corpus files, this time using lapply with exact.matches.2 to look for all non-hyphenated equivalents to the most frequent hyphenated forms -see how nicely that avoids another loop?
It involves retrieving all words from one or more texts produced by learners and classifying all words as belonging to one of several word families and word frequency bins.
As we turn to the Wikipedia entries, we scan them, strsplit them into words, and create a sorted frequency list.
We then use match to assign base word list numbers and family words to each word type of each Wikipedia entry; the more challenging part of summing the family frequencies and assigning them to each relevant type is done with tapply(...,•...,•sum) as so often before, and then we again use match and subsetting for the assignment part.
Here's the example I use to teach that difference: made-up frequencies of the words horrible, horrifying, and horrid in the Brown and the LOB corpus (lines 222-230 in the code file): The first chi-squared test tests the first scenario, the second chi-squared test tests the second, because it determines whether the frequencies in the Brown corpus differ from those expected from LOB, but not also vice versa.
Note how the two tests return very different p-values so it's important you understand that these two are different hypothesis tests.
After dealing with unannotated files, lots of XML examples, a bit of SGML, tabular versions of the COCA/COHA corpora, the Brown and the ICE-GB formats, we are now turning to the CHAT format that is widely used in language acquisition corpora, but also others.
After that we use rchoose.files to define Eve's corpus files and the function vector (with and without mode="list") to define collector structures.
We then load each corpus file with scan, merge it with paste(...,•collapse=...), gsub line-initial spaces away and replace them by spaces, strsplit the file apart again, unlist, and do further cleaning with multiple gsubs, before we strsplit up all utterances into words and retain (with grep) only those with at least one word character.
After that we store the results we obtain by sapplying functions to the list to count lengths and compute type-token ratios (with an anonymous/inline function); we compute a mean type-token ratio using all types and tokens, and a mean of all lengths of utterances with mean, but to avoid the effect that outliers might have we use the argument trim=0.05 to discard both the smallest and the largest 5 percent of the data.
We then sapply the function shapiro.test to all utterance lengths of each file and subset their p-values to see that, surprise, surprise .
We then use two nested for-loops to compute wilcox.tests on every combination of elements of lus, and store their negative log 10 p-values in a large 20 × 20 matrix called tests (20 because we have 20 corpus files).
We use lines to add the type-token ratio plot and add smoothers with lines(lowess(...)).
Most aspects of this script in terms of loops and text processing were not particularly difficult -there were many operations in the loop, but simple ones.
The book surveys the corpus linguistics discipline, providing a brief overview of 25 years of corpus linguistics studies, including descriptive corpus studies of syntax and semantics, as well as second language acquisition with specialized corpora.
In the field of corpus linguistics and its neighboring field of language education, only a few attempts have been made to explore a comprehensive and bird's eye view of the interaction among published research articles on the subject.
Furthermore, the analysis of the most-cited publications was expected to identify influential and leading works within the given time span.
Computational Linguistics was ranked at the top in the 1997-2001 time span, but the number of citations did not grow much; the ranking of the journal declined, and eventually it dropped off the list in the recent time span.
Furthermore, some journals that appeared in the middle of the time span became actively cited by the corpus linguistics papers since then.
Therefore, based on the journals most cited by corpus linguistics papers, corpus linguistics studies not only refer to general linguistic studies, but also specialized journal papers for new areas of language education, cognitive linguistics, and corpus linguistics itself.
This trend likely stemmed from the publication of grammar references based on usage-based explanations or empirical corpus data, such as Introduction to Functional Grammar and Longman Grammar of Spoken and Written English.
These introductory publications advanced the usage of the corpus for language teaching.
Thus, during the last two decades, in addition to general linguistics research being consistently referenced, specialized academic journals on corpus linguistics, cognitive linguistics, pragmatics, sociolinguistics, and language education have also been actively cited.
Co-citation patterns in each time span also revealed significantly clustered themes as well as publications within the clusters.
During that time, corpora enabled researchers to conduct studies on grammar using corpus-based parsers, such as the Penn Treebank and its parts of speech tags.
Consequently, researchers were able to computationally build corpus data, focusing primarily on grammatical or semantic aspects in discourses.
Moreover, these corpus practitioners started to compile their own personal corpora for analyzing and investigating linguistic features and examples of expressions by using corpus analysis software packages.
However, over the last few years, many researchers have also realized that generalized linear models or their extension to generalized linear mixed-effects models can run into problems especially when applied to observational data such as corpus data.
The classification tree resulting from the data can be interpreted as follows: Starting from the top, if the subordinate clause type is causal, go left and "predict" mc-sc; if the subordinate clause type is temporal, then go right and check the length difference of the main and the subordinate clause: if that difference is < -2.5, go left and also "predict" mc-sc, otherwise go right and check whether the conjunction is before.
In other words, it can happen that that single number in a regression modela slope of a numeric predictoris recoverable from a tree only by piecing together three or even more splits in different locations in a classification tree.
In this case, because of the lack of a marginally detectable main effect, none of the variables may be selected in the first split of a classification tree, and the interaction may never be discovered.
Of course, corpus data are usuallyhopefullya bit larger than the above data (and see Section 2.2 below for a larger sample size) and they do not usually exhibit the kind of complete separation shown above.
However, the Zipfian distribution that corpus data often exhibit makes it quite likely that some categorical predictors have highly frequent levels whose association to the response variable may overpower other (combinations of) predictors.
As we will see, the new sample size leads to considerable changes.
In other words, partial dependence scores would not provide the desired results in any of the above applicationswhat we would ideally like to see is (1) a variable importance score that is very small for P1l and (2) some indications that P2l and P3l on their own do not do much, but that, together, they do a lot.
Such a splitting mechanism can break any hidden structure and avoid inconsistency by forcing splits on strong variables even if they do not show any marginal effect; second, progressively muting noise variables as we go deeper down a tree so that even as the sample size decreases rapidly towards a terminal node, the strong variable(s) can still be properly identified from the reduced space; third, the proposed method enables linear combination splitting rules at very little extra computational cost.
In either case, one might use a forward model selection process, i.e.
Treebased methods have become a welcome alternative for data sets that defy regression-based methods especially in noisy and unbalanced corpus data, and that, in and of itself, is potentially a good thing.
This paper can obviously only stimulate discussion rather than settle the matter(s) at handin fact, it seems every single aspect of random forests is currently being lively discussed in bioinformatics journals: sampling of data (with or without replacement), splitting criteria (Gini vs. p-values), variable importance measures (error rate vs. permutation-based versus AUC [the latter two conditional or unconditional]), variable selection, whether random forests can capture or detect interactions in the presence of correlated predictors, imbalanced response variables, etc., all of which affect the (quality of the) results ….
In branches of linguistics concerned with chronological, geographical, and social language variation, text takes the form of collections of spoken and / or written language, or corpora.
The use of corpora in Western linguistics began in the late eighteenth century with the postulation of an Indo-European protolanguage and its reconstruction based on examination of numerous living languages and of historical written documents; after two centuries of development the importance of corpora in linguistics research has increased to the extent that a subdiscipline has come into being, corpus linguistics, whose remit is to develop methodologies for creating collections of natural language speech and text, abstracting data from them, and analysing that data with the aim of generating or testing hypotheses about the structure of language and its use in the world.
These go under a variety of names such as informatics, information science, information retrieval, text summarization, data mining, and natural language processing.
An increasingly important class of these concepts and methods is cluster analysis, which is used across a broad range of sciences for hypothesis generation based on identification of structure in data which are too large or complex, or both, to be interpretable by direct inspection.
The aim of the present discussion is to show how cluster analysis can be used for corpusbased hypothesis generation in linguistics by applying it to a case study of a dialect corpus, the Diachronic Electronic Corpus of Tyneside English , and thereby to contribute to the development of an empirically-based quantitative methodology for hypothesis generation in the linguistics community.
Understanding of these concepts and associated formalisms is, however, a prerequisite for informed application of cluster analysis, and so introductory-level explanations of them are provided.
The second chapter motivates the use of of cluster analysis for hypothesis generation in linguistics.
The third deals with data creation: the nature of data, its abstraction from text corpora, its representation in a mathematical format suitable for cluster analysis, and transformation of that representation so as to optimize its interpretability.
The fourth chapter describes a range of cluster analysis methods and exemplifies their application to data created in Chapter 3.
The fifth shows how these methods can serve as the basis for generation of linguistic hypotheses, and the sixth reviews existing applications of cluster analysis in corpus linguistics.
In addressing methodological issues in linguistic hypothesis generation using mathematically-based data creation and cluster analysis methods, the author's belief is that it satisfies this remit.
As noted in the Introduction, cluster analysis is a tool for hypothesis generation.
The number of times each speaker uses the phonetic variable or variables of interest is recorded, thereby building up a body of data.
A reasonable hypothesis based on this finding would be that there is systematic phonetic variation in the Tyneside speech community, and more specifically that the speakers who constitute that community fall into two main groups.
A text corpus is not the linguist's data -measurements of such things as lexical frequency are.
The first section deals with data creation, the second presents a geometrical interpretation of data on which all subsequent discussion is based, and the third describes several ways of transforming data prior to cluster analysis in terms of that geometrical interpretation.
The interviews record the language use of a variety of local informants from a range of social groups and extend the geographical domain covered in the earlier collections to include other parts of the North East of England.
Another, and the one adopted here, is to use quantitative values which represent the number of times the speaker uses each of the phonetic segments.
These are stringent requirements: most datasets large enough to have cluster analysis usefully applied to them probably contain error, known as 'noise', to greater or lesser degrees.
If one now defines a selection criterion, say 'loves', then the subset of pairs which satisfy the criterion constitute the relation: it is the set of all pairs of people one of whom loves the other.
In a vector space, a relation defined on an n-fold Cartesian product is a subset of vectors in the space.
A metric space M(V, d) is a vector space V on which a metric d is defined in terms of which the distance between any two points in the space can be measured.
In sociolinguistics, for example, speakers might be described by a set of variables one of which represents the frequency of occurrence of some phonetic segment in interviews, another one speaker age, and a third income.
But cluster analysis methods don't have common sense.
Such normalization is an important issue in Information Retrieval because, without it, longer documents in general have a higher probability of retrieval than shorter ones relative to any given query.
In the relevant information retrieval and data mining literature, proximity between vectors in a space is articulated as the 'nearest neighbour' problem: given a set V of ndimensional vectors and an n-dimensional vector w not in V , find the vector v in V that w is closest to in the vector space.
The remainder of this section addresses the latter alternative by presenting a range of dimensionality reduction methods.
Given the importance of dimensionality reduction in data processing generally, there is an extensive literature on it and that literature proposes numerous reduction methods.
The methods for dimensionality reduction are of two broad types.
In the relevant machine learning, artificial intelligence, and cognitive science literatures these approaches to dimensionality reduction are called 'feature selection' and 'feature extraction', but the present discussion has so far used the more generic term 'variable' for what these disciplines call features, and will continue to do so.
The dimensionality of data can be reduced by retaining variables which are important and eliminating those which are not, relative to some criterion of importance; for data in vector space format, this corresponds to eliminating the columns representing unimportant variables from the data matrix.
This part of the discussion briefly introduces the Poisson distribution, then shows how the variance-to-mean ratio relates to it, and finally describes the application of the ratio to dimensionality reduction.
The vmr can be used for dimensionality reduction as follows; a document collection D containing m documents is assumed.
Its application to dimensionality reduction is analogous to that of the methods already presented: the columns of the data matrix are sorted in descending order of t fid f magnitude, the t fid f values are plotted, the plot is used to select a suitable threshold k, and all the columns below that threshold are eliminated.
Use of t fid f for dimensionality reduction therefore runs the risk of eliminating distributionally-important variables on account of the definition of clumpiness on which it is based.
All the methods for dimensionality reduction presented so far, from frequency through to t fid f , suffer two general problems.
Where the variables are lexical, however, there is additional scope for dimensionality reduction via stemming and elimination of so-called stop-words.
Where there is such redundancy, dimensionality reduction can be achieved by eliminating the repetition of information which redundancy implies, and more specifically by replacing the researcher-selected variables with a smaller number of non-redundant variables that describe the domain as well as, or almost as well as, the originals.
Slightly more formally, given an n-dimensional data matrix, dimensionality reduction by variable extraction assumes that the data can be described, with tolerable loss of information, by a manifold in a vector space whose dimensionality is lower than that of the data, and proposes ways of identifying that manifold.
Statistics provides various measures of association, the most often used of which, Pearson's product-moment correlation coefficient, or 'Pearson's correlation coefficient' for short, is described below.
To understand Pearson's correlation coefficient, one first has to understand the concept of covariance between any two variables x and y, which is a measure of the degree to which there is a linear relationship between the values taken at successive observations in the time sequence t 1 ,t 2 .
Because it is the standard dimensionality reduction method, PCA is described in greater or lesser degrees of detail and clarity by most publications in the field.
It does this by projecting the ndimensional data reduction into a k-dimensional vector space, where k < n and closer than n to the data's intrinsic dimensionality.
To achieve dimensionality reduction, a way has to be found of eliminating any basis vectors that lie along relatively insignificant directions of variance.
In applications where the aim is simply dimensionality reduction and semantic interpretation of the new variables is not an issue, this doesn't matter.
Because the column vectors of V are an orthonormal basis for D and the values in S are ranked by magnitude, SVD can be used for dimensionality reduction in exactly the same way as PCA.
These synthetic variables may or may not have a meaningful interpretation relative to the research domain that the original variables describe, but there is no explicit or implicit claim that they necessarily do; PCA is simply a means to a dimensionality reduction end.
Where, therefore, the aim is simply dimensionality reduction and interpretation of the extracted variables is not crucial, PCA is the choice for its simplicity and optimality, but where meaningful interpretation of the extracted variables is important, FA is preferred.
This does not, however, warrant the conclusion that the MDS dimensionality reduction is better than the Sammon.
Given a linear distance matrix D L derived from a data matrix M, Isomap derives a graph-distance approximation to a geodesic distance matrix D G from D L , and D G is then the basis for dimensionality reduction using either the classical or the metric least squares MDS procedure; graph distance approximation to geodesic distance has already been described in the foregoing discussion of data geometry.
The SSE/SST term is therefore the ratio of the variability of the dependent variable relative to the regression model and its total variability relative to its mean.
Otherwise the difference in probabilities can serve as the basis for model selection.
Specifically, dimensionality is reduced to 51 using the variable selection method which combines the frequency, variance, vmr and t fid f selection criteria; variable selection rather than extraction was used for dimensionality reduction because the original variables will be required for hypothesis generation later in the discussion.
Textbook and tutorial discussions of cluster analysis uniformly agree, however, that it is difficult and perhaps impossible to give such a definition, and, if it is possible, that no one has thus far succeeded in formulating it.
In principle, this lack deprives cluster analysis of a secure theoretical foundation.
In practice, the consensus is that there are intuitions which, when implemented in clustering methods, give conceptually useful results, and it is on these intuitions and implementations that contemporary cluster analysis is built.
The fundamental intuition underlying cluster analysis is that data distributions contain clusters when the data objects can be partitioned into groups on the basis of their relative similarity such that the objects in any group are more similar to one another than they are to objects in other groups, given some definition of similarity.
The literature subcategorizes hierarchical and non-hierarchical methods in accordance with the data representation relative to which they are defined: graph-based methods treat data as a graph structure and use concepts from graph theory to define and identify clusters, distributional methods treat data as a mixture of different probability distributions and use concepts from probability theory to identify clusters by decomposing the mixture, and vector space methods treat data as manifolds in a geometric space and use concepts from linear algebra and topology.
The discussion of data creation and transformation in the preceding chapter was based on vector space representation, and to provide continuity with that discussion only vector space clustering methods are included in this chapter.
As such it is interested in the second of the above types of categorization and uses the terms 'clustering' and 'cluster analysis' with respect to it throughout the discussion, avoiding 'classification' altogether to forestall confusion.
An input vector d k ∈ D is selected.
In essence, therefore, the update to the connection vector C i j associated with the most active cell u i j is the current value of C i j plus some proportion of the difference between C i j and the input vector, as determined by the learning rate parameter.
The effect of this is to make the connection vector increasingly similar to the input vector.
Each successive input vector activates the unit in the lattice with which training has associated it together with neighbouring units, though to an incrementally diminishing degree; when all the vectors have been input, there is a pattern of activations on the lattice, and the lattice is the representation of the input manifold in two-dimensional space.
To be useful as an analytical tool, the SOM's representation of data structure has to be unambiguously interpretable on its own merits, and the problem is that an activation lattice like that in the above figures does not contain enough information to permit this in the general case.
When a SOM is used for cluster analysis, inspection of the pattern of activation on the lattice can not only be subjective but can also be based on a misleading assumption.
We have seen that each lattice cell has an associated vector which represents its connections to the input vector.
The foregoing discussion of dimensionality reduction has described linear and nonlinear ways of reducing data of observed dimensionality n to an approximation of its intrinsic dimensionality k, where k is less than n. This assumes that all the data objects are best described using the same number k of latent variables, which is not necessarily the case.
For k-means to have optimized this function, the prototypes have to be placed in the data space so that the Euclidean distances between them and their associated data points is globally minimized across all clusters.
It is easy to see that this is what k-means does: the procedure converges on stable cluster centroids, and a centroid is by definition the minimum distance from the all the points on which it is based.
Use of k-means is not restricted to Euclidean distance, though this is the most frequently used measure.
Relative to the selection criteria for inclusion in this discussion, k-means is a prime candidate: it is intuitively accessible in that the algorithm is easy to understand and its results are easy to interpret, it is theoretically well founded in linear algebra, its effectiveness has repeatedly been empirically demonstrated, and computational implementations of it are widely available.
This means that k-means essentially grows linearly with data size, unlike other clustering methods to be considered in what follows, and is therefore suitable for clustering very large data sets in reasonable time -cf.
The procedure of k-means also has several well known problems, however.
On the one hand, if the value chosen for k is incompatible with the number of clusters in the data, then the result is guaranteed to mislead because k-means will deliver k clusters whatever the actual number of clusters intrinsic to the data, including none.
No general and reliable method for selecting initial parameter values for the number of clusters and placement of prototypes in known, and given the crucial role that these play in determining the k-means result, it is unsurprising that initialization remains a research focus.
As noted, the standard k-means procedure does not guarantee convergence to a global optimum.
Like k-means, Dbscan was selected for inclusion because it is a easy to understand and interpret, is mathematically well founded, has an established user base, and is readily available in software implementations.
It has important advantages over k-means, however.
One is that Dbscan does not require and in fact does not permit prespecification of the number of clusters, but rather infers it from the data; selection of k is one of the main drawbacks of k-means, as we have seen.
As with k-means, selection of suitable parameter values strongly affects the ability of Dbscan to identify the intrinsic data cluster structure.
Where it is known or strongly suspected that the data density structure is linearly separable, the more reliable k-means method should be used, and if the data is non-linearly separable then results from Dbscan should, in view of its initialization and sparsity problems, be corroborated using some other clustering method or methods.
Hierarchical clustering is very widely used, and so is covered in most accounts of cluster analysis, multivariate analysis, and related disciplines like data mining.
Single Linkage, on the other hand, builds clusters solely on the basis of local neighbourhood proximity and without reference to cluster centres; it is to the other kinds of hierarchical clustering, therefore, as Dbscanis to k-means.
As such, the expectation is that the non-Single Linkage group will, like k-means, correctly identify the cluster structure of data when its dense regions are linearly separable but not otherwise, whereas Single Linkage will be able to identify non-linearly separable clusters.
It is, however, a commonplace of the cluster analysis literature that no currently available method is guaranteed to provide this with respect to data in general, and the foregoing discussion of a selection of methods confirms this: projection methods based on dimensionality reduction can lose too much information to be reliable, and the linear ones together with k-means and linear hierarchical methods fail to take account of any nonlinearity in the data; the reliability of the SOM, k-means, and Dbscan depends on correct parameterization; different hierarchical joining criteria can assign data points to different clusters and typically impose different constituency structures on the clusters.
One obvious and often-used approach to validation is to generate a series of results using methods based on different clustering criteria in the hope that they will mutually support one another and converge on a consistent solution: if a range of methods based on dimensionality reduction, topology preservation, proximity, and density give identical or at least compatible results, the intuition is that the reliability of the solution is supported by consensus.
This has some validity, but it is also highly subjective and runs the risk of reinforcing preconceptions and discounting the unexpected and potentially productive insights which are the prime motivation for use of cluster analysis in hypothesis generation, as already noted.
The discussion also noted that different selections of initial parameter values such as the locations of the Voronoi centroids, lattice size and shape, and different sequencings of training data items can generate different cluster results, and this calls the status of the result as a reliable representation of the intrinsic data structure into question.
Validation was until fairly recently the Cinderella of cluster analysis.
The present chapter develops a hypothesis in answer to the research question based on cluster analysis of MDECTE.
In each case k-means for k = 2 partitions the projections into two regions, which are demarcated by dashed lines for convenience of reference.
This is unsurprising because, as we have seen, k-means clusters linearly separable regions of the data space, and the regions of the projections in Figures 5.11-5.15 can be regarded as linearly separable when the distortions introduced by the projection methods are taken into account.
As can be seen, k-means k = 3 differs from k = 2 only in dividing the k = 2 cluster 2 into two subclusters.
Figures 5.17    Using the additional information that the correlation with the Dbscan and k-means provides, the relationships among the various trees becomes clearer.
That chaining keeps all the data points corresponding to those in Dbscan cluster 1 and all the points corresponding to those in Dbscan cluster 2 together, and the rest of the structure is an apparently random mix of remaining points from the k-means clusters.
In each case there are two main clusters corresponding to k-means clusters 1 and 3 which are, respectively, supersets of Dbscan clusters 1 and 2.
The Euclidean distance based Complete, Average, and Ward Linkage trees group a small number of speakers separately from the two main clusters in slightly different ways; these speakers are observable at the periphery of the main data cloud in the projection plots or as outliers to it, and correspond substantially to the smallest of the clusters in the k-means result for k = 3.
Since the aim here is methodological, that is, to exemplify the application of cluster analysis to hypothesis generation, only the first of them is addressed, though clearly the second would also have to be considered if the aim were an exhaustive investigation of the Geordie dialect as represented by DECTE .
This chapter reviews the use of cluster analysis in corpus linguistics to date.
This implic-itly excludes a range of language technologies such as information retrieval, document classification , data mining, and speech processing, as well as areas of artificial intelligence like natural language generation / understanding and machine learning.
These technologies work with natural language text and speech and thereby have much in common methodologically with corpus linguistics, including application to cluster analysis to text corpora; indeed, many of the concepts and techniques presented in the foregoing chapters come from their literatures.
In continental Europe the application of mathematical and statistical concepts and methods to analysis of corpus data for derivation of linguistic laws has continued to be developed.
Variationist linguistics, here understood as the study of how language use varies in chronological, social, and geographical domains, is fundamentally empirical in that it uses data abstracted from observation of language use either to infer hypotheses about patterns of linguistic variation in or to test hypotheses about a language community.
Because they are based on analysis of data abstracted from language use, all three are naturally suited to quantitative and more specifically statistical methods, and, as corpus linguistics has developed in recent years, the corresponding research communities have increasingly adopted it, albeit unevenly.
Since about 2000, language classification has increasingly been associated with cladistic language phylogeny, and cluster analysis is one of a variety of quantitative methods used in cladistic research.
The motivation for doing so was practical: as the size and complexity of corpora and of data abstracted from them have grown, so the traditional paper-based approach to discerning structure in them has become increasingly intractable, and cluster analysis offers a solution.
Hypothesis generation based on cluster analysis has two further advantages in terms of scientific methodology, however.
Cluster analysis and the associated data representation and transformation concepts are objective in the above sense in that they are mathematically grounded, and analyses based on them are replicable as a consequence in that experimental procedures can be precisely and comprehensively specified.
Given the methodological advantages of cluster analysis for hypothesis generation, the hope is that this book will foster its adoption for that purpose in the corpus linguistics community.
In practice, however, two have emerged as the languages of choice for quantitative natural language processing generally: Matlab and R. Both are high-level programming languages in the sense that they provide many of the functions relevant to statistical and mathematical computation as language-native primitives and offer a wide range of excellent graphics facilities for display of results.
One of these libraries is called "Math, Statistics, and Optimization", and it contains a larger range of dimensionality reduction and cluster analysis functions than any of the above software packages: principal component analysis, canonical correlation, factor analysis, singular value decomposition, multidimensional scaling, Sammon's mapping, hierarchical clustering, k-means, self-organizing map, and Gaussian mixture models.
Several such contributed libraries exist for cluster analysis, and these substantially expand the range of available methods.
Over time, the term came to be used for any natural-language dataset used by a linguist -so that, for example, a field linguist might refer to a set of sentences elicited from an informant as their 'corpus'.
The set of methods required to approach the quantitative and qualitative analysis of a collection of language data on a scale far larger than any human being could hope to analyse by hand -together with related areas such as the compilation and annotation of these corpora -constitute the modern field of corpus linguistics.
As a methodology it has certainly been a great success in fields ranging from the history of English to lexicography to language teaching to discourse analysis.
There is no room in this chapter to attempt a comprehensive review of the impact that corpus research has had across the field of linguistics.
Instead, I will provide an overview of the fundamental ideas of corpus linguistics and some key methods and practices -starting with how we approach corpus design and construction, moving on to a summary of the most widely used corpus methods, and finishing with a very brief survey of some applications and advanced forms of analysis.
It is unlikely that such a highly specialised dataset would be available in advance.
That is, there is some large phenomenon that we want to know about -a language, or some specified variety of language, as a whole -and since we cannot look at all the possible text within that language, we must select a sample.
Borrowing terms from statistics, we can talk about the whole of the language variety we are approaching as the population, and the corpus we are using to look at that language variety as the sample.
As Chomsky pointed out, language is in principle non-finite (it is always possible for a speaker of a language to create a new sentence in that language that has never been used before) and it is -of course -impossible to collect a dataset of infinite size!
Only in the case of certain historical data, where the amount of text that has survived from a particular period is finite, can we have a complete corpus of some language or language variety.
But we rarely use the whole text as the unit of analysis.
What does this mean for corpus analysis?
First, we have to be very careful in applying statistics to corpus data that are unproblematic in other fields, precisely because of this issue.
These will typically be collected in the form of plain text files -that is, computer files containing just the actual characters of the text, without any kind of formatting, as found in word-processor files, for instance.
The definitions of (especially) the terms markup and annotation in the context of corpus design and encoding vary quite a lot in the literature; the term tagging is also sometimes used as a synonym for either or both of these terms.
Markup is information added to a corpus to represent features of the original texts other than the running words themselves -for instance, in the case of a written text, features such as the beginning and end points of sentences or paragraphs, or the position of page breaks, or the position and content of elements such as illustrations which would be omitted in a plain text corpus.
Critically, markup, metadata and annotation can be processed by computer, just like the actual words of the corpus texts, and therefore they can be exploited in automated corpus analysis.
In many cases, the analysis is done at the word level, so a single analytic label or tag is assigned to each word of the corpus.
However, analyses at higher or lower levels of linguistic structure can also be represented as corpus annotation.
Let us consider four of the most widely used kinds of corpus annotation -those often pre-encoded in general-purpose corpora prior to their being distributed.
It is the longest-established form of corpus annotation, with the first efforts in the direction of automatic taggers going back as far as the early 1960s, and the first practical, high-accuracy tagging software emerging in the 1980s -although even the best POS taggers have a residual error rate of around 3-5 per cent which can only be removed by painstaking manual post-editing.
The utility of POS tagging, especially for languages like English, is that many words are ambiguous in terms of their part-of-speech.
The underlying principle of semantic tagging is the same as POS tagging, namely, the assignment to every word in a text or corpus of labels representing categories within a scheme of analysis.
Semantic tagging is a harder task for a computer than POS tagging.
For this reason, we must be prepared to work with a higher error rate when analysing the output of a semantic tagger than when working with POS tags.
The other form of corpus annotation that is often applied automatically is parsing, the annotation of syntactic structures and relations.
Unlike the forms of annotation mentioned so far, parsing does not operate strictly at the word level; rather, what is annotated are grammatical phenomena at the phrase, clause and sentence level.
Like semantic tagging, automated parsing has a much higher error rate than POS tagging, because the task is inherently more difficult.
Conversely, tagging can disambiguate the words: break is both a noun and a verb, and broken is both a participle and an adjective -POS tags allow a computer to distinguish the different grammatical functions.
For instance paragraph boundaries might be shown in XML using <p> tags; a <title> tag in the header might contain the original title of a text; and a POS tag on a word could be represented as <w pos='NN1'> (where NN1 is a common tag for a singular common noun).
Indeed, in the pre-computer age this was sometimes done; the earliest concordances were compiled manually for the study of the language of the Bible, and in the early twentieth century word frequency lists were often compiled manually to help inform foreign language teaching.
However, in practice, all techniques for corpus analysis -these two most basic methods, and all the more complex methods built upon them -are nowadays supported by the use of various pieces of corpus analysis software.
Nearly all corpus analysis software permits the generation of a concordance -a listing of all the instances of a word or phrase in the corpus, together with some preceding co-text and some following co-text (usually anything from a handful of words to a couple of sentences).
As this functionality is so common, the term concordancer is an often-used synonym for 'corpus software tool'.
In discussing the corpus frequency of words, it is useful to introduce the distinction between word types and word tokens.
So for instance, if we see on a frequency list for an English corpus that the is the most frequent word, occurring 60,000 times, then we could say that the single type the is represented in this corpus by 60,000 tokens.
We could compare the frequency of the to that of other words on the frequency list, and observe whether it is more or less frequently used than those other words.
We could, moreover, compare the frequency lists of two corpora or sub-corpora, to ascertain whether the contents and ordering of their frequency lists are similar, as a means of contrasting the types of language those corpora represent.
A frequency list is inherently quantitative in nature.
This is most straightforwardly accomplished within the context of a concordance analysis.
At one level, analysis can be more or less impressionistic -based on scanning the eye up and down the concordance lines in an attempt to observe features of note that recur in the concordance, or to identify different functions of the word or phrase that was originally searched for.
A more careful analysis will often attempt to quantify the number of concordance lines that exemplify a given function or contextual feature, and to make sure that every single concordance line has been inspected individually, to identify exhaustively all possible categories and patterns of usage.
The most rigorous form of concordance analysis will systematise the aspects of each example that are considered, building a matrix of different features -grammatical, semantic or pragmatic -and the values these features have for each concordance line.
All this can make a full concordance analysis a major undertaking; in compliance with the principle of total accountability, concordances too long to analyse in full should be reduced (or thinned) randomly to a manageable size.
It is especially important not to look at just the beginning of a concordance, as these early concordance lines may present examples from the early part of the corpus, which is not necessarily representative of the corpus as a whole.
Several difference statistics can be used, but the most common is the log-likelihood test of statistical significance, a procedure similar in principle to the better-known chisquared test.
A key items analysis is performed as follows: first, frequency lists are created for two texts or corpora -either two corpora which we wish to contrast, or a single text of interest and a generic reference corpus.
The frequency list may be of word types, lemmas or any kind of tag -thus, we often talk about keywords and key tags.
Second, the frequencies of each item on the two lists are compared by calculating a log-likelihood score from the two frequencies and the total sizes of the two corpora.
Third, the list is sorted in descending order of log-likelihood score; those items whose frequencies have the most significant differences between the two corpora under comparison will appear at the top of the list.
Key items can be classed as positive (more frequent in the first corpus) or negative (more frequent in the second or reference corpus); both can be of interest, but studies based on keywords tend to focus on the positive items.
There are many different ways to operationalise the notion of co-occurrence.
One way is manual and largely qualitative: to undertake a hand-and-eye analysis of the concordance, as explained above, where the focus of analysis is to identify co-occurring elements, the collocates of the node, in the concordance lines.
The software compiles a list of words that occur in the context (either all words, or words which stand in some particular relationship to the node) and then applies statistical analysis to identify words which are more common in the vicinity of the node than elsewhere in the corpus.
Although we can use collocation as a general covering term for different kinds of cooccurrence phenomena observed in corpora, we can also use a set of more specific terminology, developed largely by John Sinclair and colleagues, to distinguish co-occurrence patterns at different linguistic levels.
In this more specific sense, collocation refers to the co-occurrence of particular word-forms with the node, and three other terms are used to refer to grammatical, semantic or discourse-pragmatic or affective co-occurrence pattern: colligation, semantic preference and semantic prosody.
Colligation refers to a recurrent co-occurrence of some node with a particular grammatical category or structure.
Semantic preference or semantic association refers to a consistent co-occurrence with a set of words which -again, while perhaps not individually significant collocates -are drawn from a recurring semantic field.
As previously noted, corpus techniques have acquired the status of a key methodology applicable to nearly all subfields of language study (perhaps the sole exception being those Chomskyan formalist approaches whose opposition to corpus data is a matter of principle).
Not entirely unrelated to this is the second front of current progress in the field -namely, the continuing development of more refined techniques and tools for corpus analysis.
In this chapter we introduced four basic analytic methods, but a number of more complex approaches, often based on advanced corpus statistics, have been introduced over the years, such as collocation network analysis, advanced multivariate statistics applied to concordance analysis and corpus frequencies, and so on.
For the sixth and last filter, we first calculated the number of words for each tweet, which were split by white spaces to get the number of individual words.
This filtering process affects two dataset outputs which used for different purposes in the corpus.
At the top, we see the bar plot and at the bottom the word cloud.
For the text processing tasks, the use of the UDPipe and tidytext packages have been highly effective.
It has been designed to be used by a diversity of audiences who are interested in exploring linguistic patterns from corpora based on social media language.
Similar tools have been developed with invaluable contributions to the field of Corpus Linguistics.
In a future version, we aim to include analysis on emoticons, as a distinctive component of social media language.
In our times, the ability to quantitatively analyze corpus data has become an integral part of the linguist's toolbox.
Our book is intended as an educational support for students and, in general, for all those wishing to learn the use of corpora in linguistics.
This book has been designed as study material for teaching corpus linguistics at university initiatory phases, as well as a tool for students wishing to be trained in the use of corpora.
As it is an introductory work, this book is necessarily partial and does not deal with all the questions raised by the use of corpora in different linguistic disciplines.
A text corpus literally embodies a set of texts, a collection of a certain number of texts for study.
In the field of language teaching, it is also possible to collect texts written by students having different levels, and to build a corpus of these writings in order to study the typical errors that students produce at different learning stages.
For example, by making a corpus study, it is possible to determine in which textual genres the passive voice is most commonly used.
This type of information is called corpus metadata.
While the rationalist methodology leads to the formulation of categorical judgments, the empirical methodology provides a more refined answer to this question, since the observation of corpus data offers a precise indication of frequency, rather than a result in terms of absence or presence.
Around the late 1950s, the use of corpora in linguistics was almost completely interrupted in certain fields such as syntax, following the works of the American linguist Noam Chomsky.
Chomsky's first objection to the use of corpora, which is also the most fundamental one, is that corpora contain language samples produced by speakers.
Furthermore, even if we were not to include an atypical speaker, a corpus could never represent more than a tiny language sample when compared to all the oral and written productions in any language.
In fact, the size of corpora has increased exponentially over the past 20 years, and corpus analysis tools have also made considerable progress.
What is more, in many areas of linguistics such as lexicology, language acquisition and sociolinguistics, the idea of relying on the internal judgments of linguists is simply not conceivable.
This corpus-based research approach is opposed to an approach which considers corpus data as the only point of reference, both in a theoretical and a methodological sense.
In this approach, linguists begin their research without an a priori and simply let hypotheses emerge from corpus data (this is called a corpus-driven approach).
These tools also make it possible to establish the list of words contained in the corpus, together with their frequency, and to generate a list of keywords matching the content of a corpus.
This operation, called part-of-speech tagging, can be performed automatically by certain software.
Tools for analyzing the syntactic structure of sentences have also been developed in the context of works for automatic language processing.
The main objective is to test a limited number of variables, in a highly controlled environment whenever possible and on a language sample that can be representative of the phenomenon studied.
To test this hypothesis by means of a corpus study, we should first make sure that we are comparing records of men and women produced in the same context, for example, in the context of friendly discussions around a topic, or a face-to-face interview with a researcher.
We will also need to make sure that the corpus collected in this way includes approximately the same speaking time or the same number of words pronounced by men and women.
Second, we would have to choose a list of words to search within the corpus, representative of the vocabulary related to emotions, for example verbs such as to annoy, adjectives like furious or nouns like anger.
Then, by comparing the number of times these words have been produced by the two groups and by validating the significance of the differences observed between the groups through statistical tests, we would be able to provide an answer to the research question.
For example, if we want to know whether learners of French as a foreign language at an advanced level are able to use collocations as native speakers do (collocations such as "prendre une décision" -to make a decision -or "pleuvoir à verse" -to pour with rain), we can search for occurrences of these expressions in text corpora produced by learners and compare the number of times these expressions appear -and their frequency -in a corpus of similar textual productions made by native speakers.
By means of a corpus study, we will be able to identify all the types of errors produced and then quantify each of them: for example, 30 spelling mistakes, 12 lexicon errors, 20 syntax mistakes, etc., made every 100 words.
In contrast, what a corpus study will not help you to do is establish with certainty the factors influencing the number of errors.
Conversely, a corpus study focuses on linguistic productions without manipulating the data before collecting them.
In addition, the use of corpora favors the observation of a very large amount of linguistic data, whereas experiments are based on a limited number of linguistic items for the task to remain feasible for participants, who would not be able to read thousands of sentences at a laboratory, for example.
For example, if we want to decide whether learners are fluent in French idioms such as "mettre le feu aux poudres" (to stir up a hornet's nest) or "avoir un poil dans la main" (to be extremely lazy) through a corpus study, we will have to look for them in a corpus of learners' productions.
Other corpora specialize in representing other population categories, regardless of whether they are monolingual children in the process of acquiring their mother tongue, bilingual children, foreign-language learners, or even children with neuro-developmental disorders influencing language acquisition, such as autism and specific language impairment.
Conversely, some corpora specialize in the productions of speakers of a certain language variety, such as French from Frenchspeaking Switzerland, Belgium, Canada, etc.
The most common type of annotation is the assignment of a grammatical category to each word in the corpus, as we have already mentioned.
On the other hand, so-called parallel corpora contain texts produced in one language and their translation into one or more other languages.
Parallel corpora can also be annotated with exact matches between sentences.
The criticism of the potentially problematic choice of subjects who could be aphasic and not represent the normal use of language also applies to experimental methodology.
While one corpus can be compared to another reference corpus, these tools also make it possible to extract a list of keywords that are specific to the corpus studied.
In the field of multilingual corpora, aligners make it possible to align parallel corpora sentence by sentence, and then to extract a sentence and its translation by means of a bilingual concordancer.
For example, the corpus study could help identify one type of common error, and one type of rare mistake.
A large type of corpus comprising a large number of different speakers would be desirable.
In summary, studying phonology using corpus data makes it possible to bring to light the interfaces between syntax and speech.
For each speech sample, the authors counted the number of syllables spoken between two pauses.
Several studies have used corpus data to study the factors that lead speakers to pronounce the liaison or not.
This study made it possible to show that the grammatical gender of a noun can be predicted in a large number of cases, contrary to the claims of many French theoretical studies in grammar.
The use of corpora also offers the possibility of finding out the context in which a certain word was produced, with the aim of checking, for example, whether the meaning of the derived word in such a context was the one expected.
However, the use of corpora in the field of syntax has grown considerably.
In particular, the use of corpora makes it possible to compare the productions of various speakers, of different language varieties as well as different registers, providing a much more nuanced and realistic vision of the structures underlying language uses, rather than the intuitions of a single speaker.
By performing a corpus study on spoken French using the Corpus de français parlé parisien des années 2000, the authors identified all the occurrences of "il y a" or "y a" (both forms meaning "there is") and then manually chose only the cases (98 in total) in which (il) y a was followed by a definite noun phrase and a pseudo-relative.
These different functions were more easily identified thanks to the large availability of contextual language in corpus data.
This study illustrates how a corpus study can combine quantitative elements (the prevalence of different functions for a structure) with qualitative ones (the identification of semantic functions).
Another large-scale annotated corpus study was carried out to explore the question of the placement of the attributive adjective, either before or after the noun it modifies.
It is evidently impossible to list all the words existing in any language, but using large corpora, it has become possible to get a much more realistic idea of the number of words in circulation compared to the lists that could be drawn from dictionary databases, which list only part of it.
For example, in order to determine whether two words are synonymous or not, the analysis of corpus data makes it possible to determine whether these two words can appear in the same linguistic context or not.
We will focus on this type of analysis in the following section.
In Facebook conversations, spelling alterations to typical written words in social media represented less than one in three words, which contradicts the idea that social media language is entirely different from other language registers.
As in the case of these two disciplines, corpora represent valuable tools in pragmatics, because they make it possible to study the use of language in real communication situations.
As soon as a speech act is associated with a certain type of utterance (e.g.
In this section, we will introduce a study illustrating the usefulness of corpus data for the study of scalar implicatures.
The use of corpora has therefore long been a fundamental tool in sociolinguistics.
Thus, this study showed that the use of post-verbal indirect questions represents a case of language change initiated by the less privileged social strata of the population, rather than a prestige change (as is the case with other sociolinguistic changes).
The main limitation to the use of corpus data for studying linguistic changes from a diachronic perspective lies in the fact that linguistic changes generally first take place in the spoken language.
One of the main advantages of corpora is that they contain natural data providing a glimpse of different forms of language use, which can thus be studied while taking into account a rich linguistic context.
What is more, the speech act that the speaker intended to produce is sometimes ambiguous and even when having access to a linguistic production: it is not always possible to clearly identify the speaker's intention.
In particular, we will see how corpus data can be used for studying the language of specific groups such as children, individuals with language impairments and foreign language learners.
It is for this reason that this field has been a pioneer in the development and sharing of corpus data.
Language acquisition corpora show some specificities when compared to other corpora.
This is why language acquisition corpora are by nature spoken corpora, which require a written transcript in order to be analyzed.
For this reason, language acquisition corpora frequently include language samples produced by children as well as by adults.
This information represents valuable clues for studying acquisition mechanisms, and these are particularly valuable for theoretical frameworks which attribute a key role to social interactions as the source of language acquisition (e.g.
Each corpus type has its own advantages and disadvantages.
A second limitation inherent to the use of corpora is that even very dense corpora can only be used to study language productions.
The first study that we present to illustrate the use of corpora for the study of language acquisition concerns the acquisition of the French verbal system for expressing temporal references.
For example, corpus data make it possible to analyze the way in which a child with limited syntactic skills manages to ask for an object or to ask a question.
Another limitation to the use of corpora is that the latter do not make it possible to measure the connections between verbal productions and other cognitive skills, such as working memory or non-verbal intelligence.
It is for this reason that the study of language production in patients is often carried out via constrained production tasks, such as the ability to name images or to repeat non-words or sentences (see, for example, Seiger-Gardner and Almodovar 2017), rather than based on corpus data alone.
The creation of numerous learner corpora, as well as the development of new methods and annotation tools, has largely contributed to this evolution.
While linguists working on the question of second language acquisition have long used learners' productions as a source to build their theories, these data were limited to very small samples or even to single-person studies.
This led to the creation of real learner corpora, aiming to provide representative samples of this population.
Finally, most learner corpora are cross-sectional corpora, including one sample per participant and representing a given moment during the acquisition process, since most of the time learners included in a corpus have a homogeneous level of competence in the foreign language.
In her study, the author first performed a frequency analysis regarding these two markers in the three sub-corpora.
That being said, the low number of occurrences of the marker actually among French speakers (56 in all) prevents a quantitative analysis of the differences between its different functions.
Finally, the creation of learner corpora has also made it possible to bring a new dimension to language teaching, by allowing learners to consult non-native productions and to compare them with native productions.
An important question for language teaching is to determine to what extent the corpora developed for linguistic research can be reused as such in the classroom.
In order to base a language method on corpus data, it is imperative that the corpus chosen is adapted to the target audience, in particular from the point of view of the variety of the language represented, discourse genres, the age of the speakers, etc.
Each of them compared corpus data with the presentation of the same phenomenon using different language methods.
In each of the three areas studied, significant differences were observed between corpus data and the presentation of the same phenomenon in language methods.
However, corpus analysis shows that the most frequent case in many language registers is, on the contrary, the simple aspect.
Corpus data make it possible to produce better-suited educational materials to match the realities encountered by learners.
Racine and Detey (2017) also compared the information given in language methods with corpus data, focusing on the question of liaisons in French.
Indeed, searching for occurrences of words corpora rather than registering them manually over readings has permitted lexicographers to list examples much more easily and to use the frequency information provided by the corpus data, in order to decide which meanings to include and the order in which to present them in the dictionary entry.
Since the 1970s, the first dictionaries making use of corpus data became available in English.
The first large-scale lexicographic project involving the massive use of corpus data was the COBUILD dictionary, dating from 1987.
As we said above, in the Englishspeaking lexicographic tradition, the use of corpora is now the norm, but reference corpora most often refer to the same stylistic genres.
For all the words studied, the results indicated significant differences between the results of the corpus study and the entries of different French dictionaries.
However, corpus analysis has revealed that the predominant collocations are rather neutral, as in jeune mec (a young guy) or positive, as in mec bien (a good guy), beau mec (a handsome guy).
In short, this study confirmed that corpus analysis can help us to study not only the style of an author or a text genre but also the way in which the different characters in a written production are represented.
We have seen that language acquisition corpora have helped us to understand the different stages of this process, in particular regarding the study of the associations between the language that children hear in their environment and their own productions.
We have seen that corpus analysis makes it possible to better characterize the language specificities of people suffering from language and communication impairments, by studying the different ways in which these patients interact in a natural environment.
We then reviewed the multiple applications of learner corpora to better grasp second language acquisition processes and showed how these corpora can be integrated into teaching materials.
We also discussed the increasingly widespread use of corpora as a basis for the creation of dictionaries and showed that these data help us to overcome many inherent limitations of a purely qualitative approach to writing dictionaries.
Finally, we discussed the different ways in which the corpus linguistics methodology makes it possible to provide valuable tools for the stylistic analysis of texts, as well as for author identification in a legal framework.
Questions 1) In addition to morphosyntax discussed in this chapter, what are the other aspects of language acquisition that are well suited for corpus-based research?
For example, this type of analysis is quite suitable for studying the vocabulary growth of a young child or, more specifically, the emergence of certain words in their lexicon.
However, automatic corpus analysis provides many examples of politeness routines, as the ones related to the opening of a conversation, to its closure or, to speech acts such as apologizing.
First, we will discuss the advantages and disadvantages of two types of multilingual corpora, namely comparable corpora and parallel corpora.
Parallel corpora make it possible to compare texts in their original language, with the corresponding translation into one or more languages.
Parallel corpora containing texts in one or more original languages, and their translations into one or more languages, represent the second type of multilingual corpora.
It sometimes happens that parallel corpora contain only texts translated into different languages from another language that has not been included in the corpus, or it may occur that the original text cannot be identified among all the texts.
As a result, comparing languages through the use of parallel corpora is greatly simplified in contrast to comparable corpora because annotators can keep a track of translation equivalents without having to annotate syntactic or semantic features.
In order to overcome certain limitations pertaining to parallel corpora, the ideal would be to work with a bi-directional corpus, where both languages are alternately source and target, since these corpora make it possible to combine the two types of multilingual data discussed above (comparable and parallel).
Bi-directional corpora offer the possibility of studying equivalences in both translation directions through the use of parallel corpora.
Indeed, linguists working on language teaching had long observed that mistakes made by learners were often linked to transfers from their mother tongue.
Furthermore, these differences can only emerge on the basis of a quantitative corpus study, which highlights the differences in frequency and context of use.
The second study we present in this section was devoted to the analysis of the different factors that influence translations in parallel corpora.
For this study, the occurrences of the eight abovementioned connectives were drawn from three parallel corpora (Europarl for the parliamentary debate genre, a corpus of newspaper articles and the TED corpus of online conferences; see Chapter 5 for a description of these corpora).
Finally, the degree of expertise of translators also plays a role in their translation choices, and this factor should therefore be taken into account in the study of parallel corpora.
Later, we will see that the use of the empirical methodology ingrained in corpus analysis can also work as a guide for the translator when it comes to making certain translation choices.
In this section, we will refer more specifically to the role of parallel corpora in the creation of bilingual dictionaries.
To some extent, equivalences between languages obtained through the use of parallel corpora respond to such criticisms.
Many other studies have compared the translation equivalents provided by bilingual dictionaries with equivalents observed in parallel corpora.
These studies invariably highlight a discrepancy between the translation equivalents found in dictionaries and in corpus data.
In most cases, the equivalents provided by dictionaries are much more limited than the equivalents found empirically, or vice versa, dictionaries sometimes list equivalents that are completely absent from corpus data.
The study of translation often relies on parallel corpora, but can also make use of comparable corpora of texts translated into different languages, without considering the source language.
We have also shown that corpus analysis methods can be useful for uncovering recurring patterns in a source text and to better adapt the strategies used for its translation.
Finally, we argued that parallel corpora have become indispensable resources for the creation of bilingual dictionaries, since they provide rich lists of translation equivalents accompanied by their contexts of use, as well as information concerning their frequency in various genres.
Indeed, such a corpus makes it possible to establish the degree of mutual correspondences between these connectives, by counting the number of times that they can be translated by each other.
The number of words used per sentence can also be calculated, even on a corpus that has not been subjected to syntactic annotations.
Secondly, we will present a set of concordancers, which are corpus analysis tools, and discuss their main functionalities.
For example, the AntConc concordancer that we discuss can only process text format files (or files with XML or HTML tags which can also be treated as text files).
In fact, most interfaces offer user-friendly methods specifying the choice criteria, such as gender, type of speaker, time period, etc., as well as fields for typing in the element(s) to be looked for in a full text search, sometimes enabling the use of search patterns (called regular expressions, see section 5.6).
Some are limited to a continuous character string, something which prevents the search for compound words, like chemin de fer (railway) in French, which includes three separate strings of characters.
Let us take a look at an example: it is possible to look for all the occurrences of a regular verb like aimer using a single query looking for the root aim, followed by a wildcard replacing an unspecified number of characters, for example aim*.
For example, the online interface on the OFROM corpus (see section 5.3) only offers the possibility of looking for productions by women or men, but for the moment does not indicate the total number of participants of each genre, or the number of words produced by each of them.
In the same way, the consultation interface for the CLAPI corpus (see section 5.3 for a description) does not currently provide information about the total number of words included in the portion of the corpus that is available for online consultation.
Other corpora such as the new version of the Frantext literary text corpus (see section 5.2) are accessible via an annual renewable subscription.
To conclude, it is important to emphasize that, regardless of the format in which a corpus can be accessed, reusing corpus data amounts to benefiting from the often long and costly work carried out by other research teams.
The site's interface makes it possible to choose works based on different criteria, such as the time period or the author.
This piece of information makes it possible to estimate the potential difficulty of a word, for example, in the context of language teaching or for preparing experimental material, by controlling the frequency of the words used in the experimental materials.
This corpus also has an educational purpose in the area of language teaching.
In the field of written French language acquisition, the Littéracie avancée corpus produced by the Laboratoire linguistique et didactique des langues étrangères et maternelles (LIDILEM) of Grenoble Alpes University is made up of writings by undergraduate and master's degree students, covering the entire span of study.
In the area of French as a foreign language, numerous learner corpora have been collected.
A more exhaustive list of learner corpora in many languages is provided on the Center for English Corpus Linguistics (CECL) website, from UCLouvain in Belgium.
Many learner corpora are also available on the TalkBank online database.
The University of West Indies Learner Corpus or UWI L2 Corpus created by Hughes Peters includes material spoken by adult French learners (16 in total) who were also speakers of English and Jamaican Creole, and who had studied French at university.
The OPUS database includes many free access parallel corpora, including the Europarl corpus, described below, as well as corpora with subtitles and multilingual data collected from the Internet, such as Wikipedia.
This tool is very useful for quickly finding examples of word translations but it cannot be used to perform a truly quantitative contrastive analysis, as the total number of occurrences of the word is not mentioned, nor is the translation's direction.
Finally, we will discuss briefly the features of the CLAN concordancer which makes it possible to explore data coded in CHAT format, the annotation standard used in the CHILDES database.
Some concordancers can calculate the probabilities of collocations between certain words, rather than simply establishing the list of words which co-occur in the corpus.
Finally, some concordancers can be used to extract a list of keywords in a corpus by comparing them with a reference corpus (see Chapter 6).
More specifically, the concordancer determines which of the words are used significantly more (or even less used in the case of negative keywords) in the search corpus compared to the reference corpus.
In a nutshell, the keyword list of a corpus is very useful to identify its main topics, provided that the comparison with the reference corpus is appropriate.
However, this encoding does not correspond to text files containing French characters, because of accented characters.
AntConc can be used for looking up words in context and sorting their occurrences depending on the words that appear to the left or to the right of the search word.
These wildcards are mainly used for looking up all the possible endings of a regular verb in a single request, by searching for the radical of the verb followed by any number of characters (through the use of an asterisk), such as donn*.
These figures are essential for performing lexical diversity calculations on corpus data, such as the type/token ratio (see Chapter 8).
Finally, AntConc makes it possible to create a list of keywords from the corpus based on the comparison with a reference corpus.
This command makes it possible to obtain the list of words sorted by frequency, in the same way as the list of words generated by AntConc.
We have observed that, despite the absence of a reference corpus, numerous more specific corpora are available, which can be combined to carry out research in many areas of linguistics, as we will see in the subsequent exercises offered.
Indeed, the first content word only appears at the 32nd frequency rank!
We can also observe that the frequency of words in a corpus decreases rapidly.
We should also be careful to choose the French corpus and to determine a sufficiently long time period, for example from 1800 to 2000.
The fact that the type/token ratio decreases with age in the two children reflects that the total number of occurrences they produce increases a lot as recordings progress (e.g.
When it comes to creating a reference corpus, the data collection phase is so time-consuming that it can only be tackled by a group of experts.
Even for this type of corpus, several months of work are often necessary for collecting the data, and may take even longer if the latter are enriched with linguistic annotations (see Chapter 7).
The transcription process itself is very time-consuming and its complexity depends on the exact type of annotation that is added to the data (prosodic contours, etc.).
In the field of corpus linguistics, it is very common to hear that there are no good or bad corpora, rather there is only corpora which are more or less suitable to address a certain research question.
In fact, a corpus is a collection of texts or recordings specifically chosen in order to be representative of a language, of a certain register or even a language variety.
In order to be a representative, a reference corpus should contain a balanced set of samples covering the main stylistic genres, both in the spoken and written modes.
As with the question of corpus size, the balancing criterion largely depends on the question the corpus will help to study.
In general, a corpus should not be used for establishing a contrast between elements which have not been balanced during the corpus compilation phase.
Once again, the answers to these questions depend on the type of corpus the researcher has in mind.
The correct size of samples also depends on the type of text considered.
For example, if the goal is to reach a sample of 200,000 words per text genre, this number of words can almost be instantly reached by including one or two entire books in the sample.
No matter the way of acquiring data, an important point is to save the corpus files into a format which can then be used by a concordancer.
Files saved in these formats can be easily transformed into text files thanks to specific options such as the "save as" command found in word processors.
In general, it is preferable to store every language sample in a separate file.
In this way, it is later easier to combine them in different ways for creating sub-corpora, rather than having to retrieve text portions from a larger file.
In order to be able to easily group the files into sub-corpora, it is necessary to represent the features of each sample in an easily accessible manner.
Finally, if word abbreviations are used, it is desirable that each abbreviation of a category contains the same number of characters (e.g.
We have pointed out that corpus files should contain plain text, in order to facilitate data analysis.
However, for a corpus file to be used as a sample representing a certain type of language, metalinguistic information (which is not part of the text or of the dialogue) should be accessible to the researchers who will analyze it.
Indeed, some sophisticated marking formats have already been developed for corpus data.
As we have seen in Chapter 5, for corpus data to be analyzed, they should be in written format, since concordancers cannot search for words or expressions in audio files.
If, for example, the aim of a spoken corpus is to study the lexical specificities of a language variety, the prosodic information contained in the interactions will be of little use.
In order to be able to share corpus data, it is imperative to ask participants both for their authorization to use and to distribute the data before collecting them.
The responsibility of the corpus compiler is involved when it comes to texts with potentially defamatory content.
Rights holders often agree to authorize a single researcher to use a reasonable amount of their data for research, but this type of corpus often cannot be later redistributed.
Questions 1) What types of data should be collected to conduct a representative study of how young people use the discourse marker genre in French?
The most frequent content word is cinéma, at the 20th frequency rank.
The great advantage of part-of-speech tagging is that it can be done automatically with almost the accuracy of a manual annotation, regardless of the amount of text to be annotated.
As a matter of fact, part-of-speech tagging has been performed on the Google Books corpus (see Chapter 5), which contains billions of words from different languages and has made it possible to refine research on language evolution.
An example drawn from recent research on language acquisition will better illustrate the impact of annotation choices.
This example illustrates the influence of the methodological choices associated with data annotation and the conclusions that can be drawn from a corpus study.
However, lemmatization refers to the act of associating every word occurrence in a corpus with its basic morphological form.
In addition to lemmatization, words can be annotated into grammatical categories thanks to part-of-speech tagging, as we previously mentioned in relation to word annotations such as ferme and car.
Annotation also provides training and testing data for automatic word sense disambiguation.
Indeed, this type of annotation is more difficult to perform automatically than part-of-speech tagging, since it requires conceptual knowledge in context and this is still a major challenge for artificial intelligence.
As soon as a corpus has undergone a part-of-speech tagging process, it is possible to parse it, and thus reveal how grammatical categories can be grouped into smaller phrases within a sentence.
Finally, sentences may contain a pragmatic annotation of the speech act involved (e.g.
Indeed, each word belongs to a grammatical category and every sentence communicates a speech act.
Before starting the annotation process itself, we first have to define the categories which will be annotated in the corpus.
While preparing the instructions for the annotation process, it is important for each category to be clearly defined so that the annotators know how to use them, in cases where the annotation is performed by humans and not automatically.
In general, it is preferable to choose an annotation scheme as neutral as possible from a theoretical point of view and, in any case, to stick to categories clearly identified and widely accepted in the literature.
In other cases, such research will benefit from a preliminary part-of-speech tagging or even from a parsing analysis performed automatically.
For example, in order to study causal relations in French, part-of-speech tagging makes it possible to only look for occurrences of car working as conjunctions and eliminating those which are nouns (a type of vehicle).
It is often impossible to anticipate which dubious cases will appear since corpus data are always much more complex and ambiguous than the reference sentences found in dictionaries.
These examples illustrate the complexity and the ambiguity of the real corpus data when compared to invented examples.
This is why it is wise to test the categories to be annotated on a small portion of occurrences, ranging, for example, from 50 to 100 depending on the difficulty of the annotation scheme and the total number of occurrences to annotate, then refine the criteria or even redesign the categories on the basis of this first annotation.
On the one hand, there are the tools making it possible to carry out annotations in an automatic way, for example, by means of part-of-speech tagging or parsing.
We should beware that there are a very large number of them and that their more or less suitable character depends on the type of annotation the researcher has in mind.
The usefulness of part-of-speech tagging is such for corpus studies that this annotation is now directly embedded into some corpus creation tools.
Since this platform enables you to create new corpora both from the Internet and from manually inserted files, it is very convenient to use it for carrying out part-of-speech tagging.
Once annotated, the elements of the corpus can be searched by their POS tags by means of a concordancer.
The annotation of syntactic dependencies can also be done automatically using computer tools, but so far these have not generally been included in the interfaces for creating corpora, and their use requires natural language processing (NLP) skills that go beyond this book.
Taggers in XML thus provide a good way for associating annotations with corpus data.
The most widely used XML schema for coding corpus data is the one provided by the TEI (Text Encoding Initiative).
This annotation is presumed to be completely correct, that is, it matches what has been deemed appropriate in the annotation scheme.
On the one hand, the recall measures the number of occurrences of each category correctly found by the system.
On the other hand, precision measures the number of occurrences properly tagged as nouns, from among all the ones tagged by the system as such.
Finally, we can express the overall quality of the automatic annotation by calculating a mean of F1 scores per category, by weighing every category according to its number of occurrences in the reference (micro versus macro mean).
Furthermore, this double annotation process will make it possible to indicate whether the categories have been poorly defined.
In order to measure the agreement between two annotators, a first solution is to count the number of times they choose the same tag and to deduce a percentage of convergent annotations.
Sharing annotated data also implies that the annotation process and the categories used are clearly documented in an annotation manual, which will be provided to future users together with the data.
It should also provide information on the corpus processing that preceded the annotation process.
As we saw at the beginning of the chapter, these decisions influence the annotation process and must be clearly documented.
Likewise, revisions that have been made at different stages of the annotation process must be documented, as well as successive versions of the corpus that have been produced, if applicable.
We then detailed the different stages that make up an annotation process and stressed the importance of good methodological practices, so that the annotation is as valid and reusable whenever possible.
Finally, we presented some recommendations for the creation of an annotation manual, which documents both the content and the annotation process itself in order to enable other researchers to reuse it.
Questions 1) Explain three advantages of annotating corpus data.
A first example in relation to lexicon concerns the study of all polysemic words, for which part-of-speech tagging makes it possible to sort a good part of the irrelevant occurrences (e.g.
The purpose of this chapter is to introduce some simple statistical methods that are commonly used for processing corpus data.
Finally, we will illustrate the use of inferential statistics by presenting a commonly used test in corpus linguistics, namely the chi-square test, which determines whether frequency differences between categories are significant.
Before performing descriptive statistics on the data obtained in corpus studies, it is necessary to make the number of occurrences comparable, and this can be achieved through the use of different sources.
For example, in order to compare the number of passive sentences in the above-mentioned 10 texts, it would have been necessary to ensure that all the texts had a comparable number of words.
As a result, when the sources are of variable length, which is generally the case, it is necessary to transform the number of occurrences into relative frequencies, so that they can be easily compared.
Thus, in order to determine whether passive sentences are used more frequently in the written than in the spoken form, it is necessary to transform the data according to the same base of normalization, for example the number of occurrences per 10,000 words, per 100,000 words or even per million words.
To turn a number of occurrences into a relative frequency, we need to apply a rule of three, by dividing the number of occurrences found in the corpus by the total number of words in the corpus, then multiplying by the base of normalization, as shown in the example below, which has a base of normalization equal to 10,000.
To complete this task, you will count each character string one after the other to verify that you have reached the total requested.
Notions such as word type and word occurrence are both very important in corpus linguistics.
In order to avoid this type of bias, in addition to word frequency, it can be useful to calculate lexical dispersion in a corpus.
The proportions observed are calculated by taking into account the total number of occurrences of the word, whose distribution we are trying to determine in each portion of the corpus, and dividing it by the total number of occurrences of the word in the corpus.
Suppose we want to know whether the connective car was used consistently by the 10 students who compiled a dossier in the Littéracie avancée corpus (L2_DOS_SORB sub-corpus).
For example, if the research hypothesis tested is that the connectives néanmoins and toutefois are not used with the same frequency in two text genres, but we do not know which type of text is supposed to contain more connectives than the other, then a two-tailed test should be carried out.
These variables can include gender and geographical region, as we have already seen, and also the textual genre or form (spoken or written discourse), the language in the case of comparable corpora, and the age or language proficiency level in the case of children and learner corpora.
For example, in order to determine whether type of speech acts varies between two text genres, it is necessary to carry out a chi-square test.
First, we should check with corpus data whether the use of huitante varies from one group of cantons to another.
To do this, we have to look for the number of occurrences of the word huitante in each of these six cantons by means of the OFROM corpus, which compiles the French spoken in Switzerland (see Chapter 5).
Thanks to the corpus data considered, we can deduce that (at 5%) the use of huitante is almost certainly associated with a group of cantons, which corroborates the assertion made on the atlas we quoted.
Finally, note that the chi-square test of independence also makes it possible to know whether all the cantons differ from one another or not.
To conclude, we gave two variants of the chi-square test, an inferential statistics test which makes it possible to determine whether the differences observed between the distributions of several categories are significant.
In the case of the chi-square test of independence, we have shown how to represent data using a crosstab.
We introduced the notion of standardized residual, which makes it possible to determine where the significant result of the test comes from, and introduced Fisher's exact test as an alternative to the chi-square, when the conditions posed by the latter are not met.
Questions 1) The tables below show the number of occurrences of the (lemmatized) words bateau and je, as well as their English equivalent boat and I, in the bilingual corpus of children's literature ParCoGLiJe (see Chapter 5).
Raw data (number of occurrences of the words bateau, je, boat, I) drawn from these works are presented in the four tables below, as well as number of word types and word occurrences.
Normalize the data so as to be able to compare the frequency of these words throughout the texts in each table, and then between the sub-corpora (original texts vs. translated texts, French texts vs. English texts).
The effect size is moderate (Cramer's V = 0.22).
By way of conclusion, we would like to offer a list of ordered stages, making it possible to implement the concepts discussed in this book step by step, and to carry out a corpus study.
The great advantage of linguistics is that the study of language has interfaces with very many disciplines, and that it is possible to find study subjects in very varied fields.
Thus, the first step in a corpus study is to identify relevant sources that have so far explored the research subject under consideration.
For example, journals such as Corpus Linguistics and Linguistic Theory, International Journal of Corpus Linguistics and Corpora are all three specialized in the publication of corpus studies, whereas the Journal of Language Resources and Evaluation publishes articles on methodological aspects related to the compiling and processing of corpus data.
Another way to identify relevant literature is to use the Google Scholar search engine, which indexes most of the available scientific articles.
It is therefore useful to look for the article title directly in a search engine to find out whether such a version is available online.
In order to automatically search for elements in a corpus, a surface feature such as a word or list of words should be associated with it.
Data extraction can also be based on a prior automatic analysis of the chosen corpus, such as lemmatization or part-of-speech tagging (see Chapter 7, section 7.4).
The annotation process requires the prior identification of clear categories (see Chapter 7, section 7.3).
This stage notably involves the transformation of raw figures corresponding to the number of occurrences observed in the corpus into figures reporting relative frequencies, following a base of normalization.
Very often, the results of empirical studies also serve to modify and improve existing theories, and thus contribute to make linguistics a scientific study of language.
Among the assumptions that lie behind Corpus Linguistics, though, is the view that there are aspects of language use that are important but that are invisible to the human reader of texts.
This approach to Corpus Linguistics is based on the observation of pattern in concordance lines, where a word or short phrase is the node of the line and the few words occurring before and after the phrase are shown for each occurrence.
In addition, a concept known as 'semantic sequences' was developed -this suggested that concordance lines can be read to identify 'what is often said', thus moving from lexis to grammar to discourse.
The various sub-corpora are then tagged with the language features, and the strength of co-occurrence of those features is calculated.
Although we hypothesised in advance that different disciplinary discourse styles would emerge from the journal, we were not able to, and indeed did not want to, divide the articles in the journal between those disciplines in order to arrive at sub-corpora that could then be compared.
In introducing these examples I have used the term 'co-occurrence', meaning that two words frequently co-occur in the same text.
Co-occurrence of words within a short span (i.e.
In the studies described in this section, a rather different view of co-occurrence is taken.
Rather one aim is to gain novel insights into a set of texts by observing the co-occurrence of words within them.
The second aim is to gain novel insights into those words by organising them into groups according to the strength of their co-occurrence in given texts.
However, methods of identifying word co-occurrence provide a way of organising a corpus to lead to new insights.
The project relating to 'Rate My Professors', described above, is one such investigation, where the question: 'what categories of individual qualities are discernible from the comments made' is answered using the strength of co-occurrence of adjectives as the research method.
Corpus linguistic research offers strong support for the view that language variation is systematic and can be described using empirical, quantitative methods.
Variation often involves complex patterns of use that involve interactions among several different linguistic parameters but, in the end, corpus analysis consistently demonstrates that these patterns are systematic.
Against that background, it would be easy for a student to imagine that corpus linguistics developed only in the 1980s and 1990s, responding to the need to base linguistic descriptions on empirical analyses of actual language use.
As such, the handbook includes chapters dealing with a wide range of linguistic issues, including lexical variation, grammatical variation, historical change, the linguistic description of dialects and registers, and applications to language teaching and translation.
In each case, chapters assess what we have learned from corpus-based investigations to date, and provide detailed case studies that illustrate how corpus analyses can be employed for empirical descriptions, documenting surprising patterns of language use that were often unanticipated previously.
Instead, the CHECL focuses on a critical discussion of the linguistic findings that have resulted from corpusbased investigations: what have we learned about language variation and use from corpus-based research?
Each chapter also includes critical discussion of the corpus-based methods that are typically employed for research in this area, as well as an explicit summary of the state of the art: what do we know as a result of corpus research in this area that we did not know previously?
Finally, each chapter includes an empirical case study illustrating the corpus analysis methods and the types of research findings that are typical in this area of research.
We have structured the main body of CHECL around these two domains of inquiry: chapters dealing with "Corpus analysis of linguistic characteristics" in Part II and chapters dealing with "Corpus analysis of varieties" in Part III.
Those chapters are then followed by chapters on the use of corpus analysis to document the linguistic characteristics of other types of varieties: literary styles, regional dialects, world Englishes, English as a lingua franca, and learner English.
As noted above, methodological issues relating to corpus design and analysis have been dealt with at length in previous textbooks.
However, beyond those treatments, there is need for a more general discussion of the current state of the art concerning corpus design and analysis.
The three chapters included in Part I provide this discussion, dealing with current issues relating to corpus design and composition, tools and methods for the linguistic analysis of corpora, and quantitative research designs and statistical methods used to describe the patterns of use across corpora.
Corpus linguistics has had a major influence on such applications over the past two decades, so that it is now almost impossible to find a research journal in applied linguistics, language teaching, translation studies, or lexicography that does not regularly publish articles utilizing corpus research findings.
Rather, we hope to summarize and evaluate what we have learned about language use and variation from previous corpus-based research, to identify and discuss the most important of those previous studies and research findings, and to discuss the methodologies that work best for such research.
Rather than attempting to create a complete and exhaustive list, I focus on a handful of corpora (and related resources, such as text archives and the "Web as Corpus") that are representative of general classes of corpora.
The Web as corpus, seen here through the lens of Google-based searches Finally, we will consider very large "hybrid" corpora, which take data from text archives or the Web, but which then deliver this data through powerful architectures and interfaces.
As we do so, we will consider how the quantity and quality of the data are affected by the corpus size, as well as the corpus architecture and interface.
In the concluding section, we will take an (admittedly risky) "flight of fancy" and imagine what type of corpora might be available in five, ten, or twenty years.
In addition, a Brown-based frequency list (for all words in the corpus) would be quite sparse.
This is particularly true in terms of the accuracy of annotation -both at the word level (e.g.
Consider also word-level annotation, such as part-of-speech tagging.
Text archives typically do not allow searching by part of speech (or even by lemma), so we would need to search for hundreds or thousands of matching strings one by one, e.g.
With only 1,000 tokens per word or phrase, it is impossible to create a robust dataset to extract collocates.
The version of the Google Books n-grams that it uses does not include part of speech or lemma.
Google Books (BYU) cannot generate these concordance lines, because it is based just on n-grams.
I highlight some limitations of the existing tools and methods, which include for example limited support of manual categorization of concordance lines and categorization of key words.
As a result, most research articles in corpus linguistics include discussion of corpus compilation, annotation, and/or the computational methods used to retrieve linguistic data.
The following subsections will focus in turn on tools and methods related to the three key phases of corpus linguistics methodology that have already been highlighted, i.e.
Depending on the type of corpus and the age of the sources, it may be possible to find corpus material in electronic form already and then the keyboarding or scanning stages can be avoided.
Tools such as spreadsheets, databases, and word processors are usually sufficient here although the relevant information may be stored alongside the corpus data itself for later retrieval in headers encoded within the files.
Part of the manually annotated dataset could also be used as a gold-standard corpus against which to compare the output of the automatic tagging system in order to evaluate its accuracy.
Computational methods and tools for corpus annotation therefore take two forms.
First, intelligent editors to support manual annotation and second, automatic taggers which apply a particular type of analysis to language data.
Retrieval methods and tools are those most commonly and prototypically associated with the corpus user's toolbox because many linguists use pre-existing corpora and so can skip the first two stages.
Alongside the concordance method, a further four methods have emerged as central to the work of the corpus user: frequency lists, keywords, n-grams, and collocations.
Different software tools do, however, produce slightly different frequency information and word counts for the same corpus data due to the way words are defined and delimited, e.g.
The keywords approach is a method to compare two word frequency lists using statistical metrics in order to highlight interesting items whose frequency differs significantly between one corpus that is being analyzed and a much larger reference corpus.
The keywords method can also be extended by comparing three or more word frequency lists representing distributions in a larger number of corpora.
The keyness metric (usually chi-squared or log-likelihood) provides complementary information to word frequency alone and gives an indication of the aboutness of a text, or what items are worthy of further investigation.
This method is fairly simple, is easy for the computer to calculate, and represents the ability to count repeated phrases or continuous word sequences that occur in corpus data.
These lists can be seen as extensions of the simple word frequency list which is identical to a 1-gram list.
In Firthian terms, collocation refers to the relationship between a word and its surrounding context where frequent co-occurrence with other words or structures helps to define the meaning of the word.
In practical terms, collocation as a method refers to the counting of the co-occurrence of two words in a corpus depending on their relative proximity to one another, and usually includes the calculation of a statistic or metric to assign significance values to the amount or type of co-occurrence relationships.
For instance, a concordance can be produced for a certain part-of-speech tag, a frequency list of lemmas, key semantic tags, and calculate collocation statistics for which semantic tags relate to a given word.
The historical timeline of corpus retrieval software can be divided into four generations.
Other tools, such as WordSmith and BNCweb, permit the user to manually categorize concordance lines and this can be viewed as a form of corpus annotation.
In particular, spelling variation causes problems for POS tagging, concordancing, keywords, n-grams, and collocation techniques.
In order to analyze and automatically tag a 2-billion-word Hansard dataset consisting of data from 200 years of the UK parliament, 16 we recently estimated that it would take forty-one weeks of computer time.
This usually involves the inspection of concordance lines of an element and their annotation for various linguistic and/or contextual features: if one wants to determine when speakers will use the ditransitive (V NP Recipient NP Patient ) and when the prepositional dative with to (N NP Patient PP to-Recipient ), one needs to inspect the whole sentence involving these two patterns and their larger contexts to determine, for instance, the lengths of the patient and the recipient, whether the clause denotes transfer or not, etc.
Examples abound in -learner corpus research, to document potential over-/underuse by learners compared to native speakers; -language acquisition corpora, to document how children acquire patterns as they increase the number of different verbs (i.e.
However, one cannot use a simple frequency list of an English engineering corpus, because its most frequent words would still be the, of, in, .
Including such interactions is necessary if one wants to determine whether the linguistic predictors have the same effect in each L1/variety, in each register, at each time period, etc.
However, Egan did not adjust his critical chi-squared value for the fact that he runs 28 tests on a single dataset.
This is done by first generating a "word list" of the lexical items in the dataset and the reference corpus, using the appropriate program function.
It is advisable to experiment with different minimum frequency cut-off points to minimize this problem, whilst ensuring that sufficient results are generated (if the dataset is small).
A further point of good practice is to provide the raw frequencies of each keyword, in addition to its keyness value (log-likelihood or chi-square) when keyword lists are given.
WordSmith Tools offers both chi-square and log-likelihood), while others do not (Wmatrix, for example, offers only log-likelihood).
It is recognized that the notion of exactly what qualifies as key in any study is influenced by the settings and parameters of the program used, and by the comparator texts (in the reference corpus).
Rather collocational phenomena can occur beyond word level to involve the characteristic co-occurrence between words and phrases with certain grammatical categories and syntactic contexts.
Third, corpus research has provided more reliable empirical evidence than intuition that facilitates the identification of collocational behavior and semantic prosody of an extensive range of lexical items that have until recently been hidden from intuition.
Still less work has been undertaken which contrasts collocation and semantic prosody in different languages (but see Sardinha  2000; Tognini-Bonelli 2001: 131-156; Xiao and McEnery 2006; Ebeling,  Ebeling, and Hasselga º rd 2013); yet findings yielded by this kind of research can be particularly valuable in language typology, language education, contrastive linguistics, and translation studies.
Both tools allow users to set the minimum co-occurrence frequency of an item to be considered as a collocate of a given node word so that the drawback of the MI measure as noted in Section 2.2 can be partly offset.
Given the size of the comparable corpora used, we set the minimum co-occurrence frequency to 3.
Within a 4-4 window span, items which had a minimum co-occurrence frequency of 3 and a minimum MI score of 3.0 were accepted as the collocates of a node word.
When using additional data from the BNC and PDC2000 corpora, the minimum co-occurrence frequency was set at 20.
The first main section (Section 2) explores the state of the art in collocation analysis, covering definitional and methodological issues, meaning arising from collocation, collocational phenomena beyond lexical level, as well as the importance of collocation in language use.
Such findings have not only helped to achieve improved language descriptions, but they also have an important role to play in practical applications such as language teaching, translation, and natural language processing.
Beginning in the 1990s, most research on phraseological patterns has been empirical, utilizing corpus analysis.
Finally, in Section 4, we briefly present a case study illustrating the application of large-scale corpus analysis to investigate the types of discontinuous lexical frames found in spoken and written registers of English.
The corpus analysis involves recording all multiword combinations with specified characteristics that appear in a corpus, usually tracking the frequency with which each combination occurs.
As this review of research illustrates, there has been a particular interest in lexical phrases in academic registers, perhaps because much corpus research on lexical phrases has been motivated by applied concerns related to language teaching and learning.
But the total number of words is not the only important factor here: the number of different texts (and the average length of texts) is equally important.
While there has been considerable discussion of the best statistical methods to identify important phrases within a corpus, there has been almost no discussion of the replicability of phraseological findings, or the ways in which corpus design and composition influence the results of this kind of research.
For example, a frame occurring 200 times with 25 distinct fillers would have a type-token ratio of .
Discourse markers were not recognized as a part of speech in traditional grammars, and even in the present day they are an uncertain category, straddling the border between grammar, pragmatics, and discourse analysis.
Qualitative analysis of corpus examples enables a matrix table to be built up, whereby criteria showing resemblance to the be + adjective category or to the passive category can be plotted against numbers of tokens, and degrees of similarity to and difference from the passive can be established.
A reference corpus cannot in any sense represent the language, unless it is subdivided into text categories or subcorpora representing a broad range of registers, as in the BNC or the Bank of English.
With particular relevance to this chapter, a diachronic interest in PDE has developed through the compilation of corpora enabling a precise study of changes in English over the recent past: for example, the Brown family of corpora, Bas Aarts's Diachronic Corpus of Present-day Spoken English (DCPSE) and Mark Davies's Corpus of Contemporary American English (COCA) and, with a longer time span, his Corpus of Historical American English (COHA).
Although the influence of the third-person singular presenttense form is not significant at the conventional threshold of 0.05, we consider it to be marginally significant because its p-value at 0.1 means .
This does not mean that they are not relevant to the choice between zero and explicit that at all, but that for the speakers in our dataset they are less decisive than other factors.
Further similar studies on grammatical variation should take into account as many variables as feasible, since the interplay of determinants of variation needs to be re-examined for each dataset.
Comparisons of older and more recent stages of language use thus allow, in a rather straightforward fashion, the identification of grammatical changes.
If a researcher is interested in these questions, it becomes necessary to undertake a corpus analysis that goes beyond the pointwise comparison of single examples.
Characteristic grammatical behaviors of verbs concern their ability to inflect, their co-occurrence with complements, and their role as the central element in larger syntactic constructions.
What this study illustrates is that corpus data allow very detailed analyses of how a given change proceeded.
This type of analysis is furthermore relevant for the identification of given changes such as frequency change, style change, or grammatical change.
Absolute type frequencies are trivially related to corpus size, with larger corpora yielding larger type frequencies, all other things being equal.
A second variable concerns the word class of the host to which the -ment suffix attaches.
Types such as segment or nugament, which are morphologically opaque to presentday speakers, were analyzed into the parts of speech that they originally represented, so that all stems in the database were categorized as either adjectival, nominal, or verbal.
None of the speech act verbs occurs more than twenty times per million words in academic prose except explain.
The Fisher test is used to establish "collostruction strength" by comparing the number of times a word (e.g.
Collostruction strength combines two calculations of relative frequency: the number of times a word occurs in a construction compared with the total instances of the word (answering the question "how important is this construction to this word?
This is reflected in the fact that the word form decided mostly occurs as a finite form whereas decide mostly does not, but instead occurs in sequences such as should decide whether .
On further scrutiny of the concordance lines, it was found that mention that is largely used to report a negative: someone did not mention or failed to mention a fact.
In other cases, differences in frequency distribution can be accounted for by frequently occurring semi-fixed phrases.
It suggests that it is the presence of the sequence of meanings that leads to the co-occurrence of lexis and grammar.
Second, by integrating into the analysis a number of insights into how discourses function which have developed within the field of corpus linguistics.
The principle of co-selection or co-occurrence states that a far greater proportion of the language of most discourse types is made up, not of the accretion of individual items chosen from the mental lexicon, but of prefabricated or semi-prefabricated collections of items; "chunks" if we prefer.
He looks at 1,809 occurrences in his corpus data (a 100-million-word corpus of Guardian newspaper texts) and discovers first of all that it displays a clear aversion to appearing as part of the object of a sentence (4% of occurrences) but no such aversion to appearing as part of a verb complement (24%).
While in the case of RASIM the researchers track the evolution of discourses over a continuous period of time, another series of diachronic studies compares corpora from different points in time in order to identify change or stability.
In particular, we wish to show first how the concordancer's ability to collect examples of a similar linguistic phenomenon, as contained in repeated word strings or clusters, can lead to insights into the intentions of discourse participants, second how corpus techniques can enable the tracking of discourse features over time, and third how, contrary to charges from some quarters, corpora can shed light on what is absent from a dataset under examination and what this might signify.
Ru ¨hlemann uses frequency counts to demonstrate the importance of laughter to conversation, for example, if "between-speech laughter" is considered as a linguistic item in and of itself, it would be placed in 29th position on the BNC conversational subcorpus frequency list.
One of the criticisms of small corpus research is that a small corpus does not allow for generalization.
Hence, this methodology, if transferred to a larger dataset, needs to employ a sampling strategy so as to arrive at a manageable amount of vocatives.
Concordance lines were then generated using the categories as the search items so that the function and position of the vocative is placed in relief.
Indeed, it is no exaggeration to say that corpus linguistics using large computer-readable language data has established itself as the main methodology in historical pragmatics.
For example, historical sociolinguistic studies are interested in establishing correlations between the speaker's or writer's gender, age, or social class and innovative language use, whereas a pragmatic study would highlight situational uses of the novel form and momentary shifts between older and newer variants in discourse.
One of the novel software applications, the Keyword analysis, uses significance tests to distinguish words that are significantly more frequent or significantly less frequent than in a reference corpus; calculations are carried out automatically by the program and it is possible to gain valuable insights into the material that cannot be achieved with qualitative study.
It is easy for the end user to apply the method to their data, but the researcher's own input shows in the selection of the target corpus and an optimal reference corpus and the interpretation of the machine-produced key word lists, which is not simple at all (see below).
Cultural context is even broader than the genre repertoire as it includes the world view with the position of man and his relation to the surrounding world and often helps to interpret and even explain the observations about language use (see Section 6 below).
In particular, Brinton wants to find out to what extent these verbs have undergone lexicalization (rather than conversion), and whether interjection-based delocutives have undergone degrammaticalization involving grammatical "upgrading" -a shift from more minor to more major part of speech.
Speech act studies represent function-to-form mapping, which is more difficult to deal with than the form-to-functions direction of fit with corpus-linguistic methods.
One limitation of this and most other learner corpora is that most of the interviews are not rated for proficiency level.
Most corpora (spoken and written) are sampled corpora (meaning that they are sampled from a specific period of time).
New directions and challenges for spoken corpus research is the focus of Section 5, including research on fluency, prosody, and non-verbal behavior in spoken corpora, dialect studies, and discourse-level investigations.
The case study investigates stance features in nurse-patient interactions, a lesserstudied discourse domain, and also highlights differences within the interactions and across speaker groups (nurses and patients).
Corpora provide language data which represent a speaker's experience of language in a particular domain and so therefore offer evidence of typical patterning of academic texts.
Section 1 offers an overview of published studies, while Section 2 describes a study which illustrates how corpus research can inform our understanding of academic writing.
Corpus research has broadly supported the view that the schemata of L2 and L1 writers differ and influence how they write in English (e.g.
Drawing on corpora which include academic bios, acknowledgments, undergraduate essays, academic homepages, book reviews, and prize applications, the analyses seek to show how we can understand identity as a performance of writers which is informed and reinscribed over time through their use of language in disciplinary communities.
The basic idea is that a word form or cluster of words which are common in a given text are key to it, it is what the text is "about" or "what it boils down to .
This is the function of concordance analyses, which provide information about users' preferred meanings by displaying repeated co-occurrence of words, allowing us to see the characteristic associations and connections they have, and how they take on specific meanings for particular individuals and in given communities.
So, by checking the frequency of definite, indefinite and "zero" articles in a corpus of bios and then looking at concordance lines for each, we find that professors are far more likely to use naming terms that collocate with definiteness (she is professor of, he is the author of) which serve to uniquely identify them.
Comparing the features of target writers' texts with a much larger reference corpus of work in the same discipline can help to determine what is general in the norms of a community and what represents more personal choices.
These corpora were individually compared with a larger reference corpus representing a spectrum of current published work in applied linguistics and in the same genres as the target texts.
Items such as women, language, gender, men, social, talk, discourse, and work indicate Cameron's concern with the ways language functions to structure social relations, particularly in work contexts, and in the ways gender-linked patterns of language use are made significant in social relations.
Self-reference, in fact, occurs 9.1 times per 1,000 words in the Swales corpus compared with 5.2 in the reference corpus, imparting a clear authorial presence of a thoughtful reflective colleague thinking through issues.
As corpus research into academic genres continues to grow, therefore, we can anticipate an ever increasing broadening of studies beyond texts to the talk and contexts which surround their production and use, beyond the verbal to the visual, and beyond tertiary to school and professional contexts.
Section 4 summarizes the overall accomplishments and challenges of work in register variation.
In addition, the quantitative analysis that is typical of corpus research facilitates comparisons of linguistic features' distributions across registers and judgments about what is common or rare in a particular register.
It is also important to note that the study of register variation is central to one area of work -English for Specific Purposes (ESP).
First, even among sophisticated scholars, register variation is sometimes ignored or described in confusing ways.
What have we learned about register variation that should be applied to better represent the variability that exists in text categories?
Integrating corpus studies with other theoretical background or other research techniques would make register variation studies more applicable to other fields.
In the practitioner interviews, the link between language use and successful engineering practice was a consistent theme.
Section 5 provided a brief example to illustrate the benefits of supplementing register analysis with qualitative research techniques.
The combination allows for more specific applications than register analysis alone, and gives a study greater credibility among content specialists, who would otherwise be understandably skeptical of a linguist's understanding of their field.
With connections like this to other fields, more integration with theoretical perspectives, and continued descriptive linguistic work, the importance of register variation can become more widely appreciated in the future.
After this survey, we provide a more detailed examination of a selection of studies of language variation and change.
We begin by considering detailed studies of single linguistic features whose distribution changes over time in specific registers or in ways apparently conditioned partly by register variation.
Depending on which of the two perspectives mentioned above is used, the textual universe may comprise an entire language variety or specific registers in that variety.
If the aim is to represent a whole language variety, ensuring that appropriate registers are included in the corpus is one of the ways in which historical corpus linguists can enhance the representativeness of corpora.
If the corpus compiler or user has reason to suspect that a given register is linguistically heterogeneous, measures need to be taken to make period samples of the corpus comparable in this regard.
If this aspect is not controlled for, a linguistic difference between two period samples may be interpreted as language change when in reality it is due to register-internal variation.
For instance, the investigation of differences between language use in a mother country and its early transplanted colonies is a fascinating topic for research.
Registers displaying writing habits of those with little or no formal education are valuable sources for the study of dialectal variation and language change; for instance, spelling variants produced by untutored writers can display features of early pronunciation.
Using fresh manuscript material for corpus compilation is a timeconsuming enterprise: in-depth philological and computational work is often required to transfer the manuscript readings into searchable computer files.
One fundamental problem in the study of language change of past periods is that the material has been preserved in writing.
An alternative approach to register variation is to consider the occurrence of a large number of linguistic features as they cooccur in texts (see also Chapter 17).
One of the most important findings revealed by multi-dimensional historical register analysis is the existence of longterm trends in usage.
The next section will be devoted to two of these trends and their significance for historical register analysis.
During roughly the same time span, a trend towards densification has been noted in several registers.
Research has shown that register is one of the many extralinguistic categories which lie behind the linguistic variation that is a prerequisite for language change.
Typically, literary use of language is seen as creative.
Such generalizations are captured in dictionaries, grammars, and textbooks on the use of language.
The retrieval of five-word clusters in a 4.5-million-word Dickens corpus and their key comparison with a nineteenth-century reference corpus.
This functional analysis also takes account of distributions across Dickens's texts and the nineteenth-century reference corpus and hones in on detailed textual examples discussed mainly from an intrinsically explanatory point of view.
Even more fundamentally, however, the corpus-linguistic study of language has great potential not only to show differences between literary and non-literary language but to shift the focus to the similarities between the two.
Although both approaches allow for dialect variation to be observed, they provide different perspectives on language variation and change.
Although collections of language data elicited through interviews and questionnaires are now commonly referred to as corpora in sociolinguistics and dialectology (e.g.
This percentage was calculated by dividing the number of occurrences of contracted not following one of these four verb types by the number of occurrences of contracted not following one of these four verb types plus the number of occurrences of full not that could have been contracted following one of these four verbs types, and by then multiplying this value by 100.
It is therefore not surprising that the original corpus design included letters (both social and business) as a text category to be sampled for the written part of the corpus.
It is therefore not surprising that a few ICE teams have gone against the original design (which stipulated the inclusion of e-mail as a separate, additional text type) and have (also) sampled e-mails.
Qualitative analyses of the corpus data show that variable use of PPs and SPs is at times difficult to categorize.
Corpus work is therefore still rare; the databases that have been collected have mostly been small, and are perhaps best counted into the very generic category of "corpus" that in traditional philology was used to describe the language data investigated for a study.
When we look at these kinds of language use as situated in their social contexts, the differences become more pronounced.
The social parameters of being a learner in classroom settings in particular creates rather specific settings for language use.
Where ELF is special is in certain principles of corpus compilation.
In contrast to a learner corpus (see Chapter 23 this volume), an ELF corpus seeks to include speech in a natural, often complex mix, rather than selecting for given L1 backgrounds and comparable proficiency levels.
Unlike World English corpora (see Chapter 21 this volume), an ELF corpus does not seek to capture a local or regional variety of English, which would then lend itself to comparisons across others of the same kind.
Looking more closely at these word lists making up 50 percent of the corpus data, Mauranen has compared ELFA's 44 word types with the 58 found in MICASE.
By providing a manually checked, dual POS tag which encodes both form and function (VOICE Project 2013), the XML corpus can be searched for all cases of an "innovative" form, in Cogo and Dewey's terms.
This meticulously annotated VOICE dataset should put to rest the exaggerated claims of word-level variation in ELF, as the corpus findings are unequivocal.
Corpora have nevertheless already shown their powerful potential: they have helped gain a big picture of the prominent linguistic processes in ELF, and revealed new facts about second-language use (SLU).
However, ELF research looks at L2 use from the same perspective as any other natural language use, setting L1 and L2 speakers on a par.
Unlike the more experimental data types often used in SLA, where learners are forced to produce a particular form (as in fill-in-the-blanks exercises or read-aloud tasks), the focus in learner corpus data is on message conveyance and the possibility for learners to use their own wording.
In principle, like any other corpora, learner corpora need to be authentic, i.e.
A survey of the learner corpora currently available (see www.uclouvain.be/en-cecl-lcworld.html), however, reveals that some types of learner corpora are more common than others.
Thus, while in principle learner corpus data can be collected from learners at all proficiency levels, most corpora to date represent the more advanced stages.
However, this balance needs to be redressed and projects such as the Role Play Learner Corpus and the Louvain International Database of Spoken English Interlanguage are particularly welcome.
To take full advantage of these spoken learner corpora, which are usually released as transcriptions, it would also be advisable to have access to the audio files, so as to allow the investigation of learner pronunciation and prosody.
The majority of learner corpus studies are based on raw data, i.e.
Having been developed on the basis of native corpus data, POS-taggers may not perform as well when applied to learner texts.
This general research orientation is consistent with the very nature of learner corpora which are usually collected as generic resources to be used to answer a wide range of research questions not identified at the time of collection.
However, some learner corpus researchers have adopted a more explanatory research design, which uses learner corpora to revisit important SLA findings.
Several studies demonstrate how learner corpora can be used to develop pedagogical tools and methods which target more accurately the needs of the learner.
The objective of this study is to investigate the rhetorical function of causality in scientific text on the basis of an expert corpus (taken from the MicroConcord Academic Corpus Collection) and a learner corpus (taken from the Hong Kong University of Science and Technology Learner Corpus).
Capturing L2 accuracy developmental patterns: Insights from an error-tagged EFL learner corpus.
Yet, a number of directions should be further pursued before LCR can be said to meet the methodological requirements that are expected of corpus research and empirical research in general.
On the downside, however, is the fact that very few learner corpora contain truly longitudinal data, with the same learners followed for an extended period of time.
Another particularly positive development is the integrated contrastive model which establishes a close link between learner corpus studies and contrastive studies, thereby paving the way for more rigorous investigations of transfer.
The combination of learner corpus data and experimental data is another area in need of further exploration.
Through this study, we were interested in finding out how learners of English from different mother-tongue backgrounds use DMs, and how their use compares to that of native speakers (quantitatively and qualitatively), but we also wanted to demonstrate that when doing learner corpus research one should consider individual data in addition to pooled data.
The exploitation of a native corpus, used in combination with the learner corpora, makes it possible to see how the learner data are situated in relation to a certain reference norm (without being limited by it), whereas the inclusion of the L1 variable gives a glimpse of the possible influence of the mother tongue.
Like many other learner corpus-based studies, however, this one does not examine the wide range of variables that are available in learner corpora like LINDSEI and that may have an effect on the learners' use of DMs (such as the time spent in an English-speaking country or the knowledge of other languages), nor does it consider the general context in which the learners acquire English (e.g.
With massive amounts of computerized texts -now more easily obtained than ever before -at a keystroke one can generate a frequency list in a fraction of the time it would have taken to achieve the same task by hand.
Nonetheless, on a qualitative level, the corpus research and concomitant issues taken on by Thorndike and colleagues over a century ago remain largely the same.
Much like today, the researchers had to make decisions about what corpus size was needed (or adequate), and how and why they would choose the texts that comprised the corpus.
One question that was asked, however, that was often not asked as much in the wake of computerized corpus linguistics: Is it enough to simply provide a list of words?
The first one is related to corpus size.
That is, they occur frequently as multiword units (good morning, never mind), and their meaning is often not clear from the meaning of the parts (at once, set out).
WordSmith Tools (version 5.0) was chosen, among other reasons, due to its compatibility with the latest BNC XML edition (used in our study) and ability to generate both word frequency lists and lists on recurring strings of words (or "n-grams").
Once generated, the concordance was saved and then a special command -"delete to N" -was used to reduce the concordance lines to a random sample of just 100.
In the case of at first, out of 100 randomly selected concordance lines, 84 exemplars of at first in its phrasal adverbial sense remained -or 84 percent of the original total.
The expression take place, for example, in its uninflected form had a frequency count of just 3,248.
The aim of this chapter is to assess the impact of corpus data on the description of phraseology in various types of English dictionaries.
DANTE is a lexical database that provides a fine-grained description of the meanings, grammatical and collocational behavior, and text type characteristics of over 42,000 English words.
Collocation boxes where salient collocates as identified by a statistical analysis of corpus data are organized by part of speech.
LDOCE5 is the only dictionary that provides collocation boxes and phrase banks for almost each word, while MEDAL2 deserves special mention for offering collocation boxes at the level of a word sense rather than for the word in general.
Granger and Lefer's study thus also provides compelling evidence that parallel corpora can be used to improve the number and accuracy of translation equivalents.
Co-occurrence analysis lay at the core of the pioneering COBUILD project and collocations now feature prominently in (at least) British pedagogical lexicography.
There are many different types of English corpora (see Chapter 1 this volume) but the most widely used corpus in lexicography is the large monolingual reference corpus.
While the large monolingual reference corpus is an extraordinary source of lexicographic data, other types of corpora certainly deserve a more prominent place on the lexicographer's computer: specialized corpora, parallel corpora, and learner corpora.
As regards the use of learner corpora, they certainly have a major role to play in the prevention of phraseological errors.
The subsequent section takes the form of a preliminary metaanalysis in order to assess more broadly the benefits derived (or costs incurred) from the direct use of corpora by learners.
A recent development is an increase in studies at the level of text, including discourse and critical analysis, genres, sensitivity to text type or sociolinguistic variation.
The Web is used as a corpus in ten studies, whether through a generalpurpose search engine (e.g.
Experimental subjects used concordances to work with their new words exclusively, inferring meanings from multiple concordance lines and only using a dictionary to confirm their inferences, while controls used the same software but with a bilingual dictionary as the information source.
At the bottom is the combined effect size along with its standard deviation, and the 95 percent confidence intervals.
Given the broad sweep of focus in the various primary studies, it seems that corpora can be of benefit to L2 users for a range of purposes: learning and use of language anywhere on the lexico-grammatical continuum (including collocation and idiom) for both receptive and productive purposes, as well as in more extensive reading and writing tasks or in translation.
In the survey presented here, we were gratified to uncover a measure of confirmation from research to date that corpora have been not only effective in language teaching and learning, but also efficient, insofar as they produce fairly regular advantages of a standard deviation or more over other methods of achieving the same goals.
When it comes to instructed second language acquisition, however, the use of authentic vs. simplified/didacticized materials is still hotly debated, with some arguing that authenticity is key to language teaching (Ro ¨mer 2005 suggests that texts included in textbooks should mirror the frequencies and uses of present progressives in native-speaker corpora), others presenting the pros of simplified/didactized materials (e.g.
As an attempt to consider how corpus research can be incorporated into materials to reflect authenticity we propose a number of ways in which corpus research can be used to inform materials.
Linguistic descriptions have shown that intuition is often unreliable when it comes to matters related to patterns of language use.
Linguistic descriptions based on both large comprehensive corpora and descriptions of specialized corpora have greatly contributed to knowledge of the linguistic characteristics of language use across different situations and production circumstances.
These differences notwithstanding, we opted for variety rather than homogeneity of material types as this variety is more representative of the impact of corpus research on teaching materials.
Some of the entries contain (usually at the end of the entry) information about the typical errors made by learners and those errors come from the Cambridge Learner Corpus.
The integration of learner corpus data (and use of error correction exercises) is another case in point.
The fact that information from learner corpora was absent was to be expected, but much less so was the total absence of focus on errors.
Here again, whilst we agree that newspapers usually contain more passive forms than some other text types and that many learners (who often underuse the passive) need practice in using passive sentences, we feel that the exercise presented here might be counterproductive.
It must be noted, however, that it is the only book where all the examples are clearly identified as coming from corpora (the text type is always listed).
The results of corpus studies carried out on native and learner corpora can help textbook designers prioritize features, be it on the basis of frequency of use (native corpora) or of difficulty in acquisition (learner corpora).
Bilingual or multilingual parallel-corpus analysis is inherently more complex than monolingual corpus analysis, requiring a descriptive framework includingcruciallya tertium comparationis, or common platform of comparison, and for this reason mostly relying on the direct observation of parallel concordances.
These issues have been investigated by means of monolingual comparable and parallel corpora, the former being seen as more innovative and powerful than the latter.
Profile-based chi-square distances are calculated for the different varieties, distances are mapped on a two-dimensional plot, and confidence ellipses are drawn around each variety: lack of overlap indicates a statistically significant distance.
The interaction between text type and translation-related variety is also calculated and displayed graphically.
Despite some limitations (gaps in the corpus for certain text types, as acknowledged by the authors, but also lack of reference to STs and parallel observations), this study is an example of best practice, nicely combining a careful corpus design, a linguistically motivated choice of patterns, solid grounding in theory, and sophisticated statistical techniques complemented by intuitive graphic representations.
Can we attribute the use of standard language in certain text types .
In a nutshell, bigram types are first obtained from the corpora of translated and non-translated texts used for the study; they are then matched with frequency and Mutual Information data obtained from a reference corpus of English, and ranked according to these data.
As a reference corpus, the ukWaC corpus was used.
The intervening function words are used to constrain searches but are not retained in the subsequent phases: only the association between lexical words is tested against the reference corpus.
Word pairs are ranked according to their bare frequency of co-occurrence in the latter corpus, and according to their MI score, setting a cut-off point of MI>2 with FQ≥2 to exclude hapaxes and extremely low MI values.
The latter can be observed thanks to the corpus design, which includes a parallel component.
Second, I am using a linear regression model for this initial explanation.
In the following section, I discuss the example of particle placement, which will involve a binary categorical dependent variable and add the multi-level perspective that corpus data routinely require.
In both these steps, 'optimal' means according to some criterion such as significance testing/p-values or information criteria.
With p-values this would mean that the final model, m, contains (i) only random effects that make m significantly better than if these were not in m and (ii) only fixed effects predictors -again, independent variables and their interactions -that make m significantly better than if these were not in m or that are required for higher-level interactions.
Thus, the argument 'Once we have more data everything will be classified better even with BLRs' is flawed: even for some medium-or even higher-frequency verbs and particles (such as get/have and together/back), substantial improvements in classification accuracy are obtained.
Not only would this end the way in which corpus linguists nearly always violate basic assumptions of our statistical tests -because we would finally take into consideration that our data points are not, usually, independent -but it would also allow us to, again finally, take more seriously the exploration of corpus data on the multiple levels of sampling that corpora come in.
Corpora can be exported balanced, that is to say, the system automatically searches for the class with the largest number of instances and cuts instances from the other classes.
It is in this way that the corpus linguistic approach bears great potential not only for the study of language use but also for the demarcation of possible structures.
Language use involves numerous decision-taking processes whereby users choose between alternative ways of expressing the same thing during test production and recipients choose between different ways of interpreting the structures they perceive.
A major concern with language use is shared by a range of sub-disciplines in linguistics.
Sociolinguistics is concerned with the variability of language use and seeks to correlate these with the social features of language users and their interlocutors.
Other areas of linguistics where details of language use are of central concern are psycho-and neurolinguistics.
An area related to sociolinguistics, psycholinguistics, and acquisition research is that of language change and language evolution.
Corpus linguists' concern with variation is key to this programme, labelled language variation and change within variationist sociolinguistics after William Labov, since variation is a necessary condition of change.
The second part of the book focuses on some of the tasks you might do in conducting your own corpus study including querying a corpus and evaluating the results (Chapter 5), and composing and building your own corpus (Chapter 6).
A text is any instance of recorded language use that can be treated as a discrete unit: a newspaper article, a recorded university lecture, or dinner table conversation.
In linguistics we distinguish between linguistic knowledge and language use.
Language use refers to what speakers do with language and how they act in a given society by using language.
In contrast to linguistic knowledge, language use is directly observable and recordable, and the texts in a corpus essentially comprise such records of language use.
The corpus-linguistic approach is the systematic study of language use represented by the texts in corpora, which targets both linguistic knowledge and language use.
It is important to keep in mind that written language use is historically derivative of spoken language use in the sense that writing conventions developed after spoken language.
In modern corpus linguistics, transcriptions are linked to audio or audio-visual recordings of texts so that a spoken and signed language corpus consists of transcription text files as well as audio and/or video files.
Strings of words are how texts are represented prior to any further corpus annotation (cf.
The term collocation refers to the co-occurrence of multiple lexemes, often two lexemes, called a bigram and a cooccurrence of three lexemes is called a trigram.
This brings us to the problem of representativeness: shouldn't we consider all the language use in a given language?
In other words, the specific texts included in a corpus are meant to stand for language use more generally, that is, represent it.
This is the crucial component of any corpus building or compilation project (see Chapter 6) or of carefully using the metadata of existing corpora.
Wordforms are immediately observable and can be searched for; larger structures can only be detected through linguistic analysis and searching for these requires further corpus annotation (Chapter 7).
Corpus size is typically reported as the number of wordform tokens.
Despite the similarity between the two corpora in their web-based content, the striking difference in size is clearly related to a difference in corpus design: while iWeb includes just about any text from almost any website, CORE has been created with the idea of identifying as much and as clearly as possible the situational characteristics of all texts included which constitute the web registers mentioned in the title.
Corpus size is often directly dependent on limitations on resources and other more practical considerations, a point we will return to in Chapter 6 on corpus-building.
One means to represent different situationally defined text types evenly in a given corpus is to aim for balance.
These reflect rough distinctions of texts in terms of their external The rationale behind this kind of corpus composition is that each cross-category (section/year) has the same share of the overall corpus data and will thus contribute approximately equally to results from any corpus query.
The major idea here is to avoid massive skewing in results by over-representing just a single or very few text types.
For some text types, however, matters are quite obvious: for instance, the relatively small amount of spoken texts even in large or super-large corpora of well-studied languages like English is quite clearly at odds with the reality of language use, and more recent corpus building projects have been aiming at including greater proportions of spoken texts, for example, the International Corpus of English (ICE) where 60% of texts are spoken.
Language documentation-based corpora are typically less varied in terms of text type.
Certain special corpora to be discussed in 3.3 are characterised by a fairly confined set of text types that researchers are particularly interested in, for instance, the oral interactions between plane pilots and air traffic controllers.
As with our conclusions on corpus size, we take all different types of corpus composition to be viable options in corpus linguistics.
As empirical linguists we are interested in investigating all instances of language use and their conditions, whatever their nature.
The only type of text we do not consider corpus text is mere mentions of structures, for example, intuited example sentences.
Do some of the expressions extend to other text types?
Given our definition of representativeness, the major issue with very small corpora is that they will hardly have a chance to achieve any reflection of a wider range of contexts of language use.
We only need to consider the vast corpora like iWeb or corpora of the TenTen Family 3 to see this clearly: despite their massive sizes these corpora seem to not reflect well the large range of diverse instances of language use and, hence, fare relatively poorly in terms of representativeness.
We have seen above that the creators of the Brown corpus (and other subsequent corpora following the same design principles) sampled written English texts according to their intuitions about the proportions of different text types.
Relating composition back to corpus size, it should be clear that composition takes precedence over size: tremendously expanding the size of a corpus may not help improve its representativeness very much, as is the case with iWeb.
This imbalanced situation contrasts with a more prototypical situation of language use in everyday conversations, where produced texts are not normally perceived by a larger public, and where producers and receivers shift roles regularly.
Given that both production and perception of previous texts can influence people's behaviour, the perception of texts is important for considerations of representativeness and balance during corpus building.
Given that the purpose of a corpus is to serve as an empirical basis for the study of human languages, it is inevitable that a corpus has to be treated as a representation of language use beyond the specific texts included in it.
Obviously, our motivation for building corpora in the first place and investigating them is to learn about language use in a given language more generally.
Investigating saturation presupposes a dynamic perspective on corpus content and its coverage of linguistic forms available in a language variety.
As with representativeness, corpus size is a necessary condition for saturation but not a sufficient one.
We also need to consider matters of composition, since some lexemes or constructions come up only in specific text types.
Most relevant for corpus linguistic studies of variation in language use is the register dimension: this is characterised by the situational properties of texts.
It is these kinds of interrelations between situational characteristics, linguistic features, and the functional connection between them that is the core concern of a register analysis.
You could object that these formulas still serve some communicative function, at the least by announcing the type of text they also prepare recipients for their task, that is, to stop talking when a speech begins!
The major difference is that they are not motivated by communicative functions; they are due to the specific habits of language use of individual language users or groups thereof (e.g.
Metadata are data about the corpus files and the structure of the corpus as a whole and the compilation process (including design decisions), as well as data about the situational characteristics of texts.
In language documentation, raw data consists primarily of video and audio recordings of the spoken or signed text production.
Corpus composition and corpus types called corpus data, namely, the searchable text data of a corpus in a written form in digital format.
One sub-type is a reference corpus: a general corpus taken to be a standard reference for corpus-based research on a given language.
An example is DeReKo -Deutsches Referenzkorpus 'German Reference Corpus' , whose purpose is to make available a large corpus of German amenable to a large variety of research questions.
For instance, in learner corpora, we will still want to see learners of different stages, genders, age groups, L1 backgrounds, etc.
Even for very restricted corpora in terms of text types, like ATC corpora, variability in situational features is relevant, and so these will have to contain text specimens produced by female and male pilots of different age groups, different linguistic backgrounds, and so forth.
In other contexts, the situation is more intricate: an example is the work of Douglas Biber and colleagues who famously investigated English language use in US universities, their 'university language' .
These aspects are related more closely to aspects of research design and interests as well as corpus creation and curation.
As a matter of convention, we would classify as diachronic only corpora that cover at least multiple decades of language use.
This is because the purpose of a diachronic corpus is to enable comparison of language use across time spans, and that would hardly be possible if the texts included in each temporal section were vastly different, for example, personal diaries in some time spans and newspaper articles in others.
Typical morphosyntactic annotations are part-of-speech tagging (PoS tagging) which captures the word class and other morphological and syntactic properties of token wordforms in a corpus; such corpora can be loosely classified as tagged corpora.
Another type of corpus with special annotation is a speech or phonetic corpus.
This figure represents the first 2,000 words in any given text, plus the number of words to complete a sentence started so that the text snippets included contain all complete sentences.
Language use can be conditioned by who is interacting to whom and when, how much interlocutors know about each other and what they are discussing, whether the language use is spontaneous speaking or signing or writing and what the genre, register and style of speaking, signing or writing are.
Take for example written text found in a textbook and spoken conversational language use.
Many conversations are full of spontaneous, unplanned language use, whereas the language in a textbook is produced more slowly, usually has gone through multiple drafts and is more carefully created.
Corpus research of variant alternation is done at all linguistic levels: semantics, phonetics, phonology, morphology, syntax, and discourse.
And are some elements of the behavioural profile more useful for sense disambiguation than others?
More data would be needed to fully specify which elements of the behaviour profile are most significant for word sense disambiguation, but in principle this would be possible, meaning that examinations of word profiles can be useful for both issues of semantic theory and practical applications.
Phonetic corpus research is concerned with the acoustic measurements of spoken language in context.
With this extra annotation, researchers can look at issues like word length, vowel acoustics, various phonetic realisations of words, phone realisation, speech rate, and so on.
The results of this study indicate that rule-based processes affect not only word realisation but statistical tendencies built up from experience of language use affect word realisations as well.
The results from these studies are used to inform theories of language organisation and language processing.
Corpora show the probabilistic nature of morphological productivity, among other aspects of language use, and challenge traditional theories of the conception of language.
If you wanted to create a computational tool to process language data, it needs to be able to process new data it has not encountered before.
Corpora, then, are used to demonstrate that it is imperative to assess what actually happens in language use when constructing theories of language.
Fast speech may Think about other fields where language use really makes a difference to people's lives like legal proceedings or air transportation.
Levels of linguistic representation lead to a higher amount of speech for the interlocutor to understand in a shorter period of time, leading to a longer FTO for planning purposes.
Studies relating to conversational analysis are often nowhere near as large as this one, so we see the opportunity in utilising large corpus data to help bolster previous analyses and provide the springboard for future analyses.
Corpus data comes in many different formats, and there are easily hundreds of methods, tools, and strategies that are possible.
What we might recommend today (a certain corpus tool, a certain programming package, or module) might not be available three years from now, or might have advanced so much that our information is out-of-date.
The main reason to use a corpus is to find real examples of language use in context and count how frequent they are.
Many of these programs will provide basic descriptive statistics of the data, such as number of occurrences, bigram frequency (cf.
Each of them has some corpora that have been annotated with additional information like parts of speech (cf.
In some cases, you can also download the corpus data for your own processing with additional software.
Capitalised He and lowercase he should be considered to be the same word type.
Keywords are calculated by assessing all the word frequencies in each of the two (sub)corpora and doing either chi-squared tests or log-likelihood measurements to assess what words are statistically more frequent in one (sub)corpus than in another.
One of the first things many corpus linguists plot is the frequency of the words in a given corpus to see which words are most and least frequent and to examine the frequency distribution.
Many distributions of corpus data follow the Zipfian distribution, where there is a 1-to-1 logarithmic (log) relationship between the rank and frequency of events (i.e.
Some corpus programs will highlight collocates of a word of interest with different colours depending on their parts of speech.
A KWIC can usually be sorted alphabetically or by frequency of co-occurrence of w-1, w-2, w-3, w+2, w+3 etc.
Also try putting 5-gram sequences into an internet search engine surrounded by quotes.
One of the most common types of annotation is PoS tagging (cf.
One thing to watch out for with PoS tagging is that it is often automated.
In this chapter we turn our attention to the process of corpus building (or corpus compilation) itself.
However, when it comes especially to the documentation and description of smaller languages that have not previously been investigated in much depth, you may be involved in corpus compilation to a considerable degree, including having to make relevant decisions on corpus design and structure.
Finally, whether you work on well-researched or lesser-studied languages, considerations of corpus design will also apply to compilations of small, focused research corpora which may or may not draw on larger, pre-existing corpora.
Many of the considerations involved here are also relevant for the reverse perspective of a corpus user or analyst who needs to understand considerations of corpus compilation (both theoretical and practical ones) in order to properly evaluate their corpus findings.
For any of these types, corpus building involves the selection and/or collection of texts or text excerpts and their inclusion in some form of data infrastructure.
We will first outline general corpus design principles and then turn to the more practical issues of data collection and/or selection in Section 6.2.
Text selection is a non-trivial aspect of corpus building for general corpora: since we aim at a high degree of general representativeness, we need to consider carefully the composition of our corpus.
As we have discussed in 3.1.1, determination of corpus size is a non-trivial issue: we need to set the unit of measurement we find most appropriate -for example, orthographic or grammatical words -and then make sure that we can actually count tokens thereof -which may require tokenisation processes before counting can take place (cf.
After evaluation, you may determine that additional texts or text types are necessary to achieve better representativeness.
An essential requirement for corpus building is that all aspects of text production be available in some way to the users of the corpus.
In 21st-century corpora this means that the corpus text and the linked media file be accessible from a single entry point, for example, a website or a corpus query software.
In this section, we will go through a number of scenarios and conditions that pose restrictions on corpus design as desirable from a purely scientific point of view.
This does not mean, however, that considerations of corpus design become irrelevant on the ground, and we will point out what can be done better even under difficult circumstances.
For many languages, there are no written forms until language documentation projects start, so time-consuming transcription of spoken or signed texts are necessary.
There are different ways in which the raw data -the audio and/ or video recordings of the speech event -can be made accessible to corpus users: in the SBC, the solution is to store two types of file for each corpus text, one audio file containing the text recording in WAV format and a text file containing the transcription thereof.
For corpus building efforts, in particular, one needs to consider character encoding.
Having a series such as <ng> to represent a single sound may be easier for input than an engma character (ŋ), but will require consideration later on of what character counts mean for measures such as word length or consonant to vowel ratios, etc., or processes such as forced alignment (cf.
The purpose here is obvious: with our interest in modern languages encompassing thousands of different languages no corpus user can be expected to have command over every language under study.
Chapter 5), compiling the corpus data into a standard format is an important step.
All these aspects of corpus publication are intended to make the use of corpora by multiple parties, and that of any linguistic data more generally, the norm in the language sciences.
In particular, LD corpora may come with severe limitations on corpus design, given that related projects have limited funding, and also because such projects are often undertaken by individual academics (though in collaboration with community members).
This is the typical constellation for corpus building today, and it is a major reason why only a few larger corpora are web-accessible to date.
Various types of corpus annotation can make these aspects explicit and searchable.
This can be done in the form of an annotation manual that outlines the conventions that have been applied in creating a set of annotations and at the same time serves as guidelines for users who intend to implement the annotations themselves on their own corpus data.
The transcription of spoken raw data and idiomatic translations of corpus texts are dealt with in Chapter 6 as part of corpus building.
Corpus annotation involves enormous amounts of work.
Speech corpora are a special type of spoken corpus (see 4.2.2) that are intended for corpus-based investigations into the phonetic and phonological structure of spoken language use.
The first type of annotation targets individual sound segments and is essentially a type of transcription of spoken language raw data, phonetically a very close and exact one.
As the name suggests the annotation picks up the word class membership of word forms, traditionally called parts of speech.
Many of the classic and larger corpora of English contain PoS tagging, for example, COCA or the Brown family corpora.
Thus, where a corpus user is interested only in instances of matters that are plural forms of nouns they can search for a string <mat-ters_NNS> rather than just <matters>.
Tagsets are developed for specific languages, and often indeed for individual corpora, so that when you use a corpus, you need to consult the corpus metadata, manual, or other documentation.
For instance, the tag <_NNS> classifies the word form as a common noun in its plural form.
The Brown tagset also picks up specific forms that are particularly relevant for the analysis of grammatical (sentence) structure, thus breaching strict assumptions of word class membership; this is typical of tagsets for English.
What is relevant for our purposes here is that corpus and tagset developers entertain a considerable degree of freedom in designing tagsets to serve their specific needs, so that tags do not necessarily reflect the absolute exactness of linguistic analysis and classification.
But in addition to PoS tagging syntactic annotations also pick up more aspects of a syntactic structure.
Remember, however, that lexical category information will be contained in the PoS tagging that we have discussed above in its original Brown version, so that, for example, the NP-embedded PP would be fully annotated as in (7.8) (note that the separator in Penn tagging is forward slash / rather than underscore _): These searches will thus give us a count of NPs with and without recursive structures, that is, where an NP occurs embedded in a higher-order NP either as an initial possessor NP or as the complement of a preposition of an NP-embedded PP.
One major concern in the corpus-based semantic analysis is word sense disambiguation which is not inferable from the surface structure of the corpus text itself and needs to be determined by a human interpreter.
Two aspects of discourse coherence are particularly relevant for which corpus annotation systems have been developed.
Co-reference relations are probably the prime example of implicit information that can only be determined by human interpreters and that for this reason require corpus annotation to make them explicit.
One of the most prominent of these is the EAGLES standard for PoS tagging in different languages which yields basically comparable annotations (cf.
UDs are extensively annotated with PoS tags and syntactic dependencies (cf.
The first column has numerical identifiers of each corpus word form.
The next one has the word form followed by lemma annotation.
The fourth column hosts the universal PoS tag followed by a language-specific PoS tag, followed, in turn, by a list of language-specific grammatical features of the word form.
The next column contains a number cross-referencing the word form ID in column 1 that identifies the head of which the word form in question is the dependent.
Finally, constituent order in terms of grammatical relations can be determined in relation to the word form IDs.
This is achieved primarily by including very general PoS tags as well as general definitions of dependency relations; specific dependencies as cross-referenced to heads, are not language-specific anyway.
That is to say that annotators of any given language corpus are asked to annotate structures in a typological comparative perspective.
Taking a closer look at the clause structures by going back to the corpus data, try to develop some ideas as to what may potentially explain these differences, or at least be part of the story.
Now trade annotation schemas with a classmate or trial it on a different language or text type.
For instance, counting the number of times something happens under various conditions and then seeing if that is different from what we would expect if the distribution was completely random.
Corpus data are rarely normally distributed though.
Brainstorm nominal, continuous, discrete variables you might really use for a corpus study.
Can you also think of some ordinal or interval IVs or DVs that might be used in corpus research?
Histograms are a special kind of bar plot that display counts of continuous variable measurements (the figures in Section 5.4.1 on frequency were bar plots but not histograms because they were counting categorical data).
From a parsed and glossed sub-corpus 2 of 8,961 words, verbs and nouns make up 5,375 word tokens.
A very common measure of correlation is r, or Pearson's product-moment correlation coefficient.
We will completely skip non-mixed-effects regression (fixed-effects only) because it is almost never appropriate for corpus data.
Which word class has the strongest correlation?
Most analyses of corpus data should, rightly, be labelled and conducted as mixed-effects regression.
We expect that the fixed effects would show similar patterns, even if we added new data or applied our model to a different dataset.
A regression model fits a coefficient to each level of the fixed-effects predictor (how much change from the mean does each level reflect).
In a linear regression model, positive and negative values have clear mappings: IVs or levels of the IV that make the DV smaller (for instance make a word shorter) will be negative, IVs or levels of the IV that make the DV larger will be positive.
Let's make this more concrete and apply a mixed-effects logistic regression model to our Vera'a data.
The p-value represents the chance that the null hypothesis would be true if we observed this sample of data.
A high p-value like 0.823 for the P function, means that this result could easily be obtained by chance.
There is a tradition of considering p-values below a certain threshold, like 0.05 or 0.01 to be significantly unlikely to occur by chance, and then these IVs or levels are then deemed "statistically significant".
The p-values we obtain in statistical modelling are never 0 or below 0.
That would mean there is a positive interaction, and we would see this reflected with a positive coefficient for an interaction term in a model with a high standardised score and low p-value.
If the effect was additive, then we would see either a positive or negative coefficient but with a low standardised score and p-value that lets us know the result is likely to be by chance.
A categorical DV will use a classification tree and be implemented for a 2-way or 3+-way DV in a reasonably straightforward way.
A binary classification tree divides the data into two groups based on which data points are most different from each other, using the given variables.
Let' s apply a binary classification tree to our Vera' a data, excluding here Speaker and Text because their many levels may cause too many splits in the tree, making it difficult to read.
Speech act participants (the first and second person) are significantly more likely to be expressed by a pronoun, and the first person even more so, as seen in node 10.
We will not focus on clustering here, but we list below some of the main clustering types and some resources to find out more about clustering corpus data.
Despite this, reporting only a p-value is not sufficient if you want to accurately report your results.
Sociolinguistics often focuses on a shorter period of time than historical linguistics.
Finally, much of our older historical records of language use do not have the kind of detailed metadata required to carefully study communities.
This is also because longer stretches of language use occur in this portion of interviews, meaning researchers have a better context to study their targets of interest.
Unless a spoken text corpus has been phonetically annotated (cf.
This means different kinds of questions are asked for the data and that many explanations for patterns of language use come from understanding the communities, people, and personalities the linguist is working with.
However, the use of corpora is also possible for dialectometry.
Corpus-based dialectometry requires dialect corpora that are representative of multiple geographic areas associated with a particular language variety.
Through much sociolinguistic research, it is also clear that many processes of language change come about through women leading change in their communities.
Private school education means Black students now have exposure to a variety of English that was previously associated with English-speaking Whites.
Conditional inference tree analyses on F1 and F2 of the schwa vowel for different environments (word initial, final and medial) showed that female private-schooled young people use more centralised variants and are most different from their male non-private-schooled counterparts who use more peripheral variants.
An apparent time study uses data from one time period within a variety of languages, but that includes data from language users of different ages.
Fruehwald examines a set of variables in this dataset (vowel changes and whether or not speakers use um or uh for filled pauses).
Fruehwald takes these careful results not only as support for the Apparent Time model of language change but also an indication that age is a complex variable that is best studied by taking into account the language users' year of birth and the year of interview to assess changes.
Language documentation is the area of linguistics that aims at recording the observable use of language in a given society as much as possible, and doing so in as many societies and associated languages around the world as possible.
The goal of language documentation is to create a lasting record of languages' use, which crucially includes a corpus of transcribed and translated records gathered during fieldwork (cf.
Modern language documentation began in the late 1980s -beginning 1990s, at least partly as a reaction to the dramatic decrease in the diversity of human languages.
Documentary linguists want to capture examples of many kinds of language use in context so that if a language dies out, there is a record of how people used it.
There is, however, more to the enterprise of language documentation as we will see.
So, a typical record in language documentation could be a video-recorded conversation, for example, about building a canoe, where we see people around a half-hollow tree trunk point at different spots and comment on what needs to be done, and possibly explaining to the researcher what is going on.
Language documentation shares with corpus linguistics the basic goal of representativeness.
This bears some consequences for corpus building in language documentation because it means that our corpora are mostly haphazard and opportunistic.
Himmelmann' s (1998:166) use of the term linguistic practices refers not only to verbal behaviour, that is the use of language as immediately observable and, thus, directly recordable.
How can metalinguistic knowledge be recorded during language documentation and how are they relevant to corpus building?
The kinds of texts and interactions that are recorded and transcribed as part of a language documentation project may not be the kinds of texts one would record if planning a corpus.
However, the practical limitations on corpus building outlined in Chapter 6 are particularly relevant in documentation projects.
Not ideal as the sole basis of a text corpus, but could be used for phonetic measurements.
After assessing what is present in the collection, then you can plan for what you would like to add and focus on that in your corpus building enterprise.
For language communities, language documentation gives a broad sense of how their language is used.
To achieve this, we need annotations and metadata to turn collections of data into corpora that will form the basis for corpus-linguistic investigations of language use, and what this tells us about the structure of a specific language's grammar, lexicon, etc., as well as the conventions of the language use and that of the community members' thereof.
The necessary processing steps of raw data for LD corpora consists not only in time-aligned transcriptions -given that LD corpora target primarily spoken language use -but also in the translation of the transcribed text into a language widely known by anticipated user groups.
In the Matukar Panau list, we see function words and a content word (tamat, 'man').
If there is a presence or absence of grammatical marking due to certain context features, this can be appropriate for a corpus study, particularly as part of the language description processes.
Grammatical marking can also be studied in terms of co-occurrence.
Identify three real use cases of word frequency lists from your web research.
In the language documentation process, word analysis like parsing and glossing helps the researcher understand the structure of the language.
If you build a corpus out of language documentation data, you probably have a very good idea of what is in the corpus and the context for that information, much more so than with a multimillion-word corpus.
Small corpora from language documentation probably contain a great deal of spontaneous language data.
This data is often considered the best kind of language data available to understand how people really use language because it is less considered and belaboured than written text.
At the same time, it is worth noting that corpus-based work on lesser-documented languages is of particular value given how little we know about language use in many languages.
In some work each language as a whole is treated as representative of one feature value, for example, a specific number of cases.
The main concern of CBT is to systematically study language use across diverse languages.
A key concern here is what is known as usage-based approaches which seek to explain attested grammatical structures across languages in terms of the communicative function of language in use and relevant constraints on language processing.
This suggests that many aspects of language use should be universal, but there is also the possibility that language use, related communicative functions, and underlying patterns of processing are different across linguistic communities.
What is universal in language use?
All these universals on language use are statistical universals of language use 1 : They reflect tendencies in language use, and hence, require approaches like corpus linguistics to be discoverable.
This type of harmony is widely regarded as preferable for language processing since it allows language users to recognise the syntactic relations between phrases easily and quickly.
What is further significant about this reformulation of the universal is that it yields a generalisation about language use that also generalises across languages.
Corpus-based typology can be represented variably in corpora, and this means that case marking needs to be investigated as a variable of language use as well.
How does language use differ across languages and cultures?
An additional strain of corpus-based research in linguistic typology focuses on diversity in language use.
But similar to word order typology discussed in 11.2.2 above, the problem with such typologies is that it is not straightforward to classify language systems according to this parameter given the pervasive variation in language use.
Likewise, the allocation of pauses or final lengthening is not dependent on content since the regularity applies to any nouns compared to verbs, and any word form in final versus non-final position, regardless of text content.
This is significant, in particular, where relevant research is intended to reveal constraints on language processing: one would have to keep in mind that the findings may apply strictly to standardised written text production (which is often influenced strongly by traditions of formalised literacy education).
Also, we need to develop many more corpora of spoken-language texts which are particularly relevant to CBT research tapping into questions of language processing.
The findings from corpus-based typology also feed back into ideas about corpus building, composition, and annotation: more diverse languages require different considerations of register, representativeness, and potentially, require adaptations of annotation and querying strategies.
Before a multi-dimensional analysis can be performed, a corpus needs to be both lexically tagged and then analyzed by a particular statistical method termed factor analysis.
The most striking difference in type frequency for types occurring in both subcorpora is that for upon, which occurs more than 10 times as frequently in the general corpus (ratio 10.604).
These corpora are certainly impressive in terms of their size, even though they typically contain mere billions rather than trillions of 2 What is corpus linguistics?
The header section contains metadata, a description of the file, the encoding, the text profile, mainly the language, the context or participants, and even a history of its revisions.
On the one hand, these kinds of studies are relatively easy to carry out without the necessary background knowledge for a more sophisticated type of study.
Dummy coding is a way of encoding a categorical variable as a R. Schäfer distributed around 0.
What would the corpus need to look like?
If you need to convert pdf files to a text format, the best tool to use is another free program by Laurence Anthony, and it is called AntFileConverter: www.laurenceanthony.net/software/antfileconverter/.
It needs to be explained that pragmatic research which truly focuses on language in use has to be corpus-based because there is no convenient way of making speakers say what you want them to say where language is unpredictable, messy, and extended over many turns.
Among the paragraphs, we also encounter one special type, that of the heading, which acts as a kind of guide to the particular chapter or (sub-)section the individual paragraphs occur in.
The Le Monde corpus is a valuable tool not only for exploring the French journalistic style but also for studying recent developments in the language, thanks to its data spanning 25 years.
This chapter highlights genre categorizations as a pitfall at the intersection of corpus linguistics and literature and problematizes the use of the genre category tag from the perspectives afforded by both fields.
The compilers eventually arrived at a very elaborate coding scheme to describe the letter writers in their corpus, using 27 different parameters, some with very open information.
However, in order to be interpreted, a document marked up in XML coding must follow certain syntax, in the form of a document type definition (DTD), which specifies the list of tags with their potential attributes, as well as the possible order of appearance for tags.
In other countries around the world, the situation may either be handled in a more relaxed or, in contrast, even harsher way, so it's always advisable to enquire about the exact copyright situation of the country in question, especially if you later want to make your corpus available to other researchers around the world.
Lemmatization is one of the types of annotations which can be done automatically, with considerable accuracy (see section 7.4).
A more recent corpus, the Corpus of Web-Based Global English, is 1.9 billion words in length.
First, the corpus sizes range from the 1.9-billion-word GloWbE corpus to the 50-millionword Strathy corpus.
However, since an item like at first has a frequency of over 5,000 in the corpus, line-by-line searching was not a viable option.
However, where corpus pragmatics' "added value" lies is in its insistence that these patterns be considered in light of the context -the situational, interpersonal, and cultural knowledge that interactional participants share.
Nevertheless, it is not often the case that the findings of a meta-analysis will provide conclusive answers to questions of interest, particularly in a young field such as corpus 27 Meta-analyzing Corpus Linguistic Research 679 linguistics.
SFK_018 or IOM_002) where the token occurred.
Even if an automatically annotated corpus is unlikely to meet all the expectations of a researcher in terms of its categories of annotation, it can still be an invaluable resource.
In this case, the quality of the automatic annotation is measured in comparison with a reference annotation produced by human annotators.
This both makes it easier to type in the speaker of an utterance during transcription and largely anonymizes the data (given that the metadata are stored in a different file).
While it is not implausible to analyze culture in general on the basis of a literary corpus, any analysis that involves the area of publishing itself will be particularly convincing.
In providing all this information, the compilers clearly chose to collect as much metadata as possible.
Do you see any difference between the two registers in terms of the percentage of frequent (or not frequent) words in the text?
The Corpus of Global Web-Based English (1.9 billion words) also contains complete texts of varying length.
One of the ways of selecting material for a corpus is by stratified sampling, where the hierarchical structure (or 'strata') of the population is determined in advance.
While intralinguistic information helps to understand internal issues and aspects of a text, extralinguistic information provides external information based on which we have a better insight and interpretation of a text.
This software can be downloaded for free and used for converting any number of XML, HTML or sometimes even PDF files into text format.
But they are probably one reason why so much grammatical research in corpus linguistics takes a word-centered approach.
Nine covariates are tested, which represent such social factors as the sex, age and education level of the speakers, and such linguistic factors as polarity, type of determination and proximity of the copula to its referent.
Even though the processes of collecting and encoding data for inclusion in a corpus are described above as separate processes, in many senses they are closely connected: After a conversation is recorded, for instance, it may prove more efficient to transcribe it immediately, since whoever made the recording will be available either to transcribe the conversation or to answer questions about the recording to aid in its transcription.
Ratio: Selected words had to occur at a rate 50% higher (i.e., at 1.5 the 'expected' rate of occurrence) in their academic corpus than in a nonacademic corpus (the rest of COCA).
In addition, these corpora often contain an annotation of errors, which favors an explicit learning process and enables learners to become conscious of typical errors and to avoid them.
Relying on the Dutch Parallel Corpus, the authors combine two approaches in their study: monolingual comparable (Dutch translated from English and French, alongside original Dutch) and parallel (English to Dutch).
But because tweets are relatively short and are not part of any larger text, can they be considered texts themselves?
Variationist corpus linguistics, which focuses on the proportions of variant items or constructions, is not as heavily affected by the issue.
Lists of high-frequency names are often used when designing automatic taggers, which might account for the fact that with names of lower frequencies, the tagging of white in names appears less systematic.
In CFA, an intersection of variables whose observed frequency is significantly higher than expected is referred to as a type and one whose observed frequency is significantly lower is referred to as an antitype (but if we do not like this terminology, we do not have to use it and can keep talking about "more or less frequent than expected", as we do with bivariate 𝜒 2 tests).
As anyone who has ever transcribed speech knows, the flow of speech is much faster than the ability of the transcriber to type.
A 15-word sample (91 clusters) was annotated by two annotators in order to test the reliability of the general procedure, obtaining a reasonably high interannotator agreement (Cohen's kappa coefficient of 0.55).
When tri-grams occur with a particular frequency (e.g., 20 times) and in one specific register in a corpus, they are often called lexical bundles.
Furthermore, instead of revealing interesting combinations of content words, you'll often find more grammatical constructions or combinations of function + content words, especially if the corpus is not very homogeneous, as in our case.
For instance, we might see the end most often at the end of a corpus of children's stories and rarely at the beginning or in the middle of the texts in that corpus.
Is it a frequent word in the corpus?
If we consider the metadata from the ICLE (Fig.
Couldn't the generative linguist very well work with only made-up sentences, especially since his/her goal is to theorize about language, and spending additional time finding relevant data in a corpus is simply unnecessary?
Since most of the words in the non-specialized lexicon are polysemic, this annotation is essential.
Therefore, if a corpus is to be used only for non-profit academic research, this should be clearly stated in any letter of inquiry or email requesting permission to use copyrighted material and many publishers will sometimes waive fees.
Metadata is literally 'data about data' .
For instance, you could type this to load the package dplyr: library(dplyr) ¶.
In addition, many journals specializing in different areas of linguistics, such as Journal of Pragmatics, Journal of Sociolinguistics and Journal of French Language Studies, regularly publish corpus studies.
The green ones rank as the top 501-3,000 most frequently occurring words in the corpus, and the yellow ones are marked for those that are in the rank of less commonly used vocabulary in the corpus (beyond the rank of 3,000).
Topics discussed herecollocations, keywords and manual coding of concordance linesplay a key role both in the study of semantics ('dictionary' meanings of words) and in discourse analysis.
After all the above information is collected, it can be very useful to enter it into a database, which will allow the progress of the corpus to be tracked.
For instance, a general corpus of newspaper editorials would have to cover all the different types of editorials that exist, such as oped pieces and editorials produced by an editorial board.
In fact, we can select multiple reference corpora and carry out the keyword procedure with each of them and compare the results.
The FoudonReboul corpus is a longitudinal corpus of eight children with autism spectrum disorder (ASD), recorded between the ages of 4 and 9 years.
We then provide a register analytical framework for interpreting corpus findings (Chapter 2).
If, in this case, one of the students produces 80% of the occurrences and the other two students produce 10% each, it would be inappropriate to conclude that the distribution is homogeneous in the 15% of the corpus where this word appears.
In Section 2, we briefly present eight grammar textbooks (four corpus-informed and four non-corpus-informed); these textbooks are analyzed with a view to finding out the similarities and differences between these two types of materials and to answering the research questions presented in the introduction.
It is equally important to consider the quality and type of microphone to be used to make recordings.
Most of the analyses we will want to undertake with our corpus will be centred on a much smaller linguistic unit, such as the sentence, the clause or, perhaps most commonly, the word.
For instance, all of the written texts for the Brown Corpus had to be keyed in by hand, a process requiring a tremendous amount of very tedious and time-consuming typing.
It requires considerable resources to create balanced corpora, such as the BNC, whereas COCA contains texts downloaded from websites, requiring much less effort than building a corpus such as the BNC.
Much work has gone into developing efficient annotation strategies for differentiating items belonging to named entities from those representing regular uses of the words.
Type by way of into the top and the command [nn*].
For yes and no answers, we can employ a similar type of traffic signal analogy and encode them like one-way street signs.
The corpora employed in the study were the ELFA corpus for primary data, and the 1.8-million-word MICASE corpus for reference data for its close match in content and construct to ELFA, but collected in native-speaker settings.
In addition, we will suggest that you access other corpora to carry out further projects in this area, for example, the Michigan Corpus of Academic Spoken English (MICASE).
A corpus is considered balanced if its subsections are correctly sized relative to one another.
Corpus linguistics depends on computer science for various reasons.
We have highlighted tools and techniques that are already used in corpus linguistics that can be considered as visualisation: concordances, concgrams, collocate clouds, and described new methods of collocational networks and exploratory language analysis in social networks.
While this numbering system is unique to ICE components, a similar system can be developed for any corpus project.
This point can be illustrated by considering two different content words, each occurring 42 times (per 100,000 words) in the corpus of Donald Trump's campaign speeches: military (n.), and Virginia (n.).
For example, 56.18 percent of all possessive constructions in our sample are s-possessives and 43.82 percent are of -possessives; if there were a random relationship between type of construction and givenness of the modifier, we should find the same proportions for the 183 constructions with old modifiers, i.e.
Finally, the word list for the target corpus is reordered in terms of the keyness values of the words.
Corpus studies will, in tandem with other methods, have a continuing and important role to play in this endeavor.
While it is available in a machine-readable format, it is too short to be representative of the types of sentences occurring in news stories, and whether the random sampling of sentences produced a balanced corpus is questionable.
However, with a small corpus, there is probably a lot less of this "junk" to throw out.
This corpus should be specific to the population of French-speaking Switzerland.
However, the focus on words is also due to the fact that the results of corpus linguistic research quickly showed that words (individually and in groups) are more interesting and show a more complex behavior than traditional, grammarfocused theories of language assumed.
However, the creators of large corpora have agreed that it is very difficult to fulfill all the criteria to achieve a perfectly balanced corpus.
This decision is important because it will determine exactly what the corpus will look like: what types of texts will be included in it (e.g.
While many users of a spoken corpus will potentially be interested in analyzing overlapping speech, there will likely be very little interest in studying italicized words, for instance, in a written corpus.
Moreover, when initially created, the Brown Corpus, for instance, had to be loaded on to a mainframe computer for analysis, whereas many corpora such as COCA are now available for analysis over the Web or on a home computer.
To introduce more writing by females into a corpus of an earlier period distorts the linguistic reality of the period.
The statistical significance of a correlation is directly related to the number of observations (cases).
To check on 'strange items' in the list, you can use a right mouse click on the frequency to display a concordance of the item in a new tab.
The perspective that we will take is, as you can tell from the title of the book, related to an area of language study called corpus linguistics.
Second, just like a corpus, a speaker's linguistic experience is limited to certain language varieties: most English speakers have never been to confession or planned an illegal activity, for example, which means they will lack knowledge of certain linguistic structures typical of these situations.
It, therefore, taken by corpus linguistics, position and that of the anti-positivists.
The sample used here is the Diachronic Electronic Corpus of Tyneside English (henceforth DECTE), a collection of interviews with Tyneside English speakers that will be fully described in due course.
The best way to achieve this is to draw a complete sample of the phenomenon in question, i.e.
In many cases, this piece of information can be obtained by contacting the corpus creators.
Header elements may for instance be the page title (contained in the <title>…</title> tag, where the forward slash indicates the end of the tag in the closing part), which then appears in the title bar of your browser, or possibly meta-information (<meta>…</meta>), such as the text (content) type or encoding/character set (charset), as well as style (sheet) information/references (<style>…</style> or <link rel="stylesheet" href="./corpus.css" type="text/css" />, in our case) responsible for some of the page formatting.
It is a line plot that shows positions of all tokens of a vector on the x-axis (as in Section 5.1.2) and all type frequencies on the y-axis; in other words, each point's y-coordinate divided by each point's x-coordinate provides the point's type token ratio.
Moreover, although diachronic comparability improves, it may still not be optimal, because genres themselves tend to be moving targets.
We test whether the data from that sample "fit" with that of the population.
In terms of attributes, you should be able to find 27, where 'n' represents numerical identifiers for all the 14 textual elements, 'pos' the 12 PoS categories for the words, and 'type' which type of punctuation is present at the end of the syntactic unit.
To enable the study of sociolinguistic variables in the spoken part of the BNC, each conversation contains a file header, a statement at the start of the sample providing such information as the age and gender of each speaker in a conversation.
Stepping from the articles presented at the 4th International Conference that was conducted to convey many topics in the domain of Corpus Linguistics (CILC2012, Jaén, Spain).
While it is true that laws differ from country to country, it is not possible to distribute the texts of an author during his/her lifetime without demanding some type of compensation.
Some work in corpus linguistics has drawn on such resources, including panel survey data (e.g.
In most cases, the observed data in a sample provides the best possible insight into the population parameters.
However, the method of collecting citations cannot be regarded as a scientific method except for the purpose of proving the existence of a phenomenon, and hence does not constitute corpus linguistics proper.
We'll talk more about this type of issue in Section 4.4.1, where we'll explore ways of dealing with such features of the text in order to retain relatively 'clean' data.
We consequently decided to investigate the frequency of sentence-initial as well in COHA, a corpus that provided us with enough material to examine its diachronic development in recent American English.
I argue that the specific issues encountered with this database are due to fundamentally different ideas of what might constitute "good data" in the fields of Digital Humanities and corpus linguistics, respectively, and this in turn leads to different expectations regarding how the data is made available to researchers (e.g., Mehl 2021).
First of all, they should be linked to the corpus text data and their metadata so that corpus users can evaluate how annotators arrived at their annotation decisions; at the same time the annotations are ideally extractable in order to be analysed outside the corpus infrastructure itself, for example, by programmes for statistical analysis.
The corpus contains 48,569 texts -which are equivalent to web pages herecomprising 52,933,543 wordform tokens.
The results can then be displayed in the concordance format familiar to corpus linguistics with the search feature (e.g.
Furthermore, the design of the annotation and interfaces available may sometimes exhibit flaws from a linguistic perspective, as we've, for instance, seen for the CQP architecture behind BNCweb, which treats punctuation tokens in exactly the same way as genuine words, thereby potentially skewing all the statistics produced by the tool.
Compared to raw text, annotated text is complex in structure and difficult to access, but easier in analysis, better in interpretation, and more user-friendly in an application.
Inevitably, studies of co-text and phraseology are "messier" than those of lexeme and structure alone.
What therefore seems to be even more important than recognising the c-unit as the correct unit is the fact that one should always be aware that there are certain boundaries in text that form natural borders between the units of text we may want to see as belonging together, an issue that gains in importance once we move away from looking at single words only.
However, this does not necessarily mean that the text must be excluded from the corpus, since there is annotation that can be included in a corpus indicating that certain sections of a sample are "extra-corpus" material; that is, material not considered part of the corpus for purposes of word counts, generating KWIC (key word in context), and so forth.
Thus, subsets of the corpus comprising the documents with common agreement can be retrieved, and the rest of the documents can be analyzed.
Given the fact that the Europarl corpus was mainly compiled to serve as training material for machine translation systems, the difference in status between the original and translated languages is of little importance.
This means that spoken and signed texts are not immediately available for inclusion in a corpus, but are transcribed, that is, what is being said or signed is written down according to specific conventions, for example, the conventions of the International Phonetic Association (IPA), which are in turn based on specific writing systems.
Understanding the choice between possible variants is one of the main foci of corpus linguistics, especially variationist corpus linguistics.
By using the function xmlGetAttr with a specification of which attribute we want (which I hope is reminiscent of the function attr discussed above): Finally, let us do some more advanced searches, searches that tap into different kinds and levels of annotation at the same time.
On the other hand, it's usually important to make some personal information, such as the informant's age, sex, provenance, level of education, etc., available to users of the corpus, in order to allow them to conduct research of a more sociolinguistic nature.
We will again play around with the fact that the BNC is available in an XML and an SGML version, but rather than, as in Section 5.2.3, have the user state which version is being used, we will have R load the file and discover it on its own and then pick the right search expressions.
One particularly interesting type in the table in this context is wilt, which, on the surface, would appear to be an archaic form of WILL, so we'd expect to find more occurrences of it in the humanities texts.
This is also why the solutions to, and discussions of, the exercises may often represent those parts of the book that cover some of the more theoretical aspects of corpus linguistics, aspects that you'll hopefully be able to master once you've acquired the more practical tools of the trade.
The BNC contains 1 232 966 words from the Daily Telegraph (all files whose names begin with AH, AJ and AK), which will serve as our right-wing corpus, and 918 159 words from the Guardian (all files whose names begin with A8, A9 or AA, except file AAY), which will serve as our corresponding left-wing (or at least left-leaning) corpus.
The type of English used 150 years ago in the United States is quite different from the type of English used in the United States today.
Note that the probability of error depends not just on the proportion of the deviation, but also on the overall size of the sample.
This allows a corpus to reflect on the linguistic changes that take place in a language over time.
In corpus linguistics the procedure identifying and marking the boundaries of word tokens is called tokenisation.
A concept that is intrinsically related to representativeness is balance, which has to do with the proportions of the different samples included in the corpus.
In the TED Talks corpus, the only variations concern the numerous target languages.
One exception is the lemma ardent, where KNN performs slightly better than SVM.
We can describe characteristics of that sample, such as the frequency with which people use serial verb constructions.
To illustrate this, we ran a lexical bundle search in a corpus of webtexts.
This manual should also indicate how the annotation was carried out (by one or two people, throughout how many sessions) and whether successive versions were produced.
For instance, while a draft version of a text in the category of business transactions is being created, the text is saved as "S1B-071di", the "i" indicating that work on the text is incomplete.
The tagged version of the BROWN corpus does not contain quotation marks because they have intentionally been stripped from the text.
This is already an interesting finding given how widespread the consensus among corpus linguists is that tree-based approaches are good at detecting interactions: in this case, all approaches return a three-way interaction when a two-way interaction is all that would be required/desired.
Achieving gender balance in a corpus involves more than simply ensuring that half the speakers and writers in a corpus are female and half male.
The attitudes of corpus linguists seem to include both a healthy sense of humility as regards what is possible to achieve, as well as an understanding of the need to continuously remind the scholarly community of the core concerns in the field.
If you wish to show that one text is very similar to another, the higher the overlap the better.
The first of these is the one you have already seen, a text form in natural human language.
At the same time, whenever appropriate, I've also tried to point out other potential applications for such data, for example, in the development of teaching materials/textbooks, grammars, or direct application in the classroom, but of course such a list will always be incomplete as there are too many applications of corpus linguistics to be listed exhaustively.
One is to look at sequences of words which recur in a fixed order with high frequency in the corpus -these sequences are variously referred to as n-grams, clusters or lexical bundles.
Open the copy in your editor and place the cursor at the end of line 66, which should give us enough text to evaluate the tagging.
In other editors, such as Notepad++, there are additional options available via a dedicated 'Encoding' menu item, where you can specify what to encode a file in or even to convert between a limited set of encodings.
These modes differ from written texts in that the raw data is not readily amenable to inclusion in our corpus.
Each observation (i.e., each text with each normed count) will be in a different row.
We are not deontologically justified in making statements about the relevance of a phenomenon observed to occur in one discourse type unless, where it is possible, we compare how the phenomenon behaves elsewhere.
With corpus linguistics the basis has broadened and the focus has shifted to common features and everyday practices.
Importantly, the degree to which the engagement of social scientists with corpus linguistic research will occur varies, once more, according to epistemology.
The most obvious area in which the social sciences have shadowed developments in corpus linguistics is corpus markup.
When working with corpora based on language documentations, corpus linguists need to work with what they have, and this may often require flexibility.
For example, we can calculate the mean rate of occurrence for nouns in a single corpus sample of size n. However, if this sample is biased in some way, as most samples are, then our estimated rate of occurrence (i.e.
A corpus representing a specific genre can be relatively small, whereas general corpora need to be much larger.
One way to deal with this is special kinds of syntactic annotation (cf.
Of course, we could theoretically check all examples, 3 Corpus linguistics as a scientific method as there are only 42 examples overall.
Check the results and try to understand why this feature may be so useful… Tip: If you have problems in getting upper-and lowercase characters sorted separately, open the 'Tool Preferences' for 'Concordance' and check the option for 'Treat case in sort'.
The Dire autrement corpus, created in Canada by Marie-Josée Hamel and Jasmina Milicevic, contains texts mainly produced by English-speaking learners.
Conversely, even the most sophisticated quantitative analysis typically entails item-by-item annotation of each data point, which equates to a qualitative analysis of sorts; likewise, the results of a quantitative analysis demand interpretation, which typically requires a qualitative approach.
McIntyre and Walker (2010) built a corpus of 200,000 words from blockbuster film scripts.
In the case of many other phenomena, however, automatic annotation is simply not possible, or yields a quality so low that it simply does not make sense to base queries on it.
And even for the negated version of the word smoker, which may at first glance seem to be quite uncontroversial, we find the three variants non smoker (3x in 1 text), un-hyphenated nonsmoker (7x in 4 texts) and -most frequentlynon-smoker (55 in 36 texts).
This is done to establish how much proportionally each part of the corpus contributes to the overall frequency of the word or phrase.
The basic distinction in (28) looks simple, so that any competent speaker of a language should be able to categorize the referents of nouns in a text accordingly.
For instance, whether a copular verb like is is realised in its full form or appears as a clitic 's is subject to numerous factors, and corpus linguists seek to identify these and relate them to one another in modelling the variation at hand (cf.
Therefore, its occurrence in two small corpora -a 55,000-word corpus of radio phone-in data and a 52,000-word corpus of post-observation teacher trainee interaction -was examined.
However, under the definition of corpus linguistics adopted in this book, Sinclair's observations would be just the first step towards a full analysis.
If you enter a specific word in the search box, the interface will allow you to calculate an MI score for the node and that particular collocate, while selecting from the 'POS LIST' options will allow you to look for colligations directly.
In addition, sample corpus analyses are included to illustrate the many theoretical and practical applications that linguistic corpora have.
In the first loop, we identify all cases of must + V so that, at the end of it, we know all verb types ever occurring after must, and then we can look for all occurrences of all of them in the whole corpus within the second loop.
The metadata of such a corpus should at least include information of where the recording took place, its date, the context in which it happened, the conversation topic, the number of participants, the gender of the person recorded, his/her age and profession.
In the reading process patterns in the text determine which area of background knowledge or previous experience are relevant to the creation of meaning.
However, we do need to know more about all the others: "Type", "Decimals", "Label", "Values", "Missing", and "Measure".
For example, "news_15_election_00" would mean a news text from 2015 about an election in 2000.
This type of information is not a part of the text analysis but it can provide important interpretive information used in the functional analysis of your results.
The problems faced by such researchers are similar to those faced by corpus linguiststhey often wish to characterise a population which is far too large to encompass fully.
We then need nchar to determine how many three-grams we will have to create/retrieve and then store, and a for-loop that applies substr to the cleaned text vector to extract the threegrams.
If you have a tagged corpus in which verbs and particles are tagged -e.g., the corpus would look like this: John_N picked_V the_D book_N up_P -then you don't want R to do greedy matching because of how that would handle sentences with two verb-particle constructions such as John_N picked_V up_P the_D book_N and_CJ brought_V it_PN back_P.
They can be made using a set of predefined labels, such as outof-domain, positive, negative, neutral, do-notknow-do-not-answer or define a new set of tags for the corpus.
Is the corpus constructed in a balanced manner?
As before, we are defining what counts as a hapax legomenon not with reference to the individual subsamples of male and female speech, but with respect to the combined sample.
Thus, if the user chose "XML" then menu made that a 1 and switch then assigns TRUE to xml.version.
The hallmark of a corpusbased analysis, as understood in this chapter, is that a grammatical phenomenon is studied in its entirety, such that all relevant examples of a phenomenon are exhaustively retrieved from a corpus.
In order to illustrate some of the concepts in doing a situational and linguistic analysis along with a functional interpretation, we will refer to a small corpus of English as Second-Language writers who were asked to produce problem-solution paragraphs in two different conditions.
However, we cannot take for granted that this method yields a balanced sampling, especially in the case of a small corpus.
No element in the corpus should make it possible to identify any participant.
Second, texts, whether written or spoken, may contain errors that were present in the original production or that were introduced by editing before publication or by the process of preparing them for inclusion in the corpus (cf.
Especially where the body of historical data is finite, disparate and severely biased, or where a corpus is to be used to study change over very long time periods, this is a perfectly defendable strategy.
First, corpus tools can be applied to individual texts, in helping decide whether a text is appropriate and what elements to focus on.
This chapter presents an introductory survey of computational tools and methods for corpus construction and analysis.
I will turn to this point presently, but before I do so, let us discuss the third of the three consequences of large-scale testing for collocation, the methodological one.
If you set the argument lines.around to a number greater than zero, then you increase the preceding and subsequent context by that number of corpus elements.
Unlike generative grammarians, corpus linguists see complexity and variation as inherent in language, and in their discussions of language they place a very high priority on descriptive adequacy, not explanatory adequacy.
While many corpora contain only text samples, others contain entire texts.
Inexperienced writers most frequently produce text messages with opening and closing expressions.
The same applies to artificial corpora of experimentally elicited texts: even where participants produce texts narrating the exact same content under the same experimental conditions, as with the Pear Film experiment, it is vital for the corpus to cover speakers with different demographic features, as the corpora are meant to represent the behavioural reaction to the stimulus characteristic of the language community as a whole.
Thus, to be able to work more efficiently with a concordancer or similar search tool, it would be very useful to be able to specify more complex patterns that we could then look for all at once, and also make use of the other useful options we've explored before, such as sorting, etc., to help us simplify our analysis procedures.
Each time you run the search, even with the same keyword (by choosing the button "KWIC" in the top left corner of COCA), it will display a different set of randomly selected examples.
The 'Methods' section of a corpus-linguistic paper also needs to contain information on how the notions/concepts considered relevant for the analysis were operationalized as variables and how annotations were added with regard to the variables that may affect the linguistic phenomenon under study.
However, unless we carefully search our corpus manually (a possibility I will return to below), there is typically a trade-off between the two.
This study is based on BE06, a balanced corpus of contemporary British English, and AmE06, a balanced corpus of contemporary American English.
Why we might need such special programs for displaying the text is because we often want to be able to render it with special types of formatting, such as italics, boldface, etc., use a particular layout, or that we want to be able to generate a table of contents automatically in a word-processing application, where this is based on the headings inside the document.
AntConc has another feature which offers the possibility of generating a list of all the words in the corpus sorted by frequency via the Word List tab.
The utility of a corpus is increased by an elegant arrangement of texts in an archive.
They observed that this structure was much less used in this second corpus, with only two occurrences by male speakers of Moroccan origin.
We will discuss this topic in Chapter 6, which is devoted to the methodological principles underlying the construction of a corpus.
These difficulties do not keep corpus linguists from investigating grammatical structures, including very abstract ones, even though this typically means retrieving the relevant data by mind-numbing and time-consuming manual analysis of the results of very broad searches or even of the corpus itself, if necessary.
The assumption is that very short documents are unlikely to contain much connected prose, while very long documents are likely to skew the corpus unnaturally.
I then present what I take to be the methodological foundations that distinguish corpus linguistics from other, superficially Preface similar methodological frameworks, and discuss the steps necessary to build concrete research projects on these foundations -formulating the research question, operationalizing the relevant constructs and deriving quantitative predictions, extracting and annotating data, evaluating the results statistically and drawing conclusions.
The Brown Corpus was extremely important because it provided a catalyst for the many computer corpora that will be discussed throughout this book.
Audio recordings make it almost impossible to grasp this type of information, which later have to be added to the corpus so that the interactions can be understood and analyzed by the experts who will listen to these recordings.
What we did instead was to assign each article -each text -a value on each of the identified dimensions.
While it is quite common for corpora to contain meta-information about the data that they contain, should a collection of unrelated sentences and associated paraphrases be considered a corpus?
This represented the first attempt to create a historical corpus conforming to the standards of TEI.
For example, the annotation of speech acts requires setting up a list of the acts to be annotated, for example, requesting, promising, asserting, threatening, etc.
In statistical terminology, this word simply means that the results obtained in a study based on one particular sample are unlikely to be due to chance and can therefore be generalized, with some degree of certainty, to the entire population.
The latter term is to be understood here in a broad and non-technical sense, meaning simply that in order for a range of texts to form a corpus they need to be compiled in some form and accessible in some way.
Being able to annotate relations is also essential for associating anaphoric relations in a text.
The first is that you can employ the right mouse button (two-finger tap on touchpads on the Mac) to activate the context menu inside the browser in order to be able to save the material, as otherwise clicking on the link will simply get the text displayed inside a browser window, rather than saved to your computer.
This type of "literature review" is common in the introductory sections of research articles, and the effects of corpus use have been the object of several extensive narrative syntheses (e.g.
Each variable R. Schäfer from the fixed effects part of the formula which we expect to vary by LEMMA is simply repeated before the | symbol.
Even though the authors allow researchers to use the text in their corpus, some of the owners of the writing texts may not allow researchers to share the corpus with other researchers worldwide.
The management of a corpus is a complex and tedious task.
These probabilistic parsers have become increasingly common in corpus and computational linguistics, and generally work quite well for annotating arbitrary corpus texts in many languages.
Indeed, in the corpus, several articles referred to Roman Polanski's residence, which made these associations very strong, but these words do not obviously collocate in everyday language.
Select the appropriate file type that allows you to import text, generally * .txt and/or * .csv.
Note that, unlike precision, the recall rate of a query cannot be increased after the data have been extracted from the corpus.
For example, if a corpus-based analysis contains the length of a phrase as an independent variable, and we introduce the pronominality of that phrase as a second independent variable, these two variables might correlate significantly, since full lexical phrases are by necessity longer than single pronouns.
However, despite the fact that interrupted words are very common in spoken language, even that of highly fluent speakers, the CLAWS tagset provides no tag for this, something that is probably due to the CLAWS tagsets originally having been created for the morpho-syntactic annotation of written language, and later adjusted for spoken language to some extent.
One type selects a subset of the more important data variables and eliminates the remainder from the analysis, using some definition of importance.
In addition, speakers would be recorded using language in a variety of different contexts, such as conversing over the phone, lecturing in a classroom, giving a sermon, and telling a story (www.linguistics.ucsb.edu/research/santa-barbara-corpus).
It can create the nearly 70,000 KWIC results for the word "the" in the 1-million-word Brown corpus (a notoriously slow search) in under 1 sec on a modest computer.
