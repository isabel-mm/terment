INTRODUCTION

Corpus linguistics requires an understanding of various linguistic and technological issues relating to the availability of technological facilities for the generation, storage, management, annotation, processing, analysis, and dissemination of language data and information as well as a conscious realization of the importance of language corpus in the process of advancing technologies, languages, and societies. In a context where modern computer technology continues to penetrate life and living of common people, we need more clarity to visualize the importance of digital corpora in developing 'knowledge-based societies' where language data, linguistic information, and technology developed with language data and information play a beneficial role for the betterment of societies. In recent years, we have noted a positive change in approach to the use of language corpora as a reliable resource in many domains of language technology and linguistics. This phenomenon of trust in corpora is triggered through several factors such as easy and customized accessibility of corpus data, objective analysis and description of a language based on actual empirical evidence, utilization of language data and information in various domains of language application and utilization of language-based technology in development of new societies empowered with linguistic support systems. Also, it reflects on ideological and technological changes that have taken place during the last seven decades or so.

The use of computer technology in linguistics opens many new methods of collecting, storing, and processing language data, interpreting and analyzing them, and fruitfully utilizing them in different domains of humanities, social sciences, natural sciences, and technology. This was hardly possible before the use of computers in the generation and processing of linguistic data. In the earlier years, we had to be happy with a limited amount of language data for linguistic works because we had no automated system under our disposal by which we could assemble a large amount of language data from various domains of language use, analyze them, interpret them, and utilize them for our purposes. Now we use a computer to collect language data of any size, type, and variety; classify them, process them, analyze them, and utilize them in various works. The importance of a corpus is acknowledged because it contributes to making new findings, helps in utilizing data in applications, provides scopes for modifying old observations, generates opportunities for formulating new theories, and prepares new fields for applying language data in direct service to society. Moreover, it brings in new insights to look into the cognitive operation of the human mind to understand the complex cognitive processes like receiving, processing, comprehending, and sharing linguistic signals

After seven decades of corpus use in various domains of linguistics and language technology, we have now understood that the utility of a corpus is not limited to linguistics and technology alone. It has proved its usefulness in many other domains of natural and social sciences where language data is an integral part of study and application

(e)

Emphasis on scientific inquiry rather than on hypothetical speculation. (f) Emphasis on language application rather than on language description.

A corpus is a resource that serves a large number of domains of linguistics, language technology, cognitive linguistics, and sister disciplines. Due to the multidisciplinary application of real-life language data, corpus linguistics emerges as a new approach towards linguistics-a new method of studying and applying language data using techniques and tools of computer science

WHAT IS LANGUAGE TECHNOLOGY?

What is language technology? This is an age-old intriguing question. Is it a technology for language or a language for technology? The compound expression (i.e. language technology) is quite deceptive, ambiguous, and confusing. In our understanding, it is a bidirectional domain that combines both approaches. It is an applied field that includes linguistics, computer science, statistics, and other areas. It is primarily concerned with interactions between language data and computers. It is now treated as a sub-branch of Artificial Intelligence (AI) because language processing is a highly complex method of human-computer interaction. Understanding a natural language by computer is an AI problem because it requires exhaustive information and extensive knowledge about the linguistic and extralinguistic world and an ability to manipulate this information and knowledge in a computer system. The question of whether natural language processing is different from or identical to language technology is a matter of perspective. On one side, we may define language technology by way of focusing on the theoretical aspects of language processing; on the other side, we may look at it as a way of analyzing and devising systems for language application on digital platforms. All language technology systems are, therefore, primarily grounded on machine learning and a major load of information that is needed in machine learning comes from language data. As researchers of language technology, we have to depend on theories, information, and insights gathered from different disciplines of human knowledge (e.g., Linguistics, Computer Science,

The journey of language technology, in a simple count, started in the early 1950s, though researchers may trace it back to much earlier years

Recent research in language technology increasingly focuses on unsupervised and semi-supervised algorithms, which learn from multimodal language corpora-both annotated and non-annotated. This is a more difficult task than supervised learning and it typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data, which often makes up for the inferior results. Essentially, modern approaches to language technology are grounded on various machine learning strategies although the paradigms of machine learning are significantly different from those that were applied at the early stages of language processing. The modern machinelearning paradigms, instead of using general learning algorithms, often depend on statistical results derived from large language corpora (i.e., big data) to automatically learn linguistic rules through analysis of several typical real-life examples. For instance, consider the task of part-of-speech (POS) annotation (i.e., determining the correct POS of each word in a given sentence of a text). A typical machine-learningbased tool for POS annotation works in three basic sequential stages:

Over the years, many different types and classes of machine learning algorithms have been applied to language technology tasks. The most common trait of these algorithms is that they take a large set of 'features' that are generated from the analysis of input language data as inputs. For example, for a question-answering system, some of the typical features (or functions) involve the recognition of a linguistic expression as a question, analysis of the type of the question, understanding the syntactic structure of a question, capturing meaning embedded into it, recognizing the sentiment hidden in the question, searching out an appropriate document that may carry an answer, extraction of the answer from a document, evaluation of the answer before responding, and finally generating the answer either in text or speech mode. Such a complex task of language technology depends on inputs taken from two ends: linguistic data and information from corpora, and technological help from computer science. In essence, language technology is happy in engaging technological devices like computers and mobiles to do a large number of purposeful activities with natural languages, both spoken and written. It brings in new advances in computing and develops tools and systems to serve language-related human requirements. It is recognized as one of the highly productive domains of IT. We sum up the major domains of language technology in the following manner (Fig.

Fig.

[3] Digital language resource development: lexical databases, dictionaries, thesaurus, references, encyclopedias, machine-readable dictionaries, wordnets, sense nets, course books, grammars, study materials, etc.

[4] Human-machine interface development: systems for word sense disambiguation, information retrieval, optical character recognition, text summarisation, text conversion, machine translation, web-based learning, question-answering, computer-aided instruction, computer-assisted language education, text preparation, e-governance, online language education, etc.

[5] Machine translation support system: multilingual resource access, multilingual information access, transliteration, cross-lingual language information retrieval, text alignment, etc.

[6] Speech technology development: speech recognition, synthesis, and processing, pronunciation verification and recognition, voice recognition, speaker identification, text-to-speech, speech-totext, speech disorder recognition and repair, etc.

Most of these have direct real-life applications and many of them serve as sub-tasks to solve larger language technology challenges. These tasks not only refer to the volume of research that is to be carried out in future but also reiterate the fact that each task, in itself, is a well-defined problem, the solution of which depends on how it deals with information derived from corpora and the technology generated through computation. In the following sections, I shall discuss digital font generation, corpus generation, corpus processing, corpus annotation, and application of corpus in different areas of human knowledge.

DIGITAL FONT GENERATION AND CONVERSION

In typography, a font is a collection of sorts that constitutes the complete character set of a single size and style of a particular typeface (e.g., Times New

With regard to non-advanced and non-digitized languages (which are indeed very large in number), this is a crucial issue and a daunting task for scientists engaged in font technology, one of the primary areas of natural language processing. We have to keep in mind that, even at this advanced stage of language processing, there are many non-advanced and minority languages, which have not yet been successful in producing digital language texts or linguistic resources due to the non-availability of digital fonts that could be used to produced digital texts. Against this background, some of the major issues in font technology for less advanced languages are the followings:

(1) Designing fonts for all minor languages which have separate scripts and writing systems to be used for digital text generation. (2) Conversion of existing printed text documents into machine-readable texts through the application of the Optical Character Recognition (OCR) system. (3) Developing fonts of different types and styles to address various academic, administrative, and commercial requirements. (4) Conversion of non-Unicode-based fronts into Unicode-based fonts for global access to texts.

(5) Designing Unicode-based fonts for old and obsolete characters (that were once used in scripts) for proper conversion and rendering of old printed and handwritten texts into digital forms. (6) Making Unicode-compatible fonts available to people of those languages so that they can use these to produce digital texts. (7) Development of transcription modules for bidirectional-transcription of texts (e.g., English to Mundari and Mundari to English). (8) Localization of tools and systems of font technology for non-advanced languages for creating lasting impacts in the development of knowledge-based societies. (9) Sharing knowledge and font technology that is already developed and applied in advanced languages with less advanced and non-advanced language scripts. (10) Making font technology a part of digital ethnography in the preservation, protection, and promotion of scripts and writing systems of less-advanced languages.

In the present global context, these works are still unattended for many non-advanced languages. We have to think seriously about how all these problems are addressed and solved so that the benefits of language technology reach every language community-advanced or non-advanced, resource-rich or resource-poor, and technology-savvy or technology-hungry.

LANGUAGE CORPUS BUILDING

The introduction of a corpus in language study adds a new dimension to linguistics. Corpus linguistics is not a 'new area' of language study; it is a 'new approach' (or a new method) to language study. It argues for studying a language through empirical analysis of language data produced in machine-readable form with a large collection of texts. This approach, over the years, has been successful in bringing in new perspectives towards language study in several domains, namely, language description, language education, language experiments, and language computation. The underlying theoretical idea of corpus linguistics is quite broad. It refers to a process of an exhaustive analysis of a substantial body of authentic spoken and/or written texts and processing the same for various academic, social, and commercial needs. Digital corpus is now a known thing to us and we have come to a common agreement to understand what counts as a 'corpus', what are its characteristics, how it can be built and classified, how it can be annotated and processed, and how it can be analyzed and utilized. We have also understood that we can refer to corpora to understand some of the complex cognitive and linguistic questions about how people use language for communication, information generation, and knowledge sharing. Also, there are technical motivations for compiling language corpora for building intelligent devices and systems that will efficiently interact with human beings and perform many language-related tasks. All such goals have inspired computer scientists and linguists to work together to develop language corpora to be processed and utilized in designing intelligent systems like machine translation, speech recognition, information extraction, question answering, sense disambiguation, sentiment recognition, language education, machine-aided instruction, etc. What we have understood from our involvement with activities like corpus generation, processing, annotation, analysis, and applications over the years, is that both language and technology receive huge benefits from insights and information gathered from corpora. This confirms the importance of the description and analysis of data and application of corpus in diverse spheres.

The term corpus is derived from the Latin word corpus which means "body". In the present context, we use this term to refer to a large digital collection of natural language texts that are assumed to be representative of a given language, dialect, or a subset of a language, to be used for linguistic annotation, processing, and analysis. It typically contains a collection of representative samples that are obtained from texts of different varieties of language use in various domains. Theoretically, it is capable of gathering unlimited selections of texts, compatible with computers, operational in research and application, representative of a source language, processed by a machine, unlimited in the amount of data, and systematic in formation and representation. From a theoretical perspective, the salient features of a corpus are the following

[1] Quantity: A corpus should be big. It should contain a large amount of data from spoken and written sources. It is the total of components, which constitute its body.

[2] Quality: It refers to the authenticity of data. All text samples should be collected from genuine use of speech and writing. Samples should be collected from original communication (and not from experimental conditions or artificial circumstances).

[3] Representation: It should include samples from a wide range of disciplines. Samples should be balanced to all disciplines to represent wide domain varieties. Later studies devised on it will require authentication of information from texts representing a language.

[4] Simplicity: A corpus should primarily contain simple plain texts. The text should be free from any annotation that carries linguistic and extralinguistic information. Corpus users are not always willing to have additional information tagged to texts.

[5] Equality: Text varieties should be of even size concerning the number of words. It is, however, a controversial issue, and therefore, cannot be applied blindly. The sampling process varies based on a specific requirement.

[6] Retreavability: Data and information should be retrieved from a corpus. It draws attention to storage techniques of data in electronic form in an archive. Modern technology allows one to store a corpus in such a manner that one can easily retrieve data from it.

[7] Verifiability: Corpus should be open for empirical verification. Users are free to use data from it to examine earlier observations. This puts a corpus ahead of generative linguistic study.

[8] Augmentability: A corpus, unless it is a fixed one, should grow regularly. This allows a corpus to reflect on the linguistic changes that take place in a language over time. By regular addition of synchronic data, a corpus attains a diachronic dimension.

[9] Documentation: Information about corpus building should be preserved for documentation. It is to be stored as metadata in a header file. This becomes useful at subsequent stages of corpus management and reference.

[10] Management: It involves the storage of texts for faster and easier retrieval. The utility of a corpus is increased by an elegant arrangement of texts in an archive. It makes the application of data more effective and fruitful. It may involve schemes for maintenance, standardization, augmentation, up-gradation, and dissemination of data.

Classification of Corpus

Digital language corpora are new things. We are yet to come to a consensus about their classification.

The scheme that I propose here offers a reasonably workable way of classifying language corpora with delimited categories wherever possible. I argue for using the external and internal criteria for classifying corpora. The external criteria refer to a text type that is linked with participants, occasion, social setting, and function of a text. The internal criteria, on the other hand, refer to the use of language properties within a piece of text. It is known to us that corpora can be of many types with regard to texts, languages, modes of data sampling, methods of corpus creation, manners of text annotation, and goals of text utilization, etc. For instance, one corpus may contain samples of written texts, the other one may contain samples of spoken texts, and the third one may contain transcripted spoken texts.

Similarly, a corpus may contain samples of present-day use of a language, while the other may contain samples from old texts; a corpus may be monolingual with a collection of data from a single language, while another may be bilingual or multilingual by including texts from two or more languages; texts included in a corpus may be collected from one source, two sources or many sources of a particular field or across fields. This implies that numerous factors control the content, type, and use of a corpus as well as numerous ways that one can employ to classify corpora. It also implies that the kind of text included as well as a combination of different text types is crucial in classifying corpora. Each corpus developer is, however, free to adopt unique methods in language representation, text collection, and text application. These factors can make a corpus unique in form, content, feature, and function

Corpus Generation

There are various issues relating to the design, generation, and management of a corpus. These issues vary based on the type of text and the purpose of its use. For instance, issues relating to speech corpus generation differ from issues relating to text corpus development. Development of a speech corpus involves issues and factors like purpose of use, selection of speakers, choice of settings, size of a corpus, use of recording instruments, manner of data sampling, manner of data elicitation, nature of transcription, types of data encoding, management of audio files, editing of input data, processing of spoken texts, annotation of speech, analysis of speech corpus, etc. On the other hand, the development of a text corpus addresses issues like the size of a corpus, representation of text types, question of nativity of language users, determination of target users, selection of time-span of production of texts, coverage of disciplines, selection of text documents, collection of source text materials, methods of data sampling, manners of data collection, manners of text normalization, management of corpus files, types of text annotation, and issues of copyright, etc. That means, based on the type of text, one has to address various issues of corpus generation. Keeping this view, I address below some of the common issues of corpus generation. How big a corpus should be? This is related to the size of a corpus as size is an important issue in corpus generation. In the case of a speech corpus, it is related to the amount of audio data (in MB or GB) or duration of a speech (in hours and minutes). In the case of a speech corpus, on the other hand, it is linked with the total number of sentences, unique words (types), and total words (tokens) in a corpus. It also depends on the decision of how many text categories are kept in a corpus, how many text samples are put in each category, and how many words are kept in each sample. Although size does matter on the scale of the 'reliability' of a corpus, in reality, a corpus, however big it may be, is a small representation of the vast amount of speech and text varieties that are produced in a language. In the early 1960s, when computer technology was not conducive to collecting a large amount of language data, a corpus of one million words was considered good enough (e.g., Brown Corpus, Lancaster-Oslo-Bergen (LOB) Corpus, KCIE (Kolhapur Corpus of Indian English)). In the last seventy years, computer technology has undergone a sea change in storage, accessing, and processing capabilities; and due to this, size is no longer an issue. This is an age of big data. Therefore, the bigger the size of a corpus, the more is its utility, faithfulness, and reliability. In the new millennium, a corpus that contains hundreds or more millions of words is a preferred choice (e.g., British National Corpus

The issue of size becomes less relevant in the context of text representation. A large corpus does not guarantee a balanced representation of all varieties of texts of a language, if not desired so. A small corpus with wider text variations is a much better representative than a large corpus with fewer text variations.

A large yet less varied corpus cannot be used for the generalization of a language. A corpus is truly 'representative' when findings from it are generalized to a language or a part of it. Therefore, rather than focussing on the quantity of data in a corpus, it is always sensible to emphasize varieties of data. That means language data should come from the texts of all possible domains of language use. The size of a corpus should be set against the diversity of texts to achieve proper representation. In any corpus, the greater the number of unique text samples, the higher the reliability of the analysis of linguistic variables. The Brown, LOB corpus, and Survey of English Usage (SEU), for instance, contain a wide variety of text types, and therefore, they are considered much better representatives of English. However, the BNC and the ANC, which are not only very large but also far more diversified in structure and text types, empirically settle the issues relating to size and representation.

The question of the nativity of text producers is another crucial issue in corpus generation. The general argument is that texts produced by native language users should get priority over texts produced by nonnative users in a general and monitor corpus. A general corpus tends to represent a language, which is considered 'standard' in works in linguistics and language technology. The non-native use of texts in a general corpus can have an adverse effect on linguistic analysis of data and information. One of the reasons behind building a general corpus is to enable scholars to analyze 'standards' to see what does occur and what does not than looking into non-standard interpolations. The primary idea is that we should have a general corpus that includes 'benchmarked' and 'standard' data so that we can collect information about how accepted standards are commonly used in mainstream linguistic activities. Based on a general corpus, we like to produce texts and reference materials that will guide in spelling, pronunciation, grammar (i.e., syntax), meaning and usage.

In the case of a special corpus, we like to include text samples produced by non-native users as the aim of this corpus is to highlight linguistic peculiarities that are typical to non-native users. Here the question of representation is not linked with the language as a whole but with the language used by a particular type of people who are primarily non-native speakers. The question of nativity, however, becomes quite complicated and sensitive when we notice that the same language (e.g., English) is used by people separated by some geographical or political barriers (e.g., British English, American English, Australian English, Indian English, African English, Jamaican English). In this case, for instance, we may find forms that are correct by the 'rules of grammar and usage of Indian English' (and perfectly understandable); but are not the 'right' forms in the 'rules of grammar and usage of the British and American English'.

There is no fixed target user for a general corpus, as such. Anybody can use it for any purpose. In the case of a special corpus, the identification of target users is important. Since each research has a specific goal, a special corpus has to be designed accordingly. For instance, a person working on developing a tool for machine translation (MT) requires a parallel corpus than a general one. Similarly, a person working on comparative studies between two or more languages requires a multilingual comparable corpus than a monitor one. To summarise this feature, I generalize a model to relate the types of a corpus and its users (Table

Target Users

The

An important issue in corpus building is the selection of text type. It is necessary to decide if a corpus should contain all types of written text samples or specific sets of text samples. A general corpus usually includes standard text samples as it aims to identify what are central and common features of a language. It hardly cares for any typical, special, and unique features that are represented in the database. Therefore, we can furnish a general corpus with standard text samples of contemporary writings and a calculated and proportional representation of texts can suffice our requirement. On the other hand, in case of a monitor corpus, which tries to represent all possible text varieties, we may include all non-standard and ordinary text samples along with standard and established text samples. It also includes texts produced by little-known writers along with texts created by well-known and reputed writers. A heterogeneous monitor corpus includes text samples from all subject areas and the individuality of the source of a text and its writer are well attested. Thus, a monitor corpus becomes a worthy representation of texts that are ever produced in a language. Diversity is a useful safeguard for a monitor corpus against skewed representation.

The task of sampling of texts has to be done with collected text materials based on the character of a corpus. The sorting of texts can be random, regular, or selective. Since there are several methods for data sampling to ensure the optimum representation of texts in a corpus, we may pre-define the kind of data we want to study before we define a sampling procedure. The application of the random sampling method usually saves a corpus from being skewed and less representative. This method is widely used in many corpus generation projects across languages. Alternatively, we may apply selective sampling methods that are used in the Brown Corpus and the LOB corpus or consider more suitable methods of text representation keeping in mind the goal and purpose of a corpus.

The management of a corpus is a complex and tedious task. It involves diverse activities like storage, metadata generation, archiving, augmentation, upgradation, processing, retrieval, dissemination, and other works. Once a corpus is built and stored in an archive, we need robust schemes for regular maintenance, upgradation, and augmentation of the resource. There are always some errors to be corrected, some modifications to be made, and some improvements to be implemented. Adaptation to new hardware and software and addressing the requirements of users are two vital issues that have to be taken care of. Moreover, there are technical issues with retrieval, processing, and applying analytic tools and systems on databases. We have to be comfortable with the application of modern toolkits on corpus as such devices help us execute many useful tasks on a corpus.

The processes of corpus sanitation start when a corpus is made ready for use. There are many linguistic and technical issues relating to corpus sanitation (e.g., text normalization, orthographic error correction, spelling error correction, real word-error correction, grammatical error correction, punctuation error removal, and tokenization). Many of these works have to be done on a corpus before a corpus is made ready for application in technology and other works

It is necessary to dissolve issues of copyright of texts with copyright holders before a corpus is made open for application. Copyright laws are complicated and usually vary from country to country. If one uses texts for personal purposes, then perhaps, there is hardly any problem. This is fine not only for a single individual but also for a group of scholars who are working together on some areas of research. With regard to the use of a corpus for academic purposes, there are also problems with ethical 'right, wrong, legal or illegal'. As long as it is not directly used for commercial purposes, one can utilize a corpus. However, when one uses a corpus to produce tools, systems, and resources and commercialize these, one has to face copyright problems. In direct commercialization of corpus, one should seek permission from legal copyright holders.

CORPUS PROCESSING

The processing of a corpus starts after a corpus is generated and normalized. Processing is necessary for utilizing corpus data in linguistic research and technology development. There are various ways of processing (e.g., data analytics, concordance, collocation, keyword marking, local-word-grouping, lexical clustering, lemmatization, morphological processing, sentence processing, and named entity marking), which are applied to a corpus with appropriate technological supports. The outputs obtained from processing may agree with or contradict the results of earlier studies and findings. At present, there are many processing tools and software that are available mostly freely for corpora. I briefly discuss here a few well-known corpus processing techniques for understanding these technical issues. More details are available in some recent works

Studies in mathematical linguistics, computational linguistics, corpus linguistics, applied linguistics, forensic linguistics, stylometrics, and other domains require statistical and quantitative results from a corpus. Without knowledge of statistics about various properties of a language, we make mistakes in the analysis of language data and inference deduction. A corpus often becomes a subject of quantitative and qualitative statistical analysis for addressing empirical and theoretical queries. In quantitative statistical analysis, we classify linguistic properties or features in a corpus, count their frequency of occurrence, and construct statistical models to explain what we observe. We also try to discover which phenomenon is likely to be a genuine reflection of a language and which is a mere chance occurrence. In qualitative statistical analysis, on the other hand, we look at multidimensional analysis of observed phenomena in a corpus and try to find factors behind the occurrence of a feature or an element in a text. Both types of analysis have something to contribute to corpus-based language study. While descriptive statistical analysis enables summarising the most important properties of observed data, inferential statistical analysis helps to use information from descriptive statistical analysis to reply to questions, formulate hypotheses, and verify propositions. The evaluative statistical analysis, on the other hand, enables testing if hypotheses are supported by true evidence in data and how mathematical models and theoretical distributions of data are related to reality. To perform comparative studies, we apply multivariate statistical techniques (e.g., Factor Analysis, Multidimensional Scaling, Cluster Analysis, Log-linear Models, and Pearson Correlation) to extract hidden patterns of language use from frequency data obtained from a corpus.

Numerical sorting is one of the most straightforward approaches to working with quantitative data of a corpus. Here items are classified according to a particular scheme, and an arithmetical count is made on the number of items that belong to each class in the scheme within a corpus. Information collected from frequency counts is rendered in alphabetical or numerical order. Both lists are arranged in ascending and descending order based on the requirements of the study. Anyone studying a corpus may like to know the frequency and patterns of use of each item in it. A frequency list of unique words, for instance, is a useful clue in the identification of the type of text

Lexical concordance is a process of making an index to words used in a corpus

An exiting game

was played between the two teams. The winner was declared in the last game of the match. They won the Bridge competition in the last game.

The next Olympic games will be held in London.

The annual games and sports were held in December. The couples are not new to this game of love. The final game ended in six-all. They lost but played a good game.

I was just playing a game with you jokingly. It is none of your games. So that was your game, which I failed to understand.

The game of the authority annoyed the people. It was a wild game in last night's party.

The hunters went to the game reserve in a group at night The hound chased the game into the wild grass They knew the game was over for their leader.

Table

The method of lexical collocation helps to understand the role and position of word pairs in texts

Example from English: British National Corpus English Phrase: "glass of"

Fig.

The extraction of information of part-of-speech of a word and representation of information relating to inflection

Input Word : ghumāiteichhilen Root : √ghumā) Inflection Part : -āiteichhilen Number : Sing +Plural Aspect : -ite-Particle_Emphatic : -i-Auxiliary : -ch-Tense_Past : -il-Person_3rd : -en-Honorific : -en-Part-of-Speech : Finite Verb Meaning : "was indeed sleeping"

Table

CORPUS ANNOTATION

Corpus annotation is an innovative process of adding interpretative information to the text of a corpus. Since a corpus contains text samples either from a written or spoken discourse, annotation is done by adopting a value-encoding process, which attaches interpretative information at different levels of a text. The interpretative information may be related to prosody, pronunciation, orthography, grammar, meaning, sentence, anaphora, discourse, rhetorics, etymology, sociolinguistics, and other issues. Interpretative information is classified into two types: (i) intralinguistic information, which is linked with linguistic features and properties of a text, and (ii) extralinguistic information, which is related to information not linked with linguistic properties and features of a text. Analysis of corpus texts shows that apart from pure intralinguistic information, a corpus also carries several kinds of extralinguistic information. While intralinguistic information helps to understand internal issues and aspects of a text, extralinguistic information provides external information based on which we have a better insight and interpretation of a text.

Both types of information are annotated to a text-either attaching them to the elements of a text or interspersing them within a text. The primary purpose of the annotation is to add additional information to a text so that future analysis and interpretation of a text becomes easy, accurate, and useful. Compared to raw text, annotated text is complex in structure and difficult to access, but easier in analysis, better in interpretation, and more user-friendly in an application. In the last seventy years, we have come across many innovative methods of text annotation

Fig.

Intralinguistic Annotation

Based on the type of information to be added, a corpus can have two broad kinds of annotation: (a) intralinguistic annotation, and (b) extralinguistic annotation

(a) Orthographic annotation: We identify various types of characters, symbols, and signs used in a written text and mark their exact function in a text. Through interpretation and analysis of the function of orthographic symbols, we know how a piece of text is designed and developed, which script is used for the composition of a text, and in most cases, with reference to a script, we identify the language of a text

(b) Prosodic Annotation: we annotate crucial prosodic features in a spoken text. Our primary goals are to indicate and mark stress and accent in speech, patterns of intonation, spontaneous speech features, suprasegmental properties, and non-verbal cues (e.g., silence, pause, hesitation, repetition, non-ends) present in a spoken text

(c) Grammatical Annotation: we assign exact part-of-speech to words and terms after we understand their grammatical roles in a text. That is why it is also known as part-of-speech (POS) annotation. To assign a POS value to a word, we have to first analyze the role of a word in a sentence and identify in which part-of-speech it is actually used

(d) Named item Annotation: our primary goal is to identify all proper names used in a text and mark them in different types of named entities based on their form and function in a text

(e) Multiword Annotation: a multiword unit crosses a single-word boundary (or space). An expression that is composed of two or more words and is not predictable by any of the words which are used to construct it is considered a multiword unit. In multiword annotation, we identify and mark multiword units with a set of tags so that, at a later stage, a machine identifies them to analyze their forms and functions to understand their role in a text

(f)

Syntactic Annotation: It is also known as parsing. We identify and annotate all the phrases and phrasal expressions that are used in the meaningful construction of a sentence (Garside, et al. 1997;

Extralinguistic Annotation

In extralinguistic annotation, we annotate a text with that kind of information, which is not physically available inside a text. For instance, when we annotate a text at an anaphora level, there is no apparent information available in a text-based on which we can identify how words are linked with others in relation to co-reference. We go beyond the textual level to understand how words are co-indexed or bound with co-referential relations. Some common types of extralinguistic annotation include semantic annotation, anaphoric annotation, discourse annotation, etymological annotation, rhetoric annotation, and ethnographic annotation. Primary ideas about some extralinguistic annotations are presented below.

(a) Semantic Annotation: it is also known as word sense annotation. Here we assign semantic values to both open and closed classes of words. The primary goal of semantic annotation is to identify the sense a word denotes when it is used in a text. This sense, which is normally known as contextual sense, may differ from a lexicographic sense of words available in a dictionary. The contextual sense is utilized in language applications such as word sense disambiguation

(b) Anaphoric Annotation: we identify and co-index pronouns and nouns that are used in a text in a co-reference scheme following a broad framework of cohesion

(c) Discourse Annotation: we annotate a text at a level, which is beyond the level of sentence and meaning. We do this to understand discourse and pragmatic strategies deployed in a text. It is noted that the identification of specific discourse markers and pragmatic functions of units in a text gives scopes to understand the purposes and goals of a text generation

An etymologically annotated text addresses all the questions and challenges related to the etymology of words.

(e) Rhetoric Annotation: it is also known as a figure-of-speech annotation. We identify and mark various rhetorical devices used in a text. We annotate these devices based on standard methods of classification and categorization of rhetorics. The use of rhetorics is a common practice in text generation. Each piece of text (either spoken or written) contains a few rhetorics-either explicitly visible or implicitly embedded. Analysis of rhetorics sheds new insights into the theme and structure of a text. For instance, a piece of text made with rhetorical devices is different from a text without rhetorics. A rhetorically developed text differs in choice of words, order of words, sentence structure, grammatical form, sense implication, and reference. Such properties put up challenges in scientific argumentation in a text

(f) Ethnographic Annotation: it is an expected deviation from discourse annotation. It involves marking sociolinguistic cues, ethnographic elements, ecolinguistic properties and cultural information that are normally concealed in a text

Besides these major types of text annotation, we think of annotation of other types. In most cases, they posit new challenges. We have to think of new strategies to solve these challenges. For instance, we have not yet investigated how a poetic text should be annotated; and what kinds of textual, linguistic, stylistic, prosodic, and figurative annotation techniques we should design and apply to poetic texts. Problems and challenges are also there in the annotation of medical texts, texts of court proceedings, conversational texts, multimodal texts (where audio-visual elements are included), mythological texts, and others. Further challenges are involved in the annotation of heritage and classical texts, which require separate tagsets and methods for annotation of mythological characters, events, anecdotes, ecology, and sociocultural properties.

CORPUS UTILIZATION

Some questions are often raised with regard to the use of language corpus in the study of languages and designing tools of language technology: What is the use of a corpus? Who will be using it? Where will it be used? How will it be used? What purpose will it serve in general linguistics? How can it benefit applied linguistics? How does it contribute to language technology? Most of these questions are addressed in some earlier studies

In a general view, there are two major applications of a corpus: (a) as a diluted source of data to work as a yard-stick for linguistic and extralinguistic verification and authentication, and (b) as a test-bed for training and testing of devices, tools, techniques, and systems of applied linguistics and language technology. Keeping these issues in mind, we visualize the value of a corpus in the following ways:

(a)

It is an indispensable resource for developing systems, tools, and software for language technology, (b) It is a useful resource in general language description, language analysis, and language teaching and training, (c)

It is a reliable treasure-house for lexical databases, dictionaries, thesauruses, reference books, and course books, etc., (d) It is a ready-made handy resource for multi-purpose non-linguistic uses and references, (e)

It is a repository of linguistic features and properties, which are used to study the use of language across styles, genres, topics, etc. (f)

It is a customizable text for studying particular areas of interest relating to life, language, and society.

There are many benefits of a corpus in all areas and subareas of linguistics and language technology. Both from a theoretical and application point of view, corpus linguistics is a powerful method, which is empirical, scientific, realistic, authentic, open-ended, verifiable, and reliable. We must agree that quantitative data retrieved from a corpus is necessary not only in language technology but also in many areas of linguistics (e.g., speech analysis, lexicography, discourse analysis, language teaching, stylometrics, translation, and language planning). Within mainstream descriptive linguistics, a corpus is a useful resource based on which faithful language description can be made. Many successful works on language description are based on data obtained from corpora. In language teaching, information about the use of phonemes, morphemes, words, and sentences in corpora is used by teachers while they teach a language scientifically. Information about the frequency of use of language properties is not available from introspection; it is to be collected from language corpora only. In language acquisition, observation of actual evidence is a source for verification and validation, since no intuitive judgment can justify a phenomenon observed in language use by infants. Even generative linguists acknowledge the value of a speech corpus as a source of evidence in language acquisition studies. Due to many such advantages, criticisms against corpus linguistics, although partly succeeded at the initial stage, have failed to stop corpus linguistics. In the present context of linguistics and language technology, we have clear ideas about the application of corpus in various domains (Fig.

No