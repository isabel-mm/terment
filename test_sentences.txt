What this study illustrates is that corpus data allow very detailed analyses of how a given change proceeded.
As a result, most research articles in corpus linguistics include discussion of corpus compilation, annotation, and/or the computational methods used to retrieve linguistic data.
Having each word in the corpus tagged according to its part of speech (i.e., word class) permits, for instance, the study of partiallyfilled constructions (e.g., N-PROP of N-PROP; "the Einstein of Italy") and of lexical items of a specific part of speech (e.g., love_V; "I love you").
We will need to define empty character vectors to collect results using character, and we will need to use a for-loop to load each corpus file; during each iteration, we will use grep to find all corpus sentences and then use exact.matches.2 to find all matches of run, runs, walk, and walks.
Frequently, spreadsheet applications are used to store the corpus data and annotation decisions, as in the example in The first line contains labels that tell us what information is found in each column respectively.
Grammatical marking can also be studied in terms of co-occurrence.
The most frequent content word is cin√©ma, at the 20th frequency rank.
For example, historical sociolinguistic studies are interested in establishing correlations between the speaker's or writer's gender, age, or social class and innovative language use, whereas a pragmatic study would highlight situational uses of the novel form and momentary shifts between older and newer variants in discourse.
For example, by making a corpus study, it is possible to determine in which textual genres the passive voice is most commonly used.
In this way, parameters are shrunk towards zero, just as in the linear mixed effect model.
In his work on register variation, Biber develops a series of what he calls dimensions: general parameters that describe a particular style of communication.
The third deals with data creation: the nature of data, its abstraction from text corpora, its representation in a mathematical format suitable for cluster analysis, and transformation of that representation so as to optimize its interpretability.
As we do so, we will consider how the quantity and quality of the data are affected by the corpus size, as well as the corpus architecture and interface.
The discussion also noted that different selections of initial parameter values such as the locations of the Voronoi centroids, lattice size and shape, and different sequencings of training data items can generate different cluster results, and this calls the status of the result as a reliable representation of the intrinsic data structure into question.
We have seen that corpus analysis makes it possible to better characterize the language specificities of people suffering from language and communication impairments, by studying the different ways in which these patients interact in a natural environment.
From about page 24 onwards, I managed to find examples involving above this way, but remember, as we've thinned the examples randomly, your frequency distribution may be somewhat different.
On the one hand, cross-linguistic research should take full stock of recent advances in natural language processing, for tasks such as automatic alignment and multilingual annotation.
It's therefore less sensitive to corpus size, and hence probably more reliable for smaller corpora.
The great advantage of part-of-speech tagging is that it can be done automatically with almost the accuracy of a manual annotation, regardless of the amount of text to be annotated.
Multimodal interaction includes a range of different semiotic resources, and multimodal corpora, as already noted, have the potential for enabling the researcher to study the use of language along a continuum of dynamically changing contexts.
Variation often involves complex patterns of use that involve interactions among several different linguistic parameters but, in the end, corpus analysis consistently demonstrates that these patterns are systematic.
In the preceding chapter, ùúô was introduced as an effect size for two-by-two tables (see 7).
He specializes in corpus linguistics, statistics and applied linguistics, and has designed a number of different tools for corpus analysis.
Consequently, researchers were able to computationally build corpus data, focusing primarily on grammatical or semantic aspects in discourses.
For example, we may find that the independent variable under investigation has a statistically significant influence on our dependent variable, but that the effect size is very small, suggesting that the distribution of the phenomenon in our sample is conditioned by more than one influencing factor.
On the one hand, there are the tools making it possible to carry out annotations in an automatic way, for example, by means of part-of-speech tagging or parsing.
Once these determinations are made, the corpus compiler can begin to collect the actual speech and writing to be included in the corpus.
Unlike the more experimental data types often used in SLA, where learners are forced to produce a particular form (as in fill-in-the-blanks exercises or read-aloud tasks), the focus in learner corpus data is on message conveyance and the possibility for learners to use their own wording.
A specialized corpus is a corpus type represented by a collection of texts compiled from a particular genre (newspaper articles, agreement letters, academic articles, lectures, essays, etc.).
We can avoid at least some of these problems in using stop word lists by tagging our data grammatically before excluding any stop words, but there may not be such a simple solution in terms of deciding which of the semantically ambiguous types of potential stop words ought to be in-or excluded from our lists.
Note that the p-value does not say anything about the association between the observed values, it refers to the whole set of observations in relation to a larger population (for between-observation association, see the section on effect size below).
Examples abound in -learner corpus research, to document potential over-/underuse by learners compared to native speakers; -language acquisition corpora, to document how children acquire patterns as they increase the number of different verbs (i.e. the type frequency) filling a slot in a particular construction; -historical linguistics, to document the in-/decrease of use of particular words or constructions over time.
I argued in Section 1.1.1 above that this claim makes sense only in the context of rather implausible assumptions concerning linguistic knowledge and linguistic usage, but even if we accept these assumptions, the question remains whether intuited judgments are different from corpus data in this respect.
But because this kind of discussion is subjective and impressionistic, it is better to devote the bulk of a corpus study to supporting qualitative judgements about a corpus with quantitative information.
There are grey areas in copyright law and copyright infringement is looked at in different ways in different parts of the world, so it is difficult to find universally valid advice on the topic, but generally speaking copyright may prove quite a hindrance for corpus compilation.
As we have discussed in Chapter 1, language variation is a prevalent characteristic of human language.
The site's interface makes it possible to choose works based on different criteria, such as the time period or the author.
On the other hand, there is the area of diachronic historical corpus linguistics, in which corpus data are-given the relevant time spans-usually cross-sectional, covering, for instance, several centuries of the history of a language.
The exploitation of a native corpus, used in combination with the learner corpora, makes it possible to see how the learner data are situated in relation to a certain reference norm (without being limited by it), whereas the inclusion of the L1 variable gives a glimpse of the possible influence of the mother tongue.
We will discard empty character strings with nzchar and test whether each word token is "perl" with a simple logical expression testing for equality (i.e., ==) and then use plot (with type="h", for histogram) to plot the left panel (because FALSE = 0 and TRUE = 1).
It has to "learn" them from a corpus that has been annotated by hand by skilled, experienced annotators based on a reliable, valid annotation scheme.
For this reason, language acquisition corpora frequently include language samples produced by children as well as by adults.
This is a problem not only for studies that are interested in linguistic variation but also for studies in core areas such as lexis and grammar: many linguistic patterns are limited to certain varieties, and a corpus that does not contain a particular language variety cannot contain examples of a pattern limited to that variety.
This study is based on one of the large learner corpora coming out of the testing/assessment world (see Sect.
The robust evidence found in these electronic collections of language offers countless possibilities for both linguistic and social research providing a unique insight into patterns of language use.
Specifically, we will discuss three types of data (or levels of measurement) that we might encounter in the process of quantifying the (annotated) results of a corpus query (Section 5.1): nominal data (discussed in more detail in Section 5.2), ordinal (or rank) data (discussed in more detail in Section 5.3, and cardinal data (discussed in more detail in Section 5.4.
Some of these subjects engage with the study of language to a degree (psychology and sociology, for example), some might conceivably have research questions to which linguists could contribute (social work and education, for example), while others may be focused so far away from language that the likely interaction with linguistics is marginal at best (social statistics, for example).
We will focus on this type of analysis in the following section.
In the case of the HTR, decreasing the sample size is slightly more problematic than in the case of the TTR.
However, given that the intended audience of this handbook is expected to have limited resources for corpus compilation, it seems useful to provide an example of a study where it was possible to use part of an already existing corpus.
To do cognitive linguistics with corpus data, you need to interpret the data -to give it meaning.
Part II deals with corpus methods: Chapters 4-9 provide an overview of the most commonly used methods to extract linguistic and frequency information from corpora (frequency lists, keywords lists, dispersion measures, co-occurrence frequencies and concordances) as well as an introduction on the added value of programming skills in corpus linguistics.
Thus, while this is not an easy book, I hope these aids help you to become a good corpus linguist.
In the following section, we will provide some guiding principles for how to go about answering your research question(s) using a register analysis framework and corpus methods.
After the loop, we use simple arithmetic to compute Fichtner's C values and data.frame to compile the results all into one data frame.
For now, we only want to practise adding simple word class codes to our results file.
However, for larger or more complex analyses, it may also become useful to be able to split up the data frame into smaller parts, depending on the values some variable takes.
Compilers and users of learner corpora also grapple with the issue of unwanted lexical bias, stemming from task topics, writing prompts, or other input materials.
In fact, the field of corpus linguistics is often conceptualised as being part of the Digital Humanities (e.g., Roth 2018; Zottola 2020; Mehl 2021) despite the fact that it has a longer history.
For instance, we might simply choose to remove all texts shorter than, say, 400 words, 500 words, or 1,000 words from our dataset.
For example, journals such as Corpus Linguistics and Linguistic Theory, International Journal of Corpus Linguistics and Corpora are all three specialized in the publication of corpus studies, whereas the Journal of Language Resources and Evaluation publishes articles on methodological aspects related to the compiling and processing of corpus data.
We add another column to the data frame in which we identify all examples with a high frequency verb and an intervening adverb, essentially dummy-coding the interaction.
Which of the two levels is relevant in the context of a particular research design depends both on the kind of phenomenon we are counting and on our research question.
I present four multivariate exploratory techniques: correspondence analysis (henceforth CA), multiple correspondence analysis (henceforth MCA), principal component analysis (henceforth PCA), and exploratory factor analysis (henceforth EFA).
In quantitative statistical analysis, we classify linguistic properties or features in a corpus, count their frequency of occurrence, and construct statistical models to explain what we observe.
However, because it is virtually impossible for the creators of corpora to anticipate what their corpora will ultimately be used for, it is also the responsibility of the corpus user to make sure that the corpus he or she plans to conduct a linguistic analysis of is a valid corpus for the particular analysis being conducted.
The success of any research depends on the quality of the data and the effectiveness of the analytical procedure; yet, especially in corpus research, the quality of corpus data is rarely scrutinized.
Note that, since this is an effects plot, the effect shownthe interaction of complement subject length and register/mode -is represented while every other effect in the regression model is controlled for, which is important because the frequently used plots of observed means/correlations do not do that.
Dispersion is the spread of values of a variable in a dataset.
Another issue we've encountered in this context is that often tools designed for analysis almost force us into accepting the 'orthographic word' as the correct unit of analysis, which is e.g. exemplified in the fact that the BYU interfaces don't allow us to compare 'words' with 'phrases' directly.
The most economical way of reporting correlation (used especially when reporting multiple correlations in a table) is to add a single (*) or double (**) asterisk next to the correlation coefficient.
We use source to load exact.matches.2 and scan to load the text file <_qclwr2/_inputfiles/corp_indexing-1.txt> into R. We use paste to merge all elements into one long character vector and unlist plus strsplit to break it up into a character vector of pages.
We have to think seriously about how all these problems are addressed and solved so that the benefits of language technology reach every language community-advanced or non-advanced, resource-rich or resource-poor, and technology-savvy or technology-hungry.
Clearly, what probability of error one is willing to accept for any given study also depends on the nature of the study, the nature of the research design, and a general disposition to take or avoid risk.
So how a corpus linguist positions themself in the epistemological debate is important in interacting with the social sciences.
In the formula that specifies the regression model, the asterisk between the variable of high frequency verb and the variable of adverb indicates that we are testing for the main effects of these variables as well as for a possible interaction between them.
It is likely that among those learner corpora that are not listed but exist 'out there', most can be counted in tens of thousands rather than in millions of words.
It's thus well worth bearing the above-mentioned factors in mind when conducting any kind of statistics-based keyword analysis, and especially when reporting on the presumed importance of particular keywords for a given text/corpus.
Therefore, in addition to the file describing the editing process, you'll probably want to keep at least one extra file that lists the contents of the corpus file-by-file if your data contains materials from different genres, text types or domains, or, as with our web page data, that lists information about where the file was retrieved from, when, what the original file name was if you've changed it), etc.
Once we're done with the loops, we will need to compute the file sizes in percent, which means we need sum to compute the overall corpus size from sizes.of.files.in.words so we can compute each file's size as a fraction of that, and we will need lapply to access each element of freqs.of.words to do the same.
It is now treated as a sub-branch of Artificial Intelligence (AI) because language processing is a highly complex method of human-computer interaction.
In sociolinguistics, for example, speakers might be described by a set of variables one of which represents the frequency of occurrence of some phonetic segment in interviews, another one speaker age, and a third income.
We also discussed the increasingly widespread use of corpora as a basis for the creation of dictionaries and showed that these data help us to overcome many inherent limitations of a purely qualitative approach to writing dictionaries.
However, as we saw in the preceding section and in Chapter 3, it is not always possible to define a corpus query in a way that will retrieve all and only the occurrences of a particular phenomenon.
The result of this analysis is a collection of language patterns that are recurrent in the corpus and either provide an explanation of language use or serve as the basis for further language analysis.
While not every five-word text contains a first-person pronoun, it is also not that unusual if one does.
Learner corpora can be used to study language acquisition, and to develop pedagogical tools and strategies for teaching English as a second or foreign language.
To put it bluntly, then, intuition "data" are less reliable and less valid than corpus data, and they are just as incomplete and in need of interpretation.
In public projects, a proprietary search engine tailored explicitly for a specific corpus is often programmed, which cannot easily be used for other corpora.
The results of non-parametric tests, like Chi-square, cannot be generalized to the population the sample was drawn from but we can ask questions related to the given dataset.
Other researchers treat co-occurrence as a structural phenomenon, i.e., they define collocates as words that co-occur more frequently than expected in two related positions in a particular grammatical structure, for example, the adjective and noun positions in noun phrases of the form [Det Adj N] or the verb and noun position in transitive verb phrases of the form [V [ NP (Det) (Adj) N]].
A more exhaustive list of learner corpora in many languages is provided on the Center for English Corpus Linguistics (CECL) website, from UCLouvain in Belgium.
Yet another distinction is that between monolingual corpora and parallel corpora.
On the other hand, even the investigation of lexical co-occurrence by means of collocate displays can be problematic.
This is the crucial component of any corpus building or compilation project (see Chapter 6) or of carefully using the metadata of existing corpora.
Conceptually, we are looking for the mean score for each group and then the variation as to how the scores are dispersed or spread (i.e., how far away each score is from the mean).
The painful way uses gregexpr's output to retrieve the starting points of all matches in all elements of txt and then also the lengths of all matches to compute their end points, because then, once we have starting points and end points, we can use substr to extract the relevant matches from the original input vector txt.
In contrast to linguistic knowledge, language use is directly observable and recordable, and the texts in a corpus essentially comprise such records of language use.
Otherwise the difference in probabilities can serve as the basis for model selection.
There are different ways in which the raw data -the audio and/ or video recordings of the speech event -can be made accessible to corpus users: in the SBC, the solution is to store two types of file for each corpus text, one audio file containing the text recording in WAV format and a text file containing the transcription thereof.
Data extraction can also be based on a prior automatic analysis of the chosen corpus, such as lemmatization or part-of-speech tagging (see Chapter 7, section 7.4).
However, for a corpus file to be used as a sample representing a certain type of language, metalinguistic information (which is not part of the text or of the dialogue) should be accessible to the researchers who will analyze it.
It is interesting, however, that for some words at least, there appears to be a "diminishing return" with corpus size.
Accepting (or working around) the corpus creators' assumptions and decisions concerning POS tags and annotations of syntactic structure may seriously limit or distort researcher's use of corpora.
While the solutions to the problems with lexical diversity measures are not directly applicable to the problem of text length, they may still provide inspiration and starting points for new ways of approaching the problem of text length.
This should ideally be done in the form of a text file that lists all the separate editing steps, and can later provide the basis for part of a manual of information to be distributed with your corpus if you ever plan to release or distribute it.
Find five words that are recent (have a higher frequency in the most recent time period) and five examples of words that are more common in the earliest time period.
There is a substantial body of corpus-linguistic research based on designs that combine the two inherently represented variables Word (Form) and Text; such designs may be concerned with the occurrence of words in individual texts, or, more typically, with the occurrence of words in clusters of texts belonging to the same language variety (defined by topic, genre, function, etc.).
The code also runs a regression model that includes the relevant interaction term, and visualizes the interaction effect.
In each case k-means for k = 2 partitions the projections into two regions, which are demarcated by dashed lines for convenience of reference.
By way of conclusion, we would like to offer a list of ordered stages, making it possible to implement the concepts discussed in this book step by step, and to carry out a corpus study.
These lists of words that are particularly salient (i.e., frequently encountered) in a particular discourse domain have several practical applications, but perhaps the most common relate to language teaching and learning purposes.
For those interested in more information on corpus building and copyright laws, there are some sources to consult at the end of this chapter.
WordSmith Tools (version 5.0) was chosen, among other reasons, due to its compatibility with the latest BNC XML edition (used in our study) and ability to generate both word frequency lists and lists on recurring strings of words (or "n-grams").
The minimal requirement of an incremental and collaborative research cycle is what we might call retraceability: our description of the research design and the associated procedures must be explicit and detailed enough for another researcher to retrace and check for correctness each step of our analysis and when provided with all our research materials (i.e., the corpora, the raw extracted data and the annotated data) and all other resources used (such as our annotation scheme and the software used in the extraction and statistical analysis of the data) and all our research notes, intermediate calculations, etc.
A 95% confidence interval is an interval that is constructed around a statistical measure (here an effect size) based on the sample in such a way that the true value of this measure lies within this interval for 95% of the samples taken from the same population.
This script does the exact same thing as the one just discussed and most of the code is in fact identical, but it does it not by essentially treating the corpus file as a simple flat sequence of character strings as we did above, but by utilizing the hierarchical XML annotation in ways discussed in Section 3.8.2.
However, the use of corpora in the field of syntax has grown considerably.
In particular, the use of corpora makes it possible to compare the productions of various speakers, of different language varieties as well as different registers, providing a much more nuanced and realistic vision of the structures underlying language uses, rather than the intuitions of a single speaker.
The results of this study indicate that rule-based processes affect not only word realisation but statistical tendencies built up from experience of language use affect word realisations as well.
This is very much faster than most desktop corpus analysis tools, which have to deal with color highlighting and other display issues.
Finally, when we look at characteristics of individual linguistic features (e.g., article type in subject and object positions), our unit of analysis is each instance of that feature.
For example, the syllabic length of an NP is a numeric variable; other examples are pitch frequencies in hertz, word frequencies in a corpus, number of clauses between two successive occurrences of two ditransitives in a corpus file, and the reaction time toward a stimulus in milliseconds.
Let's get a first impression of this separation of texts into meta-data and content as an exercise by looking at the source of a web page.
For this study, the use of a word list offers major advantages compared to a corpus: the grammatical category of words is already known, which simplifies noun retrieval: every word is already associated with its grammatical gender.