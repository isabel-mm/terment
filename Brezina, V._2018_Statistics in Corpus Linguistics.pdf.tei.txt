Statistics in Corpus Linguistics

Do you use language corpora in your research or study, but find that you struggle with statistics? This practical introduction will equip you to understand the key principles of statistical thinking and apply these concepts to your own research, without the need for prior statistical knowledge. The book gives step-by-step guidance through the process of statistical analysis and provides multiple examples of how statistical techniques can be used to analyse and visualize linguistic data. It also includes a useful selection of discussion questions and exercises which you can use to check your understanding. The book comes with a companion website, which provides additional materials (including answers to exercises, datasets, advanced materials, teaching slides etc.) and Lancaster Stats Tools online (

vaclav brezina is a senior lecturer at the Department of Linguistics and English Language, Lancaster University. He specializes in corpus linguistics, statistics and applied linguistics, and has designed a number of different tools for corpus analysis.

Statistics in Corpus Linguistics

x

Commonwealth & Protectorate and Restoration 7.2 Comparison of two periods in the EEBO corpus: results of the bootstrap test 7.3 Final evaluation of the results: its, must and pestilence 7.

About This Book

What Is This Book About?

This book is a practical introduction to statistical procedures in corpus linguistics, a discipline that uses computers to analyse language, organized according to linguistic topics. These range from vocabulary and grammar to sociolinguistics, discourse analysis and historical investigations of language.

The book offers an overview of the state-of-the-art methodologies of language analysis using corpora and introduces new techniques that have not previously been used in the field. No prior knowledge of statistics is assumed; instead, all necessary concepts and methods are explained in non-technical language.

In addition, all procedures described in the book can be easily carried out using Lancaster Stats Tools online (see 'How Should You Use This Book?' below). Throughout the book, many examples (case studies) of the application of corpus statistics are provided and standard reporting of statistics is shown.

The emphasis of the book on the practical aspects of statistical analysis of language is also reflected in its focus on research design and the implications of different 'shapes' of data for statistical analysisfor this reason, the companion website offers complete datasets used in this book for easy replication of the analyses. Corpus linguistics is an extremely versatile methodology of language analysis applicable in a wide range of contexts, in linguistics, social science, digital humanities and elsewherethe book thus aims to facilitate meaningful use of corpora for as wide a range of users as possible.

Who Is This Book For?

The book is intended for anyone interested in corpus linguistics and quantitative analysis of language. This includes students and researchers in the field of linguistics, sociology, history, psychology, education etc. The main goal of the book is to help readers understand key principles of statistical thinking in order to be able to make informed decisions about the applications of particular statistical techniques. To facilitate this, in addition to the expository parts, the book also includes discussion questions ('Think about…') and exercises which the readers can use to better engage with the material and to check their comprehension of the subject matter; answers to the exercises are provided at the companion website (

xvii How Should You Use This Book?

The book reflects the needs of students and researchers who are looking for a practical guidebook on corpus statistics, which is grounded in the current literature in the field and reflects the best practice. The book can be used as a course book or for independent study. After reviewing general statistical principles in Chapter 1, readers can follow their own path through the book according to the linguistic topics of their interest. Statistical techniques introduced in the book are cross-referenced and included in the Index at the end of the book.

The book comes with a companion website -Lancaster Stats Tools online -(

In particular, we'll be exploring answers to five questions:

• What is the role of statistics in science and corpus research? (Section 1.2) • What are the key terms in corpus statistics? (Section

• How can we explore and visualize data? (Section 1.5)

• How can statistics be used in corpus research? (Section 1.6)   1.

Before reading this section, think about the following questions:

1. What is science? What are the basic features of scientific enquiry?

2. Which of these statements about language are scientific statements? (a) Women's speech seems in general to contain more instances of 'well', 'y'know', 'kinda', and so forth . . . (b) Words are easy, like the wind. (c) Passives are most common by far in academic prose [compared to other registers], occurring about 18,500 times per million words. (d) The faculty of language can reasonably be regarded as a 'language organ'.

(See next page for one more example) (e) Our results show that there were significant changes in at least one formant

Unlike other sources of information such as mythology, philosophy or art, science relies on the systematic collection of empirical data and testing of theories and hypotheses. One of the most influential theoreticians of science, Karl Popper, defined a scientific statement or theory as something that can in principle be falsified

Corpus linguistics is a scientific method of language analysis. It requires the analyst to provide empirical evidence in the form of data drawn from language corpora in support of any statement made about language. Another scientific requirement corpus linguists follow in principle is replicability of results. This means that researchers need to be able to confirm the findings of one study in follow-up studies (see Section 8.3). In order for the results to be replicable, corpus linguists need to make their choice of corpora and analytical techniques transparent. It is also good practice in corpus linguistics to make corpora available to other researchers who can explore the same dataset further and thus advance knowledge in the field.

Let us have a look at two examples that illustrate this point. First, imagine that we are interested in the number of adjectives different British fiction writers use in their texts. We might hypothesize that using more adjectives leads to more colourful descriptions in novels. We have randomly selected 11 fiction texts by different authors from the British National Corpus (BNC) and counted the number of adjectives in each text; this is their absolute frequency (see

The mean is calculated in the following way:

mean ¼ sum of all values number of cases exploration and must lead to doubt being cast on claims made using such corpora. If corpus linguistics wants to retain its scientific status, it should not be content with statements such as 'this feature was found in a large corpus that is, however, not available'. 5 Because the texts are of different length, we have taken the relative frequencies per 10,000 words to show how many adjectives on average each author uses in 10,000 words (see Section 2.3 for the explanation of relative frequency). The relative frequencies have been rounded to the nearest integer.

Applied to the dataset above:

¼ 591:45 ð1:1Þ

Because the mean describes our sample, it is part of what we call descriptive statistics. Another example of a mathematical representation of complex linguistic reality is a line, in statistics called a regression line or line of the best fit (see Chapter 4 for an explanation of regression models). Imagine that we are interested in whether the authors that use more adjectives also use more verbs. We can list the frequencies of verbs 6 just below the frequencies of adjectives to see whether there is any relationship between these two linguistic features:

The graph in Figure

Basic Statistical Terminology

Think about . . .

Before reading this section, think about the meaning of the following terms. Have you heard them before? If so, in what context? Would you be able to define them?

• assumption

The following is an overview of basic statistical terminology used in this book. It includes key terms with examples from corpus research and is ordered from basic concepts to more complex ones which rely on the understanding of the previous terminology. Mastering these terms will make reading of the rest of the book, and many papers in corpus linguistics, much easier.

Corpus (pl. corpora) is a specific form of linguistic data. It is a collection of written texts or transcripts of spoken language that can be searched by a computer using specialized software. A corpus usually represents a sample of language, i.e. a (small) subset of the language production of interest; in some limited cases of very specialized corpora, a corpus can include the whole population, i.e. all language of interest to the researcher (see Section 1.4). The software that is used to search a corpus usually implements basic types of statistical analysis such as the statistical identification of collocations and keywords (see

Note that preparing the spreadsheet in the appropriate format is as important as the statistical analysis that follows. The book offers many examples of datasets based on different corpora, which are suitable for different types of analysis. It is always useful to compare your data to the model examples provided (full datasets are available from the companion website) to see if your data is in the appropriate format.

Dataset is a series of corpus-based findings that can be statistically analysed. It is a systematic collection of individual results that can be stored in the form of a table in a spreadsheet program (e.g. Excel, Calc etc.), each line representing an individual data point or case and each column representing a separate variable. Figure 1.3 provides an example of a dataset with five variables and multiple cases, each case representing one speaker. Note that example datasets used in this book are available at the companion website. It is important to study them for the particular 'shape' of data that lends itself to particular types of statistical analyses.

Variable, as the name suggests, is something that can vary and take on different values. For example, speaker's age is a variable that can take on different values from about one year (when children typically learn their first words) to over 100. Much corpus research can be characterized as searching for variables in corpora and analysing the relationship between them. An important distinction needs to be made between linguistic variables and explanatory variables. Linguistic variables capture frequencies of linguistic features of interest in the corpus. Explanatory variables (sometimes called 'independent variables') capture contexts in which the linguistic features appear. For instance, an explanatory variable can be the genre/register or date of publication of a text as well as speaker's age, gender and language proficiency, to name only a few. The dataset in Figure

Variables (both linguistic and explanatory) can be either nominal, ordinal or scale variables. A nominal variable has values that represent different categories into which the cases in a dataset can be grouped; there is no order or hierarchy between the categories. Speaker's gender is an example of a nominal variable because we can assign speakers in the dataset to one of two groups:

(1) male speakers and (2) female speakers. There is no hierarchy in this classification. For convenience, we often use numbers to indicate the group membership. In the dataset in Figure

• Is there a relationship between speaker's gender (a nominal explanatory variable) and the use of personal pronouns (a scale linguistic variable)? • Does a speaker's English proficiency (an ordinal explanatory variable) have an effect on the use of the first-person pronoun (a scale linguistic variable)? • Is there a relationship between the use of the first-person and the second- person pronouns (both of which are scale linguistic variables)?

The frequency distribution of a variable provides information about the values a variable takes and their frequencies. Distributions of scale variables can be shown in a histogram (see Section 1.5). Figure

As a benchmark in statistics, one of the common distributionsthe normal distribution 8is often used. The shape of the normal distribution is a symmetrical bell as shown in Figure

Although a lot of data in the natural and social world follows the normal distribution, most linguistic data is positively skewed ( ), i.e. there is more data to the left of the distribution than the right, as we saw, for example, in Figure

introduction

Outlier or rogue value? When we look at distributions we often check for outliers. Outliers are extreme values, i.e. values that are very far from the other values. Section 1.5 will introduce boxplots, a useful means of identifying outliers. When we find an outlier we need to check if the outlier is a genuine value or a measurement errora so-called rogue value. A rogue value can be caused, for instance, by mistyping data in a spreadsheet or by a tagging error in the corpus. An outlier, instead, is a valid data point, which for some reason stands out from others. While outliers are not in themselves 'errors', they present problems for statistical models because they may obscure the general tendency (see 'measure of central tendency' below) in the data and the researcher must decide how to go about the analysis of data which includes outliers. If there is a good reason, outliers can be excluded (bracketed out) from part of the analysis that focuses on the central tendency in the data.

The measure of central tendency or 'average' provides one summary value for a series of values of a scale variable. It is a simple statistical model that is usefully paired with dispersion (see below) to complete the summary description of the data. Different types of average can be used. In corpus linguistics the most useful ones are: mean, median and 20% trimmed mean. Mean (M or x̄), as we have already seen, is the sum of all values divided by the number of cases (see Section 1.2). The mean is a useful measure in distributions which do not have extreme values (outliers) that sway the mean towards them; in distributions with outliers, the mean might represent the outlier more than the rest of the values, which leads to the mean failing to be a useful model. Take for instance the frequency of adjectives in 11 fiction texts taken from the British National Corpus (BNC) used as an example to calculate the mean in Section 1.

If we had ten instead of eleven values, which is an even number, the median would lie half way between the two central values 565 and 567, as demonstrated below.

Dispersion is the spread of values of a variable in a dataset. Take again the adjective counts analysed in Section 1.2 sorted from the lowest (508) to the highest (699) value:

An alternative dispersion measure to the range 1 and the interquartile range is standard deviation. Standard deviation (SD) is the square root of the sums of squared distances of the individual values from the mean. This gives us an indication of the overall distance of individual values from the mean (see Chapter 2 for more detail).

Statistical measure is a general term for any statistic we calculate. It can be as simple as the mean or it can involve complex statistical modelling such as mixedeffects models

1. We start with the hypothesis we want to test called the alternative hypothesis or H 1 . For example, a sociolinguistic H 1 can claim that men and women differ in the use of swearwords. 2. We formulate the null hypothesis (H 0 ) that is the reverse of H 1 . To put it very simply, the null hypothesis states that there is nothing special going on in the corpus or corpora we analyse, e.g. there is no difference between two (sub)corpora. In our example, the H 0 would therefore claim that there is no difference between men and women when it comes to the use of swearwords. 3. We test the null hypothesis using a statistical test such as the independent samples t-test. Before doing this, however, we need to check that our data satisfies the assumptions of the selected test (see 'assumptions' below). 4. We usually get two important values from a statistical test: (a) the test statistic and (b) the p-value. Based on the p-value (i.e. the probability value of the observation in the corpus by chance alone) we decide whether to reject the null hypothesis. If the p-value is small enough, usually smaller than 0.05, i.e. 5%, we reject the null hypothesis and conclude that the observed difference is unlikely to be due to chance and therefore the result is statistically significant. This means that the difference observed in the corpus (sample) is likely to be a true difference in the population (all language use). If the p-value is equal to or is larger than 0.05 (or 5%) we conclude that there is not enough evidence in the corpus to reject the null hypothesis. We need to be careful when interpreting a result like this, which should not be taken to mean that the alternative hypothesis (H 1 ) is false or that the null hypothesis (H 0 ) is true: there is simply not enough evidence to reject H 0 ; if we collect more data the statistical test might turn out significant. Note that 0.05 or 5% is the conventional cut-off point which can be imagined as the risk we are willing to take when inferring from the sample to the population (see p-value below). If we are willing to take only a smaller risk than 5%, we can decide on the p-value cut-off point 0.01 (1%) or even 0.001 (0.01%).

A p-value is often the most visible sign of a statistical test (see above). However, it would be misleading to reduce all statistics to p-values. A p-value is a probability value (p stands for probability) and is one of the outcomes of a statistical test. P-value can be defined as the probability that the data would be at least as extreme as that observed if the null hypothesis were true. In the example of a sociolinguistic research looking at the use of swearwords by men and women we would get a p-value that would give us the probability of seeing the observed or even more extreme difference between the two groups if the null hypothesis were true, that is, if there really was no difference in the population and the difference observed in the corpus (sample) was merely due to chance as the result of a sampling error.

Assumptions of a statistical test, as traditionally understood, are conditions that should be met for the statistical test to produce valid results. One of the typical assumptions of a number of statistical tests called parametric tests (e.g. the t-test or ANOVA) is the normality assumption. This assumption presupposes that the frequency distribution of the linguistic variable does not deviate considerably from the normal distribution. This is because parametric tests such as the t-test typically compare the means which may not be good models of the values of linguistic variables if the distributions are too skewed (see 'measures of central tendency'). In such cases, non-parametric tests such as the Mann-Whitney U test (non-parametric version of the t-test) can be used. These non-parametric tests typically compare sums of ranks of values rather than the means of the actual values (see Section 6.3). However, statistical research shows (e.g.

When discussing individual statistical tests in this book, the assumptions of these tests will be listed and discussed in more detail.

The confidence interval (CI) in inferential statistics is an attempt to move away from the dichotomous thinking that is often connected with NHST, statistical tests and p-values. Rather than a yes/no decision about statistical significance, the confidence interval provides an estimation of the true value of a statistical measure (such as the mean) or of a difference between two statistical 1.3 Basic Statistical Terminology measures (such as the difference between two means) in the population. A confidence interval, as the name suggests, is not a single value but a range of values (that can be visualized as error barssee Figure

Effect size in descriptive statistics is a standardized measure, that is a measure comparable across different studies (see Section 8.3 for the discussion of metaanalysis), that expresses the practical importance of the effect observed in the corpus or corpora. For example, if we establish by a statistical test (see above) that two groups of speakers (e.g. men and women) differ from each other in the use of a particular linguistic variable, i.e. there is a statistically significant difference between these two groups, we still need to see how large this difference is and whether it is practically important. To help us with this judgement, effect size measures such as r, odds ratio or Cohen's d can be used. Table

2. What does it mean to say that a corpus is representative?

3. Are large corpora always better than small corpora?

A corpus is a collection of texts

With about 400 million people who speak English as their first language and an additional hundreds of millions who speak English as their second language

In corpus linguistics the term 'representative' is often used to describe a corpus. Representative is a descriptor of a sample when it has similar characteristics to the population it is drawn from. This allows us to draw conclusions about the population from the sample. Ideally, this would be achieved by truly random sampling 10 where each text ever produced and each spoken interaction that has ever taken place would have the same chance of appearing in the sample. Random sampling is, however, impracticable because there is no catalogue of all language production that we could refer to when doing the sampling; besides, not all language produced is recorded in some form. To make the sampling manageable, corpus designers often start with a set of categories within which they aim to collect an unbiased sample. These categories are called the sampling frame. Table

The Brown family consists of 15 genre-based categories according to which the total of 500 2,000-word text samples are selected. Each Brown family corpus thus consists of approximately one million words of written English (500 × 2,000). In this traditional corpus design, the aim of the corpus creators is to achieve an unbiased sample of texts in the categories from the sampling frame. Again, we would ideally use the random sampling procedure within the categories. Yet in practice, the selection is guided by text selection principles (see below) to avoid bias in the selection process. By bias we mean a systematic but often hidden deviation of the sample from the population.

The following is a list of the most common types of bias and related text selection principles to avoid this bias:

• Text sample bias: different sections of texts (e.g. beginning, middle and end) have different linguistic properties. For example, ends of texts tend to include language that summarizes the main point of the text (in sum, to conclude, they lived happily ever after). If a corpus samples only certain sections, e.g. by taking the first or the last 2,000 words of each text, these sections will be overrepresented. In corpora that do not include whole texts but only text samples (like the Brown family) it is therefore important to achieve a balance in terms of the different sections of texts represented.

• Topic bias: topic bias is created when many texts on the same topic get included in the corpus (unless the corpus is deliberately constructed as a specialized corpus on the topic). These texts usually contain a number of specific topicrelated vocabulary items that are repeated multiple times in each text. This is connected to a so-called 'whelk problem' (see Section 2.4) when some infrequent lexical items become dramatically overrepresented in the corpus. Topic bias is a problem especially in small corpora where each individual text forms a relatively large proportion of the corpus. In most cases, corpus designers should therefore consciously select texts on a range of topics.

• Non-coverage bias: some texts are more 'visible' than others because corpus designers see them as prototypical for different reasons. For example, published texts might be given a preference over private letters or emails. Corpus designers need to actively seek to cover as wide a range of texts as possible.

• Traditional text type bias: this type of bias is a specific case of non-coverage bias. When selecting the sampling frame we are often predisposed to see as salient the text types that have traditionally been included in language corpora such as those in the Brown family sampling frame (see • Legal considerations bias: corpus designers often face the problem of copy- right; this is especially the case when corpus creators want to share their corpora with other researchers. Legal considerations may thus lead to a selection of texts to which copyright does not apply (older out-of-copyright texts, texts under creative commons licences etc.), which, however, creates a problem with biased sampling. Currently, there is no clear solution to this issue as approaches may differ according to legal requirements in individual countries. Corpus designers, however, should be mindful of this problem.

• Practicality bias: some texts such as webpages are easier to obtain than others.

This practicality consideration may lead to creating a corpus that overrepresents easily obtainable texts. Corpus creators need to resist this temptation and strive for a range of texts regardless of whether they can be obtained easily or not.

• Self-selection bias: this bias is created when contributors (i.e. authors of texts) are asked to provide texts on a voluntary basis. For instance, if we want to create a corpus of classroom writing and ask students to volunteer and contribute their texts, we may end up with texts from highly motivated students that will not reflect the written production of the class as such. Corpus designers therefore need to reach out to different groups of contributors and use a range of incentives to obtain a representative sample. So far, the traditional approach to corpus design has been considered. However, a different approach to corpus representativeness and sampling emerged from the web as corpus initiative

As discussed, a corpus is usually a sample of language. However, in some specific cases, a corpus can include the whole population. For example, in studies of literature, corpora comprising all works by a particular author include the whole population. If we wish to compare the speech of Prince Hamlet with the speech of his friend Horatio in Shakespeare's famous tragedy, we will be working with all the evidence there is about the linguistic behaviour of these two characters. Similarly, if we collect all newspaper articles about a particular topic in a given period we will be looking at the whole population of articles that were written on the particular topic.

Having discussed different aspects of corpus building, one basic question still remains to be answered: how large should a corpus be? There is no universal answer to this query because corpus size depends on the research question and the kind of linguistic features we want to investigate. For the investigation of grammatical structures such as passives, which are generally fairly common, even a small corpus (e.g. one million words or less) can be sufficient. On the other hand, many lexical items and their combinations are fairly infrequent even in very large corpora and we may easily encounter the data sparsity problem. To illustrate this issue, Table

From this example, we can derive a general rule: unless the corpus represents the whole population, the absence of evidence is not the evidence of absence. In other words, if an expression does not appear in a corpus, this doesn't mean that this expression is non-existent. As corpus users we therefore need to think critically about the nature of the evidence that corpora provide in terms of their quality (representativeness and balance) as well as their quantity (corpus size).

Finally, a few words need to be said about the role of statistics in the analysis of corpora. Let us start by considering some general principles followed by the discussion of specific research designs. In the process of corpus analysis, there are four separate but interconnected dimensions (see Table

We usually start our analysis with data exploration during which we look at frequencies and distributions of linguistic variables (see Section 1.3 for statistical terminology) and often produce graphs which capture the main patterns in the data (see Section 1.5 for data exploration and visualization). If our corpus represents a sample rather than the population (which is typically the case), we should consider the amount of evidence we have in the sample. In other words, we can use inferential statistics to enquire whether the observed effects and differences between (sub)corpora can be generalized to the population, i.e. all language that the corpus or corpora represent. Inferential statistics produces p-values or confidence intervals and we use words such as 'statistically significant' or 'nonoverlapping 95% confidence intervals' to describe the inferences. Currently, there is a debate in a number of disciplines such as psychology, sociology and applied linguistics about the place that inferential statistics, especially p-values, should have in the research process.

EFFECT SIZE

How large is the effect in the sample? (standardized measure)

effect size e.g. Cohen's d, r

LINGUISTIC INTERPRETATION

Is the effect linguistically/socially meaningful?

significant. Practical importance uses standardized statistical measures to express the size of the effect; here we are trying to evaluate the magnitude of the effect (e.g. how large the difference really is between two groups). Finally, we need to relate the observed effect back to what we know about language and society and interpret the results in the context of linguistic and social theory. This crucial step seeks to discover linguistic and social meaningfulness of what we observed in corpora.

When analysing corpora, we also need to think about the type and format of the data that needs to be obtained from the corpus in order to answer the research question. This is a so-called research design; research design is important because it has considerable implications for the specific statistical procedures that we can use with the data. In general, three main types of research design can be distinguished: (1) whole corpus design, (2) individual text/speaker design and (3) linguistic feature design. Figure

In the whole corpus design, the unit of analysis is usually the whole corpus, sometimes also large subcorpora. In Figure

In sum, selecting the right corpus, analytical procedure and corpus design is the first step in successful corpus analysis. As researchers, we need to think carefully about what the corpora we are working with represent and how they can reveal interesting findings about language and society.

1.5

Exploring Data and Data Visualization Think about . . .

Before reading this section, think about the following questions:

1. Why is looking critically at data before analysis important?

2. What types of errors can we encounter in a dataset?

3. What types of graphs do you know?

Bad data leads to bad results. No matter how sophisticated our statistical analysis is, if we come up with wrong numbers or make a mistake when copy-pasting the data from a spreadsheet into the statistical software, we will end up with results that are simply wrong. One way of preventing these accidental errors is to keep a research journal which records every step of the analysis so that the procedure can be easily checked or repeated. Another, and probably even more important, aspect of good data analysis is constant questioning of the 'sanity' of the data: Is this the expected size of the corpus or have I counted also part-of-speech tags by mistake? Does this effect make sense given what I know about the distributions of words in texts? What is the reason for this unusual data point? etc. Constantly questioning the data can help avoid making trivial errors and misinterpreting the results.

In addition, in order to understand the main trends in the data, data visualization is crucial. Effective data visualization summarizes patterns in data without hiding important features. The following example illustrates how effective visualization works. Figure

However, Figure

In sum, a boxplot provides much more information about the data than a simple barchart. While the barchart in Figure

If we want to go beyond the sample and generalize about the population we can calculate 95% confidence intervals for the mean value of variable x in the three corpora. The 95% confidence intervals are displayed as error bars in Figure

The type of visualization we use should provide a means for us to better understand the general patterns in the corpus data. This largely depends on the research question and the type of study (research design) we are dealing with. Figures 1.17 provides an overview of different types of more specific graphs used in this book with references to the chapters where these are discussed in detail.

All graphs presented in this section can be easily produced using the Graph tool from Lancaster Stats Tools online. A friend of mine who is an excellent novel writer once told me that she thinks academic writing is dry because academics use very few adjectives. I thought about this for a moment and then, instead of producing a witty reply (which I couldn't think of anyway), I said: 'that's an empirical question'. Next, I offered to check her hypothesis in the BNC, a corpus that samples both academic writing and fiction. The following is a short report I prepared for my friend. Because my friend has very little knowledge about corpora and statistics I used the margins of the report for explanatory notes. Both the report and the notes are reproduced below.

This study is based on the BNC. In particular, two subcorpora of the BNC were extracted to investigate the difference in the use of adjectives in fiction and academic writing. Table

BNC represents the use of British English in the early 1990s. So the corpus is already fairly old. However, the use of adjectives is a stable enough linguistic variable, so there is no need to worry about the representativeness too much. If you are worried about this, though, this study can be replicated with some more recent data.

In this study, each file representing a text written by a single author was considered as a separate observation. The following hypothesis was tested:

This just says that I looked at individual differences between writers, not only at group averages.

• Hypothesis (H 1 ): Academics use fewer adjectives than fiction writers.

This is your hypothesis, remember?

The data was first explored using a boxplot. Then, 95% confidence intervals were calculated for the two subcorpora and r was used as a standard effect size measure. Finally, the independent samples t-test was used to test the null hypothesis that there is no difference between the two groups of writers:

These are different statistical techniques. You don't need to understand the details.

Null hypothesis (H 0 ): There is no difference between academics' and fiction writers' use of adjectives.

Null hypothesis is part of the formal statistical procedure. It is a negation of your hypothesis. The results suggest that there is indeed a difference between academics and fiction writers in terms of their use of adjectives. However, as the boxplots in Figure

Here we go;) Larger homogeneity is signalled by a smaller box and shorter 'whiskers'.

Moving beyond the sample, 95% confidence intervals can be calculated. Figure

The difference is not only statistically significant, it also appears to be linguistically meaningful.

With a big smile on my face I presented the report to my friend. 'Hm, interesting . . . ' she said. 'But I still think that the adjectives fiction writers use are somehow richer.' 'That is a different research question!' I exclaimed in exasperation. Only much later did I add 'Maybe you are right, though . . . Why don't we find out?'

Exercises

1. As a warm-up exercise (with a twist), divide the following shape that represents three quarters of a square into four identical shapes. Feel free to skip this exercise if you want to focus on statistical techniques immediately. After you have done this, take a whole square, but this time divide it into five identical shapes.

2. Calculate the mean for the following numbers:

3. What is a model in scientific thinking? What is a 95% confidence interval? (a) An interval that shows that we can be 95% confident in the correctness of the result within this interval. (b) The measure of objectivity of our findings. (c) An interval constructed around a particular measure in a sample in such a way that the true value of the measure in the population will fall within this interval for 95% of samples.

viii. What is a p-value? (a) The probability that the null hypothesis is true. (b) The probability of seeing values at least as extreme as observed if the null hypothesis were true. (c) The probability of seeing a unicorn in Lancaster.

7. Imagine you have 500 texts (the population of interest), 250 written by a male and 250 written by a female author. However, you don't know which one is which. For the purposes of your study you want to select an unbiased sample of 40 texts that would represent the population. Use the Random number generator from the Lancaster Stats Tools online to create a list of 40 random numbers between 1 and 500 and note these down.

8. In the Answer section at the companion website, you can find which texts were written by a male speaker and which by a female speaker. Check the answers in Exercise 7 and calculate the number of male and female speakers that were selected.

Male speakers in the sample: Female speakers in the sample:

Did you get an approximately equal representation of male and female speakers in the sample?

9. Later, you also find out that half of the 500 texts were written by a young speaker and another half by an older speaker (see the Answer section at the companion website to find out which texts these are

15. Use the Graph tool from the Lancaster Stats Tools online and the data provided there to create graphs visualizing the main patterns in those datasets.

TH I NGS TO R EM EM B ER

• Corpus linguistics is a scientific method.

• Successful application of statistical techniques in corpus linguistics depends on the use of a well-constructed unbiased corpus.

• Statistics uses mathematical expressions to help us make sense of quantitative data.

• Effective visualization summarizes patterns in data without hiding important features.

• Although most visible, p-values form only a (small) part of statistics.

• 'Statistical significance', 'practical importance' and 'linguistic meaningfulness' are three separate dimensions which shouldn't be confused.

Advanced Reading

Biber

Lüdeling & M. Kytö (eds.), Corpus linguistics: an international handbook, vol. 2, pp.1287-1304. Berlin: Walter de Gruyter.

A type is a unique word form in the corpus. When we ask about word types we are asking about how many different word forms there are in the text/corpus. In the two sentences from the 'Think about' task, three formsthe (7, 20), that (10, 25) and time (11, 26)occur twice each (note that times is not counted 1 Some tools (e.g. CQPweb, Sketch Engine) include punctuation in token counts. Others (e.g.

#LancsBox) stay closer to the simple (surface) definition of the 'token' as presented in this book and do not count punctuation. In addition to punctuation, other sources of variation in token counting include: treatment of clitics (e.g. 'll in he'll) and hyphenated words (well-known), tokenization decisions based on morphological analysis (part-of-speech tagging) as well as tokenization decisions for non-segmented languages (e.g. Chinese).

together with time under this definition); each of these is counted as only one type. We are therefore left with 23 word types.

Both word tokens and word types are identified based on the form of a word (external appearance, if you like). To identify lemmas and lexemes, we first need to perform linguistic analysis of the text; lemmas are based on grammatical (morphological) analysis, while lexemes are based on both grammatical and semantic analysis. A lemma is a group of all inflectional forms related to one stem that belong to the same word class

Let us now think about how the different notions of a 'word' (type, lemma and lexeme) influence the kind of analysis we can carry out in corpus linguistics. Using types is the most straightforward approach, based on distinguishing different word forms regardless of their grammatical function or meaning. However, while type is a very useful category, it may obscure some meaningful differences, e.g. uses of the form clean as an adjective (a clean shirt) versus as a verb (to clean something). On the other hand, if we want to use lemma as the unit of analysis, we need to automatically process the corpus to assign each form its part-of-speech and lump together all inflectional forms related to the same basethis involves a certain percentage of errors. Similarly, identification of lexemes, although desirable, involves semantic tagging and semantic disambiguation, which again introduces a certain percentage of errors (even higher than part-of-speech, or POS, tagging) and, moreover, cannot be done fully automatically. Table

Finally, since word counts (mostly token and type counts) are a part of almost every statistical equation that is discussed in this book, it is important that you have a good grasp of these definitions. The exercises at the end of this chapter (see Section 2.8) together with answers, which are provided at the companion website, will help you test your understanding of these crucial concepts.

Reporting Statistics: Tokens, Types, Lemmas and Lexemes

What to Report

When talking about words in corpus linguistics, we need to specify if we mean tokens (running words), types, lemmas or lexemes. When describing corpora, we should always include the information about the exact token count. Because token counts differ from tool to tool we should also provide details about the tool and/or how the token count was arrived at.

How to Report: Examples

• The text consisted of 120 running words and included 69 different types.

• In our study, we used the 100-million-word British National Corpus (exact token count: 98,313,429; BNCweb; punctuation excluded from token count) . What are the most frequent words in English? Try to come up with a list of the top ten most frequent words:

1) , 2) , 3) , 4) , 5) ,

6)

, 7) , 8) , 9) , 10) .

Look at the wordlist in Table

relative frequency ¼ absolute frequency number of tokens in corpus Â basis for normalization ð2:1Þ

For example, the relative frequency of the definite article the, as shown in In this case, we have chosen one million as the basis for normalization, which is a common baseline in corpus linguistics. This means that, on average, there are over 61,000 instances of the definite article for every million tokens in the corpus. In fact, in all corpora of written English you can expect the definite article to be at the top of the wordlist, with an absolute frequency roughly equivalent to 6% of the overall number of tokens in the corpus. The relative frequency can therefore be considered as the mean frequency ('mean' is a statistical term for a type of average; see Section 1.3)that is, the mean of the frequencies of the word in hypothetical samples of x tokens from the corpus, where x is the basis for normalization (in this case one million). This idea about the mean frequency will be useful later, when we come to discuss other statistical measures.

In smaller corpora, smaller bases for normalization than one million are more appropriate, e.g. normalization per 10,000 or even 1,000 words. The reason for this is that relative frequency is used not only to compare frequencies of a particular type in two (or more) corpora, but it is also used to present evidence about the frequency of words and phrases in a form that is easier for a reader to grasp than the absolute frequency would be. If we choose a basis for normalization that is too large relative to the actual corpus size, this can 'blow up' our numbers artificially and thus effectively misrepresent the (limited) evidence we have. For example, imagine a very small corpus that contains 11,000 tokens, including the type homeostasis which appears only once in the corpus. If we choose one million as the basis for normalization, we will get over 90 per million words as the relative frequency of homeostasis. Although the proportion is mathematically correct, this is extremely misleading for the reader, given the behaviour of rare words. When a rare word such as homeostasis occurs just once in a small corpus, it is highly unlikely that it would occur at the mathematically equivalent 90 times in a million words. It might just as easily occur once in the hypothetical million-word corpus, or five times, or maybe not at all: the actual smaller corpus simply does not give us enough evidence to extrapolate. In this example, a more appropriate basis for normalization would therefore be 10,000 which gives us the relative frequency of approximately 0.9. It is important to stress that relative frequencies should never be used to hide the absolute frequencies but should be reported together with absolute frequencies.

It is crucial to always remember that a corpus is a sample of language (see Section 1.4). It provides evidence about the use of words and phrases; however, this evidence can be limited even in fairly large corpora. For example, in the 100-million-word BNC, over half of the types occur only once (we call these words hapax legomena

Put informally, Zipf's law tells us that when we start with the most frequent item in the wordlist (regardless of the size of the corpus), the second most frequent item will have only half of the frequency of the first item. The third most common word will have one-third of the frequency of the first item; and so on. In other words, the amount of evidence that we can get from corpora about words diminishes rapidly. This can be seen in Figure

Formally, Zipf's law can be expressed as:

absolute frequency of a word Â its rank in a wordlist ffi constant ð2:3Þ or absolute frequency ffi constant rank in a wordlist ð2:4Þ

where the constant is the frequency of the first item in the wordlist. Note that Zipf's law represents an approximation. The actual frequencies of words in a real corpus will differ to a certain extent from those predicted by this model. The practical implications of Zipf's law for corpus linguistics are as follows: we have to learn to critically evaluate the amount of evidence we have for our claims. For hapaxes and low-frequency words, this evidence is naturally limited. To answer some research questions, we therefore need very large corpora (billions rather than millions of words), in which even fairly infrequent words occur multiple times. We also need to look for further (comparable) evidence of word behaviour from multiple sources (see Section 8.3 on meta-analysis).

Reporting Statistics: Absolute and Relative Frequencies

What to Report

When reporting frequency of words, we should report both the absolute (raw) and the relative frequency. The relative frequency needs to be normalized to the appropriate basis that is similar in size to the corpus or its parts (subcorpora or texts) that we are interested in.

How to Report: Examples

• The preposition of is the second most frequent item in the BNC (AF = 3,042,376, RF = 30,945.68 per million). • The word corpus occurred 20 times in the text (13.3 per 1,000 words).

2.4

The Whelk Problem: Dispersion Think about . . .

Before reading this section, think about the following questions:

1. Do you know what a 'whelk' is?

2. How often do you think we use this word?

So far, we have been looking at frequencies of words. However, to fully describe a word or phrase in a corpus, we need to introduce another concept, namely dispersion. This concept is best illustrated with the so-called 'whelk problem'. This term was introduced by

As an example, imagine a one-million-word corpus, which is divided into six parts of unequal size (each representing a different genre/register) as described in Table

Range 2 (R) is a very basic and fairly crude measure of dispersion.

As we can see immediately (look at the row 'Includes w?'), the range for the word w in Table

The range 2 is sometimes also calculated as a percentage out of the total number of corpus parts: We can say that the range 2 of word w in the one-million-word example corpus from Table

However, range 2 is not a very good measure for quantifying the amount of dispersion across individual corpus parts because it is based on a simplistic YES/NO decision about the presence or absence of a word or phrase in each part, disregarding the actual frequencies in the different parts. Range 2 also does not take into account the size of the parts. To illustrate this, imagine a different word (w 1 ) which has the same absolute frequency in the example corpus as word w (i.e. 50), but a very different distribution, namely 46, 1, 1, 0, 1, and 1 in the six corpus parts. When we calculate the range 2 of w 1 , we'll get the same number as for word w (i.e. 5 out of 6 or 83.3%), although the large majority of all occurrences of w 1 are in only one part (Part 1) whereas word w is, in comparison, more evenly spread out across the corpus. This lack of sensitivity of range 2 as a dispersion measure is its major limitation. Because of this, range 2 can be used for a first (simple) exploration of the corpus data; but for further analyses more sensitive dispersion measures are preferable.

Standard deviation is a classic measure of dispersion, which is used very often also outside corpus linguistics. It expresses how much the individual values in a dataset (here, the relative frequencies of w in the individual parts of the example corpus from Table

In Figure

The superscript reminds us that we are looking at a simple form of standard deviation calculated assuming that we are dealing with the whole population. This form of standard deviation (SD p or σ [sigma]) differs slightly from the sample standard deviation (see below). For the data in Table

A brief explanation of the procedure: squaring the distances (

There is another (slightly modified) way of calculating the standard deviation which we will be using when we consider inferential statistics, rather than the descriptive statistics we are discussing here. This is so-called sample standard deviation (SD). It is calculated in almost the same way as the standard deviation described in equation (2.9) above. The difference is that the sum of squared distances is divided not by the total number of corpus parts but by the total number of corpus parts minus 1 (the reason for this is connected with the notion of 'degrees of freedom' explained in Section 6.3). standard deviation sample ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi sum of squared distances from the mean total no: of corpus parts À 1 s ð2:11Þ

For the purposes of describing dispersion in a corpus the basic version of standard deviation from equation (2.9) should be used. Standard deviation is a useful measure when we want to see how homogeneous or heterogeneous the distribution of a word is. Standard deviation always needs to be considered in relation to the mean (the relative frequency of the word in the corpus overall). In our example from Figure

Because the SD needs to be considered in relation to the mean, we cannot use this measure to compare the dispersions of different words (or phrases) that occur with different frequencies. In these cases, other measures of dispersion such as the coefficient of variation, Juilland's D or DP (see below) are more appropriate.

The coefficient of variation (CV) describes the amount of variation relative to the mean relative frequency of a word or phrase in the corpus. The more variation in the frequencies of a word/phrase in the individual parts there is, the more uneven the dispersion. The equation for CV is very simple:

Coefficient of variation ¼ standard deviation mean ð2:12Þ

For word w in the example corpus in Table

CV ðwÞ ¼ 4:55 5 ¼ 0:91 ð2:13Þ

The coefficient of variation is a standardized measure; this means that it can be compared across different words and phrases in one corpus. The closer the coefficient is to zero, the more even the distribution of the word or phrase is. The maximum value of the coefficient of variation depends on the number of parts in the corpus, and is equal to the square root of the number of parts minus 1 ð ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi no: of corpus parts À 1 p Þ. Sometimes, the coefficient of variation is multiplied by 100 and presented as a percentage of variation. This is, however, problematic, because the coefficient of variation can be greater than 1 (when the SD is greater than the mean) which would give a percentage higher than 100. However, a true conversion to percentage can be achieved by considering the maximum possible variation in a given corpus. We know that the maximum value of CV depends on the number of corpus parts and can be calculated as ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi no: of corpus parts À 1 p . The following equation thus converts a CV to a percentage out of the maximum possible observed variation. When we apply this to word w in the example corpus from Table

This means that the dispersion of w is less than 50% of the maximum possible variation. The maximum level of variation would be reached if the word occurred only in one part of the corpus.

Juilland's D is a measure of dispersion that builds on the coefficient of variation. It is a number between 0 and 1, with 0 signifying extremely uneven distribution and 1 perfectly even distribution. Juilland's D was originally developed for use in frequency dictionaries

In essence, Juilland's D is an inverse number to CV (and CV % ). While CV tells us about the amount of variation in the corpus (larger CV means more variation in the frequencies), Juilland's D tells us about homogeneity of the distribution (larger Juilland's D means a more even distribution and less variation). The following formula is used to calculate Juilland's D:

Coefficient of variation ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi no: of corpus parts À 1 p ð2:16Þ

For the example from Table

This value (0.59) shows an uneven distribution, which, however, is closer to 1 (perfectly even distribution) than to 0. Juilland's D has been criticized in the literature

The expected proportions are calculated by taking one-by-one the sizes of the corpus parts (number of tokens) and dividing them by the total number of tokens in the corpus; this is to establish their proportional contribution to the overall size of the corpus. The assumption is that if a word or phrase is evenly distributed in the corpus it should follow the proportional distribution calculated in this step, hence the expected (or baseline) distribution. The observed proportions are calculated by taking, again one-by-one, the absolute frequencies of the word or phrase of interest in the corpus parts and dividing these by the absolute frequency of the word or phrase in the whole corpus. This is done to establish how much proportionally each part of the corpus contributes to the overall frequency of the word or phrase. By comparing the observed and the expected proportions (taking absolute values of the difference) and putting the differences together we get the DP measure. For example, DP for the values in Table

This value (0.38) indicates an uneven distribution, which, however, is closer to 0 (perfectly even distribution) than to 1. In this case, DP provides a similar picture to Juilland's D.

Reporting Statistics: Dispersion Measures

What to Report

When reporting frequencies in corpora we should also include the information about dispersion. The decision about which dispersion measure to report should be motivated by the aims of the researchwhat aspect of dispersion is necessary for us to be able to interpret the frequency correctly. The main options (there are others) include range (R), percentage range (R%), population standard deviation (SD p ), sample standard deviation (SD), coefficient of variation (CV), percentage coefficient of variation (CV%), Juilland's D and DP.

How to Report: Examples

• The word corpus occurs 773 times in the BNC (6.9 per million) but only in 201 texts (R% = 5%). • The definite article the is the most frequent word (type) in the BE06 corpus occurring in the texts with the mean relative frequency of 51.64 per 1,000 (SD = 14.17). • Swear words are unequally distributed in the BNC64 corpus. For instance, fuck occurs 123 times in only 14 out of 64 speaker samples with DP = 0.85.

2.5

Which Words Are Important? Average Reduced Frequency Think about . . .

Before reading this section, think about the following question:

How would you find out which words in English are important to know? Arguably, words that occur frequently across a large number of contexts are important because they are very likely to be encountered in a variety of communicative situations. For example, for learners of a language it is crucial to know which lexical items they should learn first. These are often not concrete words (as we might wrongly assume) but general abstract terms.

For instance, the most widely used noun in English is time, which can be found frequently across different contexts in both speech and writing: we speak (and write) about time all the time. Recently,

Average reduced frequency (ARF) is a measure that combines frequency and dispersion

• Absolute frequency of the word • Corpus size (total number of tokens) • Positions of the word in the corpus The absolute frequency and the total number of tokens were discussed in previous sections (see

where v ¼ total corpus tokens absolute frequency of word ð2:20Þ and minðdistance n ; v Þ signifies the smaller of two values: (1) value of the distance between two occurrences of the word or (2) value of v. This is done for all of the distances between two occurrences of the word in the corpus.

Although the equation looks complicated, the idea behind ARF is fairly simple: we notionally divide (this is not done physically but as part of the calculation) the corpus into x parts of the same size (v). The number x is the absolute frequency of the word we are interested in. This means that the number of the notional parts, and their length (v in equation (2.20)), depend on the frequency of the word in the corpus. Then we simply count the number of notional parts that include the word that we are interested in. We call this count the 'reduced frequency'. The purpose of this exercise is to disregard occurrences of a word which are close to each other (i.e. fall within the same part) and count them only once. Then, in addition, to make this procedure robust, we repeat the process for every possible beginning-point in the corpus (think about the corpus as a circle rather than a line) and calculate the mean of all the reduced frequency values that we get via the procedure described above. This mean value is the value of the average reduced frequency.

Let's demonstrate this with the following example: imagine a one-million-word corpus in which we search for two words w 1 and w 2 . Both words occur five times in the corpus, i.e. with absolute frequency of 5. The first word (w 1 ) occurs only in one text, at corpus positions 1 -5 -10 -15 and 20, while the second word (w 2 ) is evenly distributed at positions 1 -200,000 -400,000 -600,000 and 800,000. Note that 'p' in Figure

The ARF for w 1 is calculated as follows: First we establish the length of the notional parts, v v ¼ 1; 000; 000 5 ¼ 200; 000

Then we calculate the distances between the individual occurrences of the word (w 1 ) in the corpusto do this we need to use the corpus positions. Note especially how distance 1 is calculated: it is the distance between the last and the first occurrence of w 1 in the corpus.

• distance 1 = 1st occurrence + (total corpus tokens -last occurrence) = 1 +

(1,000,000 -20) = 999,981 • distance 2 = 2nd occurrence -1st occurrence = 5 -1 = 4

• distance 3 = 3rd occurrence -2nd occurrence = 10 -5 = 5 • distance 4 = 4th occurrence -3rd occurrence = 15 -10 = 5

• distance 5 = last occurrence -4th occurrence = 20 -15 = 5

Finally, all the terms are inserted into the ARF equation:

As we can see from the calculations above, the ARF for word w 1 is approximately 1. This should be interpreted as follows: because the five occurrences of w 1 are all very close to each other, they should be counted as if they were only one occurrence. On the other hand, if we calculate the ARF for w 2 , we'll get a number close to 5. This means that because w 2 is evenly distributed throughout the corpus, its frequency should not be reducedwe should continue to consider each instance as a separate occurrence. Here's the mathematics: If you find it difficult to get your head around the details of this process, don't worry: the ARF is designed to be calculated automatically by a computer (see the ARF calculator in Lancaster Stats Tools online). The purpose of explaining the ARF in this section is to help you understand the general principles of this powerful measure.

Reporting Statistics: ARF

What to Report

ARF can be reported in addition to absolute and relative frequencies of a word. It can be used to rank-order words in a frequency list to highlight the most frequent and evenly dispersed items.

How to Report: An Example

• The following table shows the top four lemmas in the BNC ranked according to ARF; the absolute frequency figure is also provided. When looking at texts and corpora we can think about how different words (types) are used to communicate meanings. Some words (especially grammatical words) are often repeated, others are used only a few times. To measure whether overall a text or corpus uses a wide range of vocabulary or only a limited range of lexical items which get recycled, we can calculate a lexical diversity statistic

The simplest lexical diversity statistic is the type/token ratio (see Section 2.2 for the definition of types and tokens). Type/token ratio (TTR) expresses the proportion of types (different word forms) relative to the proportion of tokens (running words). The idea is that a larger number of different word forms (types) relative to the number of all words in text (tokens) points to a lexically more varied text. Type/token ratio is calculated as follows: type=token ratio ¼ no: of types in text or corpus no: of tokens in text or corpus ð2:23Þ

For the two texts from the 'Think about' task the type/token ratio is 0.8 (28/35) and 0.93 (28/30) respectively. This shows that text B (academic text) is more lexically diverse than text A (informal speech). This is a valid comparison because the texts are of comparable size (have a similar number of tokens). However, we have to remember that the type/token ratio is very sensitive to the length of the text; it decreases as the text becomes longer and more words get used again (recycled). The simple type/token ratio from equation

Standardized type/token ratio (STTR) is a label used by

Reporting Statistics: TTR, STTR and MATTR

What to Report

When reporting different versions of the type/token ratio it is important to also report the parameters influencing the outcome. For TTR the text length needs to be reported; for STTR and MATTR the standard segment size and the window size respectively need to be reported.

How to Report: Examples

• Simple type/token ratio (TTR) was used to compare the texts because they were of the same length (2,000 tokens). The TTR values are as follows: 0.36 (text 1), 0.33 (text 2) and 0.39 (text 3). • MATTR (window size: 100) of Dickens's Christmas Carol is 0.67.

2.7

Application and Further Examples: Do the British Talk about Weather All the Time?

In this section, we'll go through an example of a research project applying the statistical procedures that were introduced in this chapter. Imagine that our task is to investigate typical topics (content words) that are frequently discussed in British English. This can give us an insight into the general public discourse practices of British society. In particular, we want to find out if weather is one of the prominent topics typical of British culture (as the stereotype would suggest). The corpus we want to use is BE06, a corpus of current written British English,

When we transform the Zipf's law equation

The next step is to decide what we actually want to analyse. We have seen that there are different definitions of a 'word' (see Section 2.2) and therefore we need to make a decision about the definition that we will be using. Our options are type, lemma or lexeme. After considering the pros and cons, let's assume we have decided to work with lemmas in our research. Lemmas are usually a good choice if we are interested in culture or discourse, since the distinctions that get submerged are the inflectional differences which are generally not relevant for this kind of research. At this stage, we can start analysing the data. The frequency list based on lemmas confirms our initial assumption (based on Zipf's law) about the number of words with the frequency of 30 and over: there are 3,196 such lemmas in the corpus. In fact, the actual number of lemmas is higher than expected, which shows that Zipf's law provides only a rough estimate of word frequency distributions. Looking now more closely at the list of lemmas, we note that among the 3,196 lemmas meeting our inclusion criterion there are 13 lemmas related to weather (this analysis had to be done manually by going through the lemma list): cloud, cold, flood, heat, hot, ice, rain, storm, sun, temperature, warm, weather and wind. As can be seen from Table

As we can see, most of the items are fairly evenly distributed in the corpus with Range values 8 to 15 and Juilland's D values 0.7 or 0.8. Nevertheless, there is one exceptionthe lemma flood. Although this lemma occurs in a majority of corpus parts (Range 10) it has a very low Juilland's D (0.2). This signifies a very uneven distribution in the 15 corpus genres. Indeed, a closer inspection of this word confirms that flood occurs mainly in one genre-based part of the corpus representing the official documents. In fact, the majority of occurrences (38 out of 54) come from a single parliamentary report on floods.

After this initial exploration, to answer the research question about the prominent topics in British public discourse we need to look at the weather terms in the context of other words (lemmas) in the corpus. For this, we need to sort the lemmas according to their prominence in written British English and consider the position of the weather terms in this ordered list. We can calculate the average reduced frequency (ARF), which combines frequency and dispersion, and order lemmas according to their ARF values. Table

Exercises

TH I NGS TO R EM EM B ER

• There are different concepts of a 'word' -token, type, lemma and lexeme.

• Zipf's law describes the distribution of words in corpora and their rapidly diminish- ing frequency.

• To fully describe a word in a corpus we need to provide both the word's frequency and its dispersion.

• Different dispersion measures (range, SD, CV, CV%, Juilland's D, DP) are appro- priate in different situations.

• The average reduced frequency (ARF) is a measure that combines both frequency and dispersion; it can be used with corpora that are not divided into different parts (subcorpora).

• TTR is a measure of lexical diversity; it is sensitive to text length.

• STTR and MATTR are alternative measures of lexical diversity that can be used with texts of varying lengths. So far, we have looked at words in isolation. In this chapter, we will explore meanings of words in context, which is an area important to both linguistic and social analyses. Topics discussed herecollocations, keywords and manual coding of concordance linesplay a key role both in the study of semantics ('dictionary' meanings of words) and in discourse analysis. The chapter starts with a simple premise: word meanings can best be investigated through the analysis of repeated linguistic patterns in corpora. Techniques such as keywords help us to draw attention to words characteristic of particular texts or corpora that can be further investigated using methods such as collocation, i.e. investigating repeated co-occurrence of words, and concordancing, i.e. analysing examples of word use in context. Five questions in particular are addressed in this chapter:

• How do we identify collocations? (Section 3.2)

• What are collocation networks? (Section 3.3)

• How do we identify keywords and lockwords? (Section 3.4)

• How can the manual coding of concordance lines be made more reliable? (Section 3.5) • How can the techniques discussed in this chapter be used in linguistic and social research? (Section 3.6)

Collocations and Association Measures

Think about . . .

Before reading this section, think about the following questions:

1. What associations come into your mind when you see the word love?

2. Why do you think the word has these associations for you?

It is a well-known fact in corpus linguistics that words occur in combinations that we call collocations. More than fifty years ago,

The following example demonstrates how the identification of collocations works in practice:

My love is like a red, red rose that's newly sprung in June: My love is like the melody that's sweetly played in tune. As fair art thou, my bonnie lass, so deep in love am I: And I will love thee still, my dear, till a' the seas gang dry. Till a' the seas gang dry, my dear, and the rocks melt wi' the sun : And I will love thee still, my dear, while the sands o' life shall run. And fare thee weel, my only love, and fare thee weel a while! And I will come again, my love, thou'it were ten thousand mile.

(Robert Burns, 'A Red, Red Rose')

The example above is taken from an English version of Burns's famous poem (originally written in Scots) 'A Red, Red Rose'. For the purposes of illustration, all occurrences of the word love with their immediate contextone word to the left and one word to the rightare highlighted and the poem is displayed without line breaks to create one paragraph of run-on text.

Let's assume that we are interested in the use of the word love in the poem. We will call the word love, our word of interest, a 'node'. A node is a word that we want to search for and analyse. The words around the node are candidate words for collocates. Collocates are words that co-occur with the node in a specifically defined span around the node, which we call the collocation window. Metaphorically, we can imagine this as a 'magnetic field' around the node that attracts particular collocates like a magnet attracts metal objects. In this case, the span (collocation window) is one word to the left and one word to the right, sometimes abbreviated to 1L, 1R. In the example given above, within a 1L, 1R collocation window around each occurrence of the node, we can observe the words that co-occur with love, namely, in the order of appearance, my (three times), is (twice), in (once), am (once), will (twice), thee (twice), only (once), and (once) and thou (once). Note that in each case, the frequency of co-occurrence was provided in the brackets; we call this value the observed frequency of collocation. Let us consider the word which occurs most frequently with love in our example, my. At this stage, we need to ask: is my really a genuine collocate of love in the poem? In other words, is my really strongly associated with love? To find out, we need to find a way to evaluate the observed frequency. We have three basic options:

1. No baseline: we compare the observed frequencies of all individual words co-occurring with the node and produce a rank-ordered list. 2. Random co-occurrence baseline ('shake the box' model): we compare the observed frequencies with frequencies expected by chance alone and evaluate the strength of collocation using a mathematical equation which puts emphasis on a particular aspect of the collocational relationship. 3. Word competition baseline: we use a different type of baseline from random co-occurrence; this baseline is incorporated in the equation, which again highlights a particular aspect of the collocational relationship.

The first (simplest) option does not involve any statistical calculation. We merely produce a rank-ordered list of words co-occurring with the node based on their frequency, such as, in our example, my (3), is (2), thee (2), will (2) . . . The words towards the top of the list are the strongest collocates by the frequency count. The disadvantage of this approach is that the top collocates will typically be function words co-occurring with the node merely by the virtue of their frequency anywhere in the corpus. Frequency-based collocates are therefore fairly generic (we can expect a similar set of collocates for almost any node) and have only a limited usefulness.

The second option involves a comparison with a random co-occurrence baseline. We ask whether it is possible that the combination of words in question (e.g. my and love) occurs repeatedly only due to chance. Consider this:

1. The poem has 107 tokens (see Section 2.2 for a definition of 'token'). 2. Love occurs 7 times in the whole poem. 3. My occurs 8 times in the whole poem. 4. My occurs 3 times in combination with love and 4 times in combination with other words.

Imagine also that there were no associations between words in the poem and words appeared randomly in the text. This situation is illustrated in the example below, which shows the words of Burns's poem in random order: fare art And like red, sweetly in love love, And gang wi' played like dear, life shall rocks sprung the Till deep my my And still, weel, again, ten the the while! is till And As I: a' only come were sands sun: dry, and gang it a' the still, My thee will in my bonnie My red is a run. my love thee thou, melt the seas and thou' I the I lass, I melody thee a my am rose love dear, that's love newly love fare love, will o' so dry. fair thee will that's in while June: my seas tune. mile. thousand weel dear, How many times would you expect the words my and love to co-occur by chance alone? In the random example above, my love occurs once; in fact, if we run the random simulation multiple times we will get an average (mean) number close to one. We call this process establishing the random co-occurrence baseline and the resulting value is called the expected frequency of collocation. The expected frequency of collocation does not have to be established empirically but can be calculated as follows:

expected frequency of collocation ¼ node frequency Â collocate frequency no: of tokens in text or corpus ð3:1Þ

When considering collocation window sizes larger than one, a correction could be applied to account for the fact that there is a greater chance of words randomly co-occurring with the node. The corrected expected frequency of collocation is calculated as follows:

expected frequency of collocation ðcorrectedÞ ¼ node frequency Â collocate frequency Â window size no: of tokens in text or corpus ð3:2Þ

In our example, the expected frequency of collocation of love and my in the 1L-1R span would be calculated as follows:

expected frequency of collocation ðlove; my; correctedÞ

We can see that the observed frequency of collocation of love and my (3) is larger than the expected frequency (1.05). To compare the difference between the two values, different association measures can be applied (see

Finally, to avoid the potentially problematic 'shake the box' model of language, some association measures operate with a different type of baseline; these measures do not include E 11 in the equation. The baseline needs to be understood on the caseby-case basis derived from the specific formula of the association measure.

Generally, to be able to understand the equations, we need to consider the terms that enter these equations. These are best displayed in the form of contingency tables (showing all possible combinationscontingenciesof word co-occurrence). Table

The expected frequencies table is derived entirely from the observed frequencies table, using the equations shown in Table

Finally, one question remains to be answered: which is the 'best' association measure? Frustratingly for some, the answer is: it depends on which aspects of the collocational relationship we want to highlight. Some collocation measures such as MI highlight rare exclusivity of the collocational relationship, favouring collocates which occur almost exclusively in the company of the node, even though this may be only once or twice in the entire corpus. Other metrics, such as Dice and log Dice, and MI2 favour collocates which occur exclusively in each other's company but do not have to be rare. Others can take into account directionality (Delta P) or dispersion

To choose a specific association measure we first need to define the type of collocations we are interested in (based on our research question). We can think of most association measures as highlighting collocations along two main dimensions: frequency and exclusivity. Frequency refers to the number of instances in which a node and collocate occur together in a corpus. Exclusivity refers to a specific aspect of the collocation relationship where words occur only or predominantly in each other's company. According to how the association

Node absent measures rank the individual collocates, association measures can be placed on a two-dimensional scale indicating the extent to which these association measures highlight the frequency and/or exclusivity of the collocational relationship (see Figure

Reporting Statistics

What to Report

For the sake of replicability of results, all major parameters that can affect collocate identification should be reported. For this purpose,

How to Report: An Example

• The following items were identified as top five collocates of the adjec- tive new in the BE06 corpus using the MI statistic (3b-MI(

Collocation Graphs and Networks: Exploring Cross-associations

Think about . . .

Before reading this section, think about the following questions:

1. What words come to mind when you see the word university? Write down at least five associations.

2. Review the list and underline those words which you think are more closely associated with university than the rest of the words in the list.

Collocation graphs and networks build on the idea of collocation introduced in Section 3.2. A collocation graph is a visual representation of the collocational relationship between a node and its collocates. Instead of a list of collocates displayed in a table (the usual format), the graph shows the relationship between the node and the collocates by displaying collocates closer to or further apart from the node. 2) is expressed as the length of the link between the node and the collocate: the closer the collocate is to the node, the stronger the relationship (think of a magnet). The frequency is displayed as the shade of the colour of the collocate: the more intense the colour the more frequent the collocate is. Finally, the position of the collocate in text (whether it occurs predominantly before or after the node) is shown by the position of the collocate in the graph (left, middle or right). For instance, fall, falling and fell occur always before the node love, while you occurs both before and after love with approximately the same frequency.

Collocation networks are extended collocation graphs, which show larger association patterns than those seen from the immediate collocates discussed so far

It is best to illustrate this idea with an example of a relatively simple collocation network based on the one-million-word LOB corpus (see Figure

Another example that demonstrates the concept of collocation networks is the word university and its cross-associations that can be seen from Figure

Collocation graphs and networks are useful summaries of complex meanings of words in texts and corpora. These networks can provide useful information about key topics in texts and discourses as well as their connection. For effective building of collocation networks specialized software is necessary that is able to run multiple comparisons of word co-occurrence and display the data in a visual form. Such a tool, #LancsBox

Keywords and Lockwords

Think about . . .

Before reading this section, think about the following question: Which of the lists in Table

Identifying keywords is one of the crucial techniques in corpus linguistics

So, let us unpack the keyword procedure: a corpus of interest (C), sometimes referred to as a 'focus corpus'

1.

How to Choose a Reference Corpus?

Typically, a reference corpus is larger than or similar in size to the corpus of interest so as to provide a large enough amount of evidence about word frequencies (see question 2 below). Generally speaking, the larger and the more similar the reference corpus is to the corpus of interest the more reliable and focused the comparison is. This is because every two corpora are different in a number of aspects which get highlighted to different degrees in the keyword procedure. To illustrate the point of the relative heterogeneity of any two corpora, let's compare two excerpts taken from a corpus of American (AmE06) and a corpus of British (BE06) English respectively, each excerpt consisting of exactly 100 words.

Text A Text B

Democrats call those shifts too little, too late. "Changing direction in Iraq starts with changing the people we send to Washington," Shays' challenger, Diane Farrell, said Saturday in the Democratic response to Bush's radio address.

Something behind him went 'gloink'. It was a small, subtle and yet curiously intrusive sound, and it accompanied the appearance, on a shelf above Rincewind's desk, of a beer bottle where no beer bottle had hitherto been.

He took it down and stared at it. It had recently contained a pint of Winkle's Old Peculiar. There was absolutely nothing ethereal about it, except that it was blue. The label was the wrong colour and full of spelling mistakes but it was mostly there, right down to the warning in tiny, tiny print: May Contain Nuts. Now it contained a note.

Democrats, who since the Vietnam War have battled voter perceptions that they are soft on defense, are finding a more receptive audience for the argument that they could do a better job of protecting America and conducting its foreign policy. In the poll, 52% say the Iraq war has made the USA less safe from terrorism. Nonetheless, Republicans continue to view the issue of terrorism as . . . We can start enumerating the differences between the texts by pointing out that text A is about American politics, while text B is a part of a story about a person called Rincewind. Text A comes from a newspaper, text B from a novel. Text A uses American spelling (defense), whereas text B sticks with the British spelling conventions (colour). We may also begin to observe individual lexical differences reflecting the topics of the two texts. Some readers might also notice the unusual onomatopoeic word gloink in text B and words related to the war in Iraq (Bush, Iraq, terrorism, war etc.) in text A. These are only a few differences that illustrate the aspects in which two texts can be different from each otherthe same is true, on a large scale, about two corpora.

The crucial question to ask in this context therefore is: what kind of language do the corpus of interest (C) and the reference corpus (R) represent and how is the composition of each corpus reflected in the comparisonthe keyword procedure? (See Section 1.4 for the discussion of corpus representativeness.) In practice, every keyword (or lockword) identified should be related back to what we know about the composition of the two corpora (C and R) and the multiple sources of difference between them. We should also consider which words would get highlighted as keywords had we chosen a different reference corpus. In fact, we can select multiple reference corpora and carry out the keyword procedure with each of them and compare the results.

2.

How to Deal with Absent Words?

When comparing two corpora, it often happens that a word that occurs frequently in one corpus is absent from the other corpus. The question is how to deal with these words in the keyword procedure. We know that unless a corpus represents the population (all language use), absence of evidence is not evidence of absence (see Section 1.

• Is X a positive keyword or is the reference corpus not large enough?

• Is Y a negative keyword or is the corpus of interest not large enough?

In practice, some corpus tools let us set the minimum cut-off limits for the frequencies of words in C and R before considering them in the keyword procedure. This, however, needs to be done very thoughtfully.

3.

What Statistical Measure to Use for Comparing Corpora?

To answer this question, let us take as an example the two corpora from which excerpts A and B (see above) were taken, AmE06 and BE06, each consisting of approximately one million words. Let us consider five lexical items that stand out as different in texts A and Bwar, defense, pint, Rincewind and gloinkas well as two more general (grammatical) words, the and he. This time, however, the comparison will be made between the whole corpora that include texts A and B respectively. Let's also assume that AmE06 is our corpus of interest (C) and BE06 is the reference corpus (R). We can see in Table

To help make these decisions, traditionally the log-likelihood (LL) statistic has been used to establish whether the differences between C and R are likely to be due For example, to calculate the log-likelihood statistic for the word war from Table

Instead of using the values of the LL statistic, which are relatively difficult to interpret, for the identification and sorting of keywords,

The constant k simultaneously serves as a filter that allows focusing on words above certain relative frequencies in the corpus. For example, if we use 1 as the constant, we highlight low-frequency unique words, while 100 would filter out words that occur with the relative frequency smaller than 100 per million words if the relative frequency per million words is used. The simple maths parameter for the word war (with k = 100) is calculated as follows.

simple maths parameter ðwarÞ ¼ 620 þ 100 267 þ 100 ¼ 1:96 ð3:7Þ

The interpretation of the value of the simple maths parameter is more straightforward than that of the LL statistic: given that the relative frequencies of the word are larger than 100 (which we specified when choosing k), we can say that war occurs approximately twice as much in C as in R.

Currently, the question of which statistic best suits the identification of keywords is an open one. Other suggestions for sorting principles are %DIFF

In sum, the term 'keywords' might be slightly misleading because it suggests that there is a single set of words that characterize a particular corpus. However, as we have seen, the keyword list is a result of multiple decisions in the process starting with the selection of the reference corpus and finishing with the choice of the particular statistic. To demonstrate this point, let's return to the question from the 'Think about' task at the beginning of the chapter. If you have chosen your favourite keyword list for American English, you might be interested in knowing which procedure was used to identify these keywords. Table

Reporting Statistics

What to Report

The outcomes of the keyword procedure are influenced by three crucial parameters: (i) the selection of the reference corpus, (ii) implementation of minimum frequency cut-off points and (iii) the choice of the statistical measure. It is also customary to report whether all identified keywords or only the top 10, 50, 100 etc. were used. The parameters listed above are usually reported in the Procedure subsection of the Method section. This, however, needs to be complemented with a careful description of the corpus of interest C in the Data subsection of the Method section. • AmE06, which represents written American English sampled in 2006, was used. AmE06 consists of 15 genre-based subdivisions which can be grouped into four broader genre divisions: newspapers, general prose, academic prose and fiction . . .

Procedure

• Words typical of American English were identified by comparing AmE06 and BE06 using the keyword procedure. BE06 was used as a reference corpus because it was created using the same sampling frame (Brown family sampling frame) and therefore represents the same genres of written English sampled around the same point in time (

Inter-rater Agreement Measures

Think about . . .

Before reading this section, think about the following question:

Which of the concordance lines in Table

Finally, let us focus on an area that has not yet received sufficient attention in corpus-based discourse studies, the inter-rater agreement. Inter-rater agreement, which is an estimate of how reliable and consistent a coding is, should be reported in studies working with a judgement variable. This is a variable that involves categorization or evaluation of cases (e.g. concordance lines) by the analyst that might bring an element of subjectivity into the study. The larger the element of subjectivity, the larger the need for double coding and reporting of inter-rater agreement. For example, if we want to group the occurrences of the word time into ten different semantic categories (think of dictionary definitions), this would involve a certain amount of subjectivity because semantics is notoriously fuzzy. Should time in a particular context be categorized as X or should it be subsumed under Y? Another example is the categorization from the 'Think about' task where you were asked to decide which concordance lines show the use of the word religion in a positive context and which in a negative context. Clearly, your understanding of what religion is as well as your evaluation of religion may have an impact on judging the examples as positive or negative (see the discussion below). On the other hand, situations such as categorizing sentences according to the grammatical categories of the verbs into active and passive constructions involves relatively little subjective judgement and doesn't require double coding. Such situations also usually lend themselves to automatic methods; in a part-of-speech-tagged corpus, active and passive constructions can be searched for and counted automatically.

So how is inter-rater agreement approached? Before calculating inter-rater agreement, we need to find a second rater, another researcher on the project or a colleague who is willing to help. The first step is to carefully explain the coding system to themusually a written coding scheme with concrete examples which both raters can refer to is a good idea. We then ask the second rater to independently code the same dataset or (especially if the dataset is large) a random sample taken from the dataset. Finally, using an appropriate statistical procedure, we can estimate how reliable our coding is or, in other words, how much subjectivity is involved in the coding. Before discussing the details of the statistical procedure, let's have a look at some data. Let's assume that we have asked two raters, a religious person and an atheist, to code the concordance lines from the 'Think about' task. The results can be seen in Table

We can see that the religious person and the atheist agree in six cases and disagree in four cases. These four cases of disagreement represent situations in which the context does not provide enough evidence about the evaluation of the example and the raters therefore rely on their own understanding of the situations. The first statistic that we can calculate easily is the raw agreement. Raw agreement is a metric, often expressed as a percentage, which provides the proportion of agreement cases in all cases. Raw agreement is calculated as follows: raw agreement ¼ cases of agreement total no: of cases ð3:8Þ

For the example of the two raters, the raw agreement is:

We can thus say that in the example above, the raw agreement is 0.6 or 60%, which is a relatively low number. Ideally, we would be aiming at agreement of 80% and above.

Raw agreement is a useful first approximation of the reliability of the rating. However, we need to consider the fact that some agreement between the raters would be achieved even if both raters coded the cases randomly. More sophisticated inter-rater agreement measures such as Cohen's Kappa (κ) or Gwet's AC 1 therefore take this agreement by chance into account and subtract it from the raw agreement. Cohen's κ has been traditionally used for nominal variables; recent studies (e.g.

In the example above, Cohen's κ is calculated as:

agreement by chance ¼ 0:7 Â 0:3 þ 0:3 Â 0:7 ¼ 0:42 ð3:11Þ

Note: 0:7 Â 0:3 is the mathematical expression of the chance of positive rating by the first rater (7 out of 10) and by the second rater (3 out of 10). 0:3 Â 0:7 is the mathematical expression of the chance of negative rating by both raters.

κ ¼ 0:6 À 0:42 1 À 0:42 ¼ 0:31 ð3:12Þ

In the example above, AC 1 is calculated as:

Note: In 10 2Â10 , 10 is the number of positive ratings by both raters (7 + 3) and 2 Â 10 is the number of ratings by both raters. 1 À 10 2Â10 À Á is the complementary probability, i.e. the probability of not being categorized as positive by chance.

Both measures, κ and AC 1 , produced low numbers: 0.31 and 0.2 respectively. These are closer to 0 (which is the baseline for random agreement) than to 1 (which shows absolute/perfect agreement).

The scale below can help interpret the results of κ

So far, we've looked at a simple situation where we have two raters who have been coding a dataset using two categories (positive or negative). However, if we have more raters or a different number of categories or the rating is done using ranks or numeric values that can be placed on a scale, we need to select different measures of rater agreement. Table

What to Report

The following information should be provided for the reader to evaluate the reliability of coding of a judgement variable: (i) number of raters, (ii) amount of data coded (the whole dataset or random sample), (iii) inter-rater agreement measure, (iv) p-value and (v) interpretation of the result. The information described above should be reported in the Method section of the research report.

How to Report: An Example

• Following the coding scheme described above, two independent raters coded a random sample of 100 concordance lines from a total of 1,053 containing the word religion in the corpus. Gwet's AC 1 measure showed agreement between the raters (AC 1 = 0.7, p < 0.001). A review of the differences between raters found no systematic pattern of disagreement.

Given the nature of the judgement variable, the amount of agreement was deemed sufficient.

Application and Further Examples: What Do Readers of British Newspapers Think about Immigration?

This section illustrates the statistical procedures introduced in this chapter in the context of a short discourse analysis study. The study focuses on the perception of 'East European immigrants' by readers of two British newspapers, the Guardian (a 'heavyweight' newspaper, politically leaning to the left) and the Daily Mail (a right-wing mass-market newspaper), using two corpora based on readers' comments below articles on immigration on the newspaper websites. It is assumed that because the two respective newspapers attract a different readership, the analysis of the comments will reveal different perspectives on immigration.

At this stage, it might be useful to provide some historical context for the study. In January 2014, Britain opened its job market to citizens from Romania and Bulgaria. In the run-up to this event, the British press frequently debated the possible impact of this decision on the British economy and quality of life in Britain. In the media, comparisons were also made with a previous event ten years

The data covers the period from 2010 to 2013. All articles in the Guardian and the Daily Mail containing the search term 'east europeans' or 'eastern europeans' were identified and the reader comments on these articles were extracted. 'East (ern) Europeans' is a collective term frequently used by the British press to refer to people from new European Union countries (e.g. Romania, Bulgaria, the Czech Republic or Poland). Overall, 942,232 tokens were extracted from the Guardian (GU corpus) and 2,149,493 from the Daily Mail (DM corpus).

First, to show an overall difference between the two corpora, top ten positive keywords were identified for both the GU and DM corpora. When extracting keywords for one of the newspapers, the comments of the readers from the other newspaper acted as a reference corpus in order to highlight words specific to the Guardian or Daily Mail readership. For the identification of keywords,

Apart from the two 'obvious' keywords Guardian and DM referencing the two newspapers, we can see an interesting pattern of differences between the two corpora. While the keywords in the GU corpus appear more neutral and related to the theoretical aspects of the immigration debate (economic, argument, debate), the keywords found in the DM corpus predominately point to negative aspects of immigration (homeless, benefits, police, squatters). The emotional intensity in the discourse of Daily Mail readers can also be seen from the frequent use of capitalization

(3) Rarely is the distinction made between asylum seekers, immigrants and illegal immigrants. Personally, I have no time for people who easily take a swipe at hard working low-paid legal migrants who often take jobs that unemployed UK citizens sometimes find unpalatable.

(GU, 29/04/2010) (4) There you go again. Is "immigrants and asylum seekers" some kind of single entity to you? (GU, 29/03/2010)

In addition, the collocates blame and blaming also stand out in the Guardian debate. Although blame and blaming are words with negative semantic prosody, here they critically reflect the tendency to use immigrants as scapegoats for different social issues which many Guardian readers perceive as unjustifiedsee the example below.

(5) Sure there are issues, but blaming immigrants for everything isn't going to address the real issues is it? (GU, 06/06/2010) When we compare Figures 3.6 and 3.7, the most distinct difference can be seen in the labels used to denote the quantity of the immigrants coming to Britain. While the Guardian readers largely use the term influx, the Daily Mail readers also use flood, wave and swamped.

To investigate if the word immigrant(s) was used predominantly in a positive or negative context in the two reader discourses, a random sample of 100 concordance lines was extracted from each corpus and manually coded by two raters on a 5-point Likert scale from very positive (1) to very negative

We can see that Guardian readers use the term immigrant(s) in more positive than negative contexts, while the Daily Mail readers use very few positive evaluations of immigrant(s); the DM corpus is thus dominated by negative and very negative evaluations (together over 50% of comments). (a) You need to identify technical terms connected with the word process in a corpus of research articles on organic chemistry, e.g. petrochemical process. Note that technical terms are exclusive and relatively rare combinations of words with a specific meaning. (b) You want to study the associations that the word enemy (node) has in the newspaper discourse. You are interested to see content words around the node rather than frequent grammatical words. (c) You want to write a dictionary of collocations for learners of English that would include a broad range of fixed expressions such as find out, take responsibility, dire consequences etc. The collocations you include need to be recognizable as specific meaningful units and they need to occur as frequent combinations.

2. Look at the information in Table

Use the online Collocation Calculator to calculate four association measures: MI, LL, Delta P and log Dice.

• Number of tokens in the whole corpus (N): 1,001,514

• Frequency of the node in the whole corpus (R 1 ): 164

• Collocation window size: 6 (3L, 3 R)

3. Discuss how the association measures from Exercise 2 rank the six collocates. Which association measure would you choose?

Collocation Networks

4. Compare the pairs of collocation networks in Figure

7. Calculate the SMP statistic for the words in Table

Inter-rater Agreement 8. The following ratings were obtained in three situations involving a judgement variable. Calculate the inter-rater agreement in each situation.

Rater A

Transcriber A: 1, 4,

9. Look at the examples in Table

They show how speakers of English as a foreign language express disagreement. Decide how polite (or impolite) these speakers are when they express disagreement. Use the following rating on a 5-point Likert scale:  After the rating, answer the following questions:

• How confident are you about the ratings you have provided?

• Would you consider politeness a robust judgement variable?

• How important do you think it is to have another rater for this judgement variable?

10. Compare your coding in Exercise 9 with the coding of the same dataset by a different rater (e.g. ask a friend to help you with this exercise). Using the Agreement calculator, calculate the appropriate agreement measure.

Measure calculated: ,Value:

• If available, keep adding more raters and calculating the inter-rater agreement.

11. Imagine you need to produce a research report based on the dataset discussed in Exercises 9 and 10. Report the results of the inter-rater agreement measure from Exercise 10. Refer back to the 'Reporting statistics' box.

TH I NGS TO R EM EM B ER

• There are many association measures each highlighting different aspects of the collocational relationship (e.g. frequency or exclusivity). There is no one best association measure.

• Collocations can be presented in a tabular (table) or visual form (graph).

• Collocation networks show complex cross-associations in texts and discourses.

• The keyword procedure is in essence a comparison which depends on a number of parameters. There is no such thing as one set of keywords.

• For judgement variables an inter-rater agreement statistic should be reported.

Gwet's AC 1 and AC 2 , Cohen's and Fleiss's as well as interclass correlation can be used depending on the situation. This chapter focuses on the statistical analysis of lexico-grammatical features in language (such as articles, passive constructions or modal expressions). We start with a discussion of two types of approaches to lexico-grammar

• How can lexico-grammatical variation best be described? What types of research design can be used? (Section 4.2) • How can lexico-grammatical variation be summarized and what simple statis- tical measures can be computed? (Section 4.3) • How can we build complex models that account for multiple variables that predict lexico-grammatical variation? (Section 4.4) • How can the statistical techniques discussed in this chapter be used in the analysis of lexico-grammar? (Section 4.5)

4.2

Analysing a Lexico-grammatical Feature Think about . . .

Before reading this section, think about the following situation:

A friend who is learning English shows you a sentence in a newspaper article entitled 'Google unveils new logo at turning point in company's history'.

When looking at lexico-grammar from a broad perspective, we can see that there is a large amount of variation related to the situations in which language is used. For example, even purely grammatical words, such as articles in English, which we might expect to be stable in language, show considerable variation in their distribution in different registers of spoken and written English (speech, fiction, newspapers, general writing and academic writing).

The stacked bar chart in Figure

From Figure

We can now return to the 'Think about' task and reflect on the question using the evidence presented in Figure

The first step is to use a different research design (see

The dataset in Figure

(1) Lands were granted to a group of men known as feoffees, who became the legal owners of the land, while the grantor enjoyed the use of the landsin other words, all the rights and profits arising from them. But because the feoffees were the legal owners, the lands could not be taken into wardship if the grantor died leaving an heir under age . . . (BNC, file: E9V) (2) In September, a month after the RSPCA conference, she was in England for the publication of her new book. (BNC, file: A7D) (3) The kit includes a fine brass pendulum and chain along with a detailed book to point you in the right direction. (BNC, file: CBC) (4) This effect, which is strongest over the frontal lobes, was first observed in 1964 by Grey Walter, who called it the Contingent Negative Variation.

(BNC, file: A0T)

Before looking at the types of analyses we can carry out using a Linguistic feature design (see

Table

A word of caution: we need to realize that not all linguistic variables lend themselves to the type of research design described above. Ambient linguistic variables such as discourse markers, hesitations or swearwords that can appear in any possible context and do not have a clearly defined lexicogrammatical frame cannot be treated in the same way as the variables from Table

(5) It's about time that was done. (BNC, file: KBB) (6) Well, you know, it you see, time were, I don't know I suppose, I don't know but I never seemed to be afraid . . . (BNC, file: HDK)

In sum, we have seen two approaches to the analysis of lexico-grammatical variation. One approach looked at general distributions of lexico-grammatical features in broadly defined subcorpora, while the other focused on individual linguistic contexts, in which lexico-grammatical features are used. The approach that would arguably be the most fruitful for answering the question in the 'Think about' task is the exploration of the linguistic contexts in which articles in English are used (following the Linguistic feature research design). The statistical techniques employed in such an exploration are the topic of the remainder of this chapter.

4.3

Cross-tabulation, Percentages and Chi-squared Test Think about . . .

Before reading this section, think about the following questions.

1. Which of the following expressions do you say most often?

• I must go.

• I have to go.

• I need to go.

Can you think of contexts in which you would use each of them?

A good starting point for any data exploration is a simple summary table. In Chapter 3 (see Sections 3.2 and 3.4), we have seen contingency tables that were constructed for collocation and keyword analyses to show all possibilities (contingencies) of word (co)occurrence. Cross-tabulation (cross-tab), which is explored in this section, is a similar technique

A small aside: the information included in a simple cross-tab table can be visualized in a mosaic plot (Figure

Note that the presentation of the data in a mosaic plot is the reverse of the cross-tab table: in the mosaic plot, the categories of the explanatory variable are listed horizontally, while the categories of the linguistic variable are displayed vertically. For more information see

Returning to the cross-tab table: often, in addition to frequency counts, crosstabulation tables include percentages for easier comparison across categories. The percentages can be computed from row totals, column totals or the grand total according to the following equation: percentages in a cross -tabulation table = cell value relevant total Â 100 ð4:1Þ

It is important to note that each of the three percentage options (percentages out of row total, column total or grand total) has a completely different interpretation and is useful for a different type of comparison. For example, the percentage out of row total of the first cell in We can say that out of all non-determined contexts a large majority (96.2%) prefer the indefinite to the definite article. In other words, in this context, there is a 96.2% (or 0.962) probability of occurrence of the indefinite article. More generally, we can interpret the percentage based on the row total as the probability (preference where over 50% or dispreference where less than 50%) of a given variant of the linguistic variable in a given context.

Alternatively, we can calculate the percentage based on column totals. Using the same example from We can say that out of all indefinite articles, 92.6% occur in non-determined contexts. Although this may sound similar to the previous option, the logic of this statement is different. Here, we don't compare the two variants of the linguistic variable but two contexts (determined and non-determined).

Finally, we can calculate the percentages from the grand total as follows:

% contextually non-determined indefinite articles in the corpus = 25 100 Â 100 ¼ 25%

ð4:4Þ

Note that because the grand total in Table

So far, we have seen only a very simple form of cross-tabulation (2 × 2 table). With a larger number of explanatory variables (and their categories) we can create more complex cross-tabulation tables. In the 'Think about' task, you were asked to think of the contexts for the use of must, have to and need to. All of these are modal expressions with a similar meaning indicating strong obligation, in other words, indicating that something is necessary or should be done. Table

The last point to discuss in this section is the appropriate test for statistical significance that can be used with cross-tabulation; a statistical significance test evaluates the amount of evidence against the null hypothesis (see Section 1.3). 1. Independence of observations. We assume that every observation such as the use of an article in English (see Table

Chi-squared is calculated according to the following equation:

Chi-squared ¼ Sum for all cells of ðobserved frequency À expected frequencyÞ 2 expected frequency ð4:5Þ

We know from Chapter 3 (Section 3.2) that expected frequencies are frequencies that we would expect to see if there was no relationship between the variables in the data, i.e. if the null hypothesis were true. Expected frequencies function as a baseline that we use to establish if there is a real relationship between variables, in this case an effect of the explanatory variable on the linguistic variable. Expected frequencies are calculated as follows:

Expected frequency ¼ row total Â column total grand total ð4:6Þ

Let's take as an example the use of articles in English discussed above. The data, so-called observed frequencies, are provided in Table

The chi-squared test value (statistic) is then calculated as follows:

Chi-squared ¼ ð25 À 7:02Þ 63 is significant at the 0.05 level and the 0.01 level respectively. In our case, the p-value associated with the test value 85.25 is very small p < 0.0000000000000001, which is usually reported as < 0.0001. Remember, statistical testing is not a competition for the smallest pvalue (see Section 1.4). In the statistical testing procedure, we evaluate the amount of evidence which we have in the data for the rejection of the null hypothesis that states that there is no relationship between the variables in the data (the use of articles and the type of context in Table

Cramer's V ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi chi-squared total observations Â ðno: of rows or columns; whichever is smaller À 1Þ s ð4:8Þ Applied to our example we get:

The resulting value of 0.923 can be interpreted as a very large effect size. Cramer's V ranges from 0 to 1 and Table

Another option, which is in many cases preferable to the general effect size such as Cramer's V, is the probability ratio (PR, also known as relative risk or risk ratio). As the term suggests, it is a ratio of two probabilities from the crosstab table comparing the probability of a particular linguistic outcome (e.g. the definite article) occurring in one context type relative to the same outcome occurring in the other context type. Because this effect size is a ratio of two values, it is suitable only for simple 2 × 2 cross-tab tables where there are only two categories (levels) of each variable. It is calculated as follows: probability ratio ¼ probability of outcome of interest in context 1 probability of outcome of interest in context 2 ð4:10Þ

The probabilities of the definite and indefinite article in two context types are calculated in Table 4.7. 11 As can be seen, these are the values in the cells divided by the row totals. Unlike in controlled experimental research such as in medical studies,

• PR of indefinite article in non-determined vs determined context ¼ 0:962 0:027 ¼ 36

• PR of indefinite article in determined vs non-determined context ¼ 0:027 0:962 ¼ 0:03

.29

• PR of definite article in non-determined vs determined context ¼ 0:038 0:973 ¼ 0:04

• PR of definite article in determined vs non-determined context ¼ 0:973 0:038

For instance, we can see that the indefinite article is 36 times more likely to occur in a non-determined context than in a determined context. Conversely, the indefinite article's probability to occur in a determined context is 0.03 times its probability to occur in a non-determined context; that is a very small probability in comparison. The scale on which the probability ratio operates is 0 to infinity and the interpretation is as follows:

• A probability ratio of 1 means there is no difference between the two contexts.

• A probability ratio smaller than 1 means the linguistic outcome of interest is less likely to occur in context 1 than in context 2. • A probability ratio larger than 1 means the linguistic outcome of interest is more likely to occur in context 1 than in context 2.

Sometimes, an alternative measure called odds ratio is reported instead of probability ratio. Odds ratio uses odds instead of probabilities. However, its interpretation appeals less to common sense than probability ratio because we tend to understand probabilities better than odds

odds ratio ¼ probability ratio Â probability of outcome of interest in context 2 probability of outcome of interest in context 1 ð4:11Þ

For more discussion of odds ratio see Section 4.4. Finally, it should be noted that in addition to the effect size measure we should also compute the confidence intervals (95% CIs) for effect size to be able to estimate the range within which the effect is likely to occur in the population (language use in general). Reporting Statistics: Cross-tabulation and Chi-squared

What to Report

In the case of simple situations with one linguistic and one explanatory variable, we report a cross-tab table with percentages as well as the chisquared test results. For the chi-squared test the following should be reported: (i) degrees of freedom (see note 9), (ii) test value, (iii) p-value, (iv) effect size (probability ratio or Cramer's V or both) and (v) 95% confidence interval for the effect size.

In more complex situations (with more explanatory variables), the cross-tab tables alone are sufficient with detailed description of the important/interesting contrasts. If we want to report inferential statistics with complex tables, logistic regression needs to be performed (see Section 4.4).

How to Report: An Example

• There was a significant association between the context type and article type (χ2 (1) = 85.25, p < .001). The overall effect is large: Cramer's V = . In this section, we will discuss a powerful statistical technique called logistic regression. So far, we have explored the nature of lexico-grammatical variables (Section 4.2) as well as some relatively simple techniques for dealing with lexicogrammatical variation (Section 4.3). In the following discussion, we'll look at the analysis of lexico-grammatical variation with logistic regression, a technique that uses explanatory (sometimes also called predictor) variables, which can be both categorical and scale, to estimate their effect on the linguistic (outcome) variable, which has to be a categorical variable.

For those interested in the details, the following is one possible form of the logistic regression equation:

Explaining this equation introduces some new terminology: e is a mathematical constant that is approximately equal to 2.71828,

Before proceeding to the explanation of the details of the logistic regression technique, let us review the basic terminology. In the 'Think about' task, you reviewed terms that are used synonymously in the context of logistic regression applied to the study of lexico-grammar; these terms can be divided into four groups:

• Features of lexico-grammar that are the focus of the research: 'linguistic variable' = 'outcome variable'. • Terms related specifically to the structure of the outcome variables: 'variant (of a linguistic variable)' = 'outcome'. • Contextual variables that help us explain the use of lexico-grammatical fea- tures: 'explanatory variable' = 'predictor variable' = 'predictor'. • Terms related to the structure of categorical variables that are used both as outcome variables and predictor variables: 15 'category of a variable' = 'level of a variable' = 'value of a categorical variable'.

There are several stages of the logistic regression procedure: (i) Data checking (prerequisites and assumptions), (ii) Building a model and (iii) Interpreting the model. To illustrate the logistic regression technique, we'll use the example of the definite article discussed previously (see e.g. Figure

STAGE 1: Data checking (prerequisites and assumptions) Before analysing the data (which we call building a model), we need to check that the dataset is suitable for this type of analysis and that the assumptions of the statistical test are met. First, we need to make sure that the dataset is organised according to the principles of the Linguistic feature research design (see Section 1.4). This means that each occurrence of the linguistic feature of interest such as the definite and the indefinite article is on a separate line and is properly annotated for explanatory variables as shown in Figure

Statistical model:

Combination of predictors with different weights.  Second, as in any type of quantitative analysis, the variables should be measured and coded accurately and consistently. Multivariate analyses (such as logistic regression) are especially sensitive to measurement errors, which can have a multiplicative effect due to multiple variables used in model building. Because many, especially functional, aspects of lexico-grammatical variables (e.g. syntactic/semantic function of a linguistic feature) are coded manually, the consistency and accuracy of coding should not be underestimated. If a judgement variable is involved, double coding of a certain portion of the data (e.g. randomly selected 20%) is recommended (see

Third, we need to have enough data. As a general principle, the more explanatory variables we use, the more data (cases or lines in the dataset) we need to have.

Fourth, we need to check whether it makes sense to build a model. If one predictor perfectly explains (classifies) all cases, then building a model that estimates the effect of different predictors is redundant. There are also mathematical reasons for why such a model cannot be constructed

Fifth, we need to check the assumptions of the test itself (see Osborne 2015: 85-130). There are three main assumptions: (i) independence of observations, (ii) no (multi)collinearity and (iii) linearity. (i) Independence of observations. As with the chi-squared test discussed in Section 4.3, we assume that every observation such as the use of an article is independent of another observation. In corpora, this assumption is usually violated to some extent due to the nature of language, where linguistic features are interconnected, and also due to corpus sampling that is done at the level of texts, not individual linguistic features. Because when studying lexico-grammatical variation, we usually presuppose a certain uniformity among texts of the same type, we don't have to worry about this problem too much apart from making sure that in our sample, linguistic features come from a range of texts. This can be done by taking a random subsample of linguistic features from the corpus. For studies which need to control for individual texts or speakers a method called mixed-effect modelling is used (see Section 6.5 for a detailed discussion of this). (ii) No (multi)collinearity between predictors. Collinearity is characterised by high correlation (r = 0.8 and above) between predictor variables (see Section 5.2 for an explanation of correlation). This needs to be checked for both scale and categorical variables.

With the dataset in Figure

By default, statistical packages provide a comparison of any model we build with the baseline model. If our model doesn't perform better than the baseline model, it is pretty useless because the selected predictors have little effect and we can therefore discard the model. In Table

With md2 out of the way, let's focus on the comparison of md1 and md3. At first it might seem that a model with more predictors is a better one. This, however, is not true. In the same way as we compared our models with the baseline model, we can compare any two models (md1 and md3 in our case) to see if one is significantly better than the other. In addition to statistical significance, which is measured by the log-likelihood test, we also use AIC (Akaike information criterion) to establish which model is the most efficient by reaching significance with as few variables as possible. AIC is calculated as follows: Looking at the model summary, we can see that overall the model is statistically significant with a likelihood ratio test value of 89.79

Let us now focus on the effect of individual predictors by looking at the second part of the output called 'Coefficients'. The coefficients (estimates) are displayed in a table. The first row always displays a so-called intercept (or constant), a baseline value in the model estimating the situation where all predictors are at their baseline values. Remember that for categorical predictors such as the Context_type these are the values that we set as baseline; for scale predictors this is always 0 (see 'STAGE 2: Building a model' above). In md1, the intercept estimates the odds of the definite article occurring in the baseline (that is nondetermined) context. The odds are very small: 0.04 (with 95% CI 0.002, 0.189).

Let us pause for a moment and explain the units which are used to measure the effect of the predictors. The units are odds and log odds (logits), the latter being computed by taking the natural logarithm of odds. Internally, the logistic regression, as the name suggests, operates with log odds. Because these are relatively difficult to interpret, we usually convert log odds to simple odds. Odds are defined as follows: In practice, odds are often used in sports betting. There we ask questions such as what are the odds of our team winning the game? If the odds are 2 to 1 the probability of our team winning is twice as large (i.e. 66.7%) as the probability of our team losing (33.3%). In our example, the odds of the baseline value (intercept), i.e. a definite article occurring in the non-determined context, are calculated as follows (relevant probability values are available in Table

In our example, the odds ratio that shows the effect of the determined context type on the use of the definite article is calculated as follows (relevant probability values are available in Table

For each row, the output table also lists standard errors, the significance statistic Wald's z and the corresponding p-value, which tells us if the specific estimate is statistically significant. Standard errors express how accurately the estimates reflect the value in the population; the smaller standard errors are the better. The Wald statistic (z) is computed by dividing the estimate with its standard error. The effect size for each parameter is the odds ratio discussed above, which is supplemented with 95% confidence intervals, showing us where the odds ratio is likely to lie in the population.

In more complex models (those with multiple predictors), we evaluate the estimates one by one. The following table shows the addition to the coefficients output when we include the scale NP_length predictor in the model measuring the length of the noun phrase. We can see that NP_length is not a significant predictor (p > 0.05) and the odds ratio is very small. 1.037 is close to 1, which means no effect. This observation is confirmed by the confidence interval which actually includes 1; this is a sign of a statistically non-significant result, because in the population the effect can well be null (odds ratio 1). The reason for showing this output is to discuss the meaning of the odds ratio in the case of a scale predictor. The odds ratio of a scale predictor indicates how many times larger (if > 1) or smaller (if <1) the odds of the outcome of interest are compared to the odds of the baseline outcome with one unit change of the scale predictor. In our example, the unit of noun phrase length was one character. The estimate (1.037) therefore indicates that the odds of the definite article the increase 1.037 times with every character added to the noun phrase. This means the longer the noun phrase is the more likely we are to see the definite article. However, as noted, this effect is not statistically significant and we would normally exclude it from the model because there is not enough evidence for it in the data.

In sum, logistic regression is a powerful method that gives us a detailed insight into the effects of different contexts on the linguistic output. It needs to be noted that in this section we have explored a type of logistic regression called binomial logistic regression, that is a logistic regression with an outcome variable with two categories like the and a/an. This represents a typical case in lexico-grammatical research that investigates a competition between two linguistic features within a given lexico-grammatical frame. If, however, the outcome variable has more than two options (categories), a similar technique called multinomial logistic regression is used. Multinomial regression follows the same principles as binomial regression, but the comparisons are somewhat more complex (see

Reporting Statistics: Logistic Regression

What to Report

Because logistic regression is a complex procedure, its successful use depends on a number of steps, as discussed in this chapter. These steps should be briefly outlined in our research report to allow replicability. First, we should indicate how the lexico-grammatical frame was defined and which variables were used and why. Second, we should let the readers know how the data was obtained (e.g. a random subsample of all occurrences of the linguistic features from a corpus) and coded as well as whether any part of the data was double coded (if so, inter-rater agreement statistic needs to be reported). Third, we need to provide details about the overall statistics of the model (LL, p-value, C-index) as well as a table of individual coefficients, including statistical significance information, the odds ratios and 95% confidence interval for the odds ratios.

How to Report: An Example

• Because the focus of the research was the variation between the definite and the indefinite article, all occurrences of these linguistic features were found in the corpus. One hundred cases were then randomly selected and coded for the presence or absence of the definite article (outcome variable). In addition, two contextual variables (predictors) reported in the literature as having an effect on the use of articles in English were also measured. These were the context type and length of the noun phrase. • The context type was a significant predictor of the type of article used.

4.5

Application: That or Which?

While writing this chapter, I encountered the following situation: my word processor underlined with a wiggly line a phrase that included a relative pronoun which, signalling a potential grammatical error or inaccuracy (see

The correction offered was to either add a comma in front of which or use the relative pronoun that instead, with the reasoning being as follows: 'If these words are not essential to the meaning of your sentence, use "which" and separate the words with a comma' (Microsoft 2010).

Having overcome the initial resentment at the fact that a computer was correcting my grammar, I started thinking about how to test whether the rule the grammar checker was applying actually reflects how language is used. I devised a study using BE06 and AmE06, two one-million-word corpora of current written English. The study is reported below.

This study is based on BE06, a balanced corpus of contemporary British English, and AmE06, a balanced corpus of contemporary American English. Each corpus consists of four major written genres (general prose, fiction, newspapers and academic writing).

In BE06 and AmE06, there are 4,736 instances of which and 22,749 instances of that. However, while which is used predominantly as a relativizer in contexts such as the one in Figure

The suggestion from the word processor ('If these words are not essential to the meaning of your sentence, use "which" and separate the words with a comma') has two aspects: a formal and a functional aspect. The formal aspect requires which to be separated by a comma, while the functional aspect requires which to be used in situations where the clause introduced by which is not 21 Part-of-speech tags are grammatical labels automatically attached to words using a part-ofspeech tagger; for more information see

essential to the overall meaning of the sentence (can be left out), something that is in grammatical terminology called a 'non-restrictive' (or 'non-essential') clause. By implication, that, the other relativizer, is to be used in complementary distribution to which, that is in 'restrictive' ('essential') modifying clauses, not separated by a comma. To test both the formal and the functional aspect of the suggestion, two research questions were formulated:

• RQ1 (formal): Is which preceded by a comma or a dash (-) while that appears without a comma or a dash (-)? • RQ2 (functional): What are the factors that affect the use of which and that?

The results of the search of BE06 and AmE06 that answer the first question are displayed in Table

Overall, that (7,472) is used much more often than which

The chi-squared test confirmed that there is a significant association between the relativizer (which or that) and the presence of a comma or dash (χ 2 (1) = 4,595.47, p <.001). The overall effect is large: Cramer's V = 0.689, 95% CI [.669, .709]. A comma or dash is 24.8 (95% CI [21.5, 28.7]) times more likely to appear in front of which than in front of that. Conversely, no separator is 2.7 (95% CI

In sum, we can see a clear preference for no separator occurring before that; a separator may or may not appear before which, although there is a preference for a comma or dash to appear. However, it cannot be stated as a categorical rule that which should be always separated with a comma because as we can see from Table

Apple will just veto and refuse to distribute any application which does not meet its terms.

(BE06_E33) Table

From Table

Overall, the model that includes all the predictor variables ('Variety', 'Separator', 'Clause type', 'Syntax' and 'Length') is significant (LL: 222.31;

So much for the study. The burning question, however, remains: was the computer right after all? If the suggestion by the computer were to be taken as a categorical rule, the answer is certainly 'no'. The study demonstrated that there is a combination of multiple factors that favour or disfavour the use of which (and that) and these factors have to be interpreted as probabilities (or odds, to be precise), not certainty.  The following variables were coded in the dataset:

• Outcome variable: must vs have to and need to combined (baseline).

• Predictor 1 (Variety): British vs American (baseline).

• Predictor 2 (Genre): fiction vs general prose vs press vs academic writing (baseline).

• Predictor 3 (Subject): I vs you vs other subject (baseline). By virtue of being in a text together, many linguistic variables are related in some way. For example, in English the relative frequency of adjectives in texts is related to the relative frequency of nouns because adjectives modify nouns and therefore typically occur together with them; at the same time, this doesn't mean that nouns do not occur without adjectives. As can be seen from Figure

It expresses the amount of covariance (variation that the variables have in common) in the data in terms of the standard deviations (SD 1 and SD 2 ) of the two variables in question; the combination of standard deviations here is used as the standardized measurement unit of 'statistical distance'. Covariance is calculated by computing the means for variables 1 and 2 (mean 1 and mean 2 ), taking each single value of variable 1, calculating the distance from mean 1 and multiplying this by the distance of variable 2 from mean 2. This is expressed by the equation below.

covariance ¼ sum of multiplied distances from mean 1 and mean 2 total no: of cases À 1 ð5:2Þ

The process might look complicated, but the idea is very simple. For illustration, let's extract five texts from those used to produce Figure

The covariance of nouns and adjectives in the five texts from Figure

The covariance is then entered into the equation for Pearson's correlation and standardized using standard deviations. The standard deviations are calculated according to equation (2.11) from Chapter 2 (standard deviation sample). In this case, the correlation between nouns and adjectives in the five texts is positive (as is clear from

• 0 no effect • ± 0.1 small effect • ± 0.3 medium effect • ± 0.5 large effect In addition, the correlation coefficient should be complemented with a pvalue or a confidence interval (CI) to indicate whether there is enough evidence in the corpus to generalize the correlation to the population. The p-value is a result of a test that evaluates the null hypothesis which states that the correlation in the population is 0 (i.e. there is no correlation). The statistical significance of a correlation is directly related to the number of observations (cases). With large numbers, which are typical in corpus research, small correlations are statistically significant, as can be seen from the graph in Figure

Figure

Let's return to our example of the five texts from BE06 displayed in Figure

Let's move on to the Spearman's correlation. Spearman's correlation (r s ), sometimes also denoted by the Greek letter rho (ρ), is used with ordinal data (ranks) or with scale data when the parametric assumptions are violated; in the latter case, the scale data is converted into ranks. Because we are dealing with ranks, we have no means, SDs or distances from the mean at our disposal. Instead, covariance is measured by looking at the differences between the ranks. Spearman's correlation is therefore calculated as follows:

6 Â sum of squared rank differences number of cases Â ðnumber of cases squared À 1Þ ð5:5Þ

Let's take again the five texts from Figure

where z 0 ¼ 0:5 ln 1þr 1Àr À Á ; once computed, Lower/upper limit z need to be converted back to r. 3 Remember that the number 95 indicates the percentage of samples taken from the same population for which the confidence interval contains the true value (i.e. value in the population) of the measure (see Section 1.3).

When the squared rank differences from Table

In this case, the non-parametric correlation (r s ) is once again very largethe same cut-off values as for Pearson's correlation, i.e. 0.1, 0.3 and 0.5, are used for conventional interpretation of the strength of the effect. However, the p-value is larger than 0.05 (p =.083), i.e. by convention we conclude that we don't have enough evidence to reject the null hypothesis which says that the correlation in the population is 0. The five texts therefore do not provide enough evidence that Spearman's correlation is not nullwhat we are seeing could be the result of chance. The difference between the p-value associated with Pearson's and the p-value associated with Spearman's correlation can be explained by the fact that by converting the actual values to ranks we lost some information and hence also the power to reject the null hypothesis. For this reason, Pearson's correlation is preferable with scale variables, while Spearman's correlation is used with ordinal variables (ranks).

Finally, two remarks need to be made. First, Pearson's correlation coefficient r can be used to account for the amount of variability in one variable shared by the other variable. For this, the coefficient needs to be squared; the product, r 2 , is called the coefficient of determination and is calculated using the simple equation below:

Let's take, for example, the nouns and adjectives in BE06 (Figure

Second, when dealing with multiple linguistic variables we can calculate pairwise correlations (either Pearson's or Spearman's). These are usually displayed as tables (correlation matrixes), series of scatterplots or visualization matrixes. As an example, take nouns, adjectives, verbs, pronouns and coordinators in BE06 as our linguistic variables of interest. What is the relationship between these five linguistic variables? The following are three different modes of presenting the results of the same analysis.

Figure

The initial observation can be confirmed by looking at Pearson's correlations, shown in Table

For those for who prefer further visualization of the results reported in Table

To sum up, correlation is a powerful technique for exploring the relationship between variables in corpora. With large corpora, which consist of hundreds or thousands of files, even small correlations (smaller than 0.1) will be statistically significant. We therefore need to consider very carefully the meaning of the correlations for the linguistic relationships between different features of language. Traditionally, the correlation coefficients (r, r s ) are reported together with the related p-values. Often, however, reporting confidence intervals (CIs) instead of p-values is preferable because CIs provide a more precise estimate about the actual value of the correlation in the population. It is also important to interpret the size of the correlation (effect size)the observed correlation is best compared with similar correlations in the data or those reported in the literature.

The most economical way of reporting correlation (used especially when reporting multiple correlations in a table) is to add a single (*) or double (**) asterisk next to the correlation coefficient. This conventionally means p<.05 and p<.01 respectively. We can also type out the p-values (although with large corpora with many files these are always very low) or specify the CIs.

How to Report: Examples

• There is a strong positive correlation (r = .52, 95% CI [.46, .58]) between the number of nouns and the number of adjectives used in English texts. This value, however, is not as large as the correlation between verbs and pronouns (r = .81, 95% CI [.775, .836]), which explain each other's occurrence in two-thirds of the cases (r 2 = 66%). • There is a very strong negative correlation (r s = -.83, p < .01) between the number of nouns and the number of pronouns used in English texts. These show a complementary distribution. • English verbs and adjectives in written texts are in an inverse proportional relationship (r = -.63**). So are verbs and nouns (r = -.65**). The negative correlation between verbs and coordinators is only small (r = -.14**) with little observable impact on the style of writing.

Classification: Hierarchical Agglomerative Cluster Analysis

Think about . . .

Before reading this section, think about the following question: which colour terms in Figure

In the previous section, we looked at the relationship between linguistic variables. We saw that many of them are related to some degree. In this section, we'll shift the focus to 'objects' (words, sentences, texts or speakers etc.) that can be characterized using multiple linguistic variables. Instead of looking at the relationship between linguistic features, we'll be looking at the relationship between objects as characterized by these features. The question we will be asking is relatively simple and is demonstrated in the 'Think about' task: how do we classify objects based on linguistic variables? Here, we used a simple example of colour terms characterized by the frequency of use and word length. We saw that black and white are the most frequently used colour words, while more specific terms such as aquamarine, turquoise and burgundy are much less frequent. The latter terms are also the longest and for some perhaps more difficult to pronounce. So how would we go about classifying these colour terms based purely on what we know about their frequency and number of letters? An obvious way of going about this would be to use the plot (Figure

Returning to the question of how to calculate the distance between green and blue in the graph in Figure

where x A is the first coordinate of point A, x B is the first coordinate of point B, y B is the second coordinate of point B etc. We can keep adding variables determining the position of the objects and move from a two-dimensional space to a multidimensional space.

In a two-dimensional space created by the two linguistic variables as in our example, we will use only two pairs of coordinates: x A , x B and y A , y B .

Euclidean distance ðgreen; blueÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ð0:11 À 0:58Þ 2 þ ½À0:32 À ðÀ0:76Þ 2 q ¼ 0:64 ð5:11Þ

However, there are alternative approaches to calculating the distance between A and B. Imagine you are in a big city such as New York and you want to get from A to B. Unless you have a helicopter, you won't be able to go directly. Instead, you'll have to go down one street, then make a 90 degree turn and go down another street. The distance between A and B measured like this, i.e. following the grids at right angles, is called the Manhattan distance. Manhattan distance is calculated according to the following formula:

where jx B À x A j etc. is the absolute value, i. e. a positive number of the difference between the coordinates of A and B.

For green and blue in our example, we'll get:

Manhattan distance ðgreen; blueÞ ¼ j0:11 À 0:58j þjÀ 0:32 À ðÀ0:76Þj ¼ 0:91 ð5:13Þ

As expected, the Manhattan distance is larger than the direct Euclidean distance, although when used for cluster analysis both distance measures yield fairly similar results; however, Manhattan distance is more robust when dealing with outliers (see Section 1.3 for a definition of an outlier). Other types of distance measures include Canberra distance (which is a standardized form of Manhattan distance), Squared Euclidean distance (which places more emphasis on objects further apart) and Percent disagreement (used when working with categorical variables)

At this stage, we have all we need to start the cluster analysis. There are many different types of cluster analysis

As can be seen from Figure

However, with multiple clusters the cluster diagram quickly becomes too complex and potentially hard to read. For this reason, the results of the cluster procedure are often displayed as a hierarchical tree plot (or dendrogram). The tree plot seen in Figure

So far, we have assumed that the method of joining small clusters into larger ones is straightforward. However, in the cluster procedure we need to specify exactly how this is to be done. The question we need to ask is: which of the data points inside a small cluster should be taken as representing the position of the whole cluster? Typically, four answers can be provided:

(1) the closest point to the neighbouring cluster with which we want to merge our original cluster (so-called SLINK method), (

Step 1

Step 2

Step 3

Step 4

As always, there are benefits but also disadvantages to each method. The SLINK method, for instance, is very simple. It leads, however, to the 'chaining' effect visible in the right branch of the dendrogram in Figure

One final point about clusters needs to be made. So far in the example we have considered two linguistic variables in a two-dimensional space. The cluster technique can also be used with multiple linguistic variables. The procedure is similar to the simple example described above with the only difference being the fact that with multiple variables we are looking at the distances between data points in a multidimensional space. An example of the use of the cluster technique with 44 linguistic variables is offered in Section 5.5.

What to Report

Cluster analysis is largely an exploratory visual method to show patterning in the data. Both the parameters for cluster identification (data transformation, distance measure, cluster combination method) as well as the results of the analysis (tree plot) need to be reported. The Method section of the research report describes the analytical procedure and the parameters used. In the Results and Discussion, each tree plot (dendrogram) needs to be carefully discussed. The main question to be addressed is: how many meaningful factors can be observed in the plot?

How to Report: An Example

• The data was analysed using the hierarchical agglomerative cluster technique (z-score data transformation, Manhattan distance, Ward's method).

For an example of reporting and discussing results of the cluster analysis see Section 5.5. How do the two excerpts below taken from the BNC differ in terms of the language they use?

As users of a language, we are multi-stylistic. This means that we can employ a variety of styles of speech and writing depending on the situation. For example, we use a different type of language when talking informally to friends than when we are asked to write a research report. This ability to change the style of speaking/writing is directly reflected in the genres or registers of language we produce, as can be seen in the two examples from the 'Think about' task. For instance, you might have noticed that the spoken dialogue (Excerpt A) uses shorter syntactic structures (so called utterances) than the academic text (Excerpt B), which employs relatively long and complex sentences. At the same time, Excerpt B is much more 'polished'it lacks hesitations (erm), repetitions (I'm, I'm feeling) and false starts (Yes, that is also a thing to . . . erm, I'm) that are typical of informal speech. In addition, there are numerous other linguistic features that characterize informal speech such as frequent use of personal pronouns, contractions and discourse markers, while academic writing typically uses a large number of nouns, prepositions and passives. The issue we are thus presented with in the linguistic analysis of registers is how to make sense of the large amount of functional variation in the data and how to characterize individual registers by looking at the underlying principles of systematic variation

The full multidimensional analysis has four main stages: (1) Identification of relevant variables, (

Step 1: Identification of relevant variables. Before performing the statistical analysis (step 2), we need to identify a large number of linguistic variables in the texts of our corpus. The corpus needs to include different registers (e.g. informal speech, news reporting, academic writing, popular fiction etc.) and we need to identify relevant variables, that is, those that can distinguish between the registers in our corpus. For example, all the variables mentioned in Section 5.2 play an important role in register identification. Usually, several dozen (40over 140) variables are identified;

A more extensive list (141 items) combining lexico-grammatical and semantic features can be found in

Three important details need to be mentioned before we move to step 3: first, prior to carrying out factor analysis, we need to check that the individual linguistic variables correlate reasonably well (above 0.3) with some other variables in the dataset, otherwise we wouldn't be able to combine variables into factors. The checking can be done by looking at the Pearson's correlations between pairs of variables; the correlations are usually displayed in a form of a correlation matrix where each variable is correlated with the rest of the variables in the dataset.

Second, to optimize the results of the factor analysis, the factors (represented by the two axes in Figure

Third, we have to decide how many factors we want to extract. The idea is to extract as few factors that explain as much variation in the data as possible, because the whole point of factor analysis is to reduce a large number of linguistic variables into a few underlying factors. To help us decide, a visualization technique producing a scree plot is used. A scree plot is a graph exemplified in Figure

There are different answers to this question: often factors with eigenvalues above one

Factor 1 can be interpreted as follows: first we look at the variables with high positive loadings (Group A). These variables co-occur in texts and we can assume that they have a common communicative function. When we look at the range of variables in this group we can see that these variables mark informal, highly interactive language with frequent use of firstand second-person pronouns, contractions, adverbs and that deletion. Next, we look at the variables with negative factor loadings (Group B) which have a complementary distribution to the variables with positive loadings. This means that they occur infrequently in texts in which the variables from Group A occur with high frequency and vice versa. Again, variables from Group B share a communicative function, which can be labelled as 'academic-type description' with passives and many modified nouns and nominal forms ending in -tion, -ment, -ness and -ity.

Step 4: Placement of registers on the dimensions. The final step involves placement of the registers on the dimensions. This step can also help interpret the communicative functions of the dimensions from step 3 by looking at the types of texts in which the variables that load high on each factor occur. First, we need to standardize the dataset (see Figure

Dimension score text ¼ variable 1 Group A þ variable 2 Group A …

À variable 1 Group B À variable 2 Group B ð5:14Þ

A dimension score is the sums of z-scores 2 for the variables with high positive loadings (Group A) minus the z-scores 2 for the variables with high negative loadings (Group B). Finally, the mean dimension score for each register is calculated by taking all text dimension scores belonging to the same register and calculating the average value. Each register is then placed on a one-dimensional scale according to the resulting mean value of the dimension score. This can be demonstrated with Figure

We can see that registers with high Dimension 1 scores are different types of fiction in which a lot of interaction takes place. They are therefore placed towards the Involved end of the dimension. On the other hand, government documents and academic writing have a low dimension score clustering towards the Informational end of the dimension. The other registers are somewhere in the middle, some closer to the Involved end and others closer to the Informational end. In addition, to test whether there is a statistically significant difference between the registers placed on Dimension 1, one-way ANOVA is computed (see Section 6.3 for more information); R 2 is also reported showing the amount of variation in the registers explained by this factor. The process of register placement is repeated with each of the extracted factors.

Reporting Statistics: Multidimensional Analysis

What to Report

Multidimensional analysis involves a number of complex steps and statistical procedures which need to be reported for full replicability of the results. The success of MD also directly depends on the reliability of the automatic identification of linguistic variables (tagging) in corpora. The following information should be reported:

(1) Linguistic variables used: all linguistic variables should be listed together with the information on how much they overlap with

(2) The tagging procedure (i.e. which tagger and, where appropriate, which version of the tagset was used) and whether its reliability has been checked.

(3) Type of multidimensional analysis performed: Full MD or Comparison with

(4) Factor analysis rotation (e.g. promax or varimax) and number of factors extracted.

(5) Results: (a) factor loadings, (b) dimension plots, (c) ANOVA and r 2

The Method section of the research report describes the analytical procedure and the parameters used. In the Results and Discussion, each dimension plot needs to be carefully discussed. The main question to be addressed is: what is the underlying functional variation in registers that the method revealed?

2. How to Report: An Example For a contextualized example see Section 5.5.

Application: Registers in New Zealand English

This micro-study is directly inspired by Richard Xiao's Multidimensional exploration of world Englishes

past tense (1), perfect aspect (2), present tense (3), place adverbials (4), time adverbials

MAT analyser

When dealing with multiple linguistic variables, the first step is to look at the correlation between these variables. For this purpose, a 44 × 44 correlation matrix was produced (Figure

This negative correlation indicates that texts in the corpus with on average longer words (as measured by the number of characters) have very few, if any, contracted forms and vice versa. This correlation makes sense linguisticallylonger vocabulary items occur in formal written texts, which rarely use contracted forms. On the other hand, informal spoken registers have many contractions and shorter words. The relationship between contractions and mean word length can be visualized using a scatterplot

Here, we can see that these two variables alone can help distinguish between different registers. In the top left corner of the two-dimensional space, cluster informal conversations. When we move down the diagonal, we can see overlapping clusters of public dialogues, unscripted monologues, letters and pieces of creative writing (left) as well as scripted monologue (right). These are marked by a gradually decreasing number of contractions and increasing mean word length.

When we move beyond the two-dimensional space from Figure

In Figure

To explore more dimensions in the data, multidimensional analysis was performed. Because of space constraints, only a subset of the results of this analysis (first two factors/dimensions) will be reported. Rather than a comparison with

Looking at Factor 2, the largest positive loadings include different types of modals, conditional adverbials if and unless as well as verbs such as command, propose and recommend. Second-person pronouns were disregarded because they have a higher loading on Factor 1. The only negative loading is attributive adjectives. The underlining function of the positive features is modalized production typical of written instructional texts, while the other end of the dimension is descriptive production typical of academic writing and popular informational texts. We can therefore use the label 'Modalized vs descriptive' production. Note that this -0.875 a The numbers in brackets refer to the order of presentation of

dimension is completely different from Biber's (1988) Dimension 2: Narrative vs non-narrative discourse. Biber's narrative/non-narrative features are, however, salient also in New Zealand English; they appear in Factor 3 (not discussed here). Finally, it is worth mentioning that both dimensions show statistically significant differences among the registers as established by one-way ANOVA. However, in the case of the Dimension 1 almost 83% of the variation in the dimension scores of the individual texts is explained by their register membership, whereas in the case of Dimension 2 this number is less than 40%. Dimension 1 is thus a more powerful predictor of register variation than Dimension 2.

Exercises

1. Manually calculate the Pearson's and Spearman's correlations between verbs and adjectives in ten randomly selected texts from BE06. The data is provided below:  A (Press: reportage), B (Press: editorial), C (Press: reviews), D (Religion), E (Skills, trades and hobbies), F (Popular lore), G (Belles lettres, biography, essays), H (Miscellaneous government documents, foundation reports, industry reports, college catalogue, industry house organ), J (Learned and scientific writings), K (General fiction), L (Mystery and detective fiction), M (Science fiction), N (Adventure and western fiction), P (Romance and love story), R (Humour).

This classification is very useful; however, for some purposes it might be too detailed.

Group the individual text types into larger categories based on their functional similarity. Then design a study in which you could verify your grouping.

4. Table

5.

Use the data provided on the companion website and the MD tool to compare registers in current British and American English.

TH I NGS TO R EM EM B ER

• Correlations are used for the investigation of the relationship between two variables at a time.

• Pearson's correlation is suitable for scale variables, while Spearman's correla- tion assumes ordinal variables (ranks). Spearman's correlation can also be used with scale variables if the means as the measures of central tendency do not represent the data well (extremely skewed distributions).

• Hierarchical agglomerative cluster analysis is used for classification of words, texts, registers etc. The result of this analysis is a tree plot (dendrogram).

• The most complex type of analysis out of the three discussed in this chapter is multidimensional analysis (MD). MD analyses a large number of linguistic variables and reduces them to a small number of factors which are interpreted as dimensions of variation. Along these dimensions, different registers can be placed.  Is but a little way above our heads, My only love sprung from my only hate.

Staying for thine to keep him company. Either thou, or I, or both, must go with him. This shall determine that. O, I am fortune's fool.

The notion of style is central to the analyses described in this chapter. Following

But how can we identify such variables?

In variationist sociolinguistics as pioneered by

Although this formal definition of a sociolinguistic variable gives us complete control over the linguistic and social processes behind variation, it reaches its limits fairly soon when we look at other types of systematic variation beyond phonology and simple grammar

In contrast to Labov's formal approach, Biber (e.g. Biber & Conrad 2009) examines functional variation, where the analytical focus is on speakers'/writers' choices 'from the entire inventory of lexico-grammatical characteristics of a language, selecting the linguistic features that are best suited functionally to their situations and communicative purposes'

Finally, to provide a full answer to the question in the 'Think about' task (many individual aspects of the language have already been discussed): Speakers 1 and 2 are real people taken from the British National Corpus (BNC). Speaker 1 is a 17-yearold male student from the Midlands from a working-class background. Speaker 2 is a 14-year-old female student from London also from one of the lower social classes. Speaker 3 (Juliet) and Speaker 4 (Romeo) are fictional characters from Shakespeare's famous play. They are of comparable age and gender with Speakers 1 and 2, but differ in the historical variety of English they speak, social class (Romeo and Juliet come from wealthy families in Shakespeare's imaginary Verona) and, most importantly, the fact that Romeo and Juliet are products of Shakespeare's imagination, not real persons. Interestingly, the speakers deal with somewhat parallel topics: anger and aggression (Speakers 1 and 4) and dating and courtship (Speakers 2 and 3).

6.3

Group The conversation above comes from BNC64

So which statistical test should we use in this situation? The simplest option we have is the t-test, and in this case we'll use a version of the t-test that is called Welch's independent samples t-test. This test compares two groups of speakers (e.g. male and female speakers). The t-test compares the mean values of the linguistic variable (in our example, the relative frequency of personal pronouns in individual speaker samples) and takes into consideration the internal variation in each group expressed as variance. Variance (S 2 or SD 2 ) is the sum of squared distances of individual values from the group mean divided by the degrees of freedom. In fact, it is the squared version of sample standard deviation (see Section 2.4), hence SD 2 .

Variance is calculated according to the following formula:

Variance ¼ sum of squared distances from the mean degrees of freedom ð6:1Þ Figure

The logic of computing variance is simple: to see how much overall variation there is, we add the squared distances from the mean (we use squared rather than simple distances because distances below the mean are negative and would cancel out the positive distances) and divide this by the degrees of freedom. The degrees of freedom is a complex concept, which is used when dealing with calculations based on a sample (corpus) rather than a population; in corpus linguistics, this is the case most of the time. The degrees of freedom (df) signifies a number of independent ('free') components in our calculation, i.e. components that are not predictable from the previous components. In practice, it is the number of cases (texts/speakers) or groups (when looking at group variance) minus one. We subtract one from the number of cases (groups) because the last case is always predictable from the previous cases.

Let's return to the t-test, which uses variance as one of its components. Like every statistical test, the t-test has assumptions, which need to be reviewed before running the test. The most important assumption is the independence of observations. In the sociolinguistic context, the independence of observations means that each observation (text or speech sample) comes from a different (randomly sampled) speaker 4  and that the use of language by one speaker in the sample is not affected by the use of language by another speaker. We therefore need to make sure that we include each speaker as an observation (case) only once even if multiple texts/transcripts are available, otherwise we are counting the same person twice/multiple times and violating the first part of the assumption. The second part of the assumption is easy to maintain in written texts or monologues, but in a conversation, this assumption will be violated to some extent if speakers interact with each other (something known as a 'priming effect' in psycholinguistics and more generally as 'accommodation' in sociolinguistics). We have to be mindful of this effect and evaluate its impact in each individual case. Two other assumptions listed in textbooks (and often misunderstood) are the normal distribution and homoscedasticity. Normal distribution of the linguistic variable in the population is an assumed symmetrical distribution visualised as a bell-shaped curve (see Section 1.3). Homoscedasticity 5 is a technical term for the equality of variances, i.e. amount of variation, in two groups that we want to compare. As shown in the literature

Welch's independent sample t-test is calculated according to the following formula:

Variance of group2 Number of cases in group2 s ð6:3Þ 4 If we have two or more samples from each speaker and are interested in the difference in their language between sample 1 and sample 2 etc. (e.g. linguistic change/development), we are dealing with a so-called repeated measures design, which requires a different version of the statistical test (see below for more information). 5 Homoscedasticity is a bit of a tongue twister; you can use the more descriptive term 'equality of variances' instead.

As can be seen from the equation, there are three factors that have an effect on whether the test will be significant: (i) size of the mean difference, (ii) variance in each of the two groups and (iii) sample size (number of cases, i.e. speakers or texts, in both groups). The t-test value is large (and the test is significant) if there is a large difference between the means, small variance in the groups and a large number of cases; these factors combined show that there is enough evidence in the data that the two groups are different with respect to the use of the linguistic variable in question.

In our example, the means of relative frequencies (per 10 k) of personal pronouns for the male and the female subcorpora are 1,451.8 and 1,556.9 respectively, the variances are 22,256.5 and 23,820.5 and we have 32 speakers in each group (see the description of BNC64 above). When we enter this in the equation, we get: In this case, the t-test statistic (2.8), with the appropriate degrees of freedom (61.93) calculated using an equation that will not be introduced here,

In addition to the statistical test, we also need to calculate an effect size measure to evaluate in standardized terms (i.e. units comparable across linguistic variables and corpora) the size of the difference between the two groups. With the t-test, we have several options of effect size measures that include Cohen's d and r as two typically used effect size measures. Cohen's d is calculated as the difference between the two means expressed in standard deviation units.

Cohen's d ¼ Mean of group1 À Mean of group2 pooled SD ð6:5Þ

where

all cases À 2 s

In our example, the calculation of Cohen's d is as follows: This can be interpreted as a medium effect.

As with any other effect size measure, we also need to look at the 95% confidence interval for Cohen's d, which, in our example, is 0.18 to 1.21, as calculated automatically by statistical packages such as Lancaster Stats Tools online. This shows a likely range of the effect in the population (all male and female speakers of British English). Because this 95% CI is extremely wide ranging from a minimum to a large effect, we cannot be sure about the actual size of the effect in the population; this is due to a relatively small sample size (64 speakers). A small aside: for extremely skewed data (i.e. data severely violating the normality assumption) a robust version of Cohen's d has been proposed

In our case, r is 0.33, 95% CI [.08, .53], which, again, can be interpreted as a medium effect with the caveat that the true population effect may range from minimum (0.08) to large (0.53).

So far, we have compared two groups of speakers. However, what if we need to compare more? In such a case a test called one-way ANOVA

(1) cos there ain't [= isn't] no sign of them, is there ? (BNC64, M2) (2) I've won twice ain't [= haven't] I ? (BNC64, F5) BNC64 classifies speakers into four categories reflecting the socio-economic status of the speakers: AB -Managerial, administrative, professional; C1 -Junior management, supervisory; professional; C2 -Skilled manual; DE -Semi-or unskilled.

ANOVA has the following assumptions, which are similar to the assumptions of the t-test discussed above: (i) independence of observations, (ii) normality and (iii) homoscedasticity. The most important one is the independence of observations because as was shown in the literature the ANOVA test is robust against the violation of the normality assumption

One-way ANOVA ðFÞ ¼ Between-group variance Within-group variance ð6:7Þ

To illustrate the logic behind ANOVA, Figure

To calculate between-group variance, that is the variation in the data explained by social class as the explanatory variable, we need to take the group means and calculate their squared distances from the grand mean and multiply these numbers by the number of individual cases in each group. This is then divided by the appropriate degrees of freedom: number of groups minus one. Between-group variance ¼ cases group1 Â ðmean1 À grand meanÞ 2 þ cases group2 Â ðmean2 À grand meanÞ 2 þ … number of groups À 1 ð6:8Þ

Within-group variance is the sum of individual variances for each of the groups, each calculated according to equation (6.1). The degrees of freedom is the number of cases minus number of groups.

Within-group variance ¼ sum of sqared distances for group1 þ sum of sqared distances for group2 þ … number of cases-number of groups ð6:9Þ

In our example, between-group variance, within-group variance and ANOVA (F) are calculated as follows:

Between-group variance ¼ 14 Â ð2:5 À 6:6Þ 2 þ 16 Â ð3:3 À 6:6Þ 2 þ 17 Â ð10:4 À 6:6Þ 2 þ 13 Â ð10:0 À 6:6Þ The related p-value is p = .002 and hence we can conclude that the result is statistically significant.

Because ANOVA is an omnibus test, it detects statistically significant difference anywhere in the data (between any of the groups), but it does not tell us where exactly the difference lies. For this, we need to carry out so-called posthoc tests. Post-hoc tests are pair-wise comparisons of individual group means (similar to the t-test discussed above) with a correction for multiple testing; with multiple testing the probability of a falsely positive result (a so-called type I error) increases. This is because with each test that uses a p-value we are willing to accept that in a small number of cases (5%) the result will be statistically significant, even if the null hypothesis is true (there is no effect of the explanatory variable). With multiple testing, this probability dramatically increases. For instance, with four groups (as in our example), we can run six pair-wise comparisons, which will increase the type I error to 26.5%.

Like post-hoc tests, t-tests with Bonferroni correction are often reported; Bonferroni correction is fairly strict and hence the test is fairly conservative, i.e. it may not have the statistical power to detect small differences. Other options include Tuckey's HSD or different tests based on bootstrapping, a method of multiple resampling (see Section 7.3).

Finally, when reporting ANOVA, in addition to statistical significance, the effect size needs to be reported. The overall (omnibus) effect size that is sometimes reported is eta squared (η 2 ). The standard interpretation of this effect size is as follows

Interpretation of partial η 2 : η 2 > 0.01 small, η 2 > 0.06 medium and η 2 > 0.14 large effect.

A better (more specific) option, however, is to report effect sizes for differences between individual groups alongside the post-hoc tests. Cohen's d or r are the most commonly used effect sizes in this situation.

The t-test and ANOVA also have their non-parametric counterparts called the Mann-Whitney U test (also known as Wilcoxson rank-sum test) and the Kruskal-Wallis test. Non-parametric means that we don't need to know (assume) any knowledge about the parameters (such as the mean or standard deviation) of the variable of interest in the population

1. If the linguistic variable is a scale variable, all data is ranked regardless of the group membership with the highest value receiving rank 1 and tight scores both/all receiving an average rank (e.g. if all three top values are 10.5, the ranking will be 2, 2, 2 calculated as (1+2+3)/3). 2. The data is then divided into groups according to the explanatory variable, e.g. gender or socio-economic status, and the sum of ranks is calculated for each group.

For the Mann-Whitney U (Wilcoxson rank-sum) test, two U values are calculated,

ð6:13Þ

The idea behind the test is this: we take into account the actual sum of ranks for each group from which we subtract the minimum possible sum of ranks that the group can achieve [cases in group 1 × (cases in group 1 +1) /2]. Suppose that we have two groups of five speakers, one of which (group 2 ) has all the smallest ranks (1-5). The sum of ranks for this group is 1 + 2 + 3 + 4 + 5 = 15 and the minimum possible sum of ranks for this group is also 15 [(5 × 6)/2]. The resulting U, the smaller of the two options from equation (6.13), is 0 (15 -15). From the logic of the test follows that small U values show strong differences between groups as exemplified above.

The Kruskal-Wallis test (producing H values) works on a similar principle but takes into account ranks in multiple (3+) groups

Different effect size measures have been proposed for the non-parametric tests (Kerby 2014). As discussed above, an effect size that specifically quantifies the difference between two groups (rather than an omnibus effect size measure) is probably most useful to report. The Mann-Whitney U test can thus be supplemented by the rank biserial correlation

Rank biserial correlation ðr rb Þ ¼ mean rank group1 À mean rank group2 number of all cases

Like the Pearson's or Spearman's correlation coefficients (see Section 5.2), r rb can take on values from -1 to 1. The larger the value is in absolute terms, the stronger the correlation; a negative value signifies that group 2 has a larger mean rank value. Another option for effect size measure is to use probability of superiority (PS) discussed in Section 8.4.

A final note: if we have multiple samples for each individual speaker and are interested, for example, in how their language develops over time or how it was affected by another explanatory variable between two or more sampling points, a so-called repeated measures version of the tests discussed in this chapter needs to be used. Repeated measures tests match individual speakers across conditions and do not assume random distribution of speakers in different groups (see

Reporting Statistics: T-Test, ANOVA, Mann-Whitney U Test and Kruskal-Wallis Test

What to Report

When comparing groups of speakers using the t-test, ANOVA, Mann-Whitney U test and Kruskal-Wallis test, we report (i) the test statistic (t, F, U and H respectively), (ii) the degrees of freedom (for t-test and ANOVA), (iii) p-value and (iv) effect size (including 95% CI).

Exact p-values are reported unless they are smaller than 0.001; after this point p<.001 is reported. For ANOVA and the Kruskal-Wallis test, where multiple groups are involved, post-hoc tests and their relevant effect sizes should also be reported, where relevant. It is also possible to accompany the tests by data visualization such as boxplots and/or error bars showing 95% CIs.

How to Report: An Example t-test

• There was a statistically significant effect of gender on the use of personal pronouns, t(61.93) = 2.77, p = .007, with women (M = 1,556.9, SD = 154.3) using personal pronouns more than men (M = 1451.8, SD = 149.2). The size of the effect is medium, d = .69, 95% CI

• As we can see from Figure

Mann-

Whitney U

• The use of pronouns by female speakers (Mdn

Kruskal-Wallis

• There was a significant effect of social class on the use of the form ain't, H(3)= 15.57, p = .001.

Individual Style: Correspondence Analysis

Think about . . .

Before reading this section, look at the samples of transcribed speech. These come from two different speakers; three samples belong to one speaker, the remaining sample to another speaker. Can you tell which samples belong together? Are there any linguistic clues that can help you tell the speakers apart? Individual style has been investigated with a range of linguistic features both lexical and grammatical. In this section, we take a look at a technique which is somewhat similar to factor analysis discussed in Chapter 5 (Section 5.4), but that has been primarily developed for the analysis of cross-tabulation tables with categorical data, the type of data which was discussed in Chapter 4 (Section 4.3).

Correspondence analysis is a summary technique which outputs a correspondence plot. A correspondence plot is a visual depiction of a cross-tabulation table which is projected in a (typically) two-dimensional space using the chisquared distance as a measure of closeness/remoteness of the categories listed in the table. A 2D correspondence plot (e.g. Figure

1).

Let us briefly go back to the 'Think about' task. Samples 1, 3 and 4 represent speaker F1 from Figure

Conceptually, correspondence analysis is related to the chi-squared test discussed in Section 4.3, which can be performed on a two-way cross-tabulation table (e.g. Table

So how is the information in the table 'translated' into the visual representation in the correspondence plot? Let's start by taking two rows and two columns from Table

The first step in the correspondence analysis is to turn the information in the table into what is called profiles. Profiles are proportions (percentages) based on row and column totals (row and column profiles); the profiles are expressed as decimal numbers, e.g. 0.87, rather than percentages (see

The scatterplot in Figure

where x A is the first coordinate of point A, x B is the first coordinate of point B, y B is the second coordinate of point B etc. We can keep adding categories (different In effect, the chi-squared distance between two points is always larger than the simple Euclidean distance as can be seen in Figure

When we look at a correspondence plot such as the one in Figure

Reporting Statistics: Correspondence Analysis

What to Report

Correspondence analysis is best summarized by the correspondence plot, which is based on a cross-tabulation table; both the plot and the table should be reported. We should note the overall percentage of variation explained by the two factors as well as the linguistic interpretation of these factors. We should then attempt to identify meaningful clusters of speakers in the plot and comment on their relationship. Note that the chi-squared distance can be interpreted only within row/column categories and the distances between row and column categories cannot be directly interpreted. For instance, in Figure

How to Report: An Example

• The data in Table

6.5

Linguistic Context: Mixed-Effects Models Think about . . .

Before reading this section, look at the examples below. Is there a meaning difference between the two utterances? Can you imagine contexts in which you would say these utterances?

(i) This is really good.

(ii) This is very good.

One of the crucial tenets of the traditional variationist sociolinguistics is what Labov calls the principle of accountability

As an example, we will take the variation between utterances (i) and (ii) in the 'Think about' task; this example is inspired by

The first analytical step is to extract the data from the corpus and classify all the examples in the dataset (see Figure

Figure

The dataset in Figure

The statistical technique introduced in this section, whose application will be demonstrated with the dataset in Figure

Because of the similarity with logistic regression described in detail in Section 4.4, readers are referred to Chapter 4 for a detailed explanation of the basic terms and principles. In what follows, the main differences between logistic regression and mixed-effects models (mixed-effects logistic regression) are highlighted and an example of the use of this technique is provided. When we run the mixed-effects model technique we get an output such as that displayed in Figure

The interpretation of mixed-effects models has two main parts: (i) evaluation of the model characteristics and (ii) interpretation of the significance of the fixed effects. In our example, the model is statistically significant and shows the effect of age and syntactic position on the use of very as opposed to really (our baseline variant marked with the prefix A_). Older speakers prefer very and syntactic contexts with a predicative adjective (e.g. you're very lucky) also favour this intensifier. The other predictors are non-significant. This result is in line with

A Small Dictionary of Sociolinguistic Terms

In corpus linguistics and variationist sociolinguistics, different terms are used to mean the same thing or the same terms are used to mean different things

Variationist term Meaning

Corpus linguistics equivalent factor A type of speaker (e.g. male or female, young or old) or a type of context (e.g. syntactic position) which favours the use of a particular variant of the sociolinguistic variable. With mixed-effects models, it is important to report the type of mixed-effects model used as well as the details of the model. The focus of the reporting will be on the effect of the individual predictors.

How to Report: An Example

• Mixed-effects logistic regression was used with the individual speakers as random effects and gender, class, age and syntactic position as fixed effects.

The model, which was overall significant (p < .001), showed a significant effect of age and syntactic position: older speakers and predicative contexts favour the intensifier very.

6.6

Application: Who Is This Person from the White House?

I have a friend (they wish to remain anonymous), who every year, instead of a birthday card sends me a linguistic puzzle. This year, the puzzle had the form of a 'sociolinguistic riddle' with an attachment, which contained a file with transcribed speech. This is what appeared in my mailbox:

What follows is a brief description of my work over the following few days after receiving the email, which I spent solving the riddle. In forensic linguistics (my quest closely resembled the detective work of a forensic linguist), there are two basic approaches, which depend on the amount of evidence available: if the amount of evidence is small (a few sentences or paragraphs), close reading for signs of idiosyncratic language use (or shibboleths) is appropriate. If, on the other hand, as was my case, there is a lot of data available (the sample was approximately 200,000 words), the statistical approach is called for. The second part of the riddle was clear and matched the type of language in the sample. 'I work in Washington DC and speak for the President' indicated that the speech sample comes from a White House press secretary. Luckily, I could use a comparative corpus of White House press conferences (WH)

The first step was to decide whether transcript X came from a male or a female speaker. There are two women in the corpus (Dee Dee Myers and Dana Perino) and six men. The women contribute one sample each, while there are 33 speech samples from the male group. It is not possible to use traditional gender-distinguishing words such as lovely (preferred by women in informal British speech), because in the whole corpus (over 6 million words) there are only 20 occurrences of lovely. This is no surprise: we are dealing with a very specific formal spoken register of American English. We To answer this question, correspondence analysis was used with all the speech samples in the corpus plus transcript X (34 + 1). The analysis looked into the proportions of different word classes in these samples as the linguistic variables which are both frequent and independent of the topic discussed (see

The correspondence analysis clearly grouped individual speech samples from the WH press secretaries together. For instance, all speech samples from Ari Fleischer (A1-A4) cluster at the top left, while speech samples from Scott McClellan (S1-S3) cluster at the bottom right closer to the centre than speech samples from Tony Snow (T1) and Dana Perino (D1). What about the mysterious transcript X? In the graph, it clusters in the proximity of the sample from Dee Dee Myers and far apart from the other samples. With a high probability it can thus be assumed that transcript X comes from this WH press secretary. The speech of Dee Dee Myers, the first ever woman to hold the post, is characterized by the frequent use of pronouns, a relatively infrequent use of nouns, adjectives and conjunctions. These stylistic differences can be identified only by taking large speech samples and analysing them quantitatively.

Following the investigation described above, I replied to my friend's email: For those of you who are still not convinced, you can follow my example and try to search for the first sentence from transcript X on the internet: 'The good news is that the filibuster has been broken on national service.'

Exercises

1. Which linguistic disciplines use the notion of 'style' as one of their core concepts? How can it be operationalized?

2. Which of these cases of variation satisfy Labov's definition of a sociolinguistic variable and can thus be investigated using Labov's methods? Justify your answer.

(a) h-dropping in different social contexts: i.e. the variation between the pronunciation of e.g. the word hair as /heə/ or /eə/. (b) The variation between naming strategies for soft drinks in the US, e.g. soda, pop or coke. (

South (n1 = 10):

2.

3.

4.

TH I NGS TO R EM EM B ER

• Sociolinguistic variation can be operationalized in different ways: by employing the Labovian meaning-preserving sociolinguistic variable (formal approach), or by following the functional approach and looking at the distribution of linguistic features in groups of speakers.

• The t-test and ANOVA (as well as their non-parametric counterparts: Mann-Whitney U and Kruskal-Wallis) are used to investigate the effect of explanatory social variables (gender, social class) on the use of different linguistic features.

• Correspondence analysis is an exploratory analytical technique, which compares the use of multiple variables in different speakers reducing them to two factors and producing a powerful visualization -a correspondence plot.

• Mixed-effects models is a group of sophisticated statistical techniques which can account for multiple variables at the same time and include the effect of individual variation between speakers.

The notion of change over time is central to the analyses described in this chapter. From a statistical perspective, time is a continuous (scale) variable; this means that we can measure time on a continuum of centuries, decades, years, months, weeks, days, hours, minutes, seconds, milliseconds etc. A study that involves time as a variable is called a diachronic or longitudinal study. We sometimes also measure time relative to the stages of the human life, that is age, which is an important variable in sociolinguistic (see

To carry out diachronic studies, we need appropriate corpora, which we call historical or diachronic corpora. Diachronic corpora sample different stages of language or discourse development across time. Examples of historical corpora for the English language include the Brown family,

In addition, when considering diachronic representativeness, we need to deal with certain limitations inherent in historical data. We have to realize that only a small fraction of the language from the past has been recorded and preserved. A historical corpus is usually not a balanced sample of the language from a given historical period; it is, rather, a narrow lens that provides an insight into the language that has been preserved

Second, we need to consider alternative interpretations of linguistic development based on the evidence available. If there is not enough evidence, or the evidence is biased towards, for example, a certain genre or certain types of speakers/writers (see 'diachronic representativeness' above), the patterns we observe are ambiguous. Generally, the further back in history we go the less evidence has been preserved. For example, all preserved texts from the Old English period (450-1100) consist of no more than three million running words,

The issue can be mitigated by adding more data points which allow us to track the trajectory of language change more precisely. However, where acquiring more data points like this is not possible, we need to be open to alternative interpretations. To capture some language changes a straight line (the simplest model) is adequate. Yet for other diachronic processes, more complex models (complex curves) are more appropriate. For example, phonological and grammatical changes often follow an S-shape curve

Third, the fluctuation of the meaning of linguistic forms (diachronic polysemy) is a phenomenon that needs to be carefully considered when looking at the development of language; the same linguistic form often changes meaning (or its set of meanings) over time, so in diachronic analyses we also need to provide an account of the semantic development, not only an overview of changing frequencies of linguistic forms. For example,

The sensitivity to the three areas discussed above, that is the diachronic representativeness of corpora, alternative interpretations of linguistic development and fluctuation of the meaning of linguistic forms, distinguishes corpus linguistics from linguistically naïve quantitative methods of the 'big data' approaches to language, such as culturomics

We can see how culturomics and similar approaches trade the representativeness and sensitivity to meaning fluctuation for a large amount of evidence (hence the 'big data' approaches); this, however, is never a good deal

Finally, let us look at some visualization options with diachronic data. We have already seen three figures (7.1, 7.2 and 7.3) that visualize language change in the form of a line graph. A line graph is a simple display, which plots the time variable on the x-axis and the frequencies of linguistic variables on the y-axis. Line graphs help us interpret the patterns of change in corpora. Note that the actual line in the graph is produced by connecting the data points and already represents an interpretation of the data. For this reason, in Figures 7.1 and 7.2 (but not in Figure

Other options for visualizing diachronic data include boxplots and error bars, sparklines and the candlestick plot. Boxplots and error bars were discussed in detail in Section 1.5. They offer an opportunity to look inside each diachronic sampling point and analyse variation between individual texts in the historical period. The error bars can be used to display 95% confidence intervals around the mean values for each historical period. Figure

When dealing with a large number of variables that develop over time, we can use sparklines as efficient summaries of multiple individual trends. A sparkline is a small graph the size of a single word that can be seamlessly incorporated into text

The use of must in the seventeenth century is marked by a large amount of fluctuation .

In addition to the overall trend, the sparkline can provide information about the minimum and the maximum value (solid points above and below the line). It is thus an informationally rich form of display that takes incomparably smaller space than if we were to describe the data in words. The sparkline in the example above displays 100 different sampling points.

An alternative to multiple sparklines is a candlestick plot. A candlestick plot is a type of data visualization, where the development of a linguistic In addition, the colour of the box (filled vs unfilled) shows the direction of the change: a filled box shows decrease, unfilled increase. In Figure

In sum, the analysis of historical language data presents a challenge, which, in addition to the usual methodological considerations in corpus research, has special requirements connected with the diachronic dimension. Effective visualization using line charts, boxplots, error bars, sparklines, candlestick plots etc. is a useful starting point for any diachronic investigation. Broadly speaking, the most important question that we have to deal with when comparing two corpora is whether the frequencies of the relevant variables are significantly larger or smaller in corpus 1 than in corpus 2. In addition, when dealing with diachronic comparisons, we need to assess whether the observed differences in frequencies are related to change in the discourse/language over time or whether these are related to other sources of variation.

To be able to see the difference between the two sampling points a simple percentage increase/decrease was calculated. Percentage increase/decrease is a statistic that indicates by how many percentage points the value of a particular linguistic variable increased or decreased between two time periods. It is calculated using the equation below. where corpus 1 is a corpus from an earlier period and corpus 2 is a corpus from a later period. In Table

In this section, we will focus on the bootsrapping test proposed in

In practice, when performing the bootstrap test, we need to trace the distribution of the linguistic variable in individual texts and normalize the frequenciesthat is, we need to get relative frequencies (see Section 2.3). An example of such a dataset is provided in Figure

Table

(i) Standard deviation, coefficient of variation, Pearson's correlation, corrected means ratio

The dendrogram based on Table

Reporting Statistics: Variability-Based Neighbour Clustering (VNC)

What to Report

VNC is largely an exploratory visual method to show diachronic segmentation of the data. Two parameters are used for cluster identification: (i) the distance measure and (ii) the amalgamation rule as well as the results of the analysis, the dendrogram (tree plot), need to be reported. The main question to be addressed is: how many meaningful clusters representing continuous historical periods can be observed in the plot? A scree plot can be used to help determine the answer to this question.

How to Report: An Example

• The data was analysed using the variability-based neighbour clustering technique

1. Obligatory: Obtaining (relative) frequencies from the corpus of the linguistic variable of interest for each of the periods (e.g. years, decades etc.) covered by the analysis. 2. Optional: Computing differences between two consecutive values by taking value 2 and subtracting value 1; this is done to highlight high values preceded by low values (or vice versa) which indicate a more dramatic change. 3. Optional: Transformation of the values using binary logarithm (log2) to reduce extremes. This step is possible only if all transformed values are positive numbers because logarithm is not defined for negative numbers. Since step 2 typically produces also negative values, logarithmic transformation is possible with data from step 1. 4. Obligatory: Fitting a non-linear regression model (displayed as a curve in the graph), computing 95% and 99% confidence intervals (displayed as shaded areas around the curve) and identification of significant outliersdata points outside the confidence interval area.

As an example, let us look at the data from the 'Think about' task, which trace the occurrence of the term war used in The Times newspaper. Both optional and obligatory steps of the peaks and troughs procedure are shown in what follows.

1. Obligatory: For illustration, Table

1. Identification of collocates of a word of interest (node) across the time-series data. 2. Recursive estimation of the difference between collocates at any two consecutive points in time. 3. Use of the peaks and troughs technique (see above) to trace the points where major changes take place; these are identified as significant troughs because these are the largest points of meaning dissimilarity.

As an example, let's use the data from the 'Think about' task again. For UFA, instead of looking at the frequencies of war, we'll analyse the collocates of war in The Times for each period.

1. Table

. 2009 Collocates

periods are with respect to the collocates used. For example, over the period 1940-2009, 171 collocates of the node war were identified. In 1940, 19 of these collocates co-occurred with war (and 152 didn't), while in 1941, 23 of these collocates co-occurred with war, 15 of which were the same as in 1940 and 8 of which were newly introduced. This means that the collocate profiles in 1940 and 1941 overlapped in 92.98% of cases (out of 171) with AC 1 = 0.91. 3. Finally, the peaks and troughs technique is applied. Instead of frequencies, we use the inter-rater agreement index AC 1 and look for points with the lowest AC 1 scores as the points of meaning divergence. Figure

In Figure

Reporting Statistics: Peaks and Troughs and UFA

What to Report

For peaks and troughs, which has two obligatory and two optional steps, we need to report all the steps which were followed. For UFA, we need to report the details of the procedure for the identification of collocates (using collocate parameter notation described in Section 3.2) as well as the choice of the interrater agreement statistic.

How to Report: An Example

• The peaks and troughs technique was used (Gabrielatos & Marchi 2012);  the difference between the relative frequencies of the word war between consecutive points in time during the period of 1940-2009 was measured. The non-linear regression model (GAM) helped to identify four peaks

7.6

Application: Colours in the Seventeenth Century I'm sitting at my desk, the weather outside is miserablegrey and rainy. Through the window, I can see the mediaeval Lancaster castle on a hill, the Priory and the valley of the river Lune, all in a foggy mist; buildings, trees and people have the same indistinct colour. Only a flag with the typical Lancaster shade of red is flying from the castle like a drop of paint which an artist has left unintentionally in the picture. My thoughts take me back in history and I keep wondering about how colour was perceived in the past. Was it connected with the same associations, sensitivities and cultural frames? On a rainy Lancaster afternoon, I start searching the EEBO corpus. One billion words of early writing provide a unique insight into the use of colour words in the seventeenth century. I scribble notes, produce graphs and pvalues. Here's a special kind of a diary of my journey into the past with the 'corpus time machine':

My explorations, 12 November 2016

Question 1: Which colours were the most popular in the seventeenth century?

The line graph in Figure

Question 2: What is the story behind colour terms in the seventeenth century?

A summary picture can be obtained from a candlestick plot (Figure

Question 4: Does red, the most popular colour, have the same associations throughout the century?

The stable associations (consistent collocates) include nouns such as coral, dragon, flowers, iron, rose, roses, sea and wine as well as adjectives (mostly other colour terms): black, green, hot, scarlet, white and yellow. UFA (see Figure

• Visualization options include line graphs, boxplots and error bars, sparklines and candlestick plots.

• The bootstrapping test is used to compare two corpora (representing different points in time); it makes use of a technique of multiple resampling of corpus data.

• Peaks and troughs is a technique which fits a non-linear regression to historical data, producing a graph which highlights significant outliers in the process of historical development of language and discourse.

• UFA (Usage Fluctuation Analysis) is a complex procedure combining automatic collo- cation comparison in a given historical period and the peaks and troughs technique. This is the final chapter of the book; it is about bringing things together on different levels. First, it brings together the statistical knowledge discussed in this book and highlights ten key principles of statistical thinking applied to corpora. Next, the chapter introduces a statistical technique called metaanalysis. Meta-analysis is a way of bringing together results of multiple studies and combining them systematically. In this way, meta-analysis contributes to a better understanding of research results in our field. Unlike a standard narrative-form literature review, which typically considers individual studies in isolation, meta-analysis can combine results from multiple studies into a single mathematical synthesis. Although formal meta-analysis is now fairly common in a number of disciplines such as psychology, second language acquisition, medical science etc., its application in corpus linguistics has been problematic due to the general lack of reporting of effect size measures. This chapter argues in favour of standardized reporting of effect sizes in corpus research and shows how meta-analysis can be carried out. Finally, the chapter reviews common effect size measures and provides a guide for their interpretation.

In this chapter, we'll be exploring the answers to three questions:

• What are the key principles of statistical thinking in corpus linguistics? (Section 8.2) • How can we synthesize findings from multiple studies? (Section 8.3) • How should effect sizes be interpreted? (Section 8.

The devil, as the saying goes, is always in the detail. While the focus in statistical textbooks and in the field in general is on statistical techniques, interpretations of p-values etc., low-level operations such as getting data from a corpus tool into a spreadsheet and then into a statistical package often remain in the background. Nevertheless, these operations are equally important for obtaining reliable results: statistics, as we already know, is a discipline concerned not only with data analysis but also with systematic and reliable collection of data (see Section 1.2). If carefully compared, it is apparent that Tables 8.1 and 8.2 from the 'Think

One of the most important tasks for an analyst is to achieve a very good general understanding of the data. This often involves careful reading through the corpus manual to familiarize oneself with the corpus composition, inspecting the concordance lines to see the actual examples of language use behind the numbers we have obtained and producing overviews and simple graphs that reveal the main tendencies in the dataset. All this is subsumed under descriptive statistics. Without a reliable description of the data, generalizing from the sample (corpus) to the population (language as such) using sophisticated statistical tests and p-values lacks grounding. This makes meaningful interpretation of the results difficult, if not impossible. For example, the dataset from Table

4.

DATA: pay special attention to the quality of the corpus data and search procedures. --------------------------------------------------------------------------------------------------------------------------

The success of any research depends on the quality of the data and the effectiveness of the analytical procedure; yet, especially in corpus research, the quality of corpus data is rarely scrutinized. 'I found this in a corpus and therefore it must be true' is a sentiment which ascribes corpora a certain kind of magical power that they don't have. Even well-established corpora such as the BNC include errors, inconsistencies and possible bias. All corpora thus need to be approached critically. For example,

In computation and data processing, a well-known acronym, GIGO ('Garbage in, garbage out'), is used to remind us that the data needs to be approached critically, otherwise the reliability of the analyses cannot be guaranteed.

5.

EFFECT SIZE: calculate, report and interpret the size of the effect observed in the data. -------------------------------------------------------------------------------------------------------------

It is always important to think about the practical effect of the observations about language we make using corpora. To help us express this aspect of the findings, effect size measures should be used. In broad terms, effect size can be defined as the 'amount of anything that might be of interest'

Let's quickly review a brief example. When comparing two groups such as two subcorpora, the effect size measure Cohen's d is often used (see Section 6.3). Cohen's d for the difference between the use of the past tense in academic writing and mystery fiction (see Table

In addition to being aware of more general statistical principles (see Chapter 1), it is important to know how to apply these principles in the analysis of language corpora. For this, following the best practice in the field is essential. This books 7.

GRAPHICS: visualize data to identify patterns. ---------------------------------------------------

A picture, as is often said, is worth a thousand words. Effective visualization can help us discover important patterns and relationships in data. The basic principle of good visualization is simple: focus on displaying the data rather than simply adding 'visual frills'. Make the display informationally rich to allow relationships between variables to be observed

HIGHLIGHTING BOTH SIMILARITIES AND DIFFERENCES:

provide a balanced account of language use. ------------------------------------------------------

It is no surprise that when we look for differences between corpora or between various uses of a word or phrase, we usually find some. Most of the methodological thinking (not only in corpus linguistics but other disciplines as well) has been biased towards looking for differences and ignoring similarities. Similarities and null effects (effects that are small and/or not statistically significant) are thus often underreported in the literature (see the discussion of publication bias in Section 8.3). In corpus linguistics, the focus on differences, for example, leads us to reporting keywords and disregarding lockwords, words which are stable across corpora

9.

INTERPLAY BETWEEN STATISTICS AND LINGUISTICS: provide robust statistical analysis that is grounded in linguistic and social theory. ----------------------------------------------------------------------------------------------------------------------------------------------------------

Let's consider a methodological question. Which of these is worse in corpus research: linguistics without statistics or statistics without linguistics? Arguably, each of these hypothetical options fails in one fundamental respect. Linguistics without statistics lacks effective tools for analysing large quantities of language data, while statistics without linguistics can easily turn into a mindless exercise in number crunching without a connection to linguistic and social reality. We have to remember that not all linguistic research is quantitativequalitative research can bring important insights into the use of language; however, the choice of texts and examples for in-depth analysis raises the question of how typical these are and why we selected them (see

10. JARGON: use statistical terminology and notation where it helps express things clearly, but try to avoid unnecessary jargon. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------For many people, statistics can be opaque. In statistics, we often try to capture complex relationships in data using mathematical expressions, which themselves might put many people off. Throughout this book, where possible, mathematical symbolism has been replaced by glosses in plain English; for instance, X 10 i¼1 i would be expressed as 'the sum of all integers from 1 to 10' (when you work out the maths both 'renditions' will give you the number 55). However, when reporting on the results of statistical analyses it is important to provide these in a standardardized form; again, this book shows how this can be done in the 'Reporting statistics' boxes.

8.3

Meta-analysis: Statistical Synthesis of Research Results Think about . . . Before reading this section, think about the following situation.

You are in a large city such as London and are looking for a theatre, say the replica of Shakespeare's Globe. You don't have a map so you have to rely on the information from passers-by. Table

The example situation in the 'Think about' task can be understood as a metaphor for the scientific pursuit. To find out the answer to our research question we go out to collect and evaluate data. In science, this is done repeatedly to ensure the reliability of the answer. The process of repeating a study with the same research question but a different dataset is called replication. In our example, replication is demonstrated with asking different passers-by the same question. And as in our example, in science we often get (slightly) different answers from different studies. What is, however, important for the development of our field as a whole is to be able to see the larger picture; we therefore need to put individual findings together and make sense of them globally. The statistical technique that does this is called meta-analysis. Meta-analysis is a quantitative procedure of statistical  a person walking a dog synthesis of research results, which is based on combining the effects reported in multiple individual studies and calculating the summary effect

Step 1.

Identification of relevant studies --------------------------------------------------------------------------------------------This step defines the area of research which we wish to investigate in the metastudy. The area is given by a research question that is answered repeatedly in multiple studies using different corpora. Of course, the research question can be broader or more specific thus defining the granularity of a meta-study. For example, a very broad question would be: Is there an effect of gender on the use of language? Guided by this research question, we would be able to identify hundreds of relevant studies but the answer would probably be very broad and non-specific: some linguistic features show gender-based patterns, while others don't; this can be quantified as a very broad summary effect. If, on the other hand, we define the area of research using a specific research question, e.g. Is there an effect of gender on the use of pronouns?, we can tap into something more specific and arguably more interesting; however, the number of relevant studies will be considerably smaller. In practical terms, it is important to explicitly specify inclusion criteria for the studies such as what linguistic and what explanatory variables we are looking for, requirements for research design and relevant time frame for the studies, e.g. studies published since 1990

Once we define the area of research, we need to search all relevant journals and available databases (e.g. Google scholar, Linguistics and Language Behavior Abstracts, ProQuest Dissertations & Theses, EThOS etc.) for studies, both published and unpublished, that address the research question of interest and meet the inclusion criteria. The aim is to collect all available evidence related to the research question. The reason to also consider quality unpublished studies (e.g. PhD theses) is the fact that we want to reduce the effect of a so-called publication bias. Publication bias is a well-documented phenomenon (e.g.

Step 2.

Extraction of relevant pieces of information from the studies (coding) --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The second step involves obtaining relevant data from the studies we identified in step 1. This involves carefully reading through the method and results sections of the research reports, articles and books and noting down information about (i) the (sub)corpora used, (ii) the method and (iii) the effect sizes observed in the study

Because in many cases the information about different aspects of the study needs to be inferred or extrapolated from the report and this might involve an element of subjectivity, it is good practice to double code a portion of the data (e.g. 20%) and calculate an inter-coder agreement statistic (see

Let us now have a look at an example of five studies reporting the effect of gender on the use of pronouns (I, me, my, you, your etc.). Table

Step 3.

Statistical synthesis -------------------------------------------------------------------------------------------------------------------------------------

The final step of meta-analysis puts the information from the individual studies together and produces a report of the resulting (combined) effect size and a confidence interval. In its simplest form, the statistical synthesis takes as input the effect size measure (here Cohen's d) and the number of cases (texts/speakers) in each subcorpus (see Table

For example, the inverse variance for the first study

The forest plot provides a summary of a meta-analysis by displaying the effect sizes of individual studies (filled squares) and their confidence intervals (whiskers) as well as the overall effect size (diamond); the size of the square represents the weight of an individual study in the analysis. studies were weighted by inverse variance and the random-effect model was used. The procedure identified a summary effect d = .47 with the 95% CI

Effect

This example demonstrates the basic thinking about effects and effect sizes that we can also apply to linguistic data. ES measures, as we know, quantify the observed linguistic variables and the differences and changes in their frequencies. It is important to gain a good understanding of what exactly the measures mean and how to interpret them. In our half marathon example, we can show easily what we mean by one metre or one yard by drawing a line that represents the unit of measurement. However, we need to realize that the decision about the practical importance of the effect has little to do with statistics: in our example, it is our personal decision about whether a 38-second improvement in a half marathon is worth the training time and energy. To make an informed decision, the 38 seconds need to be contextualized and compared with the times of other runners etc.

This section focuses on the use of effect size (ES) measures in corpus linguistics. The concept of effect size was introduced in Chapter 1 and different effect size measures have been discussed throughout this book. Table

To address the second question (how to interpret ES measures?), we need to gain a good understanding of how a particular ES measure works in practice. In textbooks, ES measures are usually ascribed standard interpretations, typically based on

To see the practical impact of corpus linguistic effects expressed as d, r, η 2 (r 2 ), Table

Exercises

1. What is the most important thing you have learnt from this book? Write this down in the space below.

2. Provide transformations of the effect size measures in Table

Final Remarks

Statistics is a powerful analytical tool. This book has demonstrated a number of different ways in which statistical techniques can be used to explore corpora. The robust evidence found in these electronic collections of language offers countless possibilities for both linguistic and social research providing a unique insight into patterns of language use. Statistics, if applied appropriately, can facilitate the process of analysis by serving as a zoom lens through which we can observe the linguistic reality: the details of individual examples of language use as well as the larger picture of grammar, vocabulary and discourse. We need to remember, however, that the lens should always be a transparent one: what we want to observe is not the tools themselvesa showcase full of sophisticated statistical techniquesbut the linguistic data. Our analysis thus should always be primarily focused on the data and should take data seriously; if our beliefs and theories are contradicted by the data we shouldn't simply dismiss the data as 'inconvenient' evidence (or hide it behind complex statistical jargon) but, on the contrary, we should engage with it, seeking to genuinely understand and explain the findings. Only in this way can our investigation be meaningful and truly scientific. Mastery of statistics is empowering. However, as statistical tools and analyses become more complex and sophisticated, they can also become rather daunting for the users. This is because statistical analysis involves many choices. Among other things, we need to select a suitable corpus, an effective analytical technique and an appropriate interpretation of the results. These decisions can often feel challenging especially for novice researchers. The growing demand in corpus linguistics for statistical sophistication and the lack of appropriate resources for beginner and intermediate users of statistics can thus easily lead to frustration. This book (together with Lancaster Stats Tools online) hopes to be a resource addressing this issue by offering readers a guide for making informed choices about statistics in language analysis. The main message of the volume is twofold. First, statistics is not about number crunching or remembering equations (computers are much better at these tasks than humans) but about understanding core, underlying principles of quantitative analysis. Second, I would like to encourage the readers not to let themselves be overwhelmed by the complex statistical techniques or the newest fads on the statistical marketplace. Every summer, a large number of students from different parts of the world come to Lancaster to learn about corpora and statistics during a week of Lancaster summer schools in corpus linguistics. These students often ask me what the best statistical test is to use with corpora, what the best collocation measure is etc. I usually respond: in many cases, the most powerful statistical technique is common sense.

284

Final Remarks