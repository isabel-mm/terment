Representational format of corpus files and data frames 1 Introduction

Why Another Introduction to Corpus Linguistics?

In some sense at least, this book is an introduction to corpus linguistics. If you are a little familiar with the field, this probably immediately triggers the question "Why yet another introduction to corpus linguistics?" This is a valid question because, given the upsurge of studies using corpus data in linguistics, there are also already quite a few very good introductions available. Do we really need another one? Predictably, I think the answer is still "yes" and "yes, even a second edition," and the reason is that this introduction is radically different from every other introduction to corpus linguistics out there. For example, there are a lot of things that are regularly dealt with at length in introductions to corpus linguistics that I will not talk about much:

• the history of corpus linguistics: Kaeding, Fries, early 1m word corpora, up to the contemporary giga corpora and the still lively web-as-corpus discussion; • how to compile corpora: size, sampling, balancedness, representativity;

• how to create corpus markup and annotation: lemmatization, tagging, parsing;

• kinds and examples of corpora: synchronic vs. diachronic, annotated vs. unannotated;

• what kinds of corpus-linguistic research have been done.

That is to say, rather than telling you about the discipline of corpus linguistics -its history, its place in linguistics, its contributions to different fields, etc. -with this book, I will 'only' teach you how to do corpus-linguistic data processing with the programming language R (see

• aspects of how exactly data are retrieved from corpora to be used in linguistically informed analyses, specifically how to obtain from corpora frequency lists, dispersion information, collocation displays, concordances, etc. (see Chapter 2 for explanation and exemplification of these terms); • aspects of data manipulation and evaluation: how to process and convert corpus data; how to save various kinds of results; how to import them into a spreadsheet program for further annotation; how to analyze results statistically; how to represent the results graphically; and how to report your results.

A second important characteristic of this book is that it only uses freely available software:

• R, the corpus linguist's all-purpose tool (cf. R Core Team 2016): a software which is a calculator, a statistics program, a (statistical) graphics program, and a programming language at the same time. The versions used in this book are R (www.r-project.org) and the freely available Microsoft R Open 3.3.1 (

The choice of these software tools, especially the decision to use R, has a number of important implications, which should be mentioned early on. As I just mentioned, R is a full-fledged multi-purpose programming language and, thus, a very powerful tool. However, this degree of power does come at a cost: In the beginning, it is undoubtedly more difficult to do things with R than with ready-made (free or commercial) concordancing software that has been written specifically for corpus-linguistic applications. For example, if you want to generate a frequency list of a corpus or a concordance of a word in a corpus with R, you must write a small script or a little bit of code in a programming language, which is the technical way of saying you write lines of text that are instructions to R. If you do not need pretty output, this script may consist of just a few lines, but it will often also be longer than that. On the other hand, if you have a ready-made concordancer, you click a few buttons (and enter a search term) to get the job done. One may therefore ask why go through the trouble of learning R? There is a variety of very good reasons for this, some of them related to corpus linguistics, some more general.

First, let me address this very argument, which is often made against using R (or other programming languages): why use a lot of time and effort to learn a programming language if you can get results from ready-made software within minutes? With regard to the time that goes into learning R, yes, there is a learning curve. However, that time may not be as long as you think: Many participants in my bootcamps and other workshops develop a first good understanding of R that allows them to begin to proceed on their own within just a few days. Plus, being able to program is an extremely useful skill for academic purposes, but also for jobs outside of academia; I would go so far as to say that learning to program is extremely useful in how it develops, or hones, a particular way of analytical and rigorous thinking that is useful in general. With regard to the time that goes into writing a script, much of that usually needs to be undertaken only once. As you will see below, once you have written your first few scripts while going through this book, you can usually reuse (parts of) them for many different tasks and corpora, and the amount of time that is required to perform a particular task becomes very similar to that of using a ready-made program. In fact, nearly all corpus-linguistic tasks in my own research are done with (somewhat adjusted) scripts or small snippets of code from this book. In addition, once you explore how to write your own functions (see Section 3.10), you can easily write your own versatile or specialized functions yourself; I will make several of those available in subsequent chapters. This way, the actual effort of generating a frequency list, a collocate display, a dispersion plot, etc. often reduces to about the time you need with a concordance program. In fact, R may even be faster than competing applications: For example, some concordance programs read in the corpus files once before they are processed and then again for performing the actual task -R requires only one pass and may, therefore, outperform some competitors in terms of processing time.

Another point related to the notion that programming knowledge is useful: The knowledge you will acquire by working through this book is quite general, and I mean that in a good way. This is because you will not be restricted to just one particular software application (or even one version of one particular software application) and its restricted set of features. Rather, you will acquire knowledge of a programming language and regular expressions which will allow you to use many different utilities and to understand scripts in other programming languages, such as Perl or Python. (At the same time, I think R is simpler than Perl or Python, but can also interface with them via RSPerl and RSPython, respectively; see www.omegahat.org.) For example, if you ever come across scripts by other people or decide to turn to these languages yourself, you will benefit from knowing R in a way that no ready-made concordancing software would allow for. If you are already a bit familiar with corpus-linguistic work, you may now think "but why turn to R and not use Perl or Python (especially since you say Perl and Python are similar anyway and many people already use one of these languages)?" This is a good question, and I myself used Perl for corpus processing before I turned to R. However, I think I also have a good answer to why to use R instead. First, the issue of speed is much less of a problem than one may think. R is fast enough and stable enough for most applications (especially if you heed some of the advice given in Sections 3.6.3 and 3.10). Thus, if a script takes a bit of time, you can simply run it over lunch, while you are in class, or even overnight and collect the results afterwards. Second, R has other advantages. The main one is probably that, in addition text-processing capabilities, R offers a large number of ready-made functions for the statistical evaluation and graphical representation of data, which allows you to perform just about all corpus-linguistic tasks within only one programming environment. You can do your data processing, data retrieval, annotation, statistical evaluation, graphical representation . . . everything within just one environment, whereas if you wanted to do all these things in Perl or Python, you would require a huge amount of separate programming. Consider a very simple example: R has a function called table that generates a frequency table. To perform the same in Perl you would either have to have a small loop counting elements in an array and in a stepwise fashion increment their frequencies in a hash or, later and more cleverly, program a subroutine which you would then always call upon. While this is no problem with a one-dimensional frequency list, this is much harder with multidimensional frequency tables: Perl's arrays of arrays or hashes of arrays etc. are not for the faint-hearted, whereas R's table is easy to handle, and additional functions (table, xtabs, ftable, etc.) allow you to handle such tables very easily. I believe learning one environment can be sufficiently hard for beginners, and therefore recommend using the more comprehensive environment with the greater number of simpler functions, which to me clearly is R. And, once you have mastered the fundamentals of R and face situations in which you need maximal computational power, switching to Perl or Python in a limited number of cases will be easier for you anyway, especially since much of the programming languages' syntaxes is similar and the regular expressions used in this book are all Perl compatible. (Let me tell you, though, that in all my years using R, there were a mere two instances where I had to switch to Perl and that was only because I didn't yet know how to solve a particular problem in R.)

Second, by learning to do your analyses with a programming language, you usually have more control over what you are actually doing: Different concordance programs have different settings or different ways of handling searches that are not always obvious to the (inexperienced) user. For instance, ready-made concordance tools often have slightly different settings that specify what 'a word' is, which means you can get different results if you have different programs perform the same search on the same corpus. Yes, those settings can usually be tweaked, but that means that, actually, such a ready-made application requires the same attention to detail as R, and with a programming language all of your methodological choices are right there in the code for everyone to see and replicate.

Third, if you use a particular concordancing software, you are at the mercy of its developer. If the developers change its behavior, its results output, or its default settings, you can only hope that this is documented well and/or does not affect your results. There have been cases where even silent over-the-internet updates have changed the output of such software from one day to the next. Worse, developers might even discontinue the development of a tool altogether -and let us not even consider how sorry the state of the discipline of corpus linguistics would be if a majority of its practitioners was dependent on not even a handful of ready-made corpus tools and websites that allow you to search a corpus online. Somewhat polemically speaking, being able to enter a URL and type in a search word shouldn't make you a corpus linguist.

The fourth and maybe most important reason for learning a programming language such as R is that a programming language is a much more versatile tool than any readymade software application. For instance, many ready-made corpus tools can only offer the functionality they aim to provide for corpora with particular formats, and then can only provide a small number of kinds of output. R, as a programming language, can handle pretty much any input and can generate pretty much any output you want -in fact, in my bootcamps, I tell participants on day 1 that I don't want to hear any questions that begin with "Can R . . . ?" because the answer is "Yes". For instance, with R you can readily use Chapter 3 introduces the fundamentals of R, covering a variety of functions from different domains, but the area which receives most consideration is that of text processing. There are many small changes in the code and the examples (for instance, I now introduce free-spacing), but the main differences to the first edition consist of:

Chapter 4 is what used to be Chapter 5 in the first edition. It introduces you to some fundamental aspects of statistical thinking and testing. The questions to be covered in this chapter include: What are hypotheses? How do I check whether my results are noteworthy? How might I visualize results? Given considerations of space and focus, this chapter is informative, I hope, but still short.

The main chapter of this edition, Chapter 5, is brand new and, in a sense, brings it all together: More than 30 case studies in 27 sections illustrate various aspects of how the methods introduced in Chapters 3 and 4 can be applied to corpus data. Using a variety of different kinds of corpora, corpus-derived data, and other data, you will learn in detail how to write your own programs in R for corpus-linguistic analyses, text processing, and some statistical analysis and visualization in detailed step-by-step instructions. Every single analysis is discussed on multiple levels of abstraction and altogether more than 6,000 lines of code, nearly every one of them commented, help you delve deeply into how powerful a tool R can be for your work.

Finally, Chapter 6 is a very brief conclusion that points you to a handful of useful R packages that you might consider exploring next.

Before we begin, a few short comments on the nature of this book are necessary. This book is kind of a sister publication to my introduction to statistics for linguists

On the other hand, this book is an attempt to teach you a lot about how to be a good corpus linguist. As a good corpus linguist, you have to combine many different methodological skills (and many equally important analytical skills that I will not be concerned with here). Many of these methodological skills are addressed here, such as some very basic knowledge of computers (operating systems, file types, etc.), data management, regular expressions, some elementary programming skills, some elementary knowledge of statistics, etc. What you must know, therefore, is that (1) nobody has ever learned all of this just by reading -you must do things -and (2) this is not an easy book that you can read for ten minutes at a time in bed before you fall asleep. What these two things mean is that you really must read this book while you are sitting at your computer so you can run the code, see what it does, and work on the examples. This is particularly important because the code file from the companion website contains more than 6,500 lines of code and a huge amount of extra commentary to help you understand the code much better than you can understand it from just reading the book; this is particularly relevant for Chapter 5! You will need practice to master all the concepts introduced here, but will be rewarded by acquiring skills that give you access to a variety of data and approaches you may not have considered accessible to you -at least that's what happened to me when I at one point decided to leave behind the ready-made tools I had become used to. Undergrads in my corpus classes without prior programming experience have quickly learned to write small programs that do things better than many concordance software, and you can do the same.

In order to facilitate your learning process, there are four different ways in which I try to help you get more out of this book. First, there are small Think Breaks. These are small assignments which you should try to complete before you read on; answers to them follow immediately in the text. Second, there are exercise boxes with small assignments. Ideally, you should complete these and check your answers in the answer key before you read any further, but it is not always necessary to complete them right away to understand what follows, so you can also return to them later at your own leisure. Third, there are many boxes with recommendations for further study/exploration, which typically mention functions that you do not need for the section in which they are mentioned the first time, but many are used at a later stage (often this will be

• you can send questions about corpus linguistics with R to the list and, hopefully, get useful responses from some kind soul(s); • post suggestions for revisions of this book there;

• inform me and the other readers of errors you find and, of course, be informed when other people or I find errata.

Thus, while this is not an easy book, I hope these aids help you to become a good corpus linguist. If you work through the whole book, you will be able to do a large number of things you could not even do with commercial concordancing software; many of the scripts you find here are taken from actual research, and are in fact simplified versions of scripts I have used myself for published papers. In addition, if you also take up the many recommendations for further exploration that are scattered throughout the book, you will probably find ever new and more efficient ways of application.

• "Machine-readable" refers to the fact that nowadays virtually all corpora are stored in the form of plain ASCII or Unicode text files that can be loaded, manipulated, and processed platform-independently. This does not mean, however, that corpus linguists only deal with raw text files -quite the contrary: some corpora are shipped with sophisticated retrieval software that makes it possible to look for precisely defined lexical, syntactic, or other patterns. It does mean, however, that you would have a hard time finding corpora on paper, in the form of punch cards or digitally in HTML or Microsoft Word document formats; the probably most widely used format consists of text files with a Unicode UTF-8 encoding and XML annotation. • "Produced in a natural communicative setting" means that the texts were spoken or written for some authentic communicative purpose, but not for the purpose of putting them into a corpus. For example, many corpora consist to a large degree of newspaper articles. These meet the criterion of having been produced in a natural setting because journalists write the article to be published in newspapers and to communicate something to their readers, not because they want to fill a linguist's corpus. Similarly, if I obtained permission to record all of a particular person's conversations in one week, then hopefully, while the person and his interlocutors usually are aware of their conversations being recorded, I will obtain authentic conversations rather than conversations produced only for the sake of my corpus. • I use "representative

It is useful to point out, however, that the above definition of a corpus is perhaps the prototype, which implies that there are many other corpora that differ from the prototype and other kinds of corpora along a variety of dimensions. For instance, the TIMIT Acoustic-Phonetic Continuous Speech Corpus is made up of audio recordings of 630 speakers of eight major dialects of American English, where each speaker read phonetically rich sentences, a setting which is not exactly a natural communicative setting. Or consider the DCIEM Map Task Corpus, which consists of unscripted dialogs in which one interlocutor describes a route on a map to the other after both interlocutors were subjected to 60 hours of sleep deprivation and one of three drug treatments -again, hardly a normal situation. Even a genre as widely used as newspaper text -journaleseis not necessarily close to being a prototypical corpus, given how newspaper writing is created much more deliberately and consciously than many other texts -plus they often come with linguistically arbitrary restrictions regarding their length, are often not

The Four Central Corpus-Linguistic Methods 9

written by a single person, and are heavily edited, etc. Thus, the notion of corpus is really a rather diverse one. Many people would prefer to consider newspaper data not corpora, but text archives. Those would be databases of texts which • may not have been produced in a natural setting; • have often not been compiled for the purposes of linguistic analysis; and • have often not been intended to be representative and/or balanced with respect to a particular linguistic variety or speech community.

As the above discussion already indicated, however, the distinction between corpora and text archives is often blurred. It is theoretically easy to make, but in practice often not adhered to very strictly and, again, has very few implications for the kinds of (R) programming they require. For example, if a publisher of a popular computing periodical makes all the issues of the previous year available on their website, then the first criterion is met, but not the last three. However, because of their availability and size, many corpus linguists use them as resources, and as long as one bears their limitations in mind in terms of representativity etc., there is little reason not to.

Finally, an example collection is just what the name says it is -a collection of examples that, typically, the person who compiled the examples came across and noted down. For example, much psycholinguistic research in the 1970s was based on collections of speech errors compiled by the researchers themselves and/or their helpers. Occasionally, people refer to such collections as error corpora, but we will not use the term corpus for these. It is easy to see how such collections compare to corpora. On the one hand, for example, some errors -while occurring frequently in authentic speech -are more difficult to perceive than others and thus hardly ever make it into a collection. This would be an analog to the balancedness problem outlined above. On the other hand, the perception of errors is contingent on the acuity of the researcher while, with corpus research, the corpus compilation would not be contingent on a particular person's perceptual skills. Finally, because of the scarcity of speech errors, usually all speech errors perceived (in a particular amount of time) are included into the corpus, whereas, at least usually and ideally, corpus compilers are more picky and select the material to be included with an eye to the criteria of representativity and balancedness outlined above. 1 Be that as it may, if only for the sake of terminological clarity, it is useful to distinguish the notions of corpora, text archives, and example collections.

What Kinds of Corpora Are There?

Corpora differ in a variety of ways. There are a few distinctions you should be familiar with if only to be able to find the right corpus for what you want to investigate. The most basic distinction is that between general corpora and specific corpora. The former intend to be representative and balanced for a language as a whole -within the above-mentioned limits, that is -while the latter are by design restricted to a particular variety, register, genre, etc.

Another important distinction is that between raw corpora and annotated corpora. Raw corpora consist of files only containing the corpus material (see

The second part is called the body and contains the corpus data proper -i.e., what people actually said or wrote -as well as linguistic information that is usually based on some linguistic theory: Parts of speech or syntactic patterns, for example, can be matters of debate. In what follows I will briefly (and non-exhaustively!) discuss and exemplify a few common annotation schemes (see

First, a corpus may be lemmatized such that each word in the corpus is followed (or preceded) by its lemma, i.e., the form under which you would look it up in a dictionary (see

(1) I did get a postcard from him.

(2) I_I did_do get_get a_a postcard_postcard from_from him_he._punct (3) I<PersPron> did<VerbPast> get<VerbInf> a<Det> postcard<NounSing> from<Prep> him<PersPron>.<punct> (4) [@:]•I•^did•get•a•!p\ostcard•fr/om•him#•-•-

Then, there is a difference between diachronic corpora and synchronic corpora. The former aim at representing how a language/variety changes over time, while the latter provide, so to speak, a snapshot of a language/variety at one particular point in time. Yet another distinction is that between monolingual corpora and parallel corpora. As you might already guess from the names, the former have been compiled to provide information about one particular language/variety, whereas the latter ideally provide the same text in several different languages. Examples include translations from EU Parliament debates into the 23 languages of the European Union, or the Canadian Hansard corpus, containing Canadian Parliament debates in English and French. Again, ideally, a parallel corpus does not just have the translations in different languages, but has the translations sentence-aligned, such that for every sentence in language L 1 , you can automatically retrieve its translation in the languages L 2 to L n .

The next distinction to be mentioned here is that of static corpora vs. dynamic/monitor corpora. Static corpora have a fixed size (e.g., the Brown corpus, the LOB corpus, the British National Corpus), whereas dynamic corpora do not since they may be constantly extended with new material (e.g., the Bank of English).

The final distinction I would like to mention at least briefly involves the encoding of the corpus files. Given especially the predominance of work on English in corpus linguistics, until rather recently many corpora came in the so-called ASCII (American Standard Code for Information Interchange) character encoding, an encoding scheme that encodes 2 7 = 128 characters as numbers and that is largely based on the Western alphabet. With these characters, special characters that were not part of the ASCII character inventory were often paraphrased, e.g., "é" was paraphrased as "&eacute;". However, the number of corpora for many more languages has been increasing steadily, and given the large number of characters that writing systems such as Chinese have, this is not a practical approach. As such, language-specific character encodings were developed (e.g., ISO 8859-1 for Western European Languages vs. ISO 2022 for Chinese/Japanese/Korean languages). However, in the interest of overcoming compatibility problems that arose due to how different languages used different character encodings, the field of corpus linguistics has been moving towards using only one unified (i.e., not language-specific) multilingual character encoding in the form of Unicode (most notably UTF-8). This development is in tandem with the move toward XML corpus annotation and, more generally, UTF-8 becoming the most widely used character encoding on the internet. Now that you know a bit about the kinds of corpora that exist, there is one other really important point to be made. While we will see below that corpus linguistics has a lot to offer to the analyst, it is worth pointing out that, strictly speaking at least, the only thing corpora can provide is information on frequencies. Put differently, there is no meaning in corpora, and no functions, only:

• frequencies of occurrence of items -i.e., how often do morphemes, words, grammatical patterns, etc. occur in (parts of) a corpus?; and • frequencies of co-occurrence of items -i.e., how often do morphemes occur with particular words? How often do particular words occur in a certain grammatical construction? etc.

It is up to the researcher to interpret these frequencies of occurrence and co-occurrence in meaningful or functional terms. The assumption underlying basically all corpus-based analyses, however, is that formal differences reflect functional differences: Different frequencies of (co-)occurrences of formal elements are supposed to reflect functional regularities, where functional is understood here in a very broad sense as anything -be it semantic, discourse-pragmatic, etc. -that is intended to perform a particular communicative function. On a very general level, the frequency information a corpus offers is exploited in four different ways, which will be the subject of this chapter: frequency lists (Section 2.2), dispersion (Section 2.3), lexical co-occurrence lists/collocations (Section 2.4), and concordances (Section 2.5).

Frequency Lists

The most basic corpus-linguistic tool is the frequency list. You generate a frequency list when you want to know how often something -usually words -occur in a corpus. Thus, a frequency list of a corpus is usually a two-column table with all words occurring in the corpus in one column and the frequency with which they occur in the corpus in the other column. Since the notion of word is a little ambiguous here, it is useful to introduce a common distinction between (word) type and (word) token. The string "the word and the phrase" contains five (word) tokens ("the", "word", "and", "the", and "phrase"), but only four (word) types ("the", "word", "and", and "phrase"), of which one ("the") occurs twice. In this parlance, a frequency list lists the types in one column and their token frequencies in the other; often you will find the expression type frequency referring to the number of different types attested in a corpus (or in a 'slot' such as a syntactically defined slot in a grammatical construction). Typically, one out of three different sorting styles is used: frequency order (ascending or, more typically, descending; see the left panel of Table

Apart from this simple form in the leftmost panel, there are other varieties of frequency lists that are sometimes found. First, a frequency list may provide the frequencies of all words together with the words with their letters reversed. This may not seem particularly useful at first, but even a brief look at the second panel of Table

Frequency lists are sometimes more problematic than they seem because they presuppose that the linguist (and/or his computer program) has a definition of what a word is and that this definition is shared by other linguists (and their computer programs). This need not be the case, however, as the following exercise will demonstrate. Bear in mind, therefore, that you need to exercise at least some caution in comparing frequency lists from different sources. 2 Another noteworthy aspect is that frequency lists are often compiled so as not to include types from a so-called stop list. For example, one can often exclude a variety of frequent function words such as the, of, and, etc., because these are often not particularly revealing given that they occur nearly equally frequently everywhere. 3  For what follows below, it is useful to introduce the distinction between a lemma and a word-form: go, goes, going, went, and gone are all different word-forms but belong to the same lemma, namely go; in the remainder of this chapter I will only use word where the difference between lemma and word-form/token is not relevant. Computer programs normally define word-tokens as a sequence of alphabetic (or alphanumeric) characters uninterrupted by whitespace (i.e., spaces, tabs, and newlines) and allow the user to specify what to do with hyphens, apostrophes, and other special characters.

Frequency lists are useful for a variety of purposes. For example, much contemporary work in usage-based linguistics assumes that the type and token frequencies of linguistic expressions are correlated with the degrees of productivity and cognitive entrenchment of these expressions, and studies such as, e.g.,

In the domain of natural language processing or computational linguistics, the frequency of items is relevant to, among other things, speech recognition. For example, imagine a computer gets ambiguous acoustic input in a noisy environment and tries to recognize which word is manifested in the input. If the computer cannot identify the input straightforwardly, one (simplistic) strategy would be for it to assume that the word it hears is the most frequent one of all those that are compatible with the acoustic input. Another area of interest is, for example, spelling error correction, where frequency lists can be useful in two ways: First, for the computer to recognize that a string is perhaps a typo because it neither occurs in a gold-standard frequency list of words of the input language nor in, say, a list of named entities; second, for the computer to rank suggestions for correction such that the computer first determines a set of words that are sufficiently similar to the user's input and then ranks them according to their similarity to the input and their frequency. From a methodological perspective, frequency lists are useful for computing many co-occurrence statistics. Finally, frequency lists may reflect sociocultural differences.

Dispersion Information

A characteristic of words that is often, but not necessarily, correlated with their frequencies is their dispersion: words that are very similar in terms of their overall frequency in a corpus may be very differently distributed in a corpus. For instance,

Dispersion is often explored visually, but can also be quantified. The left panel of Figure

Lexical Co-occurrence: Collocations

One of the most central methodological concepts in corpus linguistics is that of co-occurrence. Corpus linguists have basically been concerned with three different kinds of co-occurrence phenomena:

1 Collocation: the probabilistic co-occurrence of word-forms such as different from vs. different to vs. different than, or the absolute frozenness of expressions such as kith and kin or by and large. 2 Colligation: the co-occurrence of words with grammatical phenomena such as parts of speech, grammatical relations, or 'definiteness', such as the preference of consequence to occur as a complement (but not an adverbial) and with indefinite articles. 3 (Grammar) patterns or collostructions: the co-occurrence of words/lemmas with morphosyntactic patterns or constructions (in the construction grammar sense) such as the ditransitive construction or the cleft construction such as the preference of to hem to occur in the passive; or the association of the ditransitive to forms of the verb to give.

In this section we will restrict ourselves to collocations because they are a natural extension of frequency lists; later in the book, we will of course deal with other kinds of co-occurrence, too. Collocations are co-occurrences of words, which are then referred to as collocates. Often, one uses the letters L and R (for 'left' and 'right') together with a number indicating the position of one collocate with respect to the main/node word to talk about collocations. For example, if you call up a concordance of the word difference, then you will most certainly find that the most frequent L1 collocate is the while the most frequent R1 collocate is between. Thus, a collocate display for a word tells you which other words occur at different positions around the target word and how frequently. In other words, a collocate display is a list of frequency lists for particular positions around a word. To look at a simple example with two adjectives, consider the two adjectives alphabetic and alphabetical: Note that all this means that a collocate display is read vertically: Row 1 of Table

The difference between the two adjectives can probably be paraphrased easiest by stating what the opposites of the two adjectives are. My suggestion would be that the opposite

THINK BREAK

of alphabetic is numeric, whereas the opposite of alphabetical is unordered, but a more refined look at the data may reveal a more precise picture.

Collocate displays are an important tool within semantics and lexicography (cf.

Another area of application is what has been referred to as semantic prosody, i.e., the fact that collocates of some word w may imbue w with a particular semantic aura even though this aura is not part of the semantics of w proper. One of the standard textbook examples is the English verb to cause. As you probably sense intuitively, to cause primarily, though not exclusively, collocates with negative things

(Lexico-)Grammatical Co-occurrence: Concordances

However useful collocate displays are, for many kinds of analysis they are still not optimal. On the one hand, it is obvious that collocate displays usually provide information on lexical co-occurrence, but the number of grammatical features that is amenable to an investigation by means of collocates alone is limited. On the other hand, even the investigation of lexical co-occurrence by means of collocate displays can be problematic. If you investigate near-synonymous adjectives such as big, great, and large (or deadly, fatal, and lethal) by looking at R1, you reduce both the precision and the recall of your results:

• Precision is defined as the quotient of the number of accurate matches returned by your search divided by the number of all matches returned by your search. The collocate approach may reduce precision because the R1 collocate of big in big shiny tricorder is shiny rather than tricorder, and the inclusion of shiny, while of course accurate, may not tell you as much about the semantics of big as tricorder. 4 • Recall is defined as the number of accurate matches returned by your search divided by the number of all possible accurate matches in the data. The R1 collocate approach may reduce recall because, as we have seen above, you miss tricorder in big shiny tricorder.

The final method to be introduced here addresses this problem, though at a cost. This method, probably the most widespread corpus-linguistic tool, is the concordance. You generate a concordance if you want to know in which (larger and more precise) contexts a particular word is used. Thus, a concordance of a word w is a display of every occurrence of w together with a user-specified context; it is often referred to as KWIC (Key Word In Context) display. 5 This user-specified context is often either the whole sentence in which the word in question occurs (usually with some highlighting or bracketing; see the use of "

While the concordance displays in Table

There was a campaign

Notes

1 It is only fair to mention, however, that (1) error collections have proven extremely useful in spite of what, from a strict corpus linguistic perspective, may be considered shortcomings, and that (2) compilers of corpora of lesser-spoken languages such as typologists investigating languages with few written records suffer from just the same data scarcity problems. 2 One important characteristic of language is reflected in nearly all frequency lists. When you count how often each type occurs in a particular corpus, you usually get a skewed distribution such that 1 a few types -usually short function words -account for the lion's share of all tokens (for example, in the Brown corpus, the ten most frequent types out of approximately all 41,000 different types (i.e., only 0.02 percent of all word types) already account for nearly 24 percent of all tokens); and 2 most tokens occur rather infrequently (for example, 16,000 of the tokens in the Brown corpus occur only once).

These observations are a subset of Zipf's laws, the most famous of which states that the frequency of any type is approximately proportional to its rank in a frequency list, and such a distribution is often referred to as a Zipfian distribution. 3 Even if the term stop list is new to you, you are probably already familiar with the concept: Search engines often omit function words that are entered into their search fields. 4 Note, however, that this may turn into an interesting finding once you find that some adjectives exhibit a tendency for such kinds of additional premodification, while others don't. Also, note in passing that in the context of regular expressions, precision and recall are sometimes referred to as specificity and sensitivity, respectively. 5 Strictly speaking, a concordance does not have to list every occurrence of a word in a corpus. Some programs output only a user-specified number of occurrences, usually either the first n occurrences in the corpus or n randomly chosen occurrences from the corpus. But writing new scripts requires programming skills that are probably beyond the capabilities of the average corpus linguist.

1 Download MRO for your operating system from the MRAN site at

The layout of these panes can be customized by going to Tools: Global Options . . . : Pane Layout. In addition, I would like to recommend that you also

As a second step, I recommend that you generate a folder <_qclwr2/> somewhere in your home directory on your hard drive (for Quantitative Corpus Linguistics With R, 2nd edition). Then, go to the companion website at

• <_qclwr2/_scripts>; this directory contains all relevant R code from this book: more than 6,000 lines of code with explanations, examples, exercises, and answer keys. The password you need to unzip these files is (without the double quotes) "_HinteRschinken". • <_qclwr2/_outputfiles>; this directory contains most output files resulting from R code in this book. The password you need to unzip these files is (without the double quotes) "_KassleR". (Even if you do not have access to those last four corpora, you will still be able to learn from the code used in the relevant case studies, plus I will actually give you practice assignments that change freely available corpora into the formats of the corpora you might not have -once you've solved those, you can still make full use of these assignments.)

Note that I am using regular slashes here in the paths to the directories because you can use those for paths in R, too, and more easily so than backslashes. Also, check out the file for errata on the website so that you can make necessary or recommended corrections. Finally, I recommend that you set the settings of your operating system's file manager such that it displays file names and their extensions, not just file names. Note also that R has much more to offer than this base installation: R is open-source software and there is a very lively community of people who have written so-called packages for R. These packages are additions to R that you can download and install (from within R) and then load into R to obtain commands (or functions, as we will later call them) that are not part of the default configuration. I would therefore suggest you do the following:

• start R -not RStudio this one time -with administrator rights: On Linux you would do this by entering sudo R ¶ at a terminal/console and provide your password; on Windows you can right-click on the R icon in your Start Menu and choose "Run as administrator"; • at the console you see, paste the first line from the code file <_qclwr2/_scripts/03-04_ allcode.r> there, which will install all packages we will be using in this book.

As mentioned above, I very strongly recommend that you read this book while sitting at your computer with the relevant code file open in RStudio -in fact, you should go to <_qclwr2/_scripts> and double-click on <03-04_allcode.r> right now, which will also make <_qclwr2/_scripts> your current working directory in RStudio, which nearly all of the code below presupposes -so that you can follow along more easily. Ideally, you would read the book and run the code I am discussing (by pressing CTRL + ENTER whenever I discuss a line of code so you see how it is executed in RStudio and what it returns); also, you can of course add your own notes to the R code files directly (ideally always preceded by the pound/hash sign # -see below for why).

As was already mentioned in the introduction, R is an extremely versatile piece of software. It can be used as a calculator, a spreadsheet program, a database, a statistics program, a (statistical) graphics program, and a scripting programming language with sophisticated mathematical and character-processing capabilities. In fact, the range of functions R can perform is so vast that I will have to restrict my discussion to a radically reduced subset of its functions. The functions that will be covered in this book are mainly concerned with

• how to load, process, and save various kinds of data structures, with a special emphasis on handling text data from corpora for corpus-linguistic analyses; and • how to load, process, and evaluate tabular data (for statistical/graphical analysis).

Thus, I will unfortunately have to leave aside many intriguing aspects of R. Also, for didactic and expository reasons I will sometimes not present the shortest or most elegant way of handling a particular problem, but rather present a procedure which is more adequate for one or more reasons. These reasons include the desires to

• keep the number of functions, arguments, and regular expressions manageable;

• highlight similarities between different functions; and • allow you to recycle parts of the code.

Thus, this book is not a general introduction to R, and while I aim at enabling you to perform a multitude of tasks with R, I advise you to also consult the additional references mentioned below and the comprehensive documentation that comes with R and RStudio; there are also many instructional videos out there, but as far as I can tell they are very heavily biased in the direction of statistical analysis rather than text processing.

Next, some notational conventions, some of which you have already seen by now. Paths/files and folders will be mentioned like this: <_qclwr2/_inputfiles/ some_name.txt>. Input to R is usually given in blocks of code as below, where "•" means a space character and " ¶" denotes a line break (i.e., an instruction for you to press ENTER).

This means: do not enter the two characters ">•" -i.e., greater-than and space. These are only provided for you so that (1) you know that this is code you are supposed to enter (i.e., don't enter gray-shaded code that does not begin with ">•") and (

(R code may also be shown inline using the same font and other conventions, like this: mean(c(1,•2,•3)) ¶.) You will also occasionally see lines that begin with "+" or "+•"; these plus signs (and spaces), which you are not supposed to enter either, begin lines where R is still expecting further input from you before it begins to execute the function. For example, when you enter "2-" and press ENTER, this is how your R interface will look:

R is waiting for you to complete the subtraction. When you enter the number you wish to subtract and press ENTER, then the function will be executed properly.

Another example: If, for instance, you wish to load a package into R to be able to use the functionality it offers, you can use the function library for that. For instance, you could type this to load the package dplyr: library(dplyr) ¶. (Note: this only works when you have installed the package as explained above.) However, if you forget the closing parenthesis, R will wait for you to provide it, and once you provide the missing closing parenthesis, R will execute the line:

Corpus files or tables/data frames will be represented as in Figure

Menus, submenus, and commands in submenus in applications are given in italics in double quotes, and hierarchical levels within application menus are indicated with colons. So, if you open a document in, say, LibreOffice.org Calc, you do that with what is given here as "File: Open . . . ".

Before we delve into R, let me finally mention several ways of using R's own documentation. The simplest way is to just use RStudio: Go the pane that has a "Help" tab, where you have access to a lot of documentation; most helpful at the beginning might be "An introduction to R" as well as "Learning R Online"; later also "R Data Import/Export". Obviously, you can also just Google the name of a function, package, or anything else since there are tons of extremely useful websites, videos, blogs, etc. out there that provide help for every level of expertise. Second, if you know the name of a function or construct and just need some information about it, type a question mark directly followed by the name of the function at the R prompt to access the help file for this function (e.g., ?help ¶, ?sqrt ¶, ?"[" ¶). Third, if you know what you would like to do but don't know the function, you can use the search field at the top right of the "Help" tab to search for anything.

Data Structures, Functions, Arguments

The simplest way of using R is using it like you would use a pocket calculator. At the R prompt, you just enter any arithmetic operation and hit ENTER (we'll talk about the

As you may recollect from your math classes in school, however, you often had to work with variables rather than the numbers themselves. R can also operate with variable names, where the names represent the content of so-called data structures. Data structures in R can take on a variety of forms. The simplest one, a vector, can contain just a number; a more complex one, a list, can contain many large tables, texts, numbers, and other data structures. Data structures can be entered into R at the prompt, but the more complex a data structure becomes, the more likely it becomes that you read it from a file, and this is in fact what you will do most often in this book: reading in text files or tables, processing text files, performing computations on tables, and saving texts or tabular outputs into text files.

One of the most central things to understand about R is how you tell it to do something other than the simple calculations from above. A command in R virtually always consists of two elements: a function and, in parentheses, arguments, where arguments can be null, in which case there could be just opening and closing parentheses. A function is an instruction to do something, and the arguments to a function represent

As another example, let us compute the base-10 logarithm of 150. One possibility would be to use log10, which, just like sqrt, only takes one argument -the number for which you want the logarithm. However, since one may also want logarithms to other bases, there is a more flexible function in R, log, which takes two arguments: The first is the number of which you want the logarithm, the second is the base to the logarithm. Thus, you can enter this:

One important aspect here is that this way of computing the log is only a short version of the more verbose version:

In this longer version, the arguments provided are labeled with the names that R expects the arguments of log to have: x for the number for which you want the logarithm and base for the base. However, if you provide the arguments in exactly the order in which R expects them, you can leave the labels out -but if you change the order or want to leave out one of several arguments, then you must label the arguments so that R knows what in your list of arguments means what.

Three other things are worth mentioning: First, outside of quotes, R does not care whether you put a space before the comma or not. Second, R's understanding of its input is case-sensitive: It doesn't know the function Log. Third, as you could see when you executed the lines of the code file in RStudio and as you now see again, R ignores everything after a #, so you can use this to comment your lines when you write small scripts and want to tell/remind yourself what a particular line is doing.

Note that, in all these cases, R does not store any result -it just outputs it. When you want to assign some content to some data structure for later use, you must use the assignment operator <-(a less-than sign and a minus, which together look like an arrow). You can basically choose any name as long as it contains only letters, numbers, underscores, or periods and starts with a letter or a period. However, to avoid confusion, you should not use names for data structures that are R functions (such as c, log, or sqrt or the many other functions you'll get to know below). As a result of assignment, the content resulting from the function is available in the data structure just defined. For example, this is how you store the square root of 5 into the kind of data structure you will get to know as a vector, which is here named aa.

>•aa<-sqrt(5) ¶

You can now check whether R actually knows about this vector by instructing R to list all the data structures it knows about (in the current R session):

And you can instruct R to output the contents of the vector (by default to the screen):

A nice short way to instruct R to assign a value to a data structure and output that data structure at the same time is by putting the assignment into parentheses:

Note that the assignment operator can also be used to change the value of an existing data structure using the name of the data structure again. The following can be read as "create a new version of aa by taking the old one and adding 2 to it":

Note that you can enter more than one command per line by separating commands with a semicolon.

If you ever want to get rid of data structures again, you can also remove them. Either you remove an individual data structure by using rm (for remove) and provide the data structure(s) to be deleted as an argument (or as arguments):

or you can just delete all data structures:

Finally, before we look at several data structures in more detail shortly, note that not only may functions not require their arguments to be labeled, many functions even have default settings for arguments which they use when the argument is not defined by the user. A function to exemplify this characteristic that is actually also very useful in a variety of respects is sample. This function outputs random or pseudo-random samples, which is useful when, for example, you have a large number of matches or a large table and want to access only a small, randomly chosen sample or when you have a vector of words and want it re-sorted in a random order.

The function sample can take up to four arguments:

• x: a data structure, most likely a vector, providing the elements from which you want a sample. If that data structure x is a vector of one number only, then R outputs a random ordering of the numbers from 1 to x; if x is a vector of two or more elements, then R outputs a random ordering of the elements of that vector. • size: an integer number determining the size of the sample; by default, that is the number of elements of x, which we will later determine with length(x); • a logical expression: replace=FALSE (if each element of the vector can only be sampled once, the default setting) or replace=TRUE (if the elements of the vector can be sampled multiple times, sampling with replacement);

• prob: a vector of probabilities with which elements of x may be sampled; the default setting is NULL, representing equiprobable sampling, i.e., sampling where each element of x has the same chance of being sampled.

Let us look at a few examples which make successively more use of label omission and default settings. First we generate a vector qwe with the numbers 1 to 10 by using: as a range operator:

If you now want to draw five elements randomly and equiprobably from qwe with replacement, you enter this:

However, since you provide the arguments in the default order, you can do away with the labels (although you now of course get different random numbers):

But since prob=NULL is the default setting, you might as well omit that, too:

And this is how we draw five elements equiprobably and randomly without replacement:

But again, since replace=FALSE is the default, why not omit it?

You have now seen that the arguments prob and replace can be omitted because of their default settings. But actually, it is also possible to omit the size argument. If the size argument is omitted, its default kicks in and R assumes we want all elements of the vector qwe back again and, thus, effectively only provides us with a random order of the elements of qwe:

In fact, if the idea is just to get a random ordering of the numbers of 1 to n, then you do not even need to provide R with a vector giving all the numbers from 1 to 10 as you did when we defined qwe above. You can just give R the number:

In fact, default settings sometimes lead to the extreme case of function calls without any argument (like help.start() above). The function q shuts R down and internally processes three arguments:

• A one-element character vector called save -i.e., a sequence of characters such as letters, numbers, or other symbols -specifying whether the current workspace, i.e., the data structures you worked with since you last started R, should be saved or whether the user should be prompted regarding whether the workspace should be saved. The latter is the default. • A one-element numeric vector called status specifying what R should 'tell' the operating system when R is shut down. The default is 0, which means "successful shut down". • A one-element logical vector called runLast (TRUE or FALSE) specifying whether some other user-specified function should be executed before R shuts down. The default is TRUE.

Thus, if you want to shut down R, you just enter:

Since you have not provided any arguments at all, R assumes the default settings. The first requires R to ask you whether the workspace should be saved. Once you answer this question, R will shut down and send 0 (i.e., "successful shut down") to your operating system. As you can see, label omission and default settings can be very useful ways of minimizing typing effort. However, especially at the beginning, it is probably wise to try to strike a balance between minimizing typing on the one hand and maximizing transparency of the code on the other. While this may ultimately boil down to a matter of personal preferences, I recommend using more explicit code at the beginning in order to be maximally aware of the options your R code uses. The next sections will introduce data structures that are most relevant to linguistic and statistical analysis.

Recommendation(s) for further study/exploration

• The functions ? or help, which provide the help file for a function (try ?sample ¶ or help(sample) ¶), and the functions args and formals, which provide the arguments a function needs, their default settings, and their default order (try args(sample) ¶ or formals(sample) ¶). • The function set.seed, which I use in the code file to make sure we obtain the same random but replicable numbers: ?set.seed ¶.

Vectors

Basics

The most basic data structure in R is a vector. Vectors are one-dimensional, sequentially ordered sequences of elements (such as numbers or character strings (e.g., words) or logical values). While it may not be obvious why vectors are important here, we must deal with them in some detail since nearly all other data structures in R can ultimately be understood in terms of vectors. As a matter of fact, you have already used vectors when we computed the square root of 5:

The "

You can also find out what kind of vector aa is by using the function class, and you can also determine that the length of aa is 1:

And, you can define vectors without contents but with a particular length, which may seem senseless right now, but is still something you must remember well because it is more memory-efficient than just letting a vector grow dynamically; this will become much clearer and in fact is necessary below.

For corpus linguists, it is of course important to know that vectors can also contain character strings -the only difference to numbers is that the character strings have to be put either between double or single quotes. You can freely choose which kind of quotes you use, but the opening and the closing quote must be identical. (For the sake of consistency, I will only use double quotes in this book.)

Note what happens when you apply length to a.name. Contrary to what you might have expected, you do not get the number of characters of "James" (i.e., 5) -you get the length of the data structure, and since the vector contains one element -"James" -R returns 1:

There are other types of vectors, but we will not distinguish any others than those mentioned above. However, vectors usually only become interesting when they contain more than one item. The function that concatenates (i.e., combines) several elements into a vector is called c, and its arguments are the elements to be concatenated into a vector.

Since an individual number such as the square root of 5 is already a vector, it is not surprising that c also connects vectors consisting of more than one element (as does append):

A characteristic that will be useful further below is that R can handle and apply vectors recursively. For example, adding two equally long numerical vectors yields a vector with all pairwise sums:

What happens if vectors are not equally long? Two things can happen. First, if the length of the longer vector is divisible without a remainder by the length of the shorter vector -i.e., if the modulus of the two lengths is 0 (try 11%%3 ¶ in R) -then the shorter vector is recycled as often as necessary to complete the function. The most frequent such case in practice is that the shorter vector has the length 1. In the following line, R multiplies the first element1 of numbers1, the 1, with the first element of bb, 10. Then, it 'wants' to multiply the second element of numbers1, the 2, with a second element of bb, but there is none, so R re-uses the first and only element of bb for that, and the same again for the 3 of numbers1:

Second, if the length of the longer vector is not divisible without a remainder by the length of the shorter vector, the operation proceeds as far as possible, but also returns a warning:

Another characteristic you will use a lot later is that elements of vectors can be named.

Here, each element of numbers1 is named according to its position:

It is important to note that -unlike arrays in Perl -vectors can only store elements of one data type. For example, a vector can contain numbers or character strings, but not really both: If you try to force character strings into a vector previously containing only numbers, R will change the data type, and since you can interpret numbers as characters but not vice versa, R changes the numbers into character strings and then concatenates them into a vector of character strings:

The double quotes around the 1 and 2 indicate that these are now understood as character strings, which also means you cannot use them for calculations anymore (unless you change their data type back using as.numeric). Apart from class, you can identify the type of a vector (or the data types of other data structures) with str (for "structure"), which takes as an argument the name of a data structure:

Unsurprisingly, the first vector consists of numbers, the second one of character strings.

In most cases to be discussed here, vectors will not be entered into R at the console but will be read in from files. In the following section, we will be concerned with how to load files containing text.

Even though we will usually load vectors from files, it is still often necessary to create quite long vectors in which (sequences of) elements are repeated. Instead of typing those into R element-by-element, you can use two very useful functions, rep and seq. In its simplest form, the function rep (for repetition) takes two arguments: the element(s) to be repeated and their number(s) of repetitions:

But rep is more powerful than that. You can also use an argument called each; here's an example with a manually entered vector and one with a range: This works for character vectors (or other data structures such as lists), too, such that seq will check their length and then return a sequence of integers from 1 to the length of the vector, something we will use a lot below in so-called for-loops:

If the numbers in the vector to be created do not increment by 1, you can set the increment to whatever value you need. The following lines generate a vector qwe in which the even numbers between 1 and 10 are repeated three times in sequence. Try it out:

Finally, instead of providing the increment, you can also let R figure it out for you, such as when you know how long your sequence needs to be and just want equal increments everywhere. You can then use the argument length.out as the third argument to sep, i.e., instead of the by argument. The following generates a seven-element sequence from 1 to 10 with equal increments and assigns it to numbers:

With c, rep, and seq, even long and complex vectors can often be created quite easily.

Recommendations for further study/exploration

• The general vector-creation function vector, which we later use to create lists:

• On how to change the types of vector to numeric and character strings:

?as.numeric ¶ and ?as.character ¶.

Loading Vectors

R has a very powerful function to load the contents of text files into vectors: scan. Since this function is central to very many corpus loading operations to be discussed below, we will discuss it and a variety of its arguments in some detail. Several useful arguments of scan, together with their default settings, are as follows:

scan(file="",•what=double(0),•sep="",•quote=if(identical(sep,• "\n"))•""•else•"'\"",•dec=".",•skip=0,•quiet=FALSE,• comment.char="",•blank.lines.skip=TRUE) ¶

• The file argument is obligatory (for loading vectors from files at least, see below) and specifies the path to the file to be loaded; usually this will look like this "home/ stgries/Corpora/BNCwe/D8Y.txt". Instead of providing a path by entering it there directly, you can also use file.choose() as the first argument, in which case R will prompt you with an Explorer/File Manager window so you can click your way to the desired file; once you choose a file, R will return the path to that file as a character string and, thus, to scan. • The sep argument specifies the character that separates individual entries in the file and that, therefore, determines what the elements of your vector will be. The default setting, sep="", means that any whitespace character will separate entries, i.e., spaces, tabs (represented as "\t"), and newlines (represented as "\r" or "\n" or both). Thus, if you want to read a text file into a vector such that each line is one element of the vector, you write sep="\n", which is what we will do nearly all of the time. • The quote argument specifies which characters surround text quotes; most of the time sep will be set as sep="\n", which entails that quote is then automatically set to quote="". • The dec argument specifies the decimal point character; if you want to use a comma instead of a period, just enter that here as dec=",". • The skip argument specifies the number of lines you wish to skip when reading in a file, which may be useful when, for example, corpus files have a fixed number of header rows at the beginning of the file. • The quiet argument specifies whether R returns the number of entries it has read in (quiet=FALSE, which is the default) or not (quiet=TRUE). • The comment.char argument specifies which character is used for comments in the file you are loading; the default is an empty character, which means nothing is ever specially treated as comments during loading. • The blank.lines.skip argument is set to TRUE, which means that empty lines will not be represented in the vector; this default setting is nearly always useful, but we will encounter a case in Section 5.4.6 where we will need to set this to FALSE.

Let us look at a few examples. First, we load into a vector x the contents of the text file <_qclwr2/_inputfiles/dat_vector-a.txt>, which looks like Figure

A slightly more complex example. Imagine you have a file, <_qclwr2/_inputfiles/dat_ vector-b.txt>, that looks like Figure

Here are two ways to load this file into a vector x. The first line reads in the file with the default setting of sep, so everything separated by whitespace in the original file becomes an element in the vector x.1; this already looks nicely like a vector of words, but think about what would happen with punctuation marks. . . . The second line reads in the file such that everything separated by line breaks becomes an element in the vector x.2:

Now, how do we load text files that involve Unicode code points? This is an important issue since corpus files these days will typically be in Unicode encoding, specifically UTF-8. Unfortunately, the way to load them and the way they are displayed in RStudio or the default R console depends on your operating system, your system locale, and the specific file. Here's an example of the kind of problems you can run into on, say, a Windows 10 system (American English locale), when you try to load <_qclwr2/_inputfiles/corp_utf8_ cyrillic.txt> (I only show a small part of the output here):

Not exactly successful. On Linux Mint 18 (same locale) to be precise, however, no problem:

How do we fix this? As many queries on various lists and websites indicate, it's unfortunately not always that straightforward. The function scan has two encoding-related argumentsencoding and fileEncoding -that sometimes do the trick, so for instance on both Windows and Linux, the following works (see iconvlist() ¶ for supported internationalization conversions): >•(cyr<-scan(file.choose(),•what=character(),•sep="\n",• encoding="UTF-8")) ¶

but the following also works on Linux, but not on Windows:

>•(cyr<-scan(file.choose(),•what=character(),•sep="\n",• fileEncoding="UTF-8") ¶

And with yet other files, even some that come with this book, the pattern might be the opposite, and sometimes it's not even clear what encoding a file comes with because that information is not always provided or obvious. The approach that I now usually end up using and that I find works best -across Windows and Linux, with UTF-8 files but also other encodings -is one based on file connections. There is a function file, which establishes a connection to a file and allows you to specify an encoding argument, and that connection will then be read with a function called readLines, which does exactly what its name suggests. This is how we will work most of the time when encodings are an issue:

This means the user chooses a file, to which file establishes a connection that it calls con and that assumes the input comes in the provided encoding, i.e., UTF-8. R then reads from that connection, i.e., from the file, the content of the file line by line and puts this into cyr (it also suppresses warnings about embedded nuls etc.). When the process is done, the connection to the file is closed again and, as you can see, the data import was successful. As mentioned above, this is the strategy that I found works most reliably, and we will use it for the various encodings we will deal with in this book.

Finally, while loading files will be the most frequent way in which you will use scan, let me mention the simplest way in which you can use scan, namely to enter vectors. If you just write scan() ¶ or scan(what=character()) ¶, you can enter numbers or strings separated by ENTER until you press ENTER twice to end your input:

Recommendations for further study/exploration

• Spector (2008: section 2.7).

• On how to customize input even more with scan: ?scan ¶.

Accessing and Processing (Parts of) Vectors

Now that we have covered some aspects of how to load vectors, let us turn to how to access parts of vectors and do something with these parts. The simplest ways to get a glimpse of what any data structure looks like are the functions head and tail. These take the name of a data structure as one argument and return the six first (for head) or last (for tail) elements of a data structure; if you want a number of elements other than six, you can provide that as a second argument to head and tail.

However, one of the most powerful ways to access parts of data structures is by means of subsetting, i.e., indexing with square brackets. In its simplest form, you can access (and of course change) a single element:

Since we have already seen how flexibly R handles variables, the following extensions of this simple principle should not come as a surprise:

With negative numbers, you choose the data structure without the designated elements:

Note also (for much later) that the square bracket can actually be used as a function (just like other function names such as head or tail), namely when you make it a character string; then its first argument becomes what data structure to extract something from and its second argument becomes what to extract:

You can use the names of vector elements for subsetting, too:

This can be very useful if, for instance, something like qwe is a potentially very long frequency list of words and you can just subset it using the word whose frequency you're interested in.

R also offers more useful functions, though. One of the most interesting ones is to let R decide which elements of a vector satisfy a particular condition. The simplest, though not always most elegant, solution is to present R with a logical expression.

R checks all elements of the argument preceding the logical operator == and outputs for each whether it meets the condition given in the logical expression or not. The one thing you need to bear in mind is that this logical expression -the test for identity -uses == rather than the = we already know from the assignment of values to arguments. Other logical operators we will use are the following: The following examples illustrate these logical expressions:

In fact, you can use such logical vectors of TRUEs and FALSEs for subsetting just like you used numbers or names. You have seen that when you subset a vector with a numeric vector in square brackets (as in x[3] ¶ or x[y] ¶ above), you get the elements in the positions indexed by the numbers. The following line shows that when you subset a vector with a logical vector of TRUEs and FALSEs in square brackets, you get the elements in the positions of the TRUEs:

And since TRUEs and FALSEs correspond to 1s and 0s, you can sum up a logical vector to find out how many TRUEs there are in it, i.e., how many vector elements satisfy a condition:

Or, for a more detailed breakdown, you can use the function table, which in its simplest use looks at one vector (but see below), counts how often each element that occurs in it at least once occurs, and provides an alphabetically ordered frequency list:

Note (again, for later) that table does not automatically also return a count of missing data -to obtain those, too, you need to add the argument useNA="ifany"). I hope you can already foresee that we will use table a lot to generate frequency lists of corpora: once a corpus is stored in R such that every word is a vector element, using table is all it takes.

While this illustrates how logical expressions work, it probably also illustrates that this is not a particularly elegant way to proceed: Regardless of how many elements of x fulfill the condition, you always get ten truth values and must then identify the cases of TRUE yourself. In other words, this approach does not scale, meaning it is not practicable once the data you are looking at are 10, 1,000, 100,000 times as large -what would you do, look at 100,000 TRUEs and FALSEs? Thankfully, there is a more elegant way of doing this using which, which takes as an argument a logical vector (e.g., something resulting from a logical expression of the kind above) and outputs the positions of TRUEs, i.e., of the element(s) satisfying a particular condition:

Note how much this actually looks like English: you are asking "which x is 4?" and get the answer "the seventh". The following examples correspond to the ones we just looked at but use which:

A central point here is not to mix up the position of an element in a vector and the element in a vector. If we come back to which(x==4) ¶, remember that it does not output 4 -the element in x -but 7 -the position that 4 occupies in x. If, however, you have been following along so far, you may already be able to guess how you get the element itself, so try to write some code that retrieves the elements of x that are larger than 8 or smaller than 3 and stores them in a vector y.

THINK BREAK

Since you can access elements of a vector with square brackets and since the output of which is itself a vector, you can simply write:

Or, because you guess that you can combine it all into one expression using the position numbers returned by which or even just the logical values returned by the logical expression:

To find out how many elements of a vector fulfill the condition, you can apply length or, since you remember that TRUE and FALSE are interpreted as 1 and 0 respectively, sum:

Thus, the fact that R uses vectors for nearly everything is in fact a great strength and makes it a very versatile language. When you combine the above with subsetting, you can also change elements of vectors very quickly. For example, if you want to replace all elements of x which are greater than 8 by 12, this is one way of achieving this:

or even directly with the position indices returned by which:

or even just with the TRUEs from a logical vector:

Apart from which and subsetting, there are also some more powerful functions available for processing vectors. We will briefly look at %in% and match, but discuss only simple cases here. The first argument of %in% is a data structure (usually a vector) with the elements to be matched; the second is a data structure (also usually a vector) with the elements to be matched against. The output is a logical vector of the length of the first argument with TRUEs and FALSEs for the elements of the first data structure that are found and that are not found in the second data structure respectively. Let me clarify this with two examples:

The first example, x•%in%•y ¶, shows that the first element of x -10 -does not appear in y, while the second element -9 -does, etc. You can combine the logical-vector output of %in% with subsetting to get at the matched elements quickly:

The function match returns a vector of the positions of (first!) matches of its initial argument in its second (again, both arguments are typically vectors):

This tells you, as above, that the first element of x does not appear in y, but that the second element of x -9 -is the third element in y. The third, fourth, and fifth element of x do not appear in y, but the sixth element -5 -appears in y, namely at position 2 of y, etc. With this, it should now be clear what the following line does, in which the arguments are reversed:

Same thing: the first element of y -the 2 -is the ninth element of x. The second element of ythe 5 -is the sixth element of x, and so on. You may ask yourself what this is good for, but you will get to see very useful applications of these two functions, in particular of match, below.

Recommendations for further study/exploration

• On how to identify elements in a vector that are duplicates of earlier elements in that vector as in duplicated(c(1,•2,•3,•4,•3,•2,•1)) ¶, which can be useful for type-token counts: The number of types is the number of all nonduplicated tokens: see ?duplicated ¶. • On how to compute cumulative sums as in cumsum(1:4) ¶: see ?cumsum ¶.

Another related way of processing two vectors uses basic set-theoretic concepts. Let me introduce three such functionssetdiff, intersect, and union, which all take two vectors as arguments -on the basis of the two vectors x and y we just used. The function setdiff returns the elements of the vector given as the first argument that are not in the vector given as the second argument:

The function intersect provides the elements of the vector of the first argument that do also occur in the vector given as the second argument; note that the order in which the elements are returned is determined by the order in which they occur in the vector given as the first argument:

The function union provides the elements that occur at least once in the combination of the two vectors given as arguments; again, the order depends on which vector is listed first:

Let me now briefly also mention the very useful functions unique, and then return to the function table. The function unique takes as its argument a vector or a factor and should be especially easy to explain to linguists since it outputs a vector containing all the types that occur at least once among the tokens (i.e., elements) of said vector/factor:

As you saw above, the function table takes as an argument one or more vectors or factors and provides the token frequency of each type or each combination of types.

That is, in g there is one "a" and there are two "b"s, etc. But now what happens with two vectors?

This is perhaps a little difficult to interpret at first sight, and you will have to remember that vectors are ordered sequences. This output tells you how often all possible combinations of elements in g and h occur. Beginning in the upper left corner and then moving to the right: There is no occasion at which there is an "a" in g and an "a" in h. There is one occasion at which there is an "a" in g and a "b" in h (element 1 of both vectors). There are no occasions where there is an "a" in g and a "c" or an "e" or an "f" in h. Analogously and in the next row, there are no positions where there's a "b" in g and an "a" or a "b" in h, but there is one position where g is "b" and h is "c" (element 2), and so on.

As mentioned above, Let me finally mention two useful functions concerned with the order of elements in vectors. The first function is used to sort the elements of a vector into alphabetical or numerical order. It is appropriately called sort and the main two arguments we will discuss here are the vector to be sorted and an argument decreasing=FALSE (the default setting) or decreasing=TRUE:

The other function is called order. It takes as arguments one or more vectors and decreasing=... -but provides a very different output. Can you recognize what order does?

THINK BREAK

The output of order when applied to a vector z is a vector which tells you in which order to put the elements of z to sort them as specified. Let us clarify this rather opaque characterization: If you want to sort the values of z in increasing order, you first have to take z's first value (the "a"). Thus, the first value of order(z,•decreasing=FALSE) ¶ is 1. The next/second value you have to take is the fifth value of z (the "b"), the next/third value you take is the second value of z (the "c"), etc. (If you provide order with more than one vector, additional vectors are used to break ties.) In other words, order produces a vector that, if you use it to subset the original element (here, z), orders that original element as specified:

As we will see below, this function will turn out to be useful when applied to data frames.

Saving Vectors

The function to output vectors we will use most is cat, which can also take a variety of arguments, some of which we will discuss here: cat(...,•file="",•sep="•",•append=FALSE) ¶

• The first, obligatory, argument ... is the vector you want to output. each vector element to get its own line, which is probably the most useful strategy for nearly all our cases, enter sep="\n", but the default separator is just a space. • The append argument specifies whether

You should now do "Exercise box 3.1: Handling vectors."

Recommendations for further study/exploration

• On how to test whether any one or all elements of a vector satisfy a logical expression: ?any ¶ and ?all ¶. • The fill-argument of cat.

• Spector (2008: sections 6.1-6.4).

Factors

The second data structure we will look at are factors. Factors are superficially similar to vectors, and in this book they are of less importance since we mostly deal with character vectors; thus, we will deal with them here only cursorily. The most straightforward way to generate a factor is by first generating a vector as introduced above and then turning it into a factor using the command factor with the vector to be factorized as the first argument:

The function factor can also take a second argument, levels, which specifies the levels the factor has. If you print out a factor, all levels of the factor that occur at least once are outputted in the same way that unique outputs all values of a vector, but, crucially and unlike unique, a factor can have levels that are not attested in the factor.

If you want to change an element of a factor into something else, you will face one of two scenarios. First, the simple one: you want to change a factor element into something for which the factor already has a level (most likely because it is already attested in the factor or was in the past). In that case, you can treat factors like vectors:

Second, the more complex one: You want to change a factor element into something that is not already attested in the factor and never has been. In that case, you must first redefine the factor by adding the level that you want to change something into, and then you can treat that new factor like a vector again, as above:

Note again that, unlike with unique, a factor can have levels that are not attested among its elements, as when you change the third element of f back into what it was:

The simplest way to get rid of those is to redefine the factor using droplevels:

Finally, note that there is a nice function called cut, which takes as its first argument a numeric vector and, in its simplest form, a number of levels as its second. It returns a factor that replaces each numeric value by the factor level that it has been categorized into. For example, the following creates a factor f from the number from 1 to 12 by creating four levels and replacing each number by the group of values it belongs in. The notation (0.989,3.75] means 'numbers greater than 0.989 and less than or equal to 3.75', which is R's way saying '1 to 3', etc. You can check that R formed the right four groups for the numbers from 1 to 12obviously 1:3, 4:6, 7:9, 10:12 -by cross-tabulating the original numeric vector and the new factor f:

Recommendations for further study/exploration

Data Frames

While vectors are the most relevant data structure for retrieving data from corpora, data frames are most relevant, first, as a tabular output form of corpus data (recall the discussion of frequency lists, collocate displays, concordances, etc. above), and, second, as the main input data structure for statistical analysis (often created/modified in a spreadsheet software and then imported into R). This data structure, which basically corresponds to a two-dimensional matrix, will be illustrated in this section.

Generating Data Frames in R

While data frames are usually loaded from text files generated with other programs, you can also generate data frames within R by combining several equally long vectors and/or factors. Let us assume you have characterized five parts of speech, as captured in a variable PARTOFSPEECH, in terms of three other variables (or parameters or characteristics):

• a variable "TOKENFREQUENCY" -i.e., the frequency of all individual words of this part of speech in a very small corpus C; • a variable "TYPEFREQUENCY" -i.e., the number of all different words of this part of speech in C; • a variable "CLASS" -i.e., whether the part of speech is an open class or a closed class.

Let us further assume the variables and the data frame you wish to generate should look like Figure

In a first step, you generate the four vectors, one for each column of the data frame:

Note that the first row of the data frame you want to generate does not contain data points, but the names of the columns. You must now also decide whether you would like the first column to contain case numbers from 1 to n or names of rows. If you prefer the former, then you can simply generate the data frame using data.frame, which in this case takes as arguments only the vectors/factors you want to combine. Note that the order of the vectors is only important in that it determines the order of the columns in the data frame. Let us now look at the data frame and its properties using the functions str (for structure) and summary:

We can see several things from this output. First, R has generated the data frame as desired. Second, R has automatically converted those vectors which contained character strings into factors (namely, PARTOFSPEECH and CLASS), and we can see that factors are internally represented as numbers (e.g., for CLASS, "closed" is 1 and "open" is 2). Third, since we have not specified any row names, R automatically numbers the rows. Finally, the column names in the output of str are preceded by a $ sign. This means that when you have stored a data frame in R, you can access columns by using the name of the data frame, a $ sign, and a column name (if the column name includes spaces, you need to put the column name in double quotes):

The above way of generating data frames is the default that we will use in most cases.

Loading and Saving Data Frames in R

The more common way of getting R to recognize a data frame is to load a file that was generated with spreadsheet software. Let us assume you have generated a spreadsheet file <_qclwr2/_inputfiles/dat_dataframe-a.ods> or a tab-or comma-delimited file <_qclwr2/_ inputfiles/dat_dataframe-a.csv> containing the above data frame x (using a text editor or a spreadsheet software such as LibreOffice Calc). Ideally your spreadsheet would only contain alphanumeric characters and no whitespace characters (to facilitate later handling in R or other software). The first step is to save that file as a raw text file. In LibreOffice Calc, you choose the Menu "File: Save As . . . " and choose "Text CSV (.csv)" from the "Save as type"/Formats menu. Then, you enter a file name, confirm "Automatic file name extension" and confirm you want to save the data into a text CSV format file, if prompted to do so. After that, ideally you choose "Unicode (UTF-8)" as the character set, "{Tab}" as the field delimiter and, usually, delete the text delimiter.

The second step is to load <_qclwr2/_inputfiles/dat_dataframe-a.csv> into R with read.table. These are the most frequently used arguments of read.table with their default settings: read.table(file=...,•header=FALSE,•sep="",•quote="\"'",•dec=".",• row.names,•na.strings="NA",•comment.char="#")

• The argument file is obligatory and specifies a data frame as saved above (as before with a one-element character vector, you can also just enter file.choose() to be prompted to choose a file interactively).

• The argument header specifies whether the first row contains the labels for the columns (header=TRUE) -as would normally be the case -or not (header=FALSE). • The argument sep, as mentioned above, specifies the character that separates data fields, and given the file we wish to import -and in fact most of the time -our setting should be sep="\t" for a tab. • We also know the dec argument, which is used as introduced above; in an English locale, the setting would not have to be changed. • The argument quote, which provides the characters used for quoted strings.

On most, if not all, occasions you should set this to quote="" to avoid input problems. • The argument row.names can either be a vector containing names for the rows or, more typically, the number of the columns containing the row names (typically 1). If you do not specify a row.names argument, the rows will be numbered automatically.

• The argument na.strings takes a character vector of strings which are to be considered as unavailable/missing data. 2 • Finally, the argument comment.char, which provides R with the character that separates comments from the rest of the line. Just like with quote, you will most likely wish to set this to comment.char="". • The argument x is the data frame to be saved.

• The arguments file, append, sep, and dec are used in the ways introduced above.

• The argument quote specifies whether you want factor levels within double quotes, which is usually not particularly useful for editing data in a spreadsheet software. • The argument eol provides the character that marks the end of lines; this will normally be eol="\n", which is also the default. • The arguments row.names and col.names specify whether you would like to include the names (or numbers) of the rows and the names of the columns in the file.

Given the above default settings and under the assumption that your operating system uses an English locale, this would probably be the 'normal' way to save such data frames:

Accessing and Processing (Parts of) Data Frames in R

There are several R functions that are useful for accessing parts of data frames that are to be used for subsequent analysis. To look at these, let us clear memory and then load the data frame from <_qclwr2/_inputfiles/dat_dataframe-a.csv> above into x again:

You have seen above that you can use the name of a data frame followed by a $ and a column name (potentially in quotes) to access columns in a data frame as in x$TYPEFREQUENCY. An alternative approach is the use of with, where you provide a data structure as the first argument and the part of that data structure that you want to access as the second:

But that's even more typing than just x$TYPEFREQUENCY. The probably simplest way to achieve the same objective is to use attach to make all columns of the data frame available without having to type the name of the data frame. This way, while R does not know the column TYPEFREQUENCY when the data frame was only loaded (see first line and its output), once it has been attached you can use its column names to access its columns without prefixing the name of the data frame:

To undo an attach(...) ¶, use detach(...) ¶. Also, note that if you attach a data frame that has the same column names as a previously attached data frame, R will do it, no problem, but will give you a warning that informs you that the columns of the previously attached data frame are no longer available by just typing the variable name. Also note that if you want to make changes to data that come from a data frame, it is safest to first detach the data frame, make your changes in the original data frame, and then attach it again: This way you make sure that you change the actual data, not the 'copy' that attach makes available.

The perhaps most general and versatile approach to accessing data from a data frame is again subsetting. We saw above that we can use square brackets to select parts of unidimensional vectors -since data frames are two-dimensional data structures, we now need two (sets of) figures, one for the rows and one for the columns:

As you can see, row and column names are not counted. Also, recall the versatile ways of juxtaposing different functions:

Sometimes, you want to investigate only one part of a data frame, namely a part where one variable has a particular value. One way to get at a subset of the data follows logically from what we have done already. Let us assume you want to define a data frame y that contains only those rows of x referring to open-class words. If you have already made the variables of a data frame available using attach, this would be one possibility to do that:

If you have not made the variable names available, you can still do this:

The maybe more readable way of achieving the same is provided by subset. This function takes as its first argument the data frame to subset and as additional arguments a logical expression specifying the selection conditions:

This way, data frames can be conveniently customized for many different forms of analysis. If, for whatever reason, you wish to edit data frames in R rather than in a spreadsheet program -for example, because Microsoft Excel or LibreOffice Calc are limited to data sets with maximally about one million rows, you can also do this in R. (One million rows may seem larger than you would ever need to you, but even a frequency list of the BNC is already at that limit.)

Finally, let me mention a very practical way of reordering data frames by means of order, introduced above. Recall that order returns a vector of positions of elements and recall that subsetting can be used to access parts of vectors, factors, and data frames (and later lists). As a matter of fact, you may already guess how you can reorder data frames. For example, imagine you want to sort the rows in the data frame x according to CLASS (ascending in alphabetical order) and, within CLASS, according to values of the column TOKENFREQUENCY (in descending order). Of course we can use order to achieve this goal, but there is one tricky issue involved here: The default ordering style of order is decreasing=FALSE, i.e., in ascending order, but you actually want to apply two different styles: ascending for CLASS and descending for TOKENFREQUENCY, so changing the default alone will not help. What you can do, though, is this:

That is, you apply order not to TOKENFREQUENCY, but to the negative values of TOKENFREQUENCY, effectively generating the desired descending sorting style. Then, you can use this vector to reorder the rows of the data frame by subsetting:

Of course, this could have been done in one line:

You can also apply a function we got to know above, sample, if you want to reorder a data frame randomly. This may be useful, for example, to randomize the order of stimulus presentation in an experiment or to be able to randomly select only a part of the data in a data frame for analysis. One way of doing this is to first retrieve the number of rows that need to be reordered using dim, whose first result value will be the number of rows and whose second result value will be the number of columns (just as in subsetting or prop.table ):

Then, you reorder the row numbers randomly using sample and then you use subsetting to change the order; of course, you may have a different (random) order:

Alternatively, you do it all in one line:

Finally, on some occasions you may want to sort a data frame according to several columns and several sorts (increasing and decreasing). However, you cannot apply a minus sign to factors to force a particular sorting style as we did above with TOKENFREQUENCY, which is why in such cases you need to use rank, which rank-orders a column with a factor first into numbers, to which then the minus sign can apply:

You should now do "Exercise box 3.2: Handling data frames."

Recommendations for further study/exploration

• On how to edit data frames in a spreadsheet interface within R: ?fix ¶ (note also that you can also view them in RStudio by clicking on the Environment tab and then on the small spreadsheet symbol next to the name of the data frame you want to view). • On how to merge different data frames: ?merge ¶.

• On how to merge different columns and rows into matrices (which can in turn be changed into data frames easily): ?cbind ¶ and ?rbind ¶. • See the package dplyr for many useful data frame operations.

• Spector (2008: chapter 2, section 6.8).

Lists

While the data frame is probably the most central data structure for statistical evaluation in R and has thus received much attention here, data frames are actually just a special kind of data structure, namely lists. More specifically, data frames are lists that contain vectors and factors which all have the same length. Lists are a much more versatile data structure which can in turn contain various different data structures within them. For example, a list can contain different kinds of vectors, data frames, and other lists, as well as other data structures we will not discuss explicitly in this book (e.g., arrays or matrices):

comment.char="") ¶ >•another.vector<-c("This",•"may",•"be",•"a",•"sentence",• "from",•"a",•"corpus","file",".") ¶ >•(a.list<-list(a.vector,•a.dataframe,•another.vector)) ¶

As you can see, the three different elements -a vector of numbers, a data frame with different columns, and a vector of character strings -are now stored in a single data structure:

It is important to bear in mind that there are two ways of accessing a list's elements with subsetting. The one with double square brackets is the one suggested by how a list is shown in the console. This way, you get each element as the kind of data structure you entered into the list, namely as a vector, a data frame, and a vector respectively. What, however, happens when you use the general subsetting approach, which you know uses single square brackets? Can you see the difference in the notation with double square brackets? What is happening here? Finally, if you named the elements of a list, you could also use those (but we did away with the names again, which is why you get NULL here):

Of course, you can also access several elements of a list at the same time. Since you will get a list as output, you use the by now familiar strategy of single square brackets:

So far we have looked at how you access elements of lists, but how do you access elements of elements of lists? The answer of course uses the notation with double square brackets (because you now do not want a list, but, say, a vector) and either of the two following options works -the latter is more intuitive to me:

And this is how you access more than part of a list at the same time:

Thus, this is how to handle data frames in lists such as, for instance, accessing the second element of the list, and then the value of its third row and second column:

And this is how you take the second element of the list, and then the elements in the third row and the second and fourth column:

And this is how you delete a list part:

As you can see, once one has figured out which bracketing to use, lists can be an extremely powerful data structure. They can be used for very many different things. One is that many statistical functions output their results in the form of a list, so if you want to be able to access your results in the most economical way, you need to know how lists work.

A second one is that sometimes lists facilitate the handling of your data. For example, we have used the data frame a.dataframe in this section, and above we introduced subset to access parts of a data frame that share a set of variable values or levels. However, for larger or more complex analyses, it may also become useful to be able to split up the data frame into smaller parts, depending on the values some variable takes. As you saw in the previous exercise box, you use split, which takes as its first argument the data frame to be split up and as its second argument the name of the variable (i.e., column name) according to which the data frame is to be split. Why do I mention this here (again)? Because the result of split is a list:

This then even allows you to access the parts of the list using the above $ notation:

Note that you can also split up a data frame according to all combinations of two variables (i.e., more than one variable); the second argument of split must be a list of these variables, which of course is not the most useful thing ever with the present tiny example, given that many combinations of a.dataframe$CLASS and a.dataframe$PARTOFSPEECH are not even attested in a.dataframe: >•split(a.dataframe,•list(a.dataframe$CLASS,•a.dataframe$ PARTOFSPEECH)) ¶

Elementary Programming Issues

In order to really appreciate the advantages R has to offer over all available corpuslinguistic software, we will now introduce a few immensely important functions, control statements, and control-flow structures that will allow you to apply individual functions as well as sets of functions to data only in certain conditions and/or more often than just the single time we have so far been covering. To that end, this section will first introduce conditional expressions; then we will look at the notion of loops; finally, we will turn very briefly to a few functions which can sometimes replace loops and are often more useful because they are faster and consume less memory.

Conditional Expressions

You will often be in the situation that you want to execute a particular (set of) (Note that the structures else•if•{•...•} and else•{•...•} are optional.) The logical expression testing a condition represents something we have already encountered above (e.g., in the context of which but also elsewhere), namely an expression with logical operators that is evaluated and returns either TRUE or FALSE. If it returns TRUE, R will execute the first (set of) function(s); if the expression condition returns FALSE, R will test whether the second condition is true. If it is, it will execute the next (set of) function(s), otherwise it will execute the last (set of) function(s). Two simple examples will clarify this; recall from above the fact that the plus sign at the beginning of lines is not something you enter but the version of the prompt that R displays when it expects further input before executing, here, a series of commands:

You may wonder what the series of leading spaces in some lines are for, especially since I said above that R only cares about space within quotes. This is still correct: R does not care about these spaces, but you will. The only function of the spaces here is to enhance the legibility and interpretability of your small script. In the remainder of the book, every indentation by three spaces represents one further level of embedding within the script. The third line of the second example above is indented by three spaces, which shows you that it is a line within one conditional expression (or loop, as we will see shortly). When you write your own scripts, this convention will make it easier for you to recognize coherent blocks of code which all belong to one conditional expression. Of course, you could also use just two spaces, a tabstop, etc.

When do you need conditional expressions? Well, while not all of the following examples are actually best treated with if, they clearly indicate a need for being able to perform a particular course of action only when particular conditions are met. You may want to

• include a corpus file in your analysis only if the corpus header reveals it is a file containing spoken language; • search a line of a corpus file only if it belongs to a particular utterance;

• use a certain part of the code only if your R instance is running on Windows, use another part of the code if your R instance is running on Linux; • include a word in your frequency list only if its frequency exceeds a particular value; or • want to use one search expression if a file has SGML annotation and another if it has XML annotation, etc.

You should now do "Exercise box 3.3: Conditional expressions."

Recommendations for further study/exploration

• On how to test an expression and specify what to do when the expression is true or false in one line: ?ifelse ¶. • On how to choose one of several alternatives and perform a corresponding action: ?switch ¶.

Loops

This section introduces loops, which are one useful way to execute one or more functions several times. While R offers several different ways to use loops, I will only introduce one of them here in a bit more detail, namely for-loops, and leave while and repeatloops for you to explore on your own (I do provide one example for each in the code file, though).

A for-loop has the following structure:

what to do as often often as seq has elements (this can be more than one line) ¶ } ¶

This requires some explanation. The expression name stands for any name you choose to assign, and seq stands for anything that can be interpreted as a sequence of values (where one value is actually enough to constitute a sequence); typically, such a sequence is a vector, and very often it is a vector with integers from 1 to n (either generated using the range operator : or seq). Let us look at the probably simplest conceivable example, that of printing out numbers (which one would of course not really do with a loop but with cat(1:3,•sep="\n") ¶ -this is just a didactic example):

This can be paraphrased as follows: Generate a variable called i as an index for looping. The value i should take on at the beginning is 1 (the first value in the sequence after the in), and then R executes the function(s) within the loop -i.e., those enclosed in curly brackets. Here, this just means printing i and a newline. When the closing curly bracket is reached, i should take on the next value in the user-defined sequence -i.e., 2 -and again perform the functions within the loop, etc. until the functions within the loop have been executed, with i having assumed the last value of the sequence, i.e., 3. Then, the for-loop is completed/exited. Again, the above kind of sequence is certainly the most frequently used one -the index i starts with 1 and proceeds through a series of consecutive integers -but it is by no means the only one: In the next example, i takes on all values of letters[4:6] (letters is a built-in constant containing the letters from a to z):

Of course, you can also use nested loops, that is, you can execute one loop within another one, but be aware of the fact that you must use different names for the looping indices (i and j in this example):

While these examples only illustrate the most basic kinds of loops conceivable (in the sense of what is done within each iteration), we will later use for-loops in much more useful ways.

Since you specify a sequence of iterations, for-loops are most useful when you must perform a particular operation a known number of times. However, sometimes you do not know the number of times an operation has to be repeated. For example, sometimes the number of times something has to be done may depend on a particular criterion, e.g., when your corpus has 200 files but you only want to search those files that contain written data, but you want R to find that out for you from the loaded corpus file's header, meaning you know what will be in the header of a file with written data, but you don't even know which and how many files those are. R offers several easy ways of handling these situations. One possibility of, so to speak, making such for-loops more flexible is by means of the control expression next. If you use next within a loop, R skips the remaining instructions in the loop and advances to the next element in the sequence/counter of the loop. The following is a (rather boring) example where you can see that, when the logical expression returns TRUE, R never reaches the line in which i is printed so the 2 does not get printed. In the above example of written corpus data, you would check whether the file contains written data or not -if yes, you do what you want to do with it; if not, you immediately go to the next file:

Another way to make loops more flexible is break. If you use break within a loop, the whole loop is exited and R proceeds with the first statement outside of the inner-most loop in which break was processed. This is how it works: When the logical expression returns TRUE, R processes break and breaks out of the for-loop, and never gets to print 3, 4, and 5:

Before we proceed, let me mention that loops slightly complicate your replicating scripts directly in the console. This is because once you enter a loop, R will not begin with the execution before the final "}" -hence the "+•" prompts -so you may not be able to easily recapitulate the inner working of the iterations. My recommendation to handle this is the following: When a loop begins like this for•(i•in•1:3)•{ ¶, but you want to proceed through the loop stepwise (for instance, in order to find and fix an error), then do not enter this line into R immediately. Rather, to see what is happening inside the loop, set the counter variable, i in this case, to its first value manually by writing i<-1 ¶. This way, i has been defined and whatever in the loop is depending on i having been defined as if the loop was actually happening can proceed. If you then want to go through the loop once more, but again do not yet want to do all iterations, just increment i by 1 manually (i<-i+1 ¶) and 'iterate' again. Once you have understood and maybe fixed the instructions in the loop and want to execute it all in one go, then just type the real beginning of the loop (for•(i•in•1:3)•{ ¶) or copy and paste from the script file. This way you will be able to understand more complex scripts more easily.

You should now do "Exercise box 3.4: Loops."

Rules of Programming

This book cannot serve as a fully fledged introduction to good programming style (in R or in general). In fact, I have not even always shown the shortest or most elegant way of achieving a particular goal because I wanted to avoid making matters more complicated or compact than absolutely necessary (and will continue to do so). However, given that there are usually many different ways to achieve a particular goal, there are two aspects of reasonable R programming that I would like to emphasize because they will help to • increase the speed with which some of the more comprehensive tasks can be completed;

• reduce memory requirements during processing; and • reduce the risk of losing data when you perform more comprehensive tasks.

The first of the two aspects is concerned with capitalizing on the fact that R's most fundamental data structure is the vector, and that R can apply functions to all elements of a vector at once without having to explicitly loop over all elements of a vector individually. Thus, while conditional expressions and for-loops are extremely powerful control structures and you can of course make R do many things using these structures, there are sometimes more elegant ways to achieve the same objectives. Why may some ways be more elegant? First, they may be more elegant because they achieve the same objective with (many) fewer lines of code. Second, they may be more elegant because the code is easier to read. Third, they may be more elegant because they are more compatible with how R processes things and manages memory. Let us look at a few examples using the vectors from above again:

Let us assume we would like to determine the overall token frequencies of open class items and closed class items in our data set. There are several ways of doing this. One is terribly clumsy and involves using a for-loop to determine for each element of CLASS whether it is "open" or "closed". Note one important thing: We know we will collect results from/during each iteration of a loop -that means the loop will contain some line in which each iteration's result is assigned to a data structure, but that in turn means that that data structure has to have been defined before the loop presupposes it, which is why the first line of the code below creates two what I will call collector data structures, numeric vectors containing 0, that will, after all iterations of the for-loop have been completed, contain the results:

The following is already a much better way: It requires only two lines of code and avoids a loop to access all elements of CLASS:

But the best way involves a function called tapply, which you have already encountered briefly in an exercise box on vectors. For now, we will discuss how this function behaves when given three arguments. The first is usually a vector to (parts of) which you would like to apply a particular function. The second is usually a vector or factor (or a list of vectors or factors) with as many elements as the first-argument vector to which you would like to apply a function. The third argument is the function you wish to apply, which can be one of many, many functions available in R (min, max, mean, sum, sd, var, etc.) as well as ones you define yourself:

This is the way to tell R "for every level of CLASS, determine the sum of the values in TOKENFREQUENCY". Note again that the second argument can be a list of vectors/factors to look at more than one classifying factor at the same time.

You should now do "Exercise box 3.5: tapply."

Another area where the apply-family of functions becomes particularly useful is the handling of lists. We have seen above that lists are a very versatile data structure. We have also seen that subsetting by means of double square brackets allows us to access various parts of lists. However, there are other ways of getting information from lists that we have not dealt with so far. Let us first generate a list as an example:

One useful thing to be able to do is to determine how long all the parts of a list are. With the knowledge we have so far, we would probably proceed as follows:

However, with sapply this is much simpler. This function takes several arguments, some of which we will discuss here. The first argument is usually a list or a vector to whose elements you want to apply a function. The second argument is the function you wish to apply. The "s" in sapply stands for "simplify," which means that, if possible, R coerces the output into a vector format (or matrix) format. From this you may already guess what you can write:

This means "to every element of another.list, apply length". Another great use of sapply allows you to access several parts of data structures

The more elegant way uses subsetting as a function. That is to say, in the previous example, the function we applied to another.list was length. Now, we want to access a subset of elements, and we have seen above that subsetting is done with single square brackets. We therefore only need to use a single opening square bracket in place of length (remember this from Section 3.2.3) and then provide as a third argument the positional index, 1 in this case because we want the first element:

Note that this works even when the elements of the list are not all of the same kind:

These last two examples hint at something interesting. The function sapply (and its sister function lapply) apply to the data structure given in the first argument the function called in the second argument; okay, so far nothing new. Interestingly, the third argument is provided to sapply (or lapply) but those two just 'pass it on' to the function mentioned as the second argument. Let me give a simple example:

In a way, lapply takes the argument decreasing=FALSE and 'hands it' to sort, which then uses this specification to sort the elements of the list ab in the specified way. These functions provide very elegant and powerful ways of processing lists and other data structures. We will see many applications of such and similar uses of tapply, sapply, and lapply below.

The second useful programming guideline is that it is often useful to split up what you want to do into (many) smaller parts and, if you do so, consider regularly saving interim results into new data structures or even into files on your hard drive that you then combine later (see Chapter 5 for several examples). While this may occasionally counteract your desire to let programs operate as quickly and memory-economically as possible -generating new data structures costs memory, saving interim results into files costs output time (hard drive access), loading them again later costs corresponding input time -it is often worth the additional effort: First, if your program crashes, then having split up your program into many smaller parts will make it easier for you to determine where and why the error occurred simply because your script/code is easier to read. This is true even for yourself: If you submit a paper and wait the customary 9-19 months that so many outlets need for reviewing, by that time you will have forgotten much of what you did and why, so you will need to decipher your code in a way that is frustratingly similar to how you would read someone else's code that you have never seen before. Second, if your program crashes, then having saved interim results into other data structures or files regularly will also decrease the amount of work you have to do again.

It is vital to realize that the way R works often strongly encourages such a modular approach. For instance, the way R handles memory is not always ideal. For instance, R is really not good at handling memory of data structures that grow (say, on each iteration of a loop). For instance, recall the last two loop examples, where we created one or two collector vectors before the loop. We did that in a way that is actually very bad practice: We made the collector vectors empty vectors and then filled, at iteration step i, their i-th slot. That means that before the first iteration the collector vector has length 0, after the first iteration (when i was 1), that vector has a length of 1, after the second iteration (when i was 2), that vector has a length of 2, and so on. That is, the vector grew a bit on each iteration, and that is something that can slow R down very much once the vector grows much more than just the four items we dealt with here. Here's an example: Save all your currently open files in all applications you may have open and then run the following three lines. You will see that even the simplest possible operation, just putting one number at a time into a vector, can really take a long time if you let the vector qwe grow dynamically:

Thus, whenever you know before a loop how long the output will be, you should define the collector structure -usually a vector or a list -such that you already reserve all required output slots in advance. So try this, where we immediately define the output vector qwe to have a length of 500,000 elements; you can see that the exact same operation takes much less time:

On my laptop the first script needed three minutes and three seconds, the second needed one second. But you might wonder now "When would I ever know the length of the result in advance?" That kind of situation is actually more frequent than you might assume: yes, when you do a concordance of a word in a corpus you probably do not already know the number of necessary results slots in advance, but here are some applications where you would:

• You are interested in the frequencies of three words in a corpus: Thus, you create a collector vector with three slots -one for each word -and then loop over the corpus files, load each one, retrieve the frequency of each word in each file, and successively add the frequency of each word in the current corpus file to the summed frequency from all previous files; that is, even before you start, you know your output vector will have three elements (this is comparable to the loop of the first tapply example above; also see Section 5.3.1 for a somewhat similar application). • You are interested in the lengths of corpus files both in words and in sentences. Your corpus has 4,049 files, so you know you will have one results vector for all files' lengths in words (with 4,049 slots), and another results vector for all files' lengths in sentences (with 4,049 slots) (see Section 5.2.4 for a similar application). • You are interested in the dispersion of three words in a corpus of 4,049 files, which means you need to collect, for each of the three words, its frequency in all 4,049 files. Thus, you know that your output structure could be a list with three elements, where each of the elements is a numeric vector of length 4,049 (see

So what do you do when you are not in a situation like this? For example, what if you wanted to generate a frequency list of a reasonably large corpus such as the 100 millionword BNC, where you do not know the exact number of word tokens and types in advance? Two possibilities: First and as briefly mentioned above, you can loop over each file, identify all word tokens in it, use table to generate a frequency table of it, and then save that frequency list into a separate file, which means you end up with 4,049 very small frequency list files, none of which will take up a lot of memory on its own. Then, in a second step, you do a second loop in which you load and amalgamate all 4,049 frequency list files. This strategy will cost much less time and memory than trying to do it all in one loop, and it is a strategy we will employ a number of times in Section 5.2.8.

A second possibility is to define a collector for the output that is so large that it will definitely be able to accommodate all results, and then fill the slots of this vector 'from left to right'. Let's pretend your collector vector is called coll (and it has 30 slots) and the first file generated four output figures: These four output figures then go into coll

Here is a small script that exemplifies this, and I again recommend that you look at this in the code file with RStudio because I have added much commentary there to explain the process in great detail, because this, too, is a strategy we will use a few times to avoid forcing R to grow vectors:

In this context, if your program involves one or more loops, it is often useful to build in a line that outputs a 'progress report' to the screen so that you can always see whether your program is still working and, if it crashes, where the crash occurred; a first example of this you see in the above script in the line beginning with cat.

One final recommendation in this section is concerned with naming practices. In much of the code below -in particular in Chapter 5 -I will often use quite long names for data structures etc., which is really only in the interest of recoverability or ease of parsing and recognizing things: Sometimes, names such as qwe or asd will be used because they are superfast and virtually error-free to type on an American QWERTY keyboard, but in the long run, i.e., for your real research projects, names such as sentences.without.tags or cleaned.corpus.file are infinitely more revealing than the kind of names (e.g., qwe or aa) I have mostly used so far.

Recommendations for further study/exploration

• On how to apply a function to user-specified margins of data structures:

?apply ¶.

• On how to apply a function to multiple list or vector elements: ?mapply ¶.

• On how to perform operations similar to tapply, but with a different output format and/or more flexibly: ?aggregate ¶, ?by ¶, and ?rowsum ¶.

Character/String Processing

So far, we have only been concerned with generating and accessing (parts of) different data structures in R. However, for corpus linguists it is more important to know that R also offers a variety of sophisticated pattern-matching tools for the processing of character strings. In this section I will introduce and explore many of these tools.

There is one extremely important point to be made right here at the beginning: KNOW YOUR CORPORA! As you hopefully recall from the first exercise box in Section 2.1, a computer's understanding of what a word is may not coincide with yours, and that of any other linguist may be different yet again. It is absolutely imperative that you know exactly what your corpus files look like -both in terms of the corpus data themselves and their annotation -which may require going over corpus documentation or, if you use unannotated texts, over parts of the corpus files themselves, in order to get to know spelling, formatting, annotation, etc.

A little example of something I came across in an undergrad corpus linguistics course was that we generated a concordance of in and found that in one of the SGML-annotated files of the BNC that we looked at, the word in was tagged as VBZ, which means "third person singular form of to be". All of us were stunned for a second, but then we noticed what was going on -can you guess?

THINK BREAK

The answer is that there were instances of innit as a reduction of isn't it in the files, and innit is tagged in the BNC in SGML annotation as "<w•VBZ>in<w•XX0>n<w•PNP>it"thus, you really must know what's in your data.

You may now think "But can't this be done more easily? Surely, ready-made programs wouldn't require me to do that?" Well, the answer is "No! And wrong!" There is no alternative to knowing your corpora, this cannot be done more easily, and any concordance programs that come with more refined search options also require you to thoroughly consider the format of the corpus files even if their interface 'hides' such decisions behind clickable buttons with smiling corpus linguists on them, in settings, or in .ini files. This issue will surface repeatedly below in this section and throughout all of Chapter 5.

Getting Information From and Accessing Character Vectors

The probably most basic character operation in R gives the length of all character strings within a vector. The function nchar takes as its most important argument the name of a character vector containing one or more elements and outputs the number(s) of characters of all elements of the vector:

Recommendations for further study/exploration

• On how to determine whether elements of a character vector have more than zero characters: ?nzchar ¶.

If you want to access a part of a character string, you can use substr, which we will always use with three arguments. The first is a character vector of which you want substrings; the second one is a numeric vector with the position(s) of the first character(s) you want to access; the third one is a numeric vector with the position(s) of the last character(s) you want to access:

From the plurals above, you probably already inferred that substr also handles longer vectors efficiently in the same way: >•some.first.vector<-c("abcd",•"efgh") ¶ >•some.other.vector<-c("ijkl",•"mnop") ¶ >•substr(c(some.first.vector,•some.other.vector),

As you can see, first some.first.vector and some.other.vector are combined into one vector with four elements, and then the four different start and end positions of characters are applied to this character vector. This can be extremely useful, for instance, when you know where in a line of annotation, say, the three-character identifier of a speaker is located (as may be the case in CHAT files from the CHILDES database) or when you want to access the line numbers in some version of the Brown corpus, which are always the first eight characters of each line, etc. (see, for example, Sections 5.4.5 and 5.4.9).

Elementary Ways to Change Character Vectors

The most basic ways in which (vectors of) character strings can be changed involves changing all characters to lower or upper case. The names of the relevant functions are tolower and toupper respectively, which both just take the relevant character vectors as their arguments; we will use this for case-insensitive frequency lists, for example:

Another elementary way to change (vectors of) character strings involves replacing x characters in (elements of) character vectors by x other characters. The name of the function is chartr, and it takes three arguments. First, the character(s) to be replaced; second, the character(s) that are substituted (this needs to have as many characters as the first argument); third, the character vector to which the operation is applied. This can be very useful, for example, to perform comprehensive transliteration operations in a single line of code or, as we will do in Section 5.4.3 below, recode a phonemic transcription into a segmental one distinguishing just vowels and consonants:

Merging/Splitting Character Vectors Without Regular Expressions

If you want to merge several character strings into one, you can use paste. These are the default arguments paste takes and their settings (where applicable):

• one or more character vectors;

• sep="•": the character string(s) used to separate the different character strings when they are merged -the space is the default; • collapse=NULL: the character string used to separate the different character strings when they are merged into just a single character string.

The use of paste can sometimes be a bit confusing to beginners, but once you think through the examples and explanations provided below, the way paste works does make sense, and the good news is that we will only use one of the six examples really often.

It is useful to distinguish three different scenarios, which differ with regard to what you are pasting together: (1) multiple one-element vectors, (2) one multiple-element vector, or (3) multiple multiple-element vectors.

In scenario (

In scenario

Finally, in scenario (3), we paste together two multiple-element vectors, and again what collapse does is make sure everything is merged into a single output string:

The 'opposite' function of paste is strsplit, with which you can split up a character string into several character strings. Its first argument is a character vector to be split up; the second argument is the character string that strsplit will delete to split up the first argument:

Note that if you use an empty character string as the second argument, R will split up the first argument character by character, which can be very useful to immediately see all characters that are attested in a file (especially when combined with unlist and table, see below):

It is important to note that the output of strsplit is not a vector -it is a list of, here, one element, which contains one vector (as you can infer from the double square brackets). The same works for larger character vectors: The list will have as many parts as strsplit's first argument has parts, which can be useful if you want to split up strsplit's first argument but nevertheless retain the information regarding where in the input each element of the output came from:

If you do not need to know where each element in the output came from, i.e., you just want one vector of all resulting items, just add unlist to change the list into one long(er) vector:

This will probably make you realize that this is close to how we will create frequency lists.

If zxc was loaded from a corpus file, then this is how you create an alphabetically sorted or a frequency-sorted frequency list respectively:

While these examples, in which character strings are split up on the basis of, say, just one space, are straightforward, strsplit is much more powerful because it can use regular expressions. However, before we introduce regular expressions, we will first treat some other functions used for searching and replacing character strings.

Searching and Replacing Without Regular Expressions

In this section we will introduce several pattern-matching functions in a very basic manner, i.e., without using the powerful regular expression techniques involving character classes and placeholders/wildcards. To that end, let us first generate a vector of character strings:

The first function is called grep. If it is executed without invoking its regular expression capabilities, its most basic form requires only two arguments: a one-element character vector to search for and a character vector (or something that can be coerced into a character vector, such as a factor) in which the first argument is to be found. The function returns the positions of the elements in which the string searched for occurs at least one time:

This output illustrates two properties of grep that beginners sometimes do not expect because (

(2) beginners have the nasty habit to think too much like humans, a problem we will encounter with regular expressions a lot. As for

THINK BREAK

However, this result will usually be produced slightly differently. Unless specified otherwise, grep assumes a default setting for a third argument, value=FALSE, which means that only the positions are returned. But if you set value=TRUE, you will get the elements of the character vectors in which the matches occurred instead of just their position numbers/indices:

Second, R does of course not know what a word is: It searches for character strings and returns the positions of the matches or the matches themselves, irrespective of whether what is searched for corresponds to what a user considers a word or not:

Third, remember that grep returns the position of matches in the vector that was searched maximally once (just like match returns only the position of the first match, not all of them). The following line returns "1" and "2" only once each, although "is" occurs twice in both first lines, once in "This" or "this" and once in "is". Thus, grep just answers the question "Does something occur somewhere at all (i.e., at least once)?", not "Does something occur somewhere at all and how often?"

If you set grep's argument invert from its default of FALSE to TRUE, then you get the positions/elements where what you searched for does not occur:

Also there is a useful function called grepl, which returns a TRUE or FALSE for every element of the vector searched depending on whether the search expression is attested in the vector searched or not, which can be useful for later subsetting or conditional expressions; thus, the following shows that the first element of txt does not contain "eco" but the second one does:

Note that in all these cases, grep returns the positions of matches in the vector that was searched -e.g., the character string "second" occurs in the second character string of the vector txt -it also does not return the positions of the matches in the elements of the vector, i.e., the fact that the character string "second" begins at character position 15 in the second element of the vector txt. However, this information can be retrieved with a different function: gregexpr. This function is one of the most powerful search functions in the default installation of R. It takes the same two arguments as the most basic form of grep, but its output is different: It returns a list with the starting positions and lengths of all matches at the same time (the companion function regexpr only finds the first matches). This list has as many vectors as the data structure that was searched has vectors (recall that behavior from strsplit?), and each of these vectors contains positive integers which correspond to the starting positions of all matches. In addition, each of these vectors comes with a match.length attribute that provides the lengths of the matches (and a useBytes attribute that is not relevant right now). It is important that you understand this hierarchical arrangement: Again, the list returned by gregexpr has vectors (one for each element of the vector that was searched), and each of the vectors in the list has attributes; the fact that the attributes are attributes of the vector, not of the list, are represented here by the indentation:

Vector with starting positions of matches attr(,"match.length")

for the first element of text

Attribute of that vector with the lengths of matches attr(,"useBytes")

Attribute of that vector indicating byte-by-

Vector with starting positions of matches attr(,"match.length")

for the second element of text

Attribute of that vector with the lengths of matches attr(,"useBytes")

Attribute of that vector indicating byte-by-

If there is no match of the search expression in an element of the vector being searched, gregexpr returns vectors of -1 for those:

But now how do we access the components of the output of gregexpr? Let's first assign the output of gregexpr to a data structure:

To extract the numeric vector that contains the starting positions of all matches for the first element of txt, you can just use the familiar double square-brackets notation, and even if the output is voluminous, the result is really still just a simple numeric vector, just one with attributes:

The above suggests two more things: you can use length to find out how many matches (of "e") are in txt

and you can unlist the list from gregexpr to find out how many matches there are in all of txt:

Now, how do you get at the attributes (because you want to know how long the matches are)? There is a function, attributes, to access attributes of objects, so you might think the following will do the trick -but why doesn't it?

THINK BREAK

This doesn't work because the attributes are attributes not of the list but of the elements of the list. Therefore, this is how you do it:

Which in turn means you can now access the attributes of this vector separately using either bracketing or their names:

An alternative to attributes is the function attr, whose first argument is the data structure whose attributes you want to access and whose second argument is a character string with the desired attribute name:

Thinking back to Section 3.6, can you write code that accesses the lengths of all 11 matches in txt in one go?

THINK BREAK

You can use sapply as follows, either to get a list with all lengths per match separately (the first line of code) or immediately unlist this to get all lengths of matches in a single vector (the second line of code):

Thus, you can also use sapply to count how many matches there are in each element of txt:

Now everything is in place to get not just the TRUEs or FALSEs indicating whether there is at least one match (as with grepl), not just the positions where at least one match was found (as with grep(...,•value=FALSE)), not just the whole elements where at least one match was found (as with grep(...,•value=TRUE)), and not just the starting positions (and lengths, as with gregexpr), but -finally -the exact matches. I will first show you the painful way (so that you learn about its logic) and then two much easier ways.

The painful way uses gregexpr's output to retrieve the starting points of all matches in all elements of txt and then also the lengths of all matches to compute their end points, because then, once we have starting points and end points, we can use substr to extract the relevant matches from the original input vector txt. However, the pain doesn't end here: When we do that -use substr and get the starting and end points from gregexpr, we also have to make sure that we get the first argument of substr right, the input vector: we cannot give substr just txt as its first argument because in the present case we need the first element of txt five times (for the five matches in it and, therefore, the five starting and end points we will get from gregexpr), but we also need the second element of txt six times (for the six matches in it). Thus, this is what we need to do (make sure you look at this in the code file, which provides a lot of commentary and visually helpful indentation):

unlist(sapply(ges.output,•attr,•"match.length")) ¶ -1) ¶ •

If that isn't painful, what is? While it is useful to understand this code well, let us now turn to the less painful ways of getting the exact matches. First, there is a function in R called regmatches, which can take three arguments: first, a character vector with the input data; second, an object created by gregexpr with match data (usually from the same input data of course); third, invert=FALSE (the default) or TRUE. Thus, we can easily do this now:

•"e"•"e"•"e"•"e"•"e"•"e" >•unlist(regmatches(txt,•ges.output)) ¶

In fact, we will later create our own function just.matches, which makes this even easier.

The second less painful way involves a function that I wrote for this book -a revision of the corresponding function in the first edition of this book. This function is called exact.matches.2 and you can use it once you have sourced it into R by running the following line and then choosing the file <_qclwr2/_scripts/exact.matches.2.r>:

The function exact.matches.2 requires minimally two arguments, but can take many more, and those first two arguments are the same that grep and gregexpr require:

(1) a search expression and (

With all default settings, the first component contains the exact matches as if we had generated them with unlist(regmatches(gregexpr(...),•...)) as above -if you add the argument vectorize=FALSE to the exact.matches.2 call, then this first component will be a list, i.e., the same as regmatches(gregexpr(...),•...)).

The second component is a numeric vector which states in which parts of the input vector there were matches (and how many): Above we can see that both the first and the second part of txt contain two instances if "is". The third component lists the percentage of corpus parts containing at least one match, which here amounts to 100 percent.

The fourth component was the main reason to write this function: It returns for each match the corpus element in which it was found but also separates the match from its preceding and subsequent contexts with a tabstop (so that, if you print the content of exact. matches.2(...) [[4]] into a. txt or. csv file, you get a nice three-column output with the context preceding a match in a first column, the match in a second, and the context following a match in a third. If you set the argument lines.around to a number greater than zero, then you increase the preceding and subsequent context by that number of corpus elements. If you set the argument characters.around to a number greater than zero, then the preceding and subsequent contexts will be as many characters (as opposed to corpus elements/lines). You can also suppress the generation of this fourth component of exact.matches.2 (if you're only interested in the first component, for example, and want to avoid the processing burden of computing potentially very many tab-delimited matches) by setting an argument gen.conc.output=FALSE. Also, note that by default this function deletes spaces around the column-delimiting tabstops -if you do not want that, set clean.up.spaces=FALSE; check out formals(exact.matches.2) ¶ for additional arguments.

Thus, with all this, we can do our searches for "e" in txt with regmatches etc. as above, or with exact.matches.2 as follows; either is a huge improvement over the manual and superlong substr(...) approach: >•exact.matches.2("e",•txt)[

The following are the main arguments of gsub and their default settings (again first without regular expressions, which will be introduced below):

The first argument is the expression that you want to replace, the second is the one you want to substitute instead. The third argument is the character vector you want to search and modify. The fourth argument specifies whether the search to be conducted will be casesensitive (ignore.case=FALSE) or not (ignore.case=TRUE). Let us apply this function to our small character vector txt. For example, we might want to change the indefinite determiner "a" to the definite determiner "the". By now, you should already know that the following will not really work well and why -don't run it, just think about it first:

THINK BREAK

This will not work because with this function call we would assume that R 'knows' that we are only interested in changing "a" to "the" when "a" is a word, the indefinite determiner. But of course R does not know that:

Thus, we need to be more explicit. What is the most primitive way to do this?

Note, however, that R just outputs the result of gsub -R has not changed txt:

Only if we assign the result of gsub to txt again or some other vector will we be able to access the vector with its substitutions:

While all these examples are probably straightforward to the point of being self-evident, the real potential of the functions discussed above only emerges when they are coupled with regular expressions -i.e., placeholders/wildcards -to which we will now turn.

Searching and Replacing With Regular Expressions

In this section, we introduce the most important aspects of regular expressions, which will be the most central tool in most of the applications to be discussed below. R offers two types of regular expressions, extended and Perl-compatible regular expressions -we will only be concerned with the latter because Perl is already widely used so chances are that you can apply what you learn here in different contexts. Thus, in order to maximally homogenize the explanations below, all regular expressions from now on will be written with the argument perl=TRUE even if that may not be necessary in all cases (and may sometimes slow code down a bit, but typically only with fairly complex regular expressions).

Let us begin with expressions that specify (or 'anchor') the position of character strings within character strings. The caret "^" and the "$" specify the beginning and the end of a character string. Thus, the following function only matches the first character string in the vector txt since only this one begins with a "t" or, ignore.case=TRUE, a "T":

Note that these expressions do not match any particular character in the string that is being searched: "^" does not match the first character of the line -it, so to speak, matches the beginning of the line before the first character: You can see that we still provided the "t" in the search string, which is, and maps onto, the first character. Thus, these expressions are also referred to as zero-width tests and we say that the caret does not consume a character, which means, for instance, that it is not a character that the regex engine would replace:

The next expressions we deal with are not zero-width tests -they do match and consume characters. The period ".", for example, means "any single character except for the newline". 3 Thus, for example, if you want to find all occurrences of "s" which are followed by one other character and then a "c", you could enter the following into R:

And of course, if you want a larger but fixed number of intervening characters, you just use as many periods as necessary:

But now this raises the question of what to do when we want to look for a period, i.e., when we do not want the period as a metacharacter but literally. Obviously, we cannot take the period as such because the period stands for "any one character but the newline", which means we would get many more matches than we want. Thus, we need to know two things: First, a character that tells R "do not use a particular character's wildcard behavior but rather the literal character as such". These are called escape characters and in R this escape character is the backslash "\". Thus, you write:

Now, why do we need two backslashes? The answer is that the two backslashes are actually just one character, namely the escape character followed by an actual backslash (as you can easily verify by entering nchar("\\");•cat("\\") ¶ into R). Thus, the two backslashes are in fact just one character -to the R interpreter, the first one instructs R to treat the second as a literal backslash (rather than part of some other sequence of characters), and the second one then says "I really want a period literally, don't use it as a regex!" but when you print them, you really only see one.

Second, we need to know which characters need to be escaped. The following is a list of such characters and their non-literal meanings, all of which will be used further below.

Line anchors

^ the beginning of a character string (but see below)  a double quote needs to be escaped if you use double quotes to delimit your character vector that makes up the regular expression ' a single quote needs to be escaped if you use single quotes to delimit your character vector that makes up the regular expression (which we don't do) While the period stands for "any one character apart from the newline", you may want to do more than retrieve exactly one kind of match. The above list already shows some of the quantifying expressions you can use. For example, if you have a vector in which both the British and the American spellings of colour and color occur and you would like to retrieve both spelling variants, you can make use of the fact that a question mark means "zero or one occurrence of the immediately preceding expression". Thus, you can proceed as shown here:

And with what search expression would you find both the singular and the plural of both spellings? With this one: "colou?rs?" of course.

Another domain of application of such quantifying expressions would be to clean up text files by, for example, changing all sequences of more than one space to just one space. How would you do this?

THINK BREAK

Well, you could write:

As a matter of fact, using the question mark and the plus sign are just two special shortcuts for a more general notation which allows you to specify, for example:

• the exact number of matches m of an expression you would like to match: just add {m} to an expression; • the minimum number of matches m of an expression you would like to match (by leaving open the maximum): just add {m,} to an expression; • the minimum number of matches m and the maximum number of matches n of an expression you would like to match: just add {m,n} to an expression.

Thus, the application of the question mark above is actually just a shorthand for an expression involving ranges. So, how could you rewrite the above line grep("co lou?r",•colors,•perl=TRUE,•value=TRUE) ¶ using a (in this case clumsy) range expression?

THINK BREAK

This would be it:

It is important here to clarify the range to which these quantifying expressions apply.

From the previous examples, it emerges that the quantifiers always apply only to the immediately preceding character -you have to resist the human impulse to apply the quantifier to, say, "colou" -the {0,1} really only applies to the "u". However, sometimes you may want to quantify something that consists of more than one element. The way to do this in R has already been hinted at above: You use opening and closing parentheses to group elements (characters and/or metacharacters). Thus, for example, if you want to match two or three occurrences of "st", you could do this:

As you can see, the expression in parentheses is treated as a single element, which is then quantified as specified. Of course, such an expression within parentheses can itself contain metacharacters, further parentheses, etc.:

Note that what the period matches the first time around does not have to be the same character for the second (and additional matches). You can see here that, in the second element, the period first matches an "s" and then an "f":

In fact, one of the probably more frequent domains in which the notation with parentheses is applied involves providing R with several alternative expressions to be matched. To do that, you simply put all alternatives in parentheses and separate them with a vertical bar (or pipe) |:

As another example, imagine that you want to replace all occurrences of "a", "e", "i", "o", and "u" by a "V" (for vowel). You could do this as follows:

But since this kind of application -the situation where one wishes to replace one out of several individual characters with something else -is a particularly frequent one, there is actually a shorter alternative notation which uses square brackets without "|" to define a class of individual characters and replace them all in one go:

(I am of course hoping that you are thinking, wait a second, [cw]ouldn't I do this with chartr?

The answer is yes, see the code file.) It is also possible to define such character classes in square brackets using the minus sign as a range operator. For example, if you want to replace any occurrence of the letters "a" to "h" and "t" to "z" by an "X", you can do this as follows:

Note that the minus sign is now not used as part of a character class: "[a-z]" does not mean "match either 'a' or '-' or 'z'" -it means "match any of the letters from 'a' to 'z'." If you want to include the minus sign in your character class, you can put it at the first position within the square brackets. Thus, to match either "a", "-", or "z", you write "[-az]" or "[-za]".

The same pattern works for numbers. For example, you could replace the numbers from 1 to 5 in the string "0123456789" with "3" as follows:

You should now do "Exercise box 3.6: A few regular expressions."

For cases in which you would like to match every character that does not belong to a character class, you can use the caret "^" as the first character within square brackets.

Note how the caret within the definition of a character class in square brackets means something else (namely, "not") than the caret outside of a character class (where it means "at the beginning of a string"); this is one of two regular expression characters that has a different meaning in a certain environment (here, in square brackets) than in another (at the beginning of a string).

While it is of course possible to define all alphanumeric characters or all numbers as character classes in this way ("[a-zA-Z0-9]" and "[0-9]" respectively), R offers a few predefined character classes. One such character class, written as "\\d" in R (see above on double backslashes), is equivalent to "[0-9]", i.e., all digits:

The opposite of this character class -i.e., what one might want to write as "[^\\d]" -can be expressed as "\\D":

Another predefined character class which will turn out to be very useful is that of word characters, written as "\\w" in R. This character class is the short form of what you could define more lengthily as "[a-zA-Z0-9]" (but many Unicode characters from other scripts can also be included), and again "\\W" is its opposite. From what you know by now, it should be obvious that you can combine this character class with a plus sign to look for what will, in the case of the Latin alphabet, correspond to most words, at least words without apostrophes and hyphens in them:

We will use this character class frequently below. Then, there is the class written as "\\s", which stands for all whitespace characters: tabstops (otherwise written as "\t"), spaces (otherwise written as " ", or here "•"), newlines (otherwise written as "\n"), and carriage returns (otherwise written as "\r"). As before, changing "\\s" into "\\S" gives you the opposite character class.

The final such metacharacter to be mentioned here is again an anchor, a zerowidth test (just like the line anchors "^" and "$" mentioned above). The expression "\\b" refers to a word boundary, which is the position between a word character and a non-word character (as defined above and in either order); again "\\B" is the opposite. That is to say, "\\b" is the position between the two characters of "\\w\\W" or "\\W\\w", but includes, or consumes, neither "\\w" nor "\\W"! Note the differences between the first two searches, where what you search for is consumed and thus replaced, and the third one using the zero-width pattern, which does not consume, and thus not replace, anything:

There are two important potential problems that we have so far simply glossed over. The first of these is concerned with what R does when there are several ways to match a pattern. Let's try it out:

The answer is clear. We ask R to match any "s" followed by zero or more occurrences of any character followed by another "s". There would be many ways to match this in txt

Obviously, R returns the third solution: It begins to match at the first occurrence of "s"at position 4 -and then chooses the longest possible match, as we can also see in the following line of code:

In regular expressions lingo, this is called greedy matching. However, this may not always be what we want. If, for example, we have a corpus line which matches a particular search pattern more than once, then, as corpus linguists, we often would not just want one long match (with unwanted material in the middle): Why don't we want this behavior? Imagine you are interested in the most interesting and important linguistic phenomenon that has ever existed, i.e., the constituent order alternation of verb-particle constructions in English, i.e., the alternation between John picked up the book and John picked the book up. If you have a tagged corpus in which verbs and particles are tagged -e.g., the corpus would look like this: John_N picked_V the_D book_N up_P -then you don't want R to do greedy matching because of how that would handle sentences with two verb-particle constructions such as John_N picked_V up_P the_D book_N and_CJ brought_V it_PN back_P. If we look for a verb followed later by a particle, greedy matching would return a match that, at first sight, suggests the first verb (picked) goes with the second particle (back):

Pay attention to the regex used above with gregexpr: Note in particular how the regex above operationalizes the notion of a word -one or more characters that are not spaces (because spaces separate words from each other, at least according to a simplistic definition adopted here) and that are not underscores (because those separate words and their tags from each other).

Thus, what we are more likely to be interested in is getting to know that there are two matches and what these are. There are two ways of setting R's matching strategy to lazy, i.e., non-greedy. The first one applies to a complete search expression, setting all quantifiers in the regular expression to non-greedy matching. You just write "(?U)" as the first four characters of your regular expression:

As you can see, now both consecutive matches are found. However, this is still bad -why?

THINK BREAK

The matches are just nine characters long, two more than before, which means they only added a space and "l"/"s". And why? Because the (?u) makes every quantifier lazy, i.e., also the "+" after "\\w" so R stops after one word character. What we want, however, is for the first quantifier to match lazily (the "*") but for the second to match greedily so that the "\\w+" gets the whole word. Thus, we would like a more flexible way to set lazy matching: Rather than using non-greedy matching for the complete regular expression, you can use greedy and non-greedy matching in one expression by adding a question mark to those quantifiers which you would like to use non-greedily. As you can see, we now get the desired result -both matches and their correct lengths are matched:

There we go. Let's apply this to vpc.example:

Not bad at all. But now note what happens if we apply this solution to the vector txt:

We still don't get all three consecutive occurrences we would probably want, namely "s•is", "s•a•firs", and "st•example•s", let alone these plus the additional three that are theoretically also possible from above ("s is a firs", "s a first example s", and "s is a first example s"). Why is that?

THINK BREAK

This is because when the regex engine has identified the first match -"s•is" -then the second "s" in txt

The second issue we need to address is that we have so far only been concerned with the most elementary replacement operations. That is to say, while you have used simple matches as well as more complex matches in terms of what you were searching for, your replacement has so far always been a specific element (a character or a character string). However, there are of course applications where, for example, you may want to add something to your match. Imagine, for example, you would like to tag the vector txt such that every word is followed by "<w>" and every punctuation mark is followed by "<p>". Now, obviously you cannot simply replace the pattern match as such with the tag because then your words and punctuation marks would be replaced rather than have tags added to them:

Obviously, this might not be the world's best tagger ever. Second, you cannot simply replace word boundaries "\\b" with tags because then you get tags before and after the words: Let us first look at the simple tagging example from above. In two steps, you can tag the 'corpus' in the above way:

That is to say, in the first step, the pattern you are searching for is defined as one or more consecutive occurrences of a word character, and the parentheses instruct R to treat the complete sequence as a single element. Since there is only one set of opening and closing parentheses, the number under which the match is available is of course 1. The replacement part, "\\1<w>", then instructs R to insert the remembered match of the first parenthesized expression in the search pattern ("\\1") -which in this case amounts to the whole search pattern -and insert "<w>" afterwards. The second step works in the same way except that now the parenthesized subexpression is a character class of multiple punctuation marks, which is treated as a single item, to which a punctuation mark tag "<p>" is added; note the fact that punctuation marks being in a character class obviates the need for writing "\\." and "\\?". Of course, this approach can be extended to handle more complex expressions, too. Imagine, for example, your data contain dates in the American-English format, with the month preceding the day (i.e., Christmas would be written as "12/25/2016") and you want to reverse the order of the month and the day so that your data could be merged with corpus files that already use this ordering. Imagine also that some of the American dates do not use slashes to separate the month, the date, and the year, but periods. And, you must take into consideration that some people write April 1 as "04/01" whereas others write "4/1". How could you change American dates into British dates using only slashes as separators (regardless of what separators are used in the American dates)? This regular expression may look pretty daunting at first, but in fact it is a mere concatenation of many simple things we have already covered. The first parenthesized expression, "(\\d{1,2})", matches one or two digits and, if matched, stores them as the first matched regular expression, "\\1". "\\D" then matches any non-digit character and, thus, both the period or the slash. Of course, if you are sure that the only characters in the data used to separate the parts of the date are a period and slash, a character class such as "[./]" would have worked just as well as "\\D". If you are not, then "\\D" may be a better choice because it would also match if the parts of the date were separated by hyphens as in "7-31-1976". The second parenthesized expression, again "(\\d{1,2})", matches one or two digits but, if matched, stores the match as the second matched regular expression, "\\2". You might actually want to think about how you can also preserve the original separators instead of changing them into slashes.

THINK BREAK

When this is all done, the replacement is easy: You want to replace all this by the potentially rhyming characters followed by "<r>", then the characters after the potentially rhyming characters, then the potentially rhyming characters, and the next non-word character. All in all, this is how it looks:

You can now also test that this regular expression does return a match on "This is not my dog" but does not match "This not is my dog", because in the latter the This and the is are not adjacent anymore; check out the code file for the (free-space commented) code. When you do so, hopefully, you look at this and immediately see a way to improve it. If not, think about (or try out) what happens if the elements of the vector txt did not have final periods. How could you improve the script in this regard and simplify it at the same time?

THINK BREAK

The solution and simpler strategy would be this:

As you can see, instead of the final non-word character, this uses a word boundary at the end. Why (is this better)? This is because the expression "\\1\\b" only consumes the rhyming characters, which has two consequences. First, this expression also matches the end of the string if there is no period because it does not require a separate non-word character to match after the rhyming character(s). Second, because the expression does not consume anything after the rhyming character(s), you need not capture anything after their second occurrence because you do not have to put anything back in; this is why this expression works with only two, not three, back-references. Thus, the second version is actually preferable. This example and its discussion gloss over the additional issue of how the approach would have to be changed to find rhyming words that are not adjacent, and you will meet this issue in the next exercise box.

The final kind of expression to be discussed here is referred to as lookaround. Lookaround is a cover term for four different constructs:

. on the left

Lookaround is sometimes a bit difficult to understand for beginners, but often exceptionally useful. The key characteristics of lookaround expressions are that

• like most regular expressions we have seen before, they match characters (rather than, say, positions); • unlike most regular expressions we have seen before, they give up the match -i.e., they do not consume characters and are therefore, just like line anchors, zero-width assertions -and only return whether they matched or did not match.

Let us look at a simple example for positive lookahead. You use positive lookahead to match something that is followed by something else (which, however, you do not wish to consume with that expression). For example, imagine you have the following vector example: >•example<-c("abcd",•"abcde",•"abcdf") ¶

If you now want to match "abc" but only if "abc" is followed by "de", you can use positive lookahead like this:

another "s" that we faced with the vector txt. The last time we managed to get two out of the three desired matches (when we introduced lazy matching), but we still had the problem that a consumed second "s" was not available as a first "s" in the next match. As you may already guess, the answer is to not consume the second "s", and you now know how to do that: with positive lookahead:

Finally, all three matches are identified: Check out the code for how you can extract them (with a crude approach that introduces the function paste0 and only works well in simple cases like the one here).

Negative lookahead allows you to match something if it is not followed by something else. In this example, you capitalize only those "d" characters that are not followed by "e":

Note the difference from what you may think is the same, but which in fact is not the same, and it is again different in how the lookahead does not consume the "e", whereas "[^e]" does:

Negative lookahead is especially useful when you want to replace unwanted strings, and here is an example we need to discuss in detail. Imagine you have a sentence with an SGML word-class annotation as in the BNC. In this format, every word is preceded by a tag in angular brackets (see www.natcorp.ox.ac.uk/docs/URG/posguide.html). The first character after the opening angular bracket of the tag is the tag's name "w" (for "word"), which is mostly followed by a space and a three character POS tag. However, the BNC also has so-called portmanteau tags, which indicate that the automatic tagger could not really decide which of two tags to apply. For example, a portmanteau tag "<w•AJ0-VVN>" would mean that the tagger was undecided between the base form of a regular adjective ("AJ0") or the past participle of a lexical verb ("VVN"). Apart from POS tags, other information is also included. For example, tags named "s" contain a name-value pair providing the number of each sentence-unit in a file; tags named "ptr" are used to indicate overlapping speech and contain a name-value pair identifying the location and the speaker of the overlapping speech. This is an example:

Imagine now you want to delete all tags that are not word tags or punctuation-mark tags. In other words, you want to keep all tags that look like this "<[wc]•...(-...)?>".

Unfortunately, you cannot do this with negated character classes: Something involving "[^wc•]" does not work because R will understand this to mean that the "w", the "c", and the space are one-character alternatives, but not as meaning "not 'w•' and also not 'c•'". Now you might say, okay, but I can do this:

And yes, this looks nice. But . . . you are just lucky because example1 does not have a tag like "<wtr•target=KB0LC004>", which begins with "<w" and which you would also want to delete. Look what would happen:

The last tag, which you wanted to delete, was not deleted. Why? Because it does not match: You were looking for something that is not "w" or "c" followed by something that is not a space, but this tag has a "w" followed by a non-space. The same problem arises when there is no "w" at the beginning of the tag in question, but a space in its second position:

One answer to our problem is negative lookahead. Here's how you do it, and you can try it out with example2 and example3 as well to make sure it works:

It is important that you understand how this works, so let me go over it in detail. First, the expression matches an opening angular bracket "<", no problem here. Then, negative lookahead says, match only if the parenthesized lookahead expression, i.e., the characters after the "?!" after the opening angular bracket, does not (negative lookahead) match "<[wc]• . The first tag "<w•UNC>" is not matched because the expression cannot match the negative lookahead: "<w•UNC>" is exactly what "?!" says not to match. The same holds for the next few tags. Once the regex engine arrives at "<ptr•target=KB0LC003>", though, something different happens. The regex engine sees the "<" and notices that the characters that follow the "<" are not "[wc]• . . . (-. . . )?>". Thus, it matches, but does not consume; the engine is still after the "<". The string "ptr target=KB0LC003" is then matched by ".*?", and the closing ">" is matched by ">". Thus, everything matches, and R can replace this by "", effectively deleting it. This is a very useful approach to know and some of the scripts below that handle BNC files will make use of this expression to "clean up" the file before further searches take place (so that overlap tags do not interrupt sequences of words we are interested in, for instance). An extension of this not only deletes unwanted tags, but also the material that follows these unwanted tags up until the next tag:

Recommendations for further study/exploration

• On how to match with positive and negative lookbehind: explore (?<=) and (?<!). Note that Perl-compatible regular expressions do unfortunately not allow variable-length lookbehind.

You should now do "Exercise box 3.7: A few more regular expressions."

Merging/Splitting Character Vectors With Regular Expressions

We saw in Section 3.7.3 that we can split up character strings using strsplit. While the above treatment of strsplit did not involve regular expressions, this function can handle regular expressions in just about the way that grep and the other functions can.

Let us exemplify this by means of a character string that may be found like this in the BNC with SGML annotation: Every word is preceded by a POS tag in angular brackets, just as in the above examples.

Imagine we want to get the words and get rid of the tags. Looking at the above, there seem to be two alternatives to do that: First, we could simply delete the tags (with gsub) and then strsplit on spaces. Second, we could strsplit on tags. Which one is better and why?

THINK BREAK

This was a bit of a trick question. Obviously, the second is better because it's just one operation. But there is a potentially more important reason. Look at the excerpt from a BNC file shown in Figure

If you pursue the first of the two strategies, you end up with the following word tokens: "now", "because", "of", "obvious", and "reasons" -if you pursue the second, you get "now", "because•of", "obvious", and "reasons" because splitting up on the tag doesn't lose the (potentially very useful) information that the BNC compilers considered because of a multi-word unit. Think how this seemingly small decision has potentially big implications: If you generate a frequency list of the BNC using the first approach, then you lose the counts of all multi-word units but every because of, in spite of, out of, etc. increases the frequencies of of as well as because, in, spite, and out. A frequency list generated with the second approach doesn't have that problem, which is why we will go with the second approach here.

Back to tagtext: If you want to split up this character string at each tag such that every word is one element of an overall vector, this is what you could do:

As you can see, the function looks for occurrences of "<w•" followed by three characters and a ">" and splits the string up at these points. Recall again that unlist is used here to let R return a vector, not a list. While this works fine here, however, life is usually not that simple. For one thing, not all tags might correspond to the pattern "<w• . . . >", and in fact not all do. Punctuation mark tags, as you already saw above, look different:

However, you should be able to find out quickly what to do. This is a solution that would work when the tag beginnings are either "<w•" or "<c•" (result not shown here):

Again, however, life is more complex than this: recall also that we have to be able to accommodate portmanteau tags:

Among the ways of handling this, this is one with alternatives, but without ranges:

As you can see, the expression either looks for any three characters or for any three characters followed by a hyphen and another three characters. The other possibility involves an expression with a range such that we are either looking for three or seven (three + "-" + three) characters; strictly speaking, the former alternative is better since it is more restrictive (by specifying the nature of the up to seven characters in more detail).

One final thing is less nice. As you can see, strsplit splits up at the tags, but it does not remove the spaces after the words. This may not seem like a great nuisance to you but, before you go on reading, look at the last word and think about in what way this may be problematic for further corpus-linguistic application.

THINK BREAK

If you could not think of something, consider what would happen if the vector tagtext looked like this:

The problem arises when you apply the above regular expression to this version of tagtext:

As you can see, there is one occurrence of "service" with a following space -the first one -and one without a following space -the second one. The second one is not followed by a space because its characters were the last ones in the vector tag text and, more often, they might not be followed by a space because they are immediately followed by a punctuation mark. The problem is that if you now wanted to generate a frequency list using table, which you already know, R would of course treat "service" and "service•" differently, which is presumably not what anyone would want:

Thus, what would a better solution look like, i.e., a solution that deletes the spaces?

This is what you could do:

Recommendations for further study/exploration

• On how to do approximate pattern matching: ?agrep ¶ and also check out ?adist ¶.

• Spector (2008: chapter 7).

• On regular expressions in general:

Two Particularly Relevant Areas: Unicode and XML

Some Notes on Handling Unicode

This section looks at non-English non-ASCII data. Just as most corpus-linguistic work has been done on English, this chapter has so far also been rather Anglo/ASCII-centric. In this section we are going to deal with data involving Unicode by briefly looking at data involving Cyrillic characters. There is good news and bad news. The good news is that nearly everything that you have read so far applies to Unicode files with Cyrillic characters, too.

There are a few small changes, such as that you cannot really use regular expressions such as "\\w", "\\W", or "\\b" to work (because they are defined with regard to a Latin alphabet), but otherwise nothing major. The bad news for Windows users is that, with R for Windows at least, it is often not as straightforward to handle Unicode as it is with R for MacOS X or R for Linux, which is mainly due to the fact that Mac OS X and Linux are UTF-8 operating systems so they play well with input files that have the same encoding, which is one of many reasons (see Section 3.8.2 for another one) why I recommend doing corpus processing with R on a Linux machine (and it takes very little time to install a long-term support version of, for instance, Kubuntu or Linux Mint on a desktop computer).

Let us load a file that has UTF-8 encoding -like actually nearly all others before -but now also non-Western European characters in it, namely the file with Cyrillic characters we looked at above, <_qclwr2/_inputfiles/corp_utf8_cyrillic.txt>:

Let us first briefly look at generating frequency lists. I will treat three ways of creating case-insensitive frequency lists of the words in this file, all of which involve the strategy of splitting up the corpus file using strsplit, but they differ in how we define how you are splitting.

The first of these is basically no different from how we did it above, which is why I reduce most of the exposition to just the code, which you will understand without problems. To make it a little more interesting, however, we will first determine which characters are in the file and which of these we might want to use for strsplit, so, how do you find out all characters that are used in the corpus file?

THINK BREAK

You need to remember that strsplit splits up (elements of) a vector characterwise if no splitting character is provided. Thus:

This tells you pretty much exactly which characters to use for splitting (and which not to worry about here, such as semicolons and single quotes), and you can proceed as above:

By the way, note that we could apply tolower and toupper in the familiar way without problems.

As mentioned above, one difference to the above treatment of ASCII data is that in an American-English locale, you cannot use characters such as "\\b" or "\\W" with perl=TRUE here because what is a word character depends on the locale, and the Cyrillic characters simply are not part of the ASCII character range [a-zA-Z]. However, there is a nice alternative worth knowing, which brings us to the second way to split up the corpus file into words, which is using character points and, more usefully, character ranges. Specifically, you can access each Unicode character in R individually by the sequence "\u" followed by a four-digit hexadecimal number. 5 This has two important consequences: First, once you know a Unicode character's number, you can use that number to call the character, which is particularly handy if, for example, you want to define a search expression in Cyrillic characters but don't have a Cyrillic keyboard and don't want to fiddle around with system keyboard settings, etc. Here are some examples:

Thus, you can look up the characters for the Russian word meaning "but" from the Unicode website or sites at Wikipedia, 'construct' it at the console, and even assign it to a character vector, which could then be the search expression for grep, gregexpr, exact. matches.2, etc.:

Second, when the characters of the alphabet you are concerned with come in ranges, you can easily define a character class using hexadecimal Unicode ranges. For instance, the small and capital characters of the Russian Cyrillic alphabet can be defined as follows:

>•(russ.char.yes.capit<-"[\u0410-\u042F\u0401]") ¶ >•(russ.char.yes.small<-"[\u0430-\u044F\u0451]") ¶ Note that the square brackets with which we define this character class are part of the character strings -this is so we can use these two character vectors as search expressions in grep etc. This of course then also means that you can easily define non-word characters in nearly the same way:

(Several regular-expression engines actually have a variety of predefined so-called Unicode blocks -i.e., contiguous ranges of code points -but the Perl-compatible regular expressions R uses do not support predefined Unicode blocks, so the above way of defining ranges manually is probably often your best choice.)

Thus, the second way to create the above kind of frequency list is this: You split on one or more occurrences of characters that are not Cyrillic letters:

If you compare this output to the one above, you should learn another important lesson: The word "daminnad" shows up four times in the first list, but not in the second. This is of course because the characters that make up this word are not Cyrillic characters as defined by the above Unicode character ranges, which is why they get used by strsplit and, thus, effectively get deleted. Also, note that just because the "a" in "daminnad" and the ASCII "a" look the same to the human eye does not mean they are the same Unicode character. For example, what may look like ASCII spaces or commas in the file <_qclwr2/_inputfiles/corp_utf8_mandarin.txt> are not ASCII characters! Thus, you must be very careful about how to define your splitting characters.

The third and final way is based on the fact that there are several predefined Unicode categories which you can use with a regular expression that defines so-called properties: the regex for that involves the syntax "\\p{ . . . }", where you replace . . . with the name of the property you wish to use. The code file provides a few examples of Unicode categories you can set with properties (and www.regular-expressions.info/unicode.html provides many more), but here I will only show how you use a property to define a socalled Unicode script for the task of creating our frequency list of Russian words with the Cyrillic alphabet: >•words.3<-exact.matches.2("\\p{Cyrillic}+",•corpus.file,• gen.conc.output=FALSE)[

As you can see, this approach returns the same results as the second one, but this one of course presupposes that the writing system you are studying is predefined as a Unicode script.

Let us now turn to concordancing with Unicode data, which is also nearly the same as with ASCII data; the main difference being again that, with perl=TRUE, you cannot use, say, "\\b". Let us begin with a simple example in which we look for a Russian word meaning "but", which is "но". You first define it as a search word (and often it is useful to immediately put parentheses around it so you can use back-referencing later when, for instance, you want to put tabstops around the match):

The simplest way to look for this is of course to just use the above search.word as the search expression with grep. However, if you look for the search expression you will find many examples where "но" is part of adjectival inflection (e.g., "ноe" or "ного") or of the derivational morpheme "ность":

To get rid of false hits, you might decide to find "но" only when it is used with nonword characters around it (because you can't use "\\b") and define a search expression such this one:

But you already know that would cause problems with matches that are at the beginnings or ends of lines because then there are no preceding or subsequent characters respectively. You have therefore two choices: Either you define a better word boundary or you use negative lookaround. I will illustrate both options using the function exact.matches.2 from above.

For a better word boundary, you can add beginnings and ends of character strings to the non-Cyrillic-character range you defined earlier:

and then create and use a new search expression: >•(search.expression<-paste0(word.boundary,•search.word,• word.boundary)) ¶

As you can see, now only the one desired match in the third line is found plus another one in the fifth line. The alternative involves negative lookaround: You look for the search word, but only if there is no character from the Cyrillic character range in front of it and no character from the Cyrillic character range after it. As you can see, the same matches are returned: However, since none of these matches actually involves a match at the beginning or end of a string, let's just make sure that the former approach, the one involving the well-defined word boundaries, works by looking for the Russian translation of in, "в":

If you define the search expressions just like above and then perform both searches with exact.matches.2, you will see that matches of "в" within words are not returned -as desired -but that the line-initial match in the eighth line is found: This concludes our brief excursus on Unicode character searches in this section. However, since this is an important issue, I would like to encourage you to not only get more familiar with this kind of encoding, but also do Exercise box 3.8 now.

You should now do "Exercise box 3.8: Unicode."

Some Notes on Handling XML Data

XML, the eXtensible Markup Language, is one of the most widely used formats that corpora come in; it is typically the format of choice in which information in the form of markup and annotation (recall Section 2.1.2) is added to the transcripts or written texts that corpora contain. It is therefore useful to at least briefly discuss how such data can be processed in R. For my discussion of XML below, I provide an input file that is a very small subpart of the file <D94.xml> from the BNC World Edition, but I will show the code and results you would obtain if you run the relevant code on the complete file; since the BNC World Edition in XML format is now freely available (from

Space precludes an exhaustive discussion of the XML format, but a few comments about it are probably in order. XML is a hierarchical format (that lends itself well to representation as a tree that does not allow cross-nesting/overlapping) in which you add to data markup and annotation in the form of either start and end tags (which may contain attribute-value pairs), or just start tags (with attribute-value pairs), and in fact you have seen examples above that are similar to that annotation already. For instance, consider Figure

If you want to process XML files with R, there are two ways in which you can do that.

One is simply what we did in the previous sections of this chapter: We treat the XML file as a regular flat text file (essentially not paying much attention to the hierarchical structure of the file) and use regular expressions to find exactly what we want. Often this is fast and unproblematic and I must admit that the vast majority of my work with XML files is really just that. The other way is using (one of) two R packages that are dedicated to XML processing: You can use the package XML (www.omegahat.net/RSXML; I used version 3.98-1.4) and/or the package xml2 (

XML files in the BNC are stored as UTF-8 text files, which you can either load with scan and then convert using the function  • c5 = "PNP" (which means "the CLAWS 5 tag of the word annotated here is PNP", which means "personal pronoun"); • hw = "it" (which means "the lemma/headword of the word annotated here is it"); • pos = "PRON" (which means "the more coarse-grained POS tag of the word annotated here is PRON", which means "pronoun").

An example where no end tag is used -these are sometimes referred to as empty elementscan be seen in Figure

Note that one can theoretically store information in either format: the information shown in Figure

You can explore the root a bit by, for instance, running xmlName(D94.root) ¶ or xmlAttrs(D94.root) ¶, but for our purposes this does not buy us much -it is more interesting to see how we can access the information we see in Figure

You can also check out the contents of both of the root's parts, either as a hierarchically represented object quite similar to what you see in a browser or as a one-element character vector (output not shown):

Now, how do you explore the header more? The header of <corp_D94_part.xml> is shown in Figure

What about the main corpus part, stext? As you can see in Figure

But that still doesn't give us easy access to a specific utterance and its characteristics and parts, or content. For brevity's sake, let's define an object containing the first utterance and look at it in more detail: we can retrieve the code of the speaker who said it ("D94PSUNK", i.e., the speaker is unknown), the name of the utterance tag ("u", from "<u who= . . . "), the size of the utterance in terms of its immediate constituents (i.e., one sentence), and what the actual data value of the utterance is: •

However, the most useful ways of accessing corpus data from such XML files are either of the following:

• The function xpathSApply with a first argument being the object returned by xmlInternalTreeParse (above that was the object D94.file), the second argument defining the hierarchical level at which you want to operate, and an optional third and fourth argument being a function that you want to apply there and maybe additional arguments for that function.

• The function xmlSApply with a first argument being the root of an object returned by xmlInternalTreeParse (above that was the object D94.root) and the second argument being a function that you want to apply to D94.root's parts.

For instance, both approaches can be used to tell you the two parts that <D94.xml> has:

The former line of code can be read as "apply XPath syntax to all elements of D94.file at the level right below the bncDoc level (i.e., the topmost one), and for each of the elements at that level, return its name." In the remainder of this section I will focus on how to use xpathSApply because I find it more straightforward and useful to work with; in particular, we will mostly use the following kind of command (don't enter this, there's no ">•":

xpathSApply(xmlInternalTreeParse(some.file), "location.in.hierarchy", FUN,•...) # FUN will usually be one of these 3 functions: # xmlValue # xmlAttrs # xmlGetAttr (Often, the code file will sometimes also provide code alternatives using xmlSApply and another function called getNodeSet, but if you want to you can skip those; the focus really is on the above approach with xpathSApply.) Most of the time now, I will not show the output because it is often too voluminous.

Given the above, you will not find it surprising that you can target the next lower levels like this: all that changes is the location description, which essentially uses a slash notation that is reminiscent of folder structures on your computer: You can see the parts of the header and how many parts each of them has. More interestingly, we can apply the same logic to the other part of the root, namely stext, and use xmlValue to immediately get the content of the corpus file without markup/annotation and without ever having had to use any regex:

Instead of an absolute path, where you move down the XML tree via the single slash-separated parts, you can also use relative paths indicated by two slashes to skip multiple intervening levels. The following jumps right down to the utterance level in stext and accesses each utterance separately, which means you can apply all sorts of text processing functions from above to it (such as nchar to get utterance lengths in characters):

This also means you can access any other level right away: sentences, words, punctuation marks, or even info from the header about the nature of the data, which here is spoken conversation (check out You can also immediately identify the lengths of all utterances (in terms of their components, i.e., sentences) and the speakers of all utterances, which are extracted from the who=" . . . " attribute-value pair of the utterance tag (see Figure

The last example was a first step toward getting at the attribute-value pairs of the XML tags. Above, we mostly looked at retrieving the data values of our XML data, but of course we want to also use the often detailed annotation that is within the tags. The above approach with xpathSApply(...,•...,•xmlAttrs) works best if tags have just one attribute, as the utterance tags do, but it is more cumbersome when elements have more attributes; see the code file for the matrix output you get from this:

For instance, we have seen that words have much more annotation and provide three attribute-value pairs: two with differently detailed POS tags and one with the word's lemma. How do we retrieve those easily? By using the function xmlGetAttr with a specification of which attribute we want (which I hope is reminiscent of the function attr discussed above): Finally, let us do some more advanced searches, searches that tap into different kinds and levels of annotation at the same time. A feature that they use is checking whether an attribute at some level has a certain value or not. The first of the following two lines should be read as "apply to those utterance elements of D94.file whose who-value (i.e., speaker) is 'D94PS000' the function xmlValue (which extracts the data value)", which returns a nice character value of D94PS000's utterances. The second line retrieves all words whose c5 tag says they are modal verbs:

Note how the logical expression that checks whether the c5 tag is "VM0" uses single quotes to keep them separate from the opening double quote before //w. The following examples work the same way but add a layer of complexity in that they do not just return the data value of the elements that meet a certain condition, but their attribute values (POS tags in the first, lemmas in the second):

Finally, let's recognize that we can combine logical conditions both on one level of hierarchical organization and on different levels. The first example determines how often will is used as a modal verb by combining two conditions on the level of the word tag, namely the c5 and the hw attribute-value pairs; the second one determines how often a certain speaker uses the lemma will as a modal verb by combining one condition on the utterance level with two additional ones on the word level; the third adds yet another condition on the sentence level to that:

As you can see, this package allows us to do very powerful searches of the kind we'd expect from a database. Many of them, for instance those just targeting words and their tags, could be done with regular expressions, but the kinds of searches exemplified here at the end show how useful some knowledge of how to use R for XML with XPath searches really is because they allow you to combine information from different levels of annotation relatively easily -all of the above is possible even when you treat an XML file as a text file -but trust me, it's painful. In what follows, I provide a very brief overview of a few additional ways in which the XML package can help you search XML corpus data in sophisticated ways. I will focus in particular on three things: (1) how speaker-specific information can be retrieved; (2) how you can search and retrieve information from different levels of the XML tree; and (3) some XPath syntax aspects that allow for simple character processing. The examples in this part will be based on the BNC Word Edition file <_qclwr2/_inputfiles/corp_H00. xml>, part of whose header is shown in Figure

Given the previous discussion, the following should be mostly obvious ways of retrieving all information regarding the speakers as lists (lines 1 and 2), retrieving all the names of all the attributes provided for each speaker (line 3, which previews the notion of an anonymous function you will learn more about in Section 3.10), and retrieving two specific characteristics for each speaker (lines 4 and 5): The next few lines should be similarly obvious: They apply the logic of logical expressions in xpathSApply applied above to sentences, words, etc. now to the person tag in the header: the following lines retrieve all the attributes of the (one and only) male speaker (line 1), his "id" attribute which is the code all his utterance tags will use in the who-attribute (line 2), and the data value of this tag, which here turns out to be his name, "Ken" (line 3):

Note that you can unfortunately not expect complete consistency in the data all the time: If you tweak the above line to retrieve data for the female speakers (by replacing ='m' with ='f'), then you will see that xmlValue does not return the female speakers' names but their occupation. Once you know the male speaker's id, it is easy to retrieve what he said in sentences (line 1) or in words (line 2):

and we can combine this kind of query with everything we discussed above to, for instance, find out which inflectional forms of the verb lemma work the male speaker uses:

Let me finally mention that XPath also allows you to exploit more features of the hierarchical tree that an XML file represents. First, you can use so-called XPath axes which define the relationships between different nodes in an XML tree using kinship and ordering terms such as ancestor, child, sibling, or parent on the one hand and preceding and following on the other. Thus, the following line retrieves from H00.file the utterances by the speaker with the id "PS2AD", specifically sentence 162, looks for words whose POS tag is "VERB" and whose lemma is "work", and then recovers the data values of the preceding words (which are siblings of work by virtue of being in the same sentence): See the code file for another similar example. Along the same lines, the next example does something similar but does not retrieve data values, but values of an attribute, namely lemma annotation. Specifically, it retrieves from H00.file the utterances by the speaker with the id "PS2AD", looks for words whose POS tag is "VERB" and whose lemma is "work", and then recovers the hw attributes (i.e., lemmas) of the next preposition(s) (which, too, are siblings of work by virtue of being in the same sentence):

Second, XPath allows you to use so-called predicates to utilize numerical and textual characteristics of the data for your searches. Such predicates work similarly to the logical expressions you already know from above (e.g., "u[@who = 'PS2AD']") by stating a condition and selecting those data values or attributes etc. for which the condition is true. Space does not permit an exhaustive discussion of everything XPath has to offer, so a few examples of predicates, whose names make it very obvious what they do, shall suffice. The next two lines, for example, return all words that have more than six characters, which may include spaces (line 1) and all words beginning with the character "q" (line 2): The next two lines return all sentences that contain a word beginning with a "q" (line 1) and their sentence numbers in the file (line 2):

>•xpathSApply(H00.file,•"//w[starts-with(text(),•'q')]/ parent::s",•xmlValue) ¶ >•xpathSApply(H00.file,•"//w[starts-with(text(),•'q')]/ parent::s",•xmlGetAttr,•"n") ¶

Finally, the next two lines retrieve all sentences beginning with "Was" (line 1) and all words beginning with "w" in sentences beginning with "Was" (line 2): Again, I hope it becomes obvious why having even only the most basic knowledge of XPath and how to use it in R is a good thing for any corpus linguist. Let me finally very briefly mention the package xml2, which is useful to avoid the XML package's memory leak on Windows. With that package, you can load an XML file as follows:

You can then access some information regarding the structure and parts of this object using the following set of functions: We can also very easily convert these results into character vectors that we can then handle with regular expressions. The following line retrieves all adjectives: The next section introduces a few functions to handle files and folders; some of these will do things you usually perform using parts of your operating system such as Explorer in MS Windows.

File and Directory Operations

You have already encountered one of the most important functions. On all three major kinds of operating systems, Windows, Mac OS X, and Linux, you can use the function file.choose to interactively choose one file in a file manager window. On Windows, the additional function choose.files allows you to interactively choose one or more files for either reading or writing (or to create one new one); on Mac OS X and Linux, this function is unfortunately not available, but there is a bit of a workaround based on the package rChoiceDialogs, which requires the package rJava and which you can load as follows:

This package provides a useful function called rchoose.files, which allows you to interactively choose multiple (already existing!) files and returns the paths of the files you chose as a character vector (which you can then loop over, for instance, to load each file with scan).

Other important functions have to do with directories. First, the package rChoiceDialogs again provides a useful function called rchoose.dir, which allows you to interactively choose an existing directory (and on Windows even to create a new one). Then, you need to know a bit about R's working directory, i.e., the directory that is accessed if no other directory is provided in a given function. By default, i.e., if you haven't changed R's default settings, then this is either R's installation directory, the logged-in user's main data directory, or, if you use RStudio and double-clicked on an R script file (< . . . .r>), then it's the directory that file is located in; most of the time while you're working through this book, it should therefore be <_qclwr2/_scripts>, which is what some output operations in Chapter 5 assume. You can easily find out what the current working directory is by using getwd without any arguments at the prompt:

If you want to change the working directory at the console, you use setwd and provide as the only argument a character string with the new working directory (either manually or again with rchoose.dir()). Note that when you do this in RStudio on Windows, the Explorer window sometimes doesn't come to the foreground so you may have to Alt-Tab your way to it (or find it in the taskbar). The first one specifies the directory whose contents are to be listed; if no argument is provided, the current working directory is used. The second argument is an optional regular expression with which you can restrict the output. If, for example, you have all 4,049 files of the BNC World Edition (the XML version) in one directory, but you only want those file names starting with "D", this is how you could proceed:

The default setting of the third argument returns only visible files; if you change it to TRUE, you would also get to see all hidden files. The default setting of the fourth argument only outputs the names of the files and folder without the path of the directory you are searching. If you set this argument to TRUE, you will get the complete path for every file and folder, which is usually recommended. By the way, the function basename does exactly the opposite: It cuts off all the path information.

>•basename("/_qclwr2/bestbookever.txt") ¶

Finally, the default setting recursive=FALSE only returns the content of the current directory, but if you set this to TRUE, the contents of all subdirectories will also be returned. This is of course useful if your corpus files come in a particular directory structure. For example, you can use the following line in a script to prompt the user to input a directory and then store the names of all files in this directory and all its subdirectories into one vector called corpus.files, and we will do just that very often in Chapter 5:

Apart from the number of files in a directory and their names, however, you can also retrieve much more useful information using R. The function file.info takes as input a vector of file names and outputs whether something is a file or a directory, the sizes of the files, the file permissions, and dates/times of file modification, creation, and last access. Thus, the easiest and most comprehensive way to take a detailed look at your working directory is therefore:

There are some other file management functions, which are all rather self-explanatory; I will only provide some of the options here and advise you to explore the documentation for more details:

• file.create(x) creates the file(s) in the character vector x;

• dir.create(x) creates the directory that is provided as a character string either as a directory in your working directory or as a relative or rooted directory; thus, dir.create("temp") will create a directory called "temp" in your current working directory and dir.create("../temp") will create a directory called "temp" one directory above your current working directory; • unlink(x) deletes the files/directories in the character vector x; file.remove(x) can only delete files; • file.exists(x) checks whether the file(s) in the character vector x exist(s) or not; • download.file(x,•...) downloads the website at the URL x (e.g., <http:// . . . >) into the file . . . specified).

Sometimes, you need to use a data structure, such as a data frame or a list, again and again; as such, you want to save it into a file. While you can always save data frames as tab-delimited text files with write.table, which are easy to use with other software, sometimes your files may be too large to be opened with other software (in particular spreadsheet software such as Microsoft Excel or LibreOffice Calc). The option of using raw text files may also not be easily available with lists. Finally, you may want to save disk space and not only save your data, but also compress them at the same time. In such cases, you may want to use save. In its simplest syntax, which is the only one we will deal with here (see the documentation for more detailed coverage), you can save any object into a binary compressed file, which will usually be much smaller than the corresponding text file and which, if you gave it the extension ".RData", makes your data available to a new R/RStudio session upon a simple double-click. As an example, consider a data frame aa with the following structure (details such as factor levels etc. are omitted): You will be prompted to choose a file, and then the data structure(s) will be available (type ls() ¶ at the console to get a listing of all objects in your current workspace). Also, you can just save your whole R workspace, not just one data structure, into a user-definable file with save.image, which is useful to store intermediate results for later processing into an. RData file. This file can either be loaded from within R, or you can just double-click on them to open R and restore your workspace.

You can now open the file with an image viewer (e.g., GIMP) and look at it.

Recommendations for further study/exploration

• On how to read tab-delimited text files: ?read.csv ¶.

• On how to produce text representations of objects into files: ?dump ¶ and ?dput ¶.

• On how to divert output to a file instead of the screen/console: ?sink ¶.

• On how to handle compressed files: ?bzfile ¶ and ?gzfile ¶.

Writing Your Own Functions and Some Final Recommendations

The fact that R is not just a statistics software but a full-fledged programming language means you are not limited by the functions that already exist in R or in any package that you may install -you can fairly easily write your own functions to facilitate and/or automate tedious and/or frequent tasks. In this section I will give a few very small examples of the logic of how to write your own functions, which is something that will become extremely useful when we turn to the case studies in Chapter 5.

Let's imagine a scenario in which we want to generate a frequency table of a vector, which we would normally do with table, but we do not want the table sorted alphabetically (table's default) or by frequency (which we would do with sort(table(...)) -we want our table to be sorted by the order of occurrence in the input vector. Let's quickly generate a vector that contains a random sample of the first ten letters of the alphabet (see again ?letters ¶):

That means we want our frequency list to first give the frequency of the letter "c", then that of "d", then "f", then "j", then "i" (because "c" is already covered), etc. The easiest way to write a function consists of the following three steps:

1 You write the code that you would use if you didn't write a function (i.e., as we have always done so far). 2 You determine which of the data structures you are using in your code are required for the code/function to work. 3 You wrap a function definition around your code and (1) make sure that the function definition requests or defines the required data structures (and any others you may want to add) and (

We begin with step 1. If we have a vector qwe of which we want a frequency list, but one sorted in a particular way, then we should probably first generate a default frequency list:

But now we want to have this output ordered as mentioned above. That means we need to find for each letter that is actually attested in qwe -i.e., the names of the frequencies in asd -at what position it shows up in qwe for the first time. This formulation should remind you of the function match, which, to quote from above, "match returns a vector of the positions of (first!) matches of its initial argument in its second":

That means the letter "a" (the first name of asd) shows up in position 10 of qwe; the letter "b" (the second name of asd) shows up in position 12 of qwe, and, crucially, the letter "c" (the third name of asd) shows up in position 1 of qwe, etc. That in turn means two things: (1) we want the above output reordered so that the 1 (for "c") comes first, the 2 (for "d") comes second etc.; and (2) once we have that ordering of these numbers, we can then apply these numbers to re-order asd.

Hence for (1):

Now that we have completed step 1 of writing a function, we now turn to step 2 and look at all the code we wrote and determine which data structures are minimally required for this whole thing to work. It turns out the only data structure required is the input vector, which above was called qwe, and this is because there is only one other data structure we use, which is asd, but asd can be derived from qwe. Thus, we now can turn to step 3: We take all the code we wrote, but wrap it into a function definition (let's assume we want to call our function table.1stocc, for table sorted by first occurrence), ensure that the user is forced to provide an input vector, and use more revealing names than qwe, asd, and the like. In the code below, I assign what we would like the function to output -the sorted frequency table -to an object called output, which then, by ending the function definition with return(output), is returned as the result of the function:

Now we can test our new function by giving it qwe as an argument. The function then maps qwe onto the function-internal more usefully named input.vector and executes everything without problems. If you save this little bit of code now, the function definition, then you never have to worry about how to do this again: If this task ever comes up (again), just remember you have this function, problem solved:

Let us briefly discuss one other interesting application, namely a function that we will use a lot in the scripts in Chapter 5, a function we will call just.matches. Let us first re-generate the example vector txt from above:

Let's assume we want to find all words that end in "is"; this is how we would use regmatches and gregexpr for this when we want the result in a vector:

Since finding exact matches like this (and maybe vectorizing them) is definitely a very frequent task for a corpus linguist, it would be nice if this could be done shorter and without having to repeat the argument txt twice. Thus, we could write a function just.matches that does this for us, and here's one way to go about this:

The first line defines a function just.matches, but also specifies that it requires at least two arguments -the search expression and the input/corpus vector. However, the function definition also states that an additional argument pcre is by default -i.e, unless otherwise specified by the user -set to TRUE, as is another additional argument vectorize. That is, if the user does not change these settings, these are used by R when this function is run. Finally, there is an ellipsis at the end of the function definition, which stands for "whatever other arguments a user might want to provide to a certain location in the body of the function, where the ellipsis is taken up again".

The body of the function then defines an object output using nearly the exact same code we used manually before, but (1) it sets gregepxr's perl argument to the value of pcre that is defined when just.matches is called (i.e., by default to TRUE);

(2) it uses the value of vectorize to decide whether the user receives a vector (the default) or a list; and (3) it passes any arguments the user may have provided to just.matches on to the regmatches function, which means we can, for example, use invert in our function:

(Of course, the former at least could also be done with the function I provide, exact. matches.2, which could also be tweaked to accept invert as well -if I ever needed that functionality, I might update exact.matches.2 accordingly.) I hope you can see by now how useful it can be to recognize that some task will be repeated a lot so it may be more efficient to define a function for it, which usually not only speeds up the process considerably, but also makes your code easier to read for you; we will use this function a lot of times in the case studies below -sometimes I will use exact. matches.2, sometimes just.matches, and sometimes both (to show how they would yield the same results).

You should now do "Exercise box 3.10: Writing your own functions."

The penultimate recommendation of this chapter is concerned with the very useful notion of so-called anonymous, or inline, functions. These are functions that are (1) defined as above but not given a name and (

We would now want to use sapply to access each element of words and count the number of word types. But there is no function to count the number of unique word typesthere is only a function to count the number of tokens, which is length. Thus, we can do the following but it doesn't give us what we want:

In other words, we are applying length to each element of words, but what we need is to apply length to the result of applying unique to each element of words, but there is no function for length(unique(...)). We could just solve this ad hoc by using the following bit of code:

It works, but for didactic purposes we want to talk about how to solve this with a function or, more importantly, an anonymous/inline function. Thus, we could define a function for that like we did above (return is not used here because it's not strictly speaking necessarythe function will automatically return the last-assigned object):

Or, because this may seem like overkill, we do the following, namely put the body of our definition of types into the second argument slot of sapply directly. In the following line, we apply to the elements of words a nameless (hence anonymous) function that requires one argument that is internally called x and to which R applies length(unique(x)); note that you of course don't have to use x -you can use any name, but just like with regular functions above, whatever name you use must be the one that is in the ... slot of length(unique(...)). This way, we avoid having to define a function and just do the definition and application of the function in one line (hence inline). Note that the x in the function definition is not available outside of the function, which means it doesn't matter if there is or isn't an object called x before you run the line below, and it means that after you run the line below there is still no object called x. x is used for the duration of the anonymous function only and does not get retained in your general workspace.

Anonymous functions can sometimes be very useful and we will use them on occasion. Finally, most of this book uses only the functionality from base R so as to minimize users' dependence on functions in additional packages, which may introduce changes that are not backwards-compatible more often than base R will. However, one of the few exceptions I want to make to this practice involves an extremely useful operator from the package dplyr (

>•library(dplyr) ¶

Remember how we generated just the exact matches with our 'normal' R code? We did it like this (or see the more heavily commented version using several lines and indentation in the code file):

The 'annoying' thing about this is that, if you want to understand what the code does, you have to process it from the inside out: The first thing that is done is the gregexpr search, the result of which is then passed outside to regmatches, the result of which is then passed outside to unlist -not exactly intuitive, and this example involves just one short line and two nested function calls. The operator %>% can simplify this by making the order in which you write functions in the code the same as the order in which they are executed. Here is a very simple example, which you can read as 'take txt and to it apply nchar':

This is the same as nchar(txt) ¶: The function nchar requires one argument and the %>% operator takes its left-hand side (lhs, the vector txt) and makes it the first argument of its right-hand side (rhs, the function nchar). Now what if the lhs is not the first argument of the rhs? Then you can use a period to indicate where in the rhs the lhs should be, as in the following example, where we make txt the second argument in the call of just. matches. So this should be read as "take txt and give it to just.matches (as the second argument) to look for "\\w*is\\b" in it":

So how does this apply to the above line returning just the matches? The following is read as "take txt, make it the thing to be searched by gregexpr (i.e., gregexpr's second argument), take the result from gregexpr and make it the second (match data) argument of regmatches, take the result from that and unlist it":

While this is of course not as reader-friendly and concise as if you already have a function just.matches, it is much more reader-friendly than the previous code version with the multiple embedding, because you read it from top to bottom, which is the order in which things are done, rather than having to read code from the inside out (rather than the usual left-to-right). Thus, check the code file for how sleek a definition of this table.1stocc this operator allows you to write. This operator takes a bit of getting used to and to make good use of it you have to remember that operators such as subsetting or subtracting can be written as functions (namely "[" and "-") etc., but it is such a useful alternative to multiple embeddings that several of the case studies in Chapter 5 will provide the traditional code, but also the %>% alternative.

Notes

1 If you let R output vectors to the screen that unlike the above are so long that they cannot be shown in one row anymore, R will use as many lines as necessary to represent all elements of the vector, and each line will start with the number (in square brackets) of the first element of the vector that is given in each line. Try it out: enter 1:100 ¶ at the prompt. 2 This is actually a generally important concept: R shows missing or unavailable data as NA. Since this may happen for many different reasons, let me mention two aspects relevant in the present context. First, you can check whether (some elements of) something is/are not available using the function is.na which takes the element to be tested as its argument. For example, if the vector a consists only of the number 2, then is.na(a) gives FALSE (because there is something, the number 2), but is.na(a

[I]t seems to me that future research should deal with frequencies in a much more empirically sound and statistical professional way. . . . In the long run, linguistic theorising without a firm grip on statistical methods will lead corpus linguistics into a dead-end street.

Introduction to Statistical Thinking

Before we begin to actually explore ways corpus data can be analyzed statistically with R, we need to cover a few basic statistical concepts. The first concept is that of a variable. A variable is a symbol of a set of values or levels characterizing an entity figuring in an investigation; in the remainder of this chapter, variables will be printed in small caps. For example, if you investigate the lengths of subjects (by counting their lengths in words), then we can say the subject of The man went to work gets a value of 2 for the variable Length. There are two ways in which variables must be distinguished: in terms of the role they play in an analysis and in terms of their information value, or level of measurement.

Variables and Their Roles in an Analysis

You will need to distinguish between dependent and independent variables. Dependent variables are the variables whose behavior/distribution you investigate or wish to explain; independent variables are the variables whose behavior/distribution you think correlates with what happens in the dependent variables. Thus, independent variables are often, but not necessarily, the causes for what's happening with dependent variables. 1

Variables and Their Information Value

In this brief chapter we will distinguish only two classes of variables. The variable class with the lower information value of the two is that of categorical variables. Given what you read in Section 3.3, you will not be surprised to read that categorical variables of this kind are usually stored as factors in R, and maybe sometimes as character vectors.

The other variable class to be distinguished here is that of numeric variables. For example, the syllabic length of an NP is a numeric variable; other examples are pitch frequencies in hertz, word frequencies in a corpus, number of clauses between two successive occurrences of two ditransitives in a corpus file, and the reaction time toward a stimulus in milliseconds. Again, given what you have read in Chapter 3, you will correctly expect that such variables are stored as numeric vectors in R.

This classification of variables in terms of their role in an analysis and their information value will be important to choose the right statistical technique for the evaluation of data because not every statistical technique can be applied to every kind of variable.

Hypotheses: Formulation and Operationalization

One of the most central notions in statistical analysis is that of a hypothesis. The term hypothesis is used here in a somewhat stricter sense than in the everyday sense of "assumption". Following

While the first criterion needs no additional explanation, the second criterion requires a little elaboration. The logic behind this criterion is that, while hypotheses typically have one of the above syntactic forms, they need not. All that is required is that the statement can be transformed into a conditional sentence. For example, the statement On average, English subjects are shorter than English objects does not explicitly instantiate a conditional sentence, but it can be transformed into one: If a constituent is an English subject, it is on average shorter than a constituent that is an English object. Or, for example, the statement More frequent words have more senses than less frequent words does not explicitly instantiate a conditional sentence, but it can be transformed into one: The more frequent a word is, the more senses it has.

The third criterion should also briefly be commented on. It means that there must be conceivable states of affairs that show the hypothesis to be false. Thus, Drinking alcohol may influence the reaction times of subjects in an experiment is not a hypothesis in this sense because may influence basically means "may influence or may not", so every result of an experiment -an influence or a lack of it -is compatible with the hypothesis. Note that this criterion does not mean that the falsifying state of affairs is ever observed -it really only means that one can imagine it so that, if it ever were to happen, one could recognize it (by virtue of the similarity to the imagined outcome).

Above, hypotheses were characterized as invoking the notion of conditionality. This characterization reiterates the first central distinction of variables, that between independent and dependent variables. Simplistically speaking, independent variables are variables that are mentioned in the if-clause or the first the more/less clause, while dependent variables are variables mentioned in the then-clause or the second the more/less clause. It is important to understand, however, that there are two parameters according to which hypotheses are classified.

The first parameter refers to the contrast between the so-called alternative hypothesis (abbreviated as H 1 ) and the so-called null hypothesis (abbreviated as H 0 ). Usually, the former is a hypothesis that,

• if you look at just one dependent variable, states that the values/levels of the dependent variable do not follow a random distribution of the investigated entities (or some other expected distribution such as the normal distribution or a uniform distribution); • if you look at dependent and independent variables, states that the values/levels of the dependent variable vary non-trivially as a function of the values/levels of the independent variable(s).

For example, On average, English subjects are shorter than English objects would be an alternative hypothesis because it states that some non-trivial part of the variation you find in the lengths of XPs is due to the fact that the XPs instantiate different grammatical relations, namely subjects and direct objects. H 0 , by contrast, is the logical counterpart of H 1 that,

• if you look at just one dependent variable, states that the values/levels of the dependent variable are randomly distributed or do not vary quasi-systematically from some specified distribution (such as the normal or the binomial distribution); • if you look at dependent and independent variables, states that the values/levels of the dependent variable do not vary non-trivially as a function of the values/levels of the independent variable(s).

You can often form a H 0 by inserting not into H 1 (that is, On average, English subjects are not shorter than English objects), but typically H 0 states there is no effect (that is, On average, English subjects and English objects do not differ in their lengths or On average, English subjects and English objects are equally long).

The second parameter is concerned with the language of the hypothesis, so to speak. Both the H 1 and H 0 come in two forms. The first of these is the one you have already seen, a text form in natural human language. The other form is its 'translation' into a statistical form in mathematical language, which brings me to the issue of operationalization, the process of deciding how the variables in your text hypotheses shall be investigated. In other words, you answer two interrelated questions. First, "What will I perceive when we perform our study and observe the values/levels of the variables involved?" Second, "Which mathematical concept will I use -counts/frequencies, averages, dispersions, or correlationswhen I couch my hypothesis into numeric terms?" This is necessary because, often, the formulation of the hypotheses in text form leaves open what exactly will be counted, measured, etc. and how. In the above case, for example, it is not yet clear how you will know whether a particular subject is longer than a particular direct object:

(1) The younger bachelors ate the nice little parrot.

With regard to the question of subject and object lengths, this sentence can support either H 1 or H 0 , depending on how you operationalize the variable Length. If you operationalize Length as "number of morphemes", the subject gets a value of 5 (The, young, comparative -er, bachelor, plural s) and the direct object gets a value of 4 (the, nice, little, parrot). On the other hand, if you operationalize Length as number of words, the subject and the object get values of 3 and 4 respectively. Finally, if you operationalize Length as "number of letters (without spaces)", both subject and object get a value of 19.

With regard to the latter question, you also must decide on a mathematical concept. For example, if you investigate, say, 99 sentences and choose to operationalize lengths as numbers of morphemes, your hypotheses could involve averages: H 0 : The average length of the subjects in morphemes is the same as the average length of the direct objects in morphemes; mean length of subjects = mean length of direct objects.

H 1 : The average length of the subjects in morphemes is different from the average length of the direct objects in morphemes; mean length of subjects ≠ mean length of direct objects.

or frequencies:

H 0 : The number of cases where the subject is longer (in morphemes) than the direct object is the same as the number of cases where the subject is not longer (in morphemes) than the direct object.

H 0 : The number of cases where the subject is longer (in morphemes) than the direct object is different from the number of cases where the subject is not longer (in morphemes) than the direct object. While the discussion so far has focused on the situation in which we have maximally two variables, a dependent and an independent one, of course life is usually not that simplethere is virtually always more than one determinant for some phenomenon. While we will not deal with statistical techniques to deal with several independent variables here (but see Gries 2013: ch. 5), it is important for you to at least understand how such phenomena would be approached conceptually. We have been talking about the alternative hypothesis that, on average, English subjects are shorter than English objects, which involves one dependent variable (Length) and one independent variable (GramRelation: subject vs. object). For the sake of the argument, let us imagine you now suspect that the lengths of some constituents do not only vary as a function of their being subjects and objects, but also in terms of whether they occur in main clauses or subordinate clauses. Technically speaking, you are introducing a second independent variable, ClauseType, with two levels, main clause and subordinate clause. Let us also assume you do a pilot study -without having a hypothesis yet -in which you look at 120 subjects and objects (from a corpus) as shown in Table

You then count for each of the 120 cases the length of the subject or object in syllables and compute the mean lengths for all four conditions, subjects in main clauses, objects in main clauses, subjects in subordinate clauses, and objects in subordinate clauses. Let us assume these are the main effects you find, where a main effect is the effect of one variable in isolation:

• Constituents that are subjects are shorter than constituents that are objects.

• Constituents in main clauses are shorter than in subordinate clauses.

There are now two possibilities for what these results can look like (or three, depending on how you want to look at it). On the one hand, the two independent variables may work together additively. That means the simultaneous combination of levels of independent variables has the effect that we would expect on the basis of their individual effects. In this case, you would therefore expect that

• the shortest constituents are subjects in main clauses while the longest constituents are objects in subordinate clauses; • the length difference between main and subordinate clause subjects is the same as the length difference between main and subordinate clause objects; and • the length difference between main clause subjects and objects is the same as the length difference between subordinate clause subjects and objects.

This perfectly additive result is represented in a so-called interaction plot in Figure

There is yet another kind of interaction, which is represented in Figure

It is still an interaction because while the lines in the interaction plot do not intersect, the slope of the black line is much steeper than that of the gray line. Put differently, while the difference between the means of subjects and objects in main clauses is only two

THINK BREAK

syllables, it is four syllables in subordinate clauses, and because of this unexpected element, this is still an interaction.

Interactions are a very important concept, and whenever you consider or investigate designs with multiple independent variables, you must be careful about how to formulate your hypotheses -sometimes you may be more interested in an interaction than a main effect -and how to evaluate and interpret your data.

One final aspect is noteworthy here. Obviously, when you formulate hypotheses, you have to make many decisions: which variables to include, which variable levels to include, which kinds of relations between variables to consider (additive vs. interaction), etc. One important guideline in formulating hypotheses you should always adhere to is called Occam's razor. Occam's razor, named after William of Ockham (1285-1349), is the name for the following Latin principle: entia non sunt multiplicanda praeter necessitatem, which means "entities should not be multiplied beyond necessity" and, generally boils down to "try to formulate the simplest explanation or model possible". I use model here in the way defined by

• Don't invoke more variables than needed: If you can explain the same amount of variability of a dependent variable with two models with different numbers of independent variables, choose the model with the smaller number of independent variables. • Do not invoke more variable levels than necessary: If you can explain the same amount of variability of the dependent variable with two models that have different numbers of levels of independent variables, choose the model with the smaller number of levels of independent variables. For example, if you can explain 60 percent of the variation in the data with a variable that distinguishes two levels of animacy (animate vs. inanimate), then prefer this variable over one that distinguishes three levels (e.g., human vs. animate vs. inanimate) but also only explains 60 percent of the variation in the data. • Choose additive relationships over relationships involving interactions: If you can explain 60 percent of the variation of the dependent variable with a model without interactions, then prefer this model over one that involves interactions and also only explains 60 percent of the variation.

One motivation for this principle is of course simplicity: simpler models are easier to explain and test. Also, this approach allows you to reject the unprincipled inclusion of ever more variables and variable levels if these do not explain a sufficiently substantial share of the data. Thus, you should always bear this principle in mind when you formulate your own hypotheses.

Data Analysis

When you have formulated your hypotheses, and only then, you begin with the analysis of the data. (In other words, it is problematic to collect data, pour over them for hours in search of any pattern that might be interesting, see some pattern, and then declare you have a hypothesis, namely, tadaah!, the pattern and then proceed with statistical testing.) For example, your analysis might begin with you writing a script to retrieve corpus data.

The best way of handling data is to store your data in a data frame that you either edit in R or, more conveniently, in a spreadsheet software such as LibreOffice Calc. There are a few crucial aspects to your handling of the data, which I cannot emphasize enough. First, every row represents one and only one analyzed case or observation of the dependent variable. Second, every column but the first represents either the data in question (e.g., (parts of) a concordance line to be investigated) or one and only one variable with respect to which the data are coded. Third, the first column just gets a counter from 1 to n so that you can always restore the data frame to one particular (the original?) order, and the first row should contain the names of all columns (i.e., variables). Fourth, missing data points are entered as NA (not as empty cells). In corpus studies, it is often useful to also have columns for variables that you might call "source of the data"; these would contain information regarding the corpus file and the line of the file where the match was obtained etc. Let us assume you investigated the alternative hypothesis Subjects are longer than objects (operationalizing length as "length in words"). First, a think break: What is the independent variable, and what is the dependent variable?

The independent variable is the categorical variable GramRelation, which has the levels subject and object, and the dependent variable is the numeric variable Length. If you now formulated all four hypotheses -H 0 and H 1 in text and statistical forms -and decided to analyze the (ridiculously small) sample in (

The data frame in Table

THINK BREAK

Table

An even more precise variant may have an additional column listing only the subject or object that is coded in each row, i.e., listing The younger bachelors in the first row. As you can see, every data point -every subject and every object -gets its own row and is in turn described in terms of its variable levels in the two rightmost columns. This is how you should virtually always store your data. Ideally, you save data from text processing with R into a format that already looks like that of Table

Hypothesis (and Significance) Testing

Once you have created a data frame such as Table

This may seem a little confusing because there are suddenly two probabilities: one that says how likely you are to get your result when H 0 is true, another one which is typically set to 0.05. This latter probability p is the probability not to be exceeded to still accept H 1 .

It is called significance level and is a threshold value defined before the analysis. The former probability p to get the obtained result when H 0 is true is the probability to err when accepting H 1 . It is called the probability of error or the p-value -this p is computed on the basis of your data, i.e., computed after the analysis. You may now wonder how this probability is computed, but this is a question we will not be concerned with in this book -see

He was locking the door. Subject 1 4

He was locking the door. Oobject 2 5

The quick brown fox hit the lazy dog. Subject 4 6

The quick brown fox hit the lazy dog. Object 3

THINK BREAK

approximations, which work very well if the data meet certain distributional criteria, or non-parametrically, e.g., on the basis of counting frequencies of different outcomes out of all possible outcomes. In the remainder of this chapter, we will use R functions to make either of those two computations. Now that the groundwork has been laid, we will look at a few statistical techniques that allow you to compute p-values to determine whether you are allowed to reject the H 0 and, therefore, accept your H 1 . For each of the tests, we will also look at all relevant assumptions. Unfortunately, considerations of space do not allow me to discuss more than some of the most elementary monofactorial statistical techniques and skip the treatment of all the techniques that would also be useful, in particular the multifactorial statistics that one ultimately always requires. Since the graphical representation and exploration of distributional data often facilitates the understanding of the statistical results, I will also briefly show how simple graphs are produced. However, again considerations of space do not allow for a detailed exposition here in the book, so do study the code files, which provide a lot of information, review the documentation of the graphics functions I mention, and refer to the documentation for par,

Categorical Dependent Variables

One of the most frequent corpus-linguistic scenarios involves categorical dependent variables in the sense that one records the frequencies of several mutually exclusive outcomes or categories. We will only distinguish two cases, one in which one just records frequencies of a dependent variable as such without any additional information (such as which independent variable is observed at the same time) and one in which you also include a categorical independent variable.

No Independent Variables

Let us look at a syntactic example, namely the word order alternation of English verbparticle constructions as exemplified in (3):

(3) a. He brought back the book.

Verb Particle DirectObject b. He brought the book back.

Verb DirectObject Particle

Among other things, one might be interested in finding out whether the two semantically so similar constructions are used equally frequently. To that end, one could decide to look at corpus data for the two constructions. The first step of the analysis consists of formulating the hypotheses to be tested. In this case, this is fairly simple: Since H 0 usually states that data are not systematically distributed, the most unsystematic distribution would be a random one, and if sentences are randomly distributed across two construction categories, then both constructions should be equally frequent, just like tossing a coin will in the long run yield nearly identical frequencies of heads and tails. Thus: Gries (

As a first step, we look at the distribution of the data. One simple way of representing this distribution graphically would be to use a bar plot:

The question that now arises is of course whether this is a result that one may expect by chance or whether the result is unlikely to have arisen by chance and, therefore, probably reflects a regularity to be uncovered. The actual computation of the test is in this case very simple, but for further discussion below it is useful to briefly comment on the underlying logic. The test that is used to investigate whether an observed frequency distribution deviates from what might be expected on the basis of chance is called the chi-squared test for goodness of fit (because it tests how good the fit of the observed data is to some expected distribution). In this case, H 0 states the frequencies are the same so, since we have two categories, the expected frequency of each construction is the overall number of sentences divided by two (Table

Like with most statistical tests, however, we must first determine whether the test we want to use can in fact be used. The chi-squared test should only be applied if:

• all observations are independent of each other (i.e., the value of one data point does not influence that of another); and • all expected frequencies are larger than or equal to 5. 2   The latter condition is obviously met, and we assume for now that the data points are independent such that the constructional choice in any one corpus example is independent of other constructional choices in the sample -note that this assumption is often not correct (because, often, one speaker contributes multiple data points, which means all the data points he contributes could be affected by that speaker's idiosyncratic behavior) and would therefore need to be carefully defended in each case.

The actual computation of the chi-squared value is summarized in the equation in (

(4) χ - . ≅ You take the observed value of each cell in the table, subtract from it the expected value for that cell, square the obtained difference, divide it by the expected value for that cell again, and sum up all values (see the code file for a quick manual computation of this value). Each of these two summands is sometimes referred to as a contribution to chi-squared.

However, we just do the whole test in R, which is extremely simple. Since R already knows what the observed data look like (from the vector Gries.2003), we can immediately use the function chisq.test to compute the chi-squared value and the p-value at the same time by providing chisq.test with three arguments: a vector with the observed data, a vector with the probabilities resulting from H 0 (i.e., two times 0.5, because according to H 0 we have two equally likely constructions), and correct=TRUE or correct=FALSE: If the size of the data set n is small (15 ≤ n ≤ 60), it is sometimes recommended to perform a so-called continuity correction; by calling correct=TRUE you can perform this correction. We immediately assign the result of this test to an object G2003. test and also check out its structure:

The result is unambiguous: The p-value is much larger than the usual threshold value of 0.05 so we conclude that the frequencies of the two constructions in Gries's sample do not differ from a random distribution. Another way of summarizing this result would be to say that Gries's data can be assumed to come from a population in which both constructions are equally frequent.

You may now wonder what the df-value means. The abbreviation "df" stands for "degrees of freedom" and has to do with the number of values that were entered into the analysis (two in this case) and the number of statistical parameters estimated from the data (one in this case); it is customary to provide the df-value when you report all summary statistics, as I will exemplify below for each test to be discussed. For reasons of space, I cannot discuss the notion of df here in any detail but refer you to full-fledged introductions to statistics instead (e.g.,

As you can see from the structure output, chisq.test also computes the expected frequencies as well as a variety of other results, which are all stored in a list. If we are interested in the expected frequencies, we just need to call the part of the result/ list we are interested in:

The usual statistical lingo to summarize this result would be something like this: "The verb-particle construction where the particle directly follows the verb occurs 194 times although it was expected 201.

You should now do "Exercise box 4.3: Unidimensional frequency distributions."

Recommendations for further study/exploration

• On another way to test whether an observed frequency differs from an expected percentage: ?prop.test ¶.

One Independent Categorical Variable

The probably more frequent research scenario with dependent categorical variables, however, is that one records not just the frequency of some dependent variable but also that of an independent categorical variable that one might suspect is correlated with the dependent one. Fortunately, the required method is very similar to that of the previous section: It is called the chi-squared test for independence and has the same two requirements as the chi-squared test in the previous section. In order to explore how this test works when an additional variable is added, let us revisit the example of

Again, we first formulate the hypotheses. Since we have seen in the previous section how the chi-squared statistic is computed -namely with the absolute differences between observed and expected values in the numerator of (4), we immediately add the statistical hypotheses as well:

H 0 : The frequencies of the two constructions (V Part DO vs. V DO Part, the dependent variable) do not vary depending on whether the referent of the direct object is abstract or concrete (the independent variable); chi-squared = 0.

H 1 : The frequencies of the two constructions (V Part DO vs. V DO Part, the dependent variable) vary depending on whether the referent of the direct object is abstract or concrete (the independent variable); chi-squared > 0.

We first read the data from <_qclwr2/_inputfiles/stat_vpc.csv> into R:

Since we now know the column names etc., we tabulate (as usual, the first-named variable goes into the rows) and use the generic plot command to get a so-called mosaic plot of a table of our two variables (we use t(table(...)) to transpose the table so that the plot has the same row-column arrangement as the table ):

In this kind of plot, the frequencies of the two levels of Concreteness are reflected in the widths of the bars, while the frequencies of the two constructions within each level of

Note that you can compute the expected frequency for any of the four cells of the table of the observed frequencies by multiplying the row total of the row of a cell by the column total of the column of that cell and dividing that by the overall total of the table, as is here shown for the top left cell:

The results show that the variable Concreteness is highly significantly correlated with the choice of construction. However, there are two things we still don't really know. One is how strong the effect is: It is important to note that one cannot use the chi-squared value as a measure of effect size, i.e., as an indication of how strong the correlation between the two investigated variables is. This is due to the fact that the chisquared value is dependent on the effect size, but also on the sample size. We can test this in R very easily:

As is obvious, when the sample is increased by one order of magnitude, so is the chi-squared value. This is of course a disadvantage: While the sample size of this increased table is ten times as large, the relations of the values in the table have of course not changed. In order to obtain a measure of effect size that is not influenced by the sample size, one can transform the chi-squared value into a measure of correlation. This is the formula for the measure φ (read: phi, for k × 2 / m × 2 tables, where k and m are the numbers of rows and columns respectively) or Cramer's V (for k × m tables with k, m > 2):

The theoretically extreme values of this correlation coefficient are 0 (no correlation) and 1 (perfect correlation). With R, this computation can be done in one easy step, but for expository reasons we break it down into smaller steps:

Thus, the correlation is not particularly strong but highly significant. But where does it come from and how can the results be interpreted? In this simple case, the mosaic plot above already gave it all away, but in tables with more rows and columns, interpreting the result from such a plot may be more difficult. Thus, the most straightforward way to answer these questions involves inspecting (1) the so-called Pearson residuals and/or (2) an association plot. The Pearson residuals indicate the degree to which observed and expected frequencies differ: The more they deviate from 0, the more the observed frequencies deviate from the expected ones. Each residual of a cell is obs-exp / sqrt(exp) , so if you square them you get the above-mentioned contributions to chi-squared, whose sum in turn corresponds to the overall chi-squared value. They are generated just like the expected frequencies, by calling up the result of the chi-squared test again, this time requesting another part of the output normally not provided:

This table is interpreted as follows: Positive values mark observed frequencies which are larger than expected and negative values mark observed frequencies which are smaller than expected, and you already know that the more the values deviate from 0, the stronger the effect. Thus, the strongest effect in the data is the strong preference of abstract objects to occur in V DO Part; the second strongest effect is the dispreference of abstract objects to occur in V Part DO etc. (In this case, all residuals are similarly high, but in tables where this is not so, one could distinguish the cells that matter from those that do not.)

A visual approach to the same issue involves a so-called association plot (Figure

In this representation, black and gray boxes represent table cells whose observed frequencies are greater and smaller than the expected ones respectively (i.e., what corresponds to positive and negative Pearson residuals), and the area of the box is proportional to the difference in observed and expected frequencies (in a way you can read up on at ?assocplot ¶).

In sum, the data and their evaluations would be summarized as follows: "According to a chi-squared test for independence, there is a statistically highly significant albeit moderate correlation between the choice of a verb-particle construction and the abstractness/ concreteness (χ 2 = 46.18; df = 1; p < 0.001; φ = 0.34). The significant result is due to the fact that the construction where the particle follows the verb directly is preferred with abstract objects while the construction where the particle follows the direct object is preferred with concrete objects."

Let me briefly mention one important area of application of the chi-squared test and other related statistics: measures of collocational strength. Collocational statistics quantify the strength of association or repulsion between a node word and its collocates. Node words tend to attract some words -such that these words occur close to the node word with greater than chance probability -while they repel others -such that these words occur close to the node word with less than chance probability -while they occur at chance-level frequency with yet others. Let us look at one example from the case study chapter below, the collocation of alphabetical order. In the BNC, there are 6,024,359 numbered sentences (occurrences of "<s•n") and 225 and 28,662 such sentences that

As you can see, this distribution is highly significant because the observed co-occurrence of alphabetical and order (96) is much larger than the expected one (1.07, check example. test$expected). Two issues have to be mentioned, however. First, the chi-squared test is actually not the best test to be applied here given the small expected co-occurrence frequency, which is much smaller than five. As a matter of fact, there is a huge number of other statistics to quantify the reliability and the strength of the co-occurrence relationship between two (or more) words (see

You should now do "Exercise box 4.4: Two-dimensional frequency distributions."

Numeric Dependent Variables

Another frequent kind of scenarios involves numeric dependent variables. In Section 4.3.1 I will briefly review a few useful descriptive statistics for such dependent variables, but will not discuss significance tests for this scenario (cf. Gries 2013 for much discussion and worked examples). In Sections 4.3.2 and 4.3.3 we will then distinguish two cases, one in which you have a two-level categorical independent variable and one in which you have a numeric independent variable.

No Independent Variables

First, let us briefly review some useful descriptive statistics with which you can summarize numeric data. Usually, one distinguishes measures of central tendency -what you probably know as averages -and measures of dispersion.

Measures of central tendency serve to summarize the central tendency of a numeric variable in a single statistic. The most widely known measure of central tendency is the mean. You compute the mean by summing all observations/cases and dividing that sum by the number of observations. In R you use mean, which takes as its only argument a vector of values:

Despite its apparent simplicity, there are two important things to bear in mind. First, the mean is extremely sensitive to outliers: A single very high or very low value can influence the mean so strongly that it stops being a useful statistic. In the example below, the mean value of 168.1667 neither represents the first five nor the sixth value particularly well:

In such cases, you should better either trim the data of extremes (using mean's argument trim) or choose the median as a less sensitive measure of central tendency. If you sort the values in ascending order, the median is the middle value (or the mean of the two middle values). As you can see below, the median of this vector is 2, which represents the distribution of the first five values very well. Thus, whenever you look at a distribution as extreme as the above example or whenever your data are better conceived of as ordinal (i.e., rank data), you should use the median, not the mean:

The second important thing to bear in mind is to never, ever (!) report a measure of central tendency without a measure of dispersion because without a measure of dispersion, i.e., the variability of the data around its central tendency, one can never know how well the measure of central tendency does in fact summarize all of the data: Obviously, if in the case of the above extreme distribution you only report a mean in 168.17 without reporting the huge dispersion of the distribution, you're really not providing anything informative at all. 3 Let us look at a less contrived example by comparing two cities' average temperature in one year. >•City1<-c

From the means alone, it seems as if the two cities have very similar climates -the difference between the means is very small. However, if you look at the data in more detail, especially when you do so graphically as in Figure

Obviously, the mean of City2 summarizes the central tendency in City2 much better than the mean of City1 because City1 exhibits a much greater degree of dispersion of the data throughout the year. Measures of dispersion quantify this and summarize it into one statistic. One widely used dispersion measure for numeric data is the so-called standard deviation, which is computed as represented in (

(6) sd= Σ -  As you can see, now the measures of dispersion for City1 and its derivative are the same, and they are still considerably higher than that of City2, which is to be expected, given the similarities of the means. Another way of summarizing the dispersion of a numeric variable is again using a measure that helps in cases when the data are decidedly non-normal (i.e., not likely bell-shaped) and/or when you have outliers etc. (just like the median may replace the mean for measures of central tendencies in such cases). This measure of dispersion for ordinal variables is the so-called interquartile range (IQR), which gives you the length of the interval around the median that includes about half of the data:

Again, the conclusions are the same: City1 is more heterogeneous than City2.

Recommendations for further study/exploration

• On how to get quartiles or deciles for a numeric vector: quantile(0:100) ¶ and ?quantile ¶.

• On how to get the range of a vector (i.e., a shortcut for max(City1)min(City1)): ?range ¶.

• On how to get the median absolute deviation, another robust measure of dispersion: ?mad ¶.

A good way to look at such data is the boxplot, a graph that provides a lot of information about the distribution of a vector; see the top panel of Figure

This plot tells you something about the central tendencies of both cities because the bold horizontal lines represent the medians of each distribution. Second, it also tells you something about dispersion because the horizontal lines delimiting the boxes at the top and at the bottom extend from the upper to the lower hinge (roughly, the two data points delimiting the highest 25 percent and the lowest 25 percent of all the data). Third, the whiskers -the dashed vertical lines with the horizontal limits -extend to the most extreme data points which are no more than 1.5 times the interquartile range from the box (the default value of 1.5 can be changed; enter ?boxplot ¶ at the R prompt). Fourth, outliers beyond the whiskers would be represented by small circles. Finally, the notches extend to ±1.58*IQR/sqrt(n) (enter ?boxplot.stats ¶ at the R prompt for information on hinges and whiskers). If the notches of two boxplots do not overlap, this is strong prima facie evidence that the medians are significantly different from each other (but of course you would still have to test this properly). The function grid() draws the dashed gray line grid into the coordinate system.

If we apply this to our example, we can see all of what we have observed so far separately in one glance: The central tendencies of both cities are very similar (because the medians are close to each other and the notches overlap). The first city exhibits much more heterogeneous values than the second city (because the boxes and the whiskers of the first city cover a larger range of the y-axis; also, the notches of City1 are huge). Note finally the difference between the top and the bottom of the box in the top plot. While the bottom poses no problems, the top is sometimes difficult to understand because the horizontal line, so to speak, folds back-/downwards. This is R's way of giving you both the correct top end of the box and the full extension of the notch. In this case, the upper end of the box does not go up as far as the notch, which is why the line for the notch extends higher than the upper limit of the box.

The lower panel of Figure

Each point indicates how much in percent of the data of one city (on the y-axis) are covered by the corresponding temperature on the x-axis and all smaller ones. For instance, we can estimate from the plot that 25 percent of all temperature values of City1 are 1 or less, that 33 percent of all temperature values of City1 are 5 or less, etc. This plot is often more informative than a boxplot because it doesn't bin data points (into a box covering the middle 50 percent of the data): Rather, every unique observed data point (here a temperature) is represented with a point and, for instance, the fact that the temperatures of City2 are much less variable is revealed by the fact that the gray curve for City2 covers a much smaller range of the x-axis limits than the black curve for City1.

Recommendations for further study/exploration

One Independent Categorical Variable

Another very frequent scenario involves the situation in which you have a numeric dependent variable and a categorical independent variable; the more detailed discussion here will be restricted to the case where the independent categorical variable has just two levels.

As an example for this scenario, we can return to the example we discussed above: the different lengths of subjects and (direct) objects. Let us first formulate our hypotheses, assuming we operationalize the lengths by the numbers of syllables:

H 0 : On average, English subjects are as long as English direct objects; mean length of English subjects = mean length of English direct objects.

H 1 : On average, English subjects not as long as English direct objects; mean length of English subjects ≠ mean length of English direct objects.

(One might in fact hypothesize that subjects are not just of different length, but also how they differ from objects, namely by being shorter, because referents of subjects are often assumed to be given or accessible information, and given/accessible information is usually encoded with less linguistic material than new(er) information. In the interest of keeping matters simple, we will stick with H 1 above, which only postulates a difference in length, but not also the direction of the difference.) Let us further assume we have investigated so many randomly drawn subjects and direct objects from a corpus until we had 152 instances of each and stored them in a data frame that conforms to the specifications in Section 4.

Next, we look at the data graphically by again using boxplot. Although this was not mentioned above, there are two different ways of using boxplot. One is to provide the function with vectors as arguments, as we did above for the two cities. We could do the same here like this:

but if the data frame looks as I told you it should, then R also offers the possibility to use a formula notation in which the dependent variable is followed by a tilde (~) and the independent variable. We also immediately add the ecdf plot for objects and subjects (and to practice, you should also generate histograms for these data):  As we can see, the median of the objects is slightly larger than that of the subjects, but we can also immediately see that there is a lot of variability and that the ecdf curves are very similar. (Given the nature of the data, a logged y-axis for the boxplot might make the graph easier to grasp; also, see the code file for how to generate side-by-side histograms of the object and subject lengths.)

Let us now also compute the means and standard deviations as well as medians and interquartile ranges:

The usual test for such a case -a dependent numeric variable and an independent categorical variable -is either the so-called t-test for independent samples (if the independent categorical variable has two levels, as it does here) or a one-way ANOVA (if the independent categorical variable has three or more levels); if you use a one-way ANOVA with a binary independent variable, the results will be identical to those of the t-test. The next step again consists of determining whether we are in fact allowed to perform a t-test here. The t-test for independent samples is a parametric test and may only be used if:

• the observations of the samples are independent of each other such that there is no meaningful relation between, say, pairs of data points; • the populations from which the samples are drawn are normally distributed (especially with sample sizes smaller than 30); and • the variances of the two samples are homogeneous.

As to the former assumption, the subjects and objects were randomly drawn from a corpus so there is no relation between the data points. As to the latter two assumptions, Figure

Thus, we compute the non-parametric alternatives to the t-test/the one-way ANOVA, which are the U-test (if the independent categorical variable has two levels, as it does here) or the Kruskal-Wallis test (if the independent categorical variable has three or more levels). The function for the U-test is called wilcox.test, the function for the Kruskal-Wallis test is kruskal.test, and both accept the formula notation we have seen above:

Note how, since the independent variable has only two levels, the p-values of both tests are virtually completely identical so you could have used either test, but once your independent variable has more than two levels, you have to use kruskal.test.

Here, both p-values are much smaller than 0.05, which is why we conclude that the difference in the median lengths is most likely not due to chance. Thus, this result would be summarized as follows: "The median length of direct objects was four syllables (interquartile range: three) while the median length of subjects was three syllables (interquartile range: four). Since the data violated the assumption of normality, a U-test/Kruskal-Wallis test was computed, which showed that the difference between the two lengths is highly significant (for the U-test: W = 14,453, p < 0.001): In the population of English for which our sample is representative, direct objects are longer than subjects."

You should now do "Exercise box 4.5: Averages."

One Independent Numeric Variable

The final statistical method we look at is one where both the dependent and the independent variables are numeric. As an example, let us assume that we are again interested in the lengths of XPs. Let us also assume that we generally believe that the best way of operationalizing the length of an element is by counting its number of syllables. However, we may be facing a data set that is so large that we don't think we have the time to really count the number of syllables, something that could require a lot of manual counting (but see

This kind of question can be addressed using a linear correlational measure. Linear correlation coefficients such as r or τ (see below) usually range from -1 to +1:

• negative values indicate a negative correlation which can be paraphrased by sentences of the form "the more . . . , the less . . . " or "the less . . . , the more . . . "; • values near 0 indicate a lack of a correlation between the two variables;

• positive values indicate a positive correlation which can be paraphrased by sentences of the form "the more . . . , the more . . . " or "the less . . . , the less . . . ".

The absolute size of the correlation coefficient, on the other hand, indicates the strength of the correlation. Our hypotheses are therefore as follows:

H 0 : The lengths in syllables do not correlate with the lengths in words; r / τ = 0.

H 1 : The lengths in syllables correlate positively with the lengths in words such that the more words the XP has, the more syllables it has; r / τ > 0.

Let us now load the data from the file <_qclwr2/_inputfiles/stat_lengths.csv> into R:

As usual, we begin by inspecting the data visually. Conveniently, we can again use the generic plot function to produce the very simple scatterplot in the top panel of Figure

However, the plot in the top panel of Figure

This plot

(2) uses the rgb function to create transparent grayscale colors (with overplotting leading to darker colors); (3) adds a thicker, smoother (lowess) line (lines) that summarizes the trend we see in the data; and (4) adds a main diagonal marking the cases where the word length is the same as the syllabic length: abline can take an intercept and a slope so if you set them values as above, you get a line representing the situations where y = x.

It is immediately obvious that there is a positive correlation between the two variables: The larger the syllabic length, the larger the lexical length. The correlation between two numeric variables is referred to as Pearson's product-moment correlation r (for regression), but its significance test, too, comes with an assumption that corpus data usually do not meet, namely that the population from which your samples were taken is bivariately normally distributed, which in practice is often approximated by testing whether each variable is distributed normally. Figure

It turns out that the correlation is rather high and highly significant so that we might want to be confident that the length in syllables can be reasonably well approximated by the length in words that is computationally easier to obtain. We can thus say: "There is a and definitions of what is a word will differ across software applications. Thus, in order for others to be able to understand and replicate your results, you must outline your approach as precisely as possible.

Of course, if you use R, all of this is much less problematic because you can just provide your regular expression(s) or even your complete program as pseudocode. Pseudocode is a kind of structured English that allows you to describe what a program/script does without having to pay attention to details of the programming language's syntax (see the very useful page by John Dalbey at www.csc.calpoly.edu/~jdalbey/SWE/pdl_std.html for more details), and Chapter 5 will make use of something very similar to pseudocode all the time.

Here is an example of pseudocode for a script that generates a concordance of right when tagged as an adjective:

01 clear memory 02 choose corpus files to be searched 03 for each corpus file 04 load the file 05 if the corpus file is from the right register 06 downsize the file to all lines with sentence numbers 07 retrieve all matches (i.e., lines containing right as an adjective)

from the file 08 store all matches with the names of the files successively 09 end if 10 end for 11 insert tab stops for better output 12 delete unwanted tags and unnecessary spaces 13 output result into a file If you provide this information together with the actual regular expressions you used to

• check the right register in line 5 ("<teiHeader type.*S conv</classCode>"); • downsize the file in line 6 ("<s•n="); • retrieve the matches in line 7 ("<w•AJ0(-AV0)?>right\\b"); • delete unwanted tags and unnecessary spaces in line 12 ("<.*?>" and "•*\t•*"); then you will be more explicit than many corpus linguists are at present, and than I myself have been on too many occasions in the past.

Now we are at the point where we have covered nearly all the programming knowledge you need, we can finally get down to business and deal with the case studies.

Notes

3 The same is actually true of corpus frequencies. Too many studies content themselves by reporting frequencies of occurrence or co-occurrence only, without also reporting a measure of corpus dispersion, which can sometimes hide a considerable bias in the data; see

Using R in Corpus Linguistics

Case Studies

Now that corpus linguistics is turning more and more into an integral part of mainstream linguistics, . . . we have to face the challenge of complementing the amazing efforts in the compilation, annotation and analysis of corpora with a powerful statistical toolkit and of making intelligent use of the quantitative methods that are available.

(2) several scripts involve more than one of these aspects; they cover a wide range of things in the hope that you can generalize from them to your own applications. Within each group the tasks or case studies are ordered according to difficulty, but this, too, is only approximate because difficulty is hard to operationalize objectively. In some sense, you have arrived in a 'good news, bad news' kind of situation. The good news is that you actually already know nearly all the functions we will use: You will have to learn only a very small number of new functions for this chapter and in fact you already know virtually all functions I use for my own research. The bad news is that it's still not an easy chapter because it is all about putting these functions together in the right ways to get stuff done, and in that sense, R is like a natural language, where it's often easy to learn vocabulary and their local syntax (functions and their arguments), but much less easy to employ them properly in sentences and paragraphs (scripts). But fear not, the chapter is written in a way that I hope will be useful and also in a way that will make it easy for you to recycle (parts of) scripts for your own work: Wherever possible I wrote the code such that I didn't hardcode particular numbers of files etc. into the script but have R determine the lengths of vectors and lists (so that you don't have to fiddle with details as you recycle the script for your own work).

With very few exceptions, the case study sections all have the same structure: They begin with a short characterization of what the goal of the task/script is; sometimes, this will be just a prose explanation, sometimes I will also show you the results of the scripta results table or a plot, for instance. Then, each case study has four sections, which are designed to help you understand the programming process in a stepwise fashion. This sequence is important because it is very easy to show someone the solution to a programming problem, but very hard to teach someone to see why that solution is a good one and even harder to teach someone to develop a way of thinking that enables them to find such solutions themselves, because programming requires an analytical and to some extent taxonomic and modular kind of thinking. In this context, a small half-humorous side-remark is in order: One thing that you should do right now is drop your human way of thinking and your human expectations, because one other important aspect of writing scripts is to understand that the 'thing' implementing your instructions is a 'stupid computer,' i.e., an entity that will cover up your gaps in scripts with well-intentioned base understandings of what words are, which words you might be interested in etc. -no, you must try to write scripts such that all eventualities are covered. That means you may think that every corpus file contains matches of an expression, but your script still needs to be able to handle cases when a corpus file does not contain a match. That also means you may think a certain website may be downloadable, but your script still needs to be able to handle the situation that it's not. And so on.

Back to the sequencing of each section: The sequencing I am using here is designed to help you zoom in from a general description of the task into the more specific aspects of the code such as (

• What are the things we will need to do? This section explains in plain English which steps the relevant task involves; it uses hardly any R code but already introduces the kinds and names of a few data structures that the script will contain.

• What are the functions we will need for that? This section lists all the main functions one will need to use to perform the things implied by the description formulated in the previous step. • Thus, this is the overall structure of the script. This section provides a skeleton of the R code we will use in what is called pseudocode: a description of the algorithm and structure of a program that performs a particular task. It is imperative that you read this part with the relevant script open in RStudio because in the pseudocode I will provide the line numbers of the relevant R script from the companion website so you can see exactly which lines of code in the script do which part of the pseudocode, or how the pseudocode is 'translated' into actual R code. The files with all the scripts are in the folder <_qclwr2/_scripts/> and all begin with "05_"; this part will greatly help you understand the logic of the scripts. • Which aspects of the script are worth additional comment? If you look at the script files, you will see that they are very heavily commented: Often even a single function call is broken up into several lines so that each argument can be explained, and sometimes I will break down regular expressions into multiple lines (remember freespacing from above?) to explain everything in detail. However, sometimes scripts involve something that I think merits additional explanation here, or they involve something you haven't seen yet at all or in the form in which something is used. If there are such situations -and not every case study has such parts -then this section provides additional discussion of these aspects of the scripts.

Note that sometimes scripts provide more than one solution to a problem, and note that even if scripts do not provide alternative solutions, there are usually many ways to solve a problem. The solutions I provide here are probably relatively good ones, but there will be others and probably better ones, so if, after thinking about a task or reading my solution you come up with a better one, great -post it on the newsgroup for the Google Group for this book! Ok, let's get started.

Dispersion

Dispersion 1: HIV, Keeper, and Lively in the BNC

As mentioned in Section 2.3, it is often extremely important to not just consider the frequency of morphemes, words, constructions, etc. in a corpus, but also their dispersion, because elements may have very similar overall frequencies in a corpus but nevertheless be very differently dispersed in that corpus. Above, I mentioned

(1) 0 5 Here's a little made-up example of a corpus with three files: the files make up 50 percent, 30 percent, and 20 percent of the corpus, and of all occurrences of the word in question, 70 percent, 20 percent and 10 percent are in the corresponding files. Then, DP is computed like this: Simplifying a bit, DP ranges from 0 (a word is perfectly evenly distributed in the corpus, i.e., in accordance with the sizes of the corpus files) to 1 (a word is completely unevenly distributed in the corpus). If we want to compute DP for a word w, we therefore need to determine (1) the frequency of w in every single corpus file (which may be 0 or higher) and (

What are the things we will need to do?

• We need to define a vector corpus.files that contains the paths to all corpus files, which should all be in one directory (potentially with sub-directories). • We need to define a vector words that contains the words for which we want to compute DP values and then also a vector search.expressions that changes words into regular expressions we can search for in the corpus files (using the annotation of the corpus). Recall from Section 3.6.3, this is a script in which we know the dimensions of the output in advance: If we have three words and 4,049 corpus files, we know, for instance, that the vector corresponding to sizes.of.files.in.words above will need to have 4,049 slots, and we know that the list that collects the three words' frequencies will need three components each with 4,049 slots. Thus, we make sure that we define data structures with these dimensions before we enter any of the loops. • We will access each of these files, i.e., use a first for-loop (using i as a counter)

where on each iteration we load (with scan) one corpus file; once we have that file in memory, we need to determine its size in words, which we save into a slot in sizes.

of.files.in.words.

• Also, while we have that corpus file in memory, we need a second for-loop (using j as a counter) that iterates over each search.expression to determine how frequent each word w in words is in each corpus file c 1-n , once we have the frequency of word w j in corpus file c i , we store its frequency in the corresponding slot of freqs.

of.words[[j]][i].

• We complete the inner for-loop for the words and then the outer for-loop for the corpus files; we do that in this order so that we load every corpus file just once to look for the three words in it rather than loading each corpus files three times. • Finally, once we have the results of this loop, we can compute DP for each word along the lines discussed above and then generate a plot with, say, the frequencies of the words on the x-axis and the DP values of the same words on the y-axis.

What are the functions we will need for that? Obviously, we need to be able to define the corpus files we want to search, which means it will be useful to use rchoose.dir to define the directory containing the corpus files; also, we will need to be able to retrieve all the file names from that directory using dir. We will need to define vectors and a list (with integer and list), and we will need to use one for-loop nested into another. We will use scan to load each corpus file in the outer loop, use tolower to make everything case-insensitive, and we will use grep to make sure we only search for words in the part of the corpus file that's not the header. To find the words we are interested in, we can use just.matches (which we would then define at the beginning of the script again) or exact.matches.2 (which we would then have to source at the beginning of the script).

Once we're done with the loops, we will need to compute the file sizes in percent, which means we need sum to compute the overall corpus size from sizes.of.files.in.words so we can compute each file's size as a fraction of that, and we will need lapply to access each element of freqs.of.words to do the same. Then, we loop over each word w 1-3 to compute DP as defined in

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_01_dispersion1.r>; read this side by side with the actual script in RStudio: clear memory and define and/or load all relevant packages and/or functions

Another thing that's useful to talk about is the creation of the data structures that will collect the results. Creating sizes.of.files.in.words is easy: The following line (44 in the script) creates an integer vector that has as many slots as you have corpus files: >•sizes.of.files.in.words<-integer(length=length(corpus.files)) ¶

But creating the list to collect the words' frequencies in each corpus file is more involved. We proceed in two steps: First, we create a temporary vector that has as many slots as there are corpus files and make sure that each slot has the name of one corpus file:

But then we also need our list with as many elements as there are words for which to compute DP. Thus, we create a list with that many empty components (line 51), give each list element the name of a word we are computing DP for (line 52), and then use the subsetting function "[" to put temp into each list element:

Remember that freqs.of.words is a three-element list. We use lapply to access each element of the list and then apply an anonymous/inline function to it that takes the list element -which you know is a numeric vector -temporarily refers to it as x, and divides each number of that vector x (i.e., each frequency of a word in a corpus file) by the sum of all numbers in x (i.e., the overall frequency of a word in the whole corpus). That way, we now have the two vectors that at the beginning of this section were exemplified as sizes. of.files.in.words and freqs.of.words, which means we can use a small for-loop to compute DP for each word and plot the results.

Dispersion 2: Perl in a Wikipedia Entry

Let us now consider another application of dispersion: This time around we want to visualize the dispersion of the string "Perl" (case-insensitively) in an older version of its Wikipedia entry (already cleaned and available in <_qclwr2/_inputfiles/corp_perl.txt>). Specifically we want to create the results shown in Figure

The left panel is a dispersion plot whose x-axis represents all the words in the 'corpus' which shows a vertical line when a word is "Perl" (case-insensitively) and no line otherwise. We can easily see how, for instance, there are a lot of occurrences at the very end -presumably that's the section with references and links. The right panel is a similar plot but it bins the words in the corpus (here into ten equally large parts), and again we can see that there are a lot of occurrences of "Perl" in the last 10 percent slice of the corpus.

What are the things we will need to do?

• We need to load and switch to lower case our single corpus file and split it up into words (say, into a character vector word.tokens); we may have to get rid of empty character strings resulting from the splitting.

• We need to check for every word token in word.tokens whether it is "perl" or not.

• For the left panel, we plot a vertical line whenever a word token is "perl".

• For the right panel, we need to create a vector corpus.parts that lists for every element of the vector word.tokens which of ten equally sized parts it's in and then count how often "perl" shows up in each of those parts; from that we can compute DP as above.

• Then, since we then already have the observed percentages of how often "perl" shows up in each of the ten parts, we generate a bar plot that represents these ten percentages.

What are the functions we will need for that? We need scan and tolower to load and prepare the corpus file, and we will need strsplit to retrieve the word tokens as well as unlist to turn the list returned by strsplit into a vector word.tokens. We will discard empty character strings with nzchar and test whether each word token is "perl" with a simple logical expression testing for equality (i.e., ==) and then use plot (with type="h", for histogram) to plot the left panel (because FALSE = 0 and TRUE = 1). For the right panel, we will define a number of corpus parts we want (here ten) so that the script can easily be changed to accommodate different divisions of the corpus into parts. Then, we create a vector of word positions in word.tokens, which will run from 1 (for the first word in word.tokens) to n = 6,065 (for the last word in word.tokens) with seq and we then split that vector up into ten equally long parts using cut. The factor that cut returns will have ten levels and can be cross-tabulated (using table) with the TRUEs and FALSEs from the logical vector created to plot the left panel.

The two final steps are computing DP and plotting the bar plot. For the former, our table contains the frequencies of "perl" in each corpus part (in the column for TRUEs), which we can divide by the overall frequency of "perl" in the file to get percentages (to be stored in a vector called obs.percs), and we can use the function rowSums to compute the corpus part sizes in percentage in a vector exp.percs (which should all be really close to 10 percent, given how we split the corpus up into ten parts above), from which we can compute DP. Finally, we use the function barplot to plot the observed percentages of "perl" in the corpus parts and customize the plot.  recall Section 3.6.3) and create a vector all.3grams1 of that length to collect the results.

• We will then go through the one long character string that is now our input file with a loop and extract all character three-grams, storing each one of them in all.3grams1. • Finally, we will create a sorted frequency list of all character three-grams, explore it briefly statistically, and plot it -can you guess what the most frequent three-gram will be?

In addition to the above, I will present a version of this script that does not use a loop but a function from the family of apply functions (see ?mapply ¶), and I will show how you can use that kind of code to write and apply a function called character.ngrams for your future work.

What are the functions we will need for that? We need scan and tolower to load and prepare the corpus file, and we will need gsub to clean up characters we do not want to consider, and we will need paste and gsub again to create a clean version of a character vector with one element that contains the whole text.

We then need nchar to determine how many three-grams we will have to create/retrieve and then store, and a for-loop that applies substr to the cleaned text vector to extract the threegrams. Finally, we will use sort and table to create our frequency list of the three-grams, quantile to explore their distribution, and plot with type="h" again to visualize the results.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_03_char-ngrams.r>; read this side by side with the actual script in RStudio: clear memory

That is, the above is the short version of this:

Now what about the alternative solution that does not require a for-loop? This one is shown in lines 72-77 and is a bit more tricky and involves a function I haven't discussed, but only recommended to you for further exploration before, mapply. As the help says, it's a multivariate version of sapply that takes as its first argument a function and as additional arguments it can take arguments to the first-named function. In the following line, this means, use the function substr with textfile.oneline.3 as substr's first argument (i.e., as the thing from which to extract substrings), with the positions 1, 2, . . . 17,093, 17,094 as the second argument to substr (i.e., the starting points of the substrings to extract), and with the positions 3, 4, . . . , 17,095, 17,096 as the third argument to substr (i.e., the end points of the substrings to extract), and we do not allow mapply to name the output elements, because that makes the output extremely unwieldy:

A third alternative to create all character three-grams also avoiding a loop is shown in lines 81-86, which involves just using substr, but using rep to make its first argumentthe string from which to extract substrings -as long as its arguments start and stop:

Explore the rest of the code file, lines 90-110, to see how the second approach -the one using mapply -is then used to define a function character.ngrams, where you or any other user provides a character vector of length 1 as the first argument and the desired gram length as the second to get the job done very quickly. As a 'homework assignment' you may want to think about how you could create a version of character.ngrams that works on input vectors of length 1 or longer ones. Also, check out ?stringdist::qgrams ¶, which is the R notation to say "the help file for the function qqgrams from the package stringdist".

Word N-Grams

The next case study is very similar to the preceding one: we are again interested in n-gramsthree-grams, to be precise -but this time in three-grams of words, not characters; for the sake of simplicity, we will work with the same input file, <_qclwr2/_inputfiles/corp_gpl_ long.txt>.

What are the things we will need to do?

• We need to load and switch to lower case our single input file.

• We will retrieve all words of the file by splitting on everything that's not a letter from a to z and a space (again, this [cw]ould need to be tweaked for other kinds of input files). • We will then determine how many word three-grams there will be (so as to avoid having a vector of three-grams grow all the time -same as above) and create a vector all.3grams1 of that length to collect the results. • We will then go through the vector of all words that now is our input file with a loop and extract all word three-grams, storing each one of them in all.3grams1. • Finally, we will create a sorted frequency list of all word three-grams, explore it briefly statistically, and plot it.

As before, I will also present a version of this script that does not use a loop, but mapply, and I will also show how you can use that kind of code to write and apply a function called word.ngrams for your future work.

What are the functions we will need for that? We need scan and tolower to load and prepare the corpus file, and we will need strsplit and unlist to split up the corpus file into words (and nzchar to eliminate empty strings) and put them into a vector called textfile.words. We then need length to determine how many three-grams we will have to create/retrieve and then store, and a for-loop that applies paste (with collapse) to the text vector to create the three-grams. Finally, we will use sort and table to create our frequency list of the three-grams, quantile to explore their distribution, and plot with type="h" again to visualize the results.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_04_word-ngrams.r>; read this side by side with the actual script in RStudio: clear memory (1) load the corpus file into a vector textfile and switch it to lower case (3-8) use strsplit to split up at every occurrence of 1+ characters you do not want

(continued) determine how many words textfile.words has with length, that number minus 3 -1 = 2 (because you are generating three-grams) will be the number of threegrams; reserve a vector all.3grams1 that has that many slots) (21-30) use a for-loop to retrieve every three-gram from textfile.words and store each in the relevant slot of all.3grams1 (32-39) create a sorted frequency list of all.3grams1; call it sorted.ngram. Which aspects of the script are worth additional comment? Not much really, because we've dealt with pretty much all of it in the previous section on character n-grams. However, if you look at lines 59-71 of the code file, you can see an alternative way of generating the word three-grams without a for-loop, but this time the solution is quite complicated and involves several nested steps:

• We are again using mapply, but this time we do it to generate a matrix with 2,950 columns (one for each three-gram to create) and three rows (with positions of the three words in each three-gram for subsetting. Here are the first six columns of this matrix:

• Then we use the function apply, which takes three arguments: First, a two-dimensional data structure such as the above matrix; second, a 1 or a 2 depending on whether you want to do something to each row

• The function we pass on to apply is an application of paste with collapse="•":

We want each vector of numbers from each column to be used in subsetting from textfile.words so that we can paste those three words together with spaces between them.

This is a pretty complicated application and it doesn't matter if you do not fully understand it right away: Some of the apply functions are difficult at first -revisit this once you have more practice. However, do note that lines 75-90 provide you with a function word.ngrams that uses that apply(mapply(...)) approach and that you can use whenever you need to create n-grams from a character vector each element of which is a word.

(continued)

Case Studies 189

Zero-Derivation of Run and Walk in the BNC

In this example we look at how the word forms run, runs, walk, and walks are used in the BNC. Specifically, we explore whether the two lemmas differ with regard to how frequently they are used as nouns or as verbs. We will do so in the BNC but, to add a slight twist to things, we will write our script such that it prompts the user to interactively choose what annotation the files to search have (XML or SGML), and then it proceeds with the required search expressions; (

(2) a <w•c5="NN1"•hw="walk"•pos="SUBST">walk

The desired output of this script <05_05_run-walk.r> is a data frame result like this (shown for XML results), which lists for every occurrence of run, runs, walk, and walks:

• a coarse-grained tag (the pos-value in the XML annotation and the first letter of the tag in the SGML annotation) and a fine-grained tag (the c5 tag in the XML annotation and the full tag in the SGML annotation); • the form that was found and the lemma of the form that was found:

What are the things we will need to do?

• We need to define a vector corpus.files that contains the paths to all corpus files, which should all be in one directory (potentially with sub-directories). • We will let the user make a choice of which kind of corpus annotation the script will deal with, and depending on what the user chooses, the script will define all search expressions accordingly. Also, the script generates empty vectors to collect search results for each verb.

• We will load each corpus file and use our search expressions to find the four forms in question with the right tags and store all their matches. • We will process the matches so as to retrieve the coarse-grained and the fine-grained tags as well as the lemmas of the matches and save all of the results in a data frame result.

• Finally, we will split up that data frame by the lemma and determine whether, within each lemma, there is a significant correlation between the two forms and whether they are used as nouns or verbs: We will perform chi-squared tests and create some plots.

What are the functions we will need for that? Obviously, we need to be able to define the corpus files we want to search, which means we use rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory. We will need the functions switch and menu (which you do not know yet so you may want to briefly look at their help pages -they are not difficult and the script will show you how they are used anyway) to prompt the user to choose the annotation format that will be processed, and we need a conditional with if to then define regular expressions for either choice. We will need to define empty character vectors to collect results using character, and we will need to use a for-loop to load each corpus file; during each iteration, we will use grep to find all corpus sentences and then use exact.matches.2 to find all matches of run, runs, walk, and walks.

After the loop, we will again use exact.matches.2 to retrieve the tags from the matches, we will use gsub to retrieve the forms only (i.e., discard all annotation), and we will use substr to define the lemma of each match (by retrieving the first letter, which gives us enough information to distinguish run and walk) and using ifelse to define a lemma column). All those results will then be put into the data frame result using data. frame. 1 That data frame will then be split up using split and cleaned using droplevels, and then we use chisq.test, mosaicplot, and assocplot to explore the distributions of nouns and verbs for all inflectional forms of each lemma; in that process, I will also introduce you to the functions with.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_05_run-walk.r>; read this side by side with the actual script in RStudio: clear memory and define and/or load all relevant packages and/or functions

define the search expressions conditional on which annotation will be processed; the search expressions must cover the retrieval of the overall form with its tag, how to retrieve just the tags, and just the forms (18-30) define empty character vectors to collect matches: all.runs and all.walks (32-33)

for-loop (counter = i, over corpus.files): (35, -61) output a progress report: the name of the file currently being processed (36) and load that corpus file i (37) discard the header/find the sentences and switch them to lower case (39-44) find and store in all.runs the matches for run and runs when tagged as a verb or a noun (46-53) find and store in all.walks the matches for walk and walks when tagged as a verb or a noun (54-60) retrieve from all.runs and all.walks all coarse-grained tags with exact.matches.2 (63-70) retrieve from all.runs and all.walks all fine-grained tags with exact.matches.2 (71-78) retrieve from all.runs and all.walks all forms by gsubbing away everything else

Which aspects of the script are worth additional comment? It is maybe useful to briefly comment on how the user is prompted to choose the annotation format: The function menu takes as its first argument a character vector of options to choose from and can be further customized with graphics (do you want a window pop-up or a prompt in the console?) and title; it returns the number of the option the user chose, i.e., in our case 1 or 2. In the code here, this 1 or 2 is then the input to switch, which takes that number and picks the corresponding item from its arguments. Thus, if the user chose "XML" then menu made that a 1 and switch then assigns TRUE to xml.version.

A second aspect worth mentioning is the definition of the search expressions; here's one example:

This looks for the beginning of an XML tag, with the c5 tag beginning either with "v" or with "n", followed by one or more characters that are not already the closing angular bracket of that tag till you reach that closing angular bracket, followed by "run" or "runs" but only if, looking to the right, you can see a space (optional) and the next opening angular bracket. Okay, but why are there three backslashes? To understand this, it's useful to compare the difference between how R represents those strings as a vector in the console and how they are actually printed as a string:

That is, the first line is how we defined it, but when R, so to speak, 'uses' it then the third backslash is one that makes R recognize the " as not being the " that closes the opening " of the search expression, and the first backslash is the one that marks the second one as an actual backslash, not a backslash of a special character. A little confusing, but if you ever struggle with this -as I admittedly do all the time myself -it helps to just cat the expression to the screen to see what you have really defined.

Finally, a brief note on the function with as used in the following line:

The purpose of using with as above is that R does not know the two objects that are the arguments of table : If you tried to retrieve TAGSCOARSE, you will get an error message:

Thus, recall from Section 3.4.3 that with tells R where TAGSCOURSE and FORMS can be found, namely in the data frame results.

The rest of the script should be more or less self-explanatory but why don't you, as a final task, try to take the two main search expressions, the ones used in the loop in lines 50 and 57, and devise a new one that finds all four forms at the same time? The solution to this little exercise is in the code file with free-spacing in lines 157-165. Also, why don't you think about (for later) how this script could be changed so as to avoid growing results vectors in the loop.

Word and Sentence Lengths in the BNC

In this small case study, we will go over all files in the BNC and determine for each sentence how long it is (in words) and how long the words in it are (in characters) so we can generate a plot that checks whether there is a correlation between the two.

What are the things we will need to do?

• We need to define a vector corpus.files that contains the paths to all corpus files, which should all be in one directory (potentially with sub-directories). • We need to generate two lists to store for each file

• To determine sentence lengths, we count all the word tags we find in each sentence;

we save the sentence lengths for a file in the relevant element of all.sent.lengths for this file. • To determine word lengths, we first use the word tags to find words, but then delete the tags and count the number of characters of what's left; we save the word lengths for a file in the relevant element of all.word.lengths for this file. • After the loop, we compute the median sentence and word lengths for each file (very crude approach!) and summarize the distribution of these medians. • Finally, we generate a scatterplot with word lengths on the x-axis and (logged) sentence lengths on the y-axis and try to determine whether there's a correlation between the two.

What are the functions we will need for that? As usual, we use rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory. We define two lists with as many empty elements as there are files using the function vector with mode="list". We then use the usual for-loop to load each corpus file during each iteration and use grep to find all corpus sentences. We then use exact.matches.2 to find all word tags but only retain the second component of exact.matches.2, namely the locations of the matches, i.e., the numbers of the sentences they occur in (because we can tabulate those with table ). After that, we use exact.matches.2 again to find all words and their tags as well as gsub to delete the tags and nchar to determine the numbers of characters of what's left. At the end of each iteration of our for-loop, we save all sentence lengths of a file and all word lengths of a file into the relevant components of all.sent.lengths and all.word.lengths.

After the loop, we use sapply together with median and summary to compute the median sentence lengths and median word lengths per file and use plot to create a scatterplot as defined above; to avoid overplotting of many points onto the same coordinates, we use jitter and rgb (for grayscale and transparency effects) just as we did in Section 4.3.3 when we discussed correlations as well as log to log the sentence lengths. Finally, we use abline and text to add some annotation into the plot; we use lines(lowess(...)) to add a trendline, and I introduce mtext to add colored axis labels.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_06_wrd-sentlengths.r>; read this side by side with the actual script in RStudio: clear memory and define and/or load all relevant packages and/or functions

for-loop (counter = i, over seq(corpus.files)):

find the numbers of word tags for each sentence and store them all in all.sent. lengths [[i]] (27-31)

find the lengths of all words (identified by their tags (33-36), but with the tags deleted by gsub) and store them all in all.word.lengths

Which aspects of the script are worth additional comment? This script has no particularly complicated components -the only thing I recommend is that you study the code for the plot and all its commentary in great detail because it involves many useful functions.

Approximating Syntactic Complexity: Fichtner's C

In the context of an applied linguistics study, you want to compute for every file in the BNC World Edition a measure of syntactic complexity. A particularly easy one to compute uses information that our last case study made us collect: Fichtner's C, which is computed as shown in (

(4) Fichtner s numberof verbs numberof sentences

What are the things we will need to do?

• We need to define a vector corpus.files that contains the paths to all corpus files, which should all be in one directory (potentially with sub-directories): thus, as usual, we need rchoose.dir and dir. • We need to generate three vectors to store for each file (1) the number of verbs in it;

(2) the number of words in it; and (3) the number of sentences in it. Thus, each of these vectors needs to have as many empty elements as there are files in corpus.files.

Let's call those vectors nos.of.verbs, nos.of.words, and nos.of.sentences. • We will load each corpus file, discard the header and keep only the sentences.

• To determine the number of sentences, we just determine the length of the vector remaining after the previous step, which was precisely aiming at only keeping sentences; we save the number of sentences in the relevant element of nos.of.sentences. • To determine the number of words, we count how many closing word tags ("</w>") we find in the sentences; we save that number into the relevant element of nos.

of.words.

• To determine the number of verbs, we count how many instances of the pos-tag "VERB" we find in the sentences; we save the number of verbs in the relevant element of nos.of.verbs. • After the loop, we compute Fichtner's C values for all files as defined above in (4) and, just for housekeeping, give the values the names of the corpus files; also, we compile all the results in a data frame result that looks like this:

• Finally, we explore the data visually by (

What are the functions we will need for that? Obviously, we need rchoose.dir to define the directory containing the corpus files and dir to retrieve all the file names from that directory. We will need to define three vectors to collect numeric results using integer, and we will need to use a for-loop to load each corpus file. During each iteration, we will use grep to find all corpus sentences. We then use length to find the number of sentences, and use length applied to searches for closing word tags and verb tags using exact.matches.2. to find the numbers of words and verbs.

After the loop, we use simple arithmetic to compute Fichtner's C values and data.frame to compile the results all into one data frame. We then use par(mfrow=c(rows,•columns)) to define the number of rows and columns we want to have in the plotting windows and then use apply (see above) and hist (and log) to create histograms of the data in the first five columns of result. Finally, we use an alternative to par, layout, to divide up the plotting window into five regions and then plot five scatterplots into them with plot, log, lines(lowess(...)), and abline; we also do a correlation test to see how strongly Fichtner's C is related to the number of verbs/sentence and the number of words/ sentence.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_07a_Fichtners-C.r>; read this side by side with the actual script in RStudio: clear memory and define and/or load all relevant packages and/or functions

for-loop (counter = i, over seq(corpus.files)):

find the numbers of sentences and store it in nos.of.sentences[i] (23) find the numbers of closing word tags and store it in nos.of.words[i] (25-28) find the numbers of verb tags and store it in nos.of.verbs[i] (30-34) compute Fichtner's C for all files (37-39)

compile the results into one data frame (41-49), summarize each column (50), and save the result (52) set up a plotting window with two rows and five columns (line 57) and use apply to plot histograms of the raw and the logged values of all variables but Fichtner's C (add the argument breaks="FD" to hist for more fine-grained bins in the histrograms (lines 58-69), then reset the plotting window to its default (line 70)

(learn about the layout function and how it defines panels in the plotting window (

set up a plotting window for five plots (line 76) and plot scatterplots of Fichtner's C against every other variable in isolation

Which aspects of the script are worth additional comment? This script also has no particularly complicated components but let me again recommend that you study the code for the plots in detail, in particular (1) how apply is used to do something (namely hist) to the first five columns of result (check Section 5.2.2 again), and (2) how layout works: It takes as input a matrix whose numbering shows where different plots go: equal numbers other than 0 cover space for one panel, and 0s reserve space that R doesn't plot into. As you probably noticed, there is a second case study script file on Fichtner's C, namely <_qclwr2/_scripts/05_07b_Fichtners-C_XML.r>. This script does the exact same thing as the one just discussed and most of the code is in fact identical, but it does it not by essentially treating the corpus file as a simple flat sequence of character strings as we did above, but by utilizing the hierarchical XML annotation in ways discussed in Section 3.8.2. Thus, most of the things discussed above do not change -the only changes occur in the loop. Remember that the most powerful package to handle XML data in R is XML, but also remember that it has a bad memory leak when used on Windows systems. The script therefore does the following within the loop: It checks whether it is currently running on a Windows computer or not (lines 18, 31, -42) and then,

• if the script detects it is running on a Windows computer, it uses functions from the xml2 package to get the job done -it loads the corpus file with read_xml and retrieves the frequencies of elements using xml_find_all (lines 19-30); • if the script detects it is not running on a Windows computer, it uses functions from the XML package -it loads the corpus file with xmlInternalTreeParse and retrieves the frequencies of elements using either the summary of the parsed XML object or using xpathApply with xmlGetAttr (lines 32-41).

(continued)

Thus, check out how you can query your R instance with regard to what system it's running on (line 18) and, more importantly, how I use both of these XML processing packages to get the job done.

Key Words

In this section, we will discuss a script that computes different measures of keyness for words occurring in a target corpus by comparing their frequencies in that target corpus to that of a reference corpus and quantifying the relative attraction of each word to the target corpus by means of a keyness statistic. The most frequently used keyness statistics are based on a 2 × 2 occurrence table such as the one shown in Table

The most widely used statistic is the log-likelihood ratio G 2 (see

Finally, one can use Damerau's relative frequency ratio rfr

visualize the results in a scatterplot either simply (87-90, see ?rug ¶) or more nicely (92-102)

Which aspects of the script are worth additional comment? Most aspects of this script should be straightforward, given that everything happening in the loop is relatively simple text and data processing. In my experience, the only difficult part here may be the generation of the observed frequencies and the subsequent computations. For that you have to appreciate that R uses vectors even if they are very long. For instance, here's a part of

You can see the frequencies of "perl" in both corpora, which are the numbers you know from Table

Frequencies of -ic and -ical Adjectives

In this case study, we are studying English adjectives ending in -ic and -ical on the basis of an already stored frequency list of the BNC Version 1; this frequency list was compiled by Adam Kilgarriff and is provided here in the file <_qclwr2/_inputfiles/corp_bnc_sgml_freql. txt>, which looks like this:

Specifically, we want to do two things: First, we want to generate a table that states for every adjective that is attested at least once with either -ic (e.g., public) or -ical (e.g., physical) -or of course with both suffixes (e.g., electric(al)) -how often it is attested with either suffix (both in absolute and relative frequencies. This is supposed to look like this:

Second, we want to explore whether there is a correlation between the overall frequency of an adjective stem (with either suffix) and the percentage of these adjectives ending in -ical: Is it the case that -ical adjectives are more frequent (as has been claimed by Marchand 1969: 242)? What are the things we will need to do?

• We load the BNC frequency list file into a data frame called freqs.

• So as to not have to deal with such a large date structure all the time (nearly 940,000 rows), we then extract from it two data frames: one with all rows that contain adjectives ending in "ic" (freqs.ic.adj), and one with all rows that contain adjectives ending in "ical" (freqs.ical.adj). Note that we are interested in being memoryefficient: We will therefore first trim down all of freqs to a data frame that just contains adjectives (freqs.adj); we then remove freqs from memory and look for -ic and -ical adjectives, but only in freqs.adj. • Noting that some adjectives may be represented in that file with more than one tag (remember the existence of portmanteau tags, check out physical for instance), we then make sure we sum up the frequencies of all tags for each adjective in either data frame, which will give us one separate vector with these frequencies for each suffix. Let's call those ic.adjectives.freqs and ical.adjectives.freqs (for mild amusement, check out freqs.ical.adj for spelling mistakes, e.g., the different spelling of hierarchical attested in the file). • Next, we need to, so to speak, join those vectors such that we can see for each adjective stem how often it occurs with -ic and how often with -ical. To that end, we apply the logic of comparing frequency lists (from the first exercise box on vectors) and from the previous case study: First, we create a vector with all -ic adjective tokens (called ic.adjectives) and another one with all -ical adjective tokens (called ical.adjectives). Second, we create a vector their.suffixes that lists "ic" and "ical" as many times as there are -ic and -ical adjectives (which we know from the lengths of ic.adjectives and ical.adjectives respectively). Third, we combine ic.adjectives with a version of ical.adjectives from which we deleted "al" into a vector called both.adjectives. As a result, we have one vector with only -ic tokens (both.adjectives), but another one that says which suffix each token was attested with originally (their.suffixes), which we can then tabulate for both raw frequencies and percentages. • Finally, we can then plot the row sums of the frequency table result (the overall frequency of an adjective stem) against the second column of results.perc (the relative frequency of the -ical form) to see whether there is a correlation between the two, and we can explore whether the -ic adjectives differ in frequency from that of -ical adjectives. Let me recommend here to do that once for all the data and once for adjectives that occur with a certain minimum frequency (such as three) -you will be amazed at the difference in results!

What are the functions we will need for that? We need read.table and rchoose.files to load the frequency list file, and we need grepl, subsetting, and droplevels to extract the sub-data frames with -ic and with -ical adjectives from it; using droplevels is important: It makes sure that all the adjectives that are not -ic and -ical adjectives don't "stick around" as factor levels and in frequency tables! Next, we need tapply and sum to merge the frequencies of adjectives that are attested with more than one tag; after that we use rep and c to merge the vectors and gsub to reduce -ical adjectives to -ic forms. Finally, we need table to compute raw adjective frequencies, prop.table to change that table into one of percentages, and order to re-order the rows of that table by the combined adjective stem frequencies.

For the statistical and visual exploration of the correlation between stem frequency and the percentage of -ical, we will need plot, log, and rowSums as well as lines(lowess(...)) again. For the exploration of potential differences between -ic and -ical adjective frequencies, we can use boxplot and wilcox.test as well as plot(ecdf(...)). To create a downsized version of the frequency table, use apply and min with subsetting.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_09_ic-ical.r>; read this side by side with the actual script in RStudio: clear memory

Frequencies of All Word-Tag Combinations in the BNC

Now we're getting serious. The next script does something seemingly elementary -we will create a frequency list of word-tag combinations (so as to be able to distinguish run as a noun from run as a verb) -but we will do it on a relatively large data set, the complete BNC World Edition with XML annotation, and we will use the annotation well by utilizing information provided by the BNC's multi-word tags, i.e., tags that mark multi-word units such as because of, in spite of, on behalf of, etc. That is, this is output we want: Crucially, what that means is that you need to be careful to not count things twice: Once even when as a subordinating conjunction (CJS) has been counted, you must not count even as an adverb (AV0) and when as another subordinating conjunction againotherwise you imply that Figure

Also, we will create and use a function called whitespace, which can clean excess whitespace -leading and trailing spaces, sequences of more than one space -as well as empty character strings. Finally, and this is maybe the biggest new thing here: Given the size of the data and R's memory handling, we will most likely not be able to have two vectors -one for about 100 million words and multi-word units, one for all their tagsjust grow dynamically in memory. Your computer session might not survive that and either take forever or even crash (each of my two computers has 16 GB of RAM and I stopped the scripts because they were taking forever). Therefore, we will create a subdirectory into which we will save interim output after each corpus file, and after having gone through all 4,049 corpus files we will then merge the 4049 output files into the one data structure we want, results.df.

What are the things we will need to do?

• We set the warnings option so that any problems will be indicated during the iteration in which they arise, not just later at the end (see ?options ¶). • We define a function whitespace, which takes as a first argument a character vector, from which it deletes leading and trailing spaces, in which it reduces sequences of 2+ spaces to 1, and from which it can delete empty character strings. • We define a vector corpus.files with all paths to all 4,049 BNC corpus files; also, we create a directory <05_10_freqoutput> into which to save our 4,049 interim results. • We load each file, switch it to lower case, and keep only the corpus sentences.

• Then, we retrieve the multi-word units by looking for things beginning with a "<mw•" and ending with "</mw>". • if there are such units, we retrieve their c5 attribute and, by splitting on all tags within the multi-word unit, the multi-word unit itself; we paste the tags and the multi-word units together and, for memory management reasons, do not store all instances but just a frequency list of the instances. Finally, before exiting this conditional expression, we delete the multi-word units from the corpus sentences so that their constituent words are not counted again. • Then, we use nearly the same approach to retrieve the tags and the words of the 'normal,' meaning 'one-word words,' which we also paste together and tabulate. • Then we merge the results for the 'normal' words and the multi-word units and save them into the directory for the interim results under the name of the current corpus file. • After the loop, there's the next hurdle: it is again likely that just growing all 4,049 frequency tables together into one super-long one will be too much for a 'normal computer'. Thus, we reserve memory space in advance by creating two very long vectors, one for word-tag combinations (all.word.tags) and one for their frequencies (all. word.tag.freqs), each with, say, 25 million elements (more than enough for a 100 million word corpus). Then, we use a loop to load each of the 4,049 interim frequency tables and put the word-tag combinations and their frequencies into the relevant slots in all.word.tags and all.word.tag.freqs respectively. We do this by having a counter that always specifies the first position of new input and then inserting the next n elements, where n is the number of word-tag combinations from the current interim-results file -this is like having a large number of small empty containers in front of us and we're filling them from left to right. • The last step is then to sum up all frequencies that belong to the same word-tag combinations so that we can save the output as a .csv file and an .Rdata file.

What are the functions we will need for that? One piece of good news is that, in terms of the functions this script requires, it is not challenging. We need function, several if-conditionals, and gsub as well as nchar to define whitespace. Next, the usual rchoose.dir and dir, but now also dir.create to create the folder to collect all interim-results files. Then, we do the usual for-loop to load each file with scan and get the sentences with grep. We then use exact.matches.2 to find potential multi-word units. We then use if to check, with is.null, whether there are any matches for multi-word units at all, and if there are, we use exact.matches.2 again to get the tags, and then use strsplit to split on tags, access the resulting list elements with sapply and use paste to glue the words together, and then paste again to glue them together with the tags so that we can tabulate them with table.

Then, before proceeding with the words, we delete all multi-word units from the sentences. We then use pretty much the same logic to extract the one-element words and paste them together with their tags so we can again tabulate. At the end of the loop, we use c to combine the multi-word unit table and the one-element word table and then save the result into the interim results folder.

To merge the results, we need to create long empty character and integer vectors all.word.tags and all.word.tag.freqs, and we define a position counter, which is initially set to 1 (because on the first iteration, we begin to put things into the first position of these two long vectors). With another for-loop, we load each interim result and add them into the first empty slots -the one indicated by counter of the two long collector vectors. Once we're done with all 4,049 frequency list files, we use nzchar and subsetting to delete all empty character strings that might remain in our long collector vectors and use tapply to sum up all frequencies of each word-tag combination and save the results as a .csv file with cat and as a data frame (created by strsplitting the words and tags) into an .RData file with save.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_10_bncxml-freqs.r>; read this side by side with the actual script in RStudio: clear memory, load the rChoiceDialogs package, and source the function exact.

matches.2 (1-3) set options(warn) to return warnings immediately when they arise, not later (4) (continued) define the function whitespace (6-18) define the vector corpus.files and create a directory for interim results files

and this part after it: >•result<-tapply(all.word.tag.freqs,•all.word.tags,•sum) ¶

As is often the case, a complex procedure like this is best understood if it is applied to a very small simulated case (repeating and paraphrasing some discussion from Section 3.6.3 here simply because it's my experience that this part is challenging to beginners). In the code file, you find that, beginning in line 150:

• I define two frequency lists freqs.1 and freqs.2 (152-154), which simulate the interim-results files i we load from the sub-directory (106). • I define two collector vectors lett.coll and freq.coll

• Then, to simulate our for-loop (102-117), I manually go through two iterations of it: Lines 163 to 175 show how the data from freqs.1 are inserted into lett.coll and freq.coll and how counter gets incremented; lines 177 to 186 show how the data from freqs.1 are inserted into lett.coll and freq.coll and how counter gets incremented again. • We remove unused slots from lett.coll and freq.coll

This manual execution of a really small example and going through a simulated loop should clarify this way of making use of predefined results vectors, and it is generally a good way to understand more complex parts of scripts, which I encourage you to use whenever you run into problems with loops (recall the end of Section 3.6.2).

As an additional practice assignment, now that this is all done, why don't you try to revise the script so that it uses the corpus files' XML annotation, i.e., try to use the packages XML and/or xml2.

Co-Occurrence Data: Collocation/Colligation/Collostruction

We are now turning to a variety of case studies that involve the exploration of co-occurrence data.

The Collocation Alphabetical Order in the BNC

We begin with a very simple example: We want to determine the collocation strength of the collocation alphabetical order in the BNC but, for now, in a very simplistic way. We will just count how many sentences there are with alphabetical in it, how many there are with order in it, how many there are with both, and how many sentences there are altogether. This characterization maybe already allows you to guess what we will do with this information, namely compile it in a 2 × 2 table like Table

Once we have this table, we will compute two kinds of association measures. First, a very simple one -namely pointwise mutual information MI, which is computed as shown in (

What are the things we will need to do?

• We need to define the corpus files and we will again define search expressions to find alphabetical tagged as an adjective (AJ0) and order tagged as singular noun (NN1).

We will again play around with the fact that the BNC is available in an XML and an SGML version, but rather than, as in Section 5.2.3, have the user state which version is being used, we will have R load the file and discover it on its own and then pick the right search expressions. We will use the fact that the SGML version has no closing word tags. • We create four vectors that we set to 0 each: freq.both will be cell a of

What are the functions we will need for that? We use rchoose.dir and dir as nearly always. We use a for-loop with scan and grep to load each file and retain the sentences.

Then we use if and any (see the very simple definition at ?any ¶) to let R check whether the file has XML or SGML annotation; depending on that, search.expression.alph and search.expression.ord will be defined in the required way. We use length to count the number of sentences and grep to find our search expressions (and length again for the numbers of their matches); the number of sentences with both search words we obtain with length and intersect (on the results of the greps). After the loop, we create Table

The Reduction of to be Before Verbs

In this case study, we will explore corpus data in a way inspired by

With these data, we can test the first hypothesis as well as generate a nice visualization, but to test the second hypothesis we will again use Adam Kilgarriff's frequency list data frame so that we can correlate the percentages of be-reductions with the frequencies of the gerunds in the BNC as a whole.

(continued)

What are the things we will need to do?

• We define the functions just.matches and whitespace and define the corpus files.

• After defining collector vectors, we load each corpus file within a loop, output a progress report, but then immediately check whether the corpus file contains spoken data or not, and if not we immediately iterate to load the next file. • If the file contains spoken data, we discard everything that's not a sentence and everything that's not a word and its tag. • We then retrieve sequences of to be (using the BNC's lemma annotation) followed by a gerund (using the BNC's c5 tags) (note that there are a few tagging errors where forms of to be are tagged as the lemma was). • We extract the forms of to be and the gerunds and store them in separate vectors; after the loop, all the results go into a data frame whose rows we order by row sums. • We then cross-tabulate the gerunds with whether their form of to be was reduced or not both using raw frequencies and percentages of reduction (see above). • We then plot reduction percentages against gerund frequencies and compute Kendall's tau. • Then, for the second exploration, we load Kilgarriff's data frame again, trim it down to only gerunds, and extract our gerunds' overall corpus frequencies from it so that we can generate an analogous plot and compute an analogous Kendall's tau for those frequencies, too.

What are the functions we will need for that? After defining just.matches and whitespace as above, we of course use rchoose.dir and dir as nearly always. We use a for-loop with scan to load each file and if as well as grepl to check for whether the corpus file contains spoken data. If it does, we use grep to retain the sentences and gsub to delete all non-word-tags and their values (using negative lookahead). To retrieve all sequences of to be plus gerund we use just.matches, and we extract forms of to be and the following gerunds using just.matches and gsub (plus whitespace to clean them up).

After the loop, we compile all data into a data frame and use table as well as prop. table to create reductions and reductions.perc as well as order and rowSums to re-order their rows. Then, we use plot, text, abline, and lines(lowess(...)) for the scatterplot and cor.test for the correlation. For the second study, we load the data frame with read.table, trim it down with subsetting and grepl, and tapply with sum to sum up all frequencies per gerund. With those data, we can then create a new plot and do a new correlation test with the same code as before.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_13_reduction-be.r>; read this side by side with the actual script in RStudio: clear memory, load the rChoiceDialogs package, and source exact.matches.2 (just in case)

The third line retrieves the frequencies with which all your gerunds are attested in Kilgarriff's data frame just using subsetting of names again, but of course we now know that two of those will not be found, which is why lines 4 and 5 set those frequencies to 1 (for the later logging) and the names of these frequencies to the two missing verbs.

Verb Collexemes After Must

At the very beginning of this section, in Section 5.3.1, we dealt with computing association measures for the collocation alphabetical order -but even then I already mentioned that, typically, one doesn't just compute an association measure for one collocationrather, one computes them for potentially many collocations and then ranks those by their strength of association. This is what we will do here: We will explore which verbs are most strongly attracted to the position after the modal verb must in the folder K of the BNC World Edition. In other words, now we need a table like Table

But that raises two interesting questions: First, as you know and can see here again, this kind of association measure computation requires us to know not only how often, say, must admit occurs -we also need to know how often must occurs altogether (a + b), how often admit occurs altogether (a + c), and a corpus size (a + b + c + d) -and actually we need all that for all potentially hundreds or thousands of verb types after must. But we can't determine those frequencies all in one loop: Imagine we write a loop that looks for must as a modal verb and then extracts all verbs v 1-n after it so that it can then also determine those verbs' frequencies. Okay, but what if must admit is not attested in the first file but only in the second? Then we will already have missed the frequency of admit in the first file. Thus, we can't do it like this -rather, we must have a second independent loop. In the first loop, we identify all cases of must + V so that, at the end of it, we know all verb types ever occurring after must, and then we can look for all occurrences of all of them in the whole corpus within the second loop.

This brings us to the second question, namely the tricky and often misunderstood question of granularity: What level of resolution are we adopting for the counts of our table(s)? Above, we only had one table and since we were counting occurrences in sentences, the level of resolution was clear: sentences. But here we are looking at something much  more specific: verbs after the modal verb must. The solution, which weirdly enough seems surprising to some, is in fact fairly obvious: We choose a level of resolution that is as similar to the phenomenon being studied as possible (see

Finally, it will turn out that there are 440 verbs occurring after must as a modal verb, so how do we compute all these tables and then also an association measure for each? Thankfully, since this kind of question was one I frequently used in works using collostructional analysis

What are the things we will need to do?

• We define the function just.matches, the corpus files, and two collector vectors: one character vector infs.after.must that will collect all infinitives of verbs occurring after must as a modal verb (that is, from this we can compute all cells a and a + b), the other is the numeric vector modals.plus.inf, which is initially set to 0 but will then determine the sample size a + b + c + d. • We load each corpus file, trim it down to just the sentences, and then look for all sequences of any modal verb followed by an infinitive; we store that in current. mpis; we add the number of matches to modals.plus.inf. • Then we extract from current.mpis the cases where the modal is must and extract from those cases the lemma of the verb after must, which we add to infs.after. must. This way, not only does infs.after.must collect those infinitives, but its length will ultimately determine cell a + b. • After this first loop, we can determine for each table the frequency of must + V (a + b)

and corpus size (a + b + c + d), but also all must + V 1 , must + V 2 , etc., i.e., all as. Also, we know how many different verbs occur after must (440) and what those are, so after creating a collector vector for the cells a + c, we do a second loop over the corpus files where now we determine the frequencies of these verbs after modals in general, not just after must) by looking into this loop's current.mpis. • We then save the results in the format that <coll.analysis.r> needs for a simple collexeme analysis (see readme.txt at

What are the functions we will need for that? Nothing challenging here: rchoose.files and dir as nearly always. A for-loop will load each corpus file with scan, we use tolower and grep to get lower-case sentences, and we use just.matches to find our modal verbs plus infinitives as well as grep and gsub to extract the infinitives after must. We then tabulate the infinitives after must, create a vector freqs.overall (using rep) with a 0 for each infinitive, and enter into a second loop that does everything as before but now has another for-loop in it in which we look for each of the infinitives using a search expression created with paste0; we add the frequency of each infinitive to the relevant slot of freqs.overall and then, after the loop, use data.frame to save the results in the relevant format. After that, we start <coll.analysis.r> by sourcing it from www.linguistics.ucsb.edu/ faculty/stgries/teaching/groningen/coll.analysis.r, answer its questions, and let it do the rest. Thus, this is the overall structure of the script <_qclwr2/_scripts/05_14_must-V.r>; read this side by side with the actual script in RStudio: clear memory, load the tcltk package, and source exact.matches.2 (just in case)

(1-3) define just.matches as before

(check out the code alternative to the inner loop, which is not really much easier (94-104) compile all results in a data frame and save it into a .csv file for the collocation/collexeme analysis

Noun Collocates After Speed Adjectives in COCA (Fiction)

After having worked through so many examples using the BNC with XML annotationbecause the BNC is such a widely used corpus and XML such a widely used form of annotation -we will now explore two corpora in a different format. In this case study specifically, we will look at four near-synonymous speed adjectives -fast, quick, rapid, and swiftand the nouns they premodify in the fiction component of the Corpus of Contemporary American English (COCA), which is available for download from

Second, from that we want to generate a list top.10.collocates that provides for each year for which we have corpus data the top ten most attracted noun collocates of each adjective. We will then save that into a spreadsheet.

One question may arise here: What if you don't have access to COCA (and COHA, which we'll deal with in the next section)? If that's the case, jump ahead to the end of this section for a moment, do the assignment mentioned there, and then come back here.  fact that it's two capital letters in an otherwise tolower-ed corpus will indicate that it's not a real word.

The trickiest part here is probably the two loops that compute and then collect as well as sort the residuals of the adjective-noun co-occurrences in lines 83-95. One important thing to note is again the use of droplevels to make R not show zero frequencies for all sorts of words that are not even attested in the data. Lines 85 to 88 are then just a compact representation where a table of nouns and adjectives is created, but never stored or used other than as immediate input to chisq.test, and even the result of that main significance test is never used but we immediately jump to the residuals part of that test's output, which we essentially use as an association measure -this may strike you as strange, but recall (1) from Section 5.3.1 that, for instance, an association measure such as MI is based on the ratio of observed and expected frequencies, and (2) from Section 4.2.2 that the residuals of a chi-squared test are, too -a simulation with 1,000 random 2 × 2 tables shows that MI values and the residuals are in fact very highly correlated with each other (adj. R 2 of a polynomial regression (second degree) of residuals against MIs was 0.87).

Now, what if you don't have COCA and COHA? These corpora are 'available' but not 'freely available' so not everyone will have them. If you do in fact not have them, here's a very nice solution to the problem in the form of an extra assignment: Write a corpus-conversion script that changes the whole BNC into the COCA format exemplified in Figure

Collocates of Will and Shall in COHA (1810-90)

In this case study we will again look at collocates, this time at collocates in a window of three words to the left and three words to the right of the verbs will and shall in the nineteenth-century data from the Corpus of Historical American English (COHA). Like COCA, COHA is available for download from

Given this similarity in format, several aspects of the code will be quite similar to the previous case study. However, this time we want two kinds of different output. The first one is essentially just a small concordance display and looks like this:

For this, we will retrieve positions of matches, i.e., where will and shall will occur tagged as modal verbs, which will be numbers such as 6, 29, etc., and that means we will need to retrieve the words in positions 3, 4, 5, 6 (the match), 7, 8, 9 for the first match, and 26, 27, 28, 29 (the match), 30, 31, 32 for the second match, etc. That could be achieved using something like (matchposition-3):(matchposition+3) -but what if the match is the first or second word in the corpus, or the penultimate or the last, meaning the subtraction and addition of 3 will result in nonsensical position values? In order to handle such cases, it is useful to define a function that I will here call ranger and that is provided in the code file, which you should explore (in general and for its use of lapply with an anonymous function). This function takes as input:

• a vector of numbers called positions, which typically will be positions of matches in a vector of words (i.e., in the above example 6, 29, etc.); • an argument before.and.after, which says how many collocate, i.e., slots, to the left and right of the matches one wants (the default is set to five but we want only three here). As a programming exercise, you might want to tweak the function such that it can have different numbers of collocates on the left and on the right; • an argument desired.min, which indicates the earliest vector position you might want as collocate slots (the default is of course 1, the first word slot in the corpus (file)); • an argument desired.max, which indicates the last vector position you might want as collocate slots (the default is the maximum of positions, but you should set it to the length of the vector of words that you will subset so that the last word in the corpus (file) could be shown as a collocate)); • two more arguments padded and with.center, which you usually shouldn't need to change from their default setting of TRUE, which is why I will not explain them hereplay around with them if you want to get to know them. Check out the small example in the code file to see how ranger gives you all possible collocate positions for a corpus vector of a certain length (and pads the remaining positions with NAs so all output is reliably equally long).

The other display we want is that exemplified in Table

What are the things we will need to do?

• We define ranger and a vector coha.files with all paths to the 10,000 or so COHA files from the nineteenth century (or your BNC surrogate); also, we define the search terms and the window span.

• We create a matrix with seven columns (see Figure

• We go through an outer loop to load each corpus file (in the same way as in the previous section); we split it up into columns, from which we access the word lemmas and the tags; also, we determine where the modal verb tags are. • We go through an inner loop to search for each search word by looking for the intersections of the positions of the word in the lemmas and the positions of the modal verb tags (this wouldn't have to be a loop -you could just use the same code twice, once for will, once for shall -but putting in a loop there right away makes the script easy to reuse if you have more than two search expressions: You just change the vector with the search terms and all else stays the same). • For the positions of each search word, we use ranger to recover the relevant 3L to 3R collocate slots around the matches and put them into a matrix, which we add to all collocates from the previous search words and files. • We then change the results into a data frame that we can nicely save and, to get results like Figure

What are the functions we will need for that? We use the function ranger, which uses lapply to apply seq to all numbers in its first argument positions, with seq's starting point being the maximum of desired.min and the smallest starting point of collocates and its end point the minimum of desired.max and the end point of the input vector. We use rchoose.dir and dir as nearly always but, since the COHA files are in separate directories, may have to use sapply and dir to look into all user-specified directories to retrieve the files. We use matrix to create the beginning of the matrix results into which we will dump all collocates. We for-loop over the files, which we load with readLines(file(...)) and process with tolower and then strsplit (as in the previous section); thus, we can use sapply and "[" to extract lemmas and tags. We determine modal-verb positions with grep and, in a second for-loop, find the intersections of the search words' positions with those of the modal tags. Once we have the position of the intersecting matches, it's time for ranger to return collocate slots 3L to 3R around them. The collocate slots are then unlisted and used with subsetting on the vector with all lemmas in the file, which is in turn passed on to matrix, which generates a representation of the type of Figure

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_16_modals-incoha.r>; read this side by side with the actual script in RStudio: Which aspects of the script are worth additional comment? The most difficult part of this script, I think, is the retrieval of the collocates: first the slots, then the actual words.

The slots are retrieved in lines 88-91 with ranger and they return an object collocate. slots, which, if one interrupted or simulated the loops, would look like this: That means there are matches (here of shall) in positions

The remaining lines, 96-101, just create nice dimension names: The name of the corpus file is repeated as many times as there are matches (with collocates around them). From that, everything till the first underscore is deleted with sub, which makes the remainder of the string begin with the four-digit year of the file name. That year is then extracted using substr(...,•1,•4), and those become the row names of the matrix. The column names are just numbers from -3 to +3, which represent 3L, 2L, 1L, 0 (for the match, here shall), 1R, 2R, and 3R. The final result:

Split Infinitives

The final case study in this section on co-occurrence deals with split infinitives, i.e., examples such as to boldly go, which contrast with to go boldly. We will return to the BNC XML (just folder A this time) to answer the following questions:

• Which adverbs and which verbs are most often observed in the split construction compared to the overall frequency of split vs. non-split constructions? • Which combinations of adverbs and verbs are most often used in the split construction? • To get an idea of how frequent both constructions are, we'd like to know which words are about as frequent as split and non-split infinitives in the same corpus? Saying that a certain construction occurs x many times per million words is not intuitive to understand, whereas saying that a construction is as frequent as words a, b, c is more straightforward.

What are the things we will need to do?

• We define the function whitespace and a vector corpus.files; also, we define two search expressions -one to find split infinitives, the other to find non-split ones. • We create two collector vectors for split (all.splits) and non-split infinitives (all.

nonsplits) as well as an interim-results directory (<_qclwr2/_outputfiles/05_17_ freqoutput>) in which to store frequency lists of all words per file (which we later will amalgamate into one frequency list as in Section 5.2.8). • We load each corpus file, trim it down to what we need, and look for split and nonsplit infinitives and add them to their respective collector vectors. • We also create a frequency list of each file and save it as an .RData into the interimresults folder. • After the loop, we extract all adverbs and verbs from both the split and the non-split infinitives, which means we can look at the most frequent adverbs and verbs in either construction and also count the numbers of split and non-split infinitives. • We then identify the adverbs that are attested in both constructions, how often they are attested in each construction (raw frequencies), how often in percent they are used in the split construction, and how much higher that number is than the overall (baseline) percentage of split infinitives out of split and non-split infinitives (e.g., if split infinitives made up 10 percent of all infinitives and actually is attested in split constructions 25 percent of the time, we'd get 15 percent as a result); we then plot these 'split-preference' values. • Then, we do the same for the verbs.

• We also create a frequency table of all split constructions to see which combinations of adverbs and verbs are most frequent in split infinitives. • Finally, we amalgamate all frequency lists from <05_17_freqoutput> and list all words that are as frequent as both kinds of infinitives plus/minus 5 percent.

What are the functions we will need for that? We use whitespace, which mostly uses gsub, and as always we use rchoose.dir and dir. We use the usual for-loop with scan and grep to choose and process the files, and then we use exact.matches.2 to retrieve all the infinitives as well as all lemmas for the overall frequency list. After the loop, we use strsplit as well as sapply with whitespace to extract adverbs and verbs; we use length to count how many of each infinitive construction there are, and we use intersect to determine which adverbs are used in both constructions. We summarize those in a matrix, to which we apply prop.table to get the percentages of split infinitives, and then plot with text and abline to visualize the adverbs' preference for the split construction.

Then we do the same for the verbs. For the frequencies of verb-adverb combinations, we use paste and then table.

The last step consists of merging the frequency list files: We generate an empty table first, and then use another for-loop to load each frequency list file and merge them into one long table (with c). Then we group the frequencies by the lemmas with tapply and sum up all frequencies of each lemma (we don't do the insertion into a long vector with counter here because, since we're only looking at BNC folder A, the data set is small enough for this to work even on a computer that's not state-of-the-art). Finally, we compute the absolute difference of all frequencies from the construction frequencies and sort and display those that do not differ from the construction frequencies by 5 percent or more.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_17_split-infs.r>; read this side by side with the actual script in RStudio: clear memory, load the rChoiceDialogs package, and source exact.matches.2

abs(all.lemma.freqs-length(all.nonsplits))•<•(length(all. nonsplits)*0.05)

That logical vector is used to subset all.lemma.freqs, the result of which is then sorted.

Other Applications

We are now turning to a variety of case studies that involve applications that are not so easily grouped into (just one of) the previous sections on dispersion, frequencies, and concordancing or co-occurrence; thus, the following scripts are, I hope, good examples to showcase the variety and potential of corpus-linguistic methods and the different kinds of data you can use and/or explore.

Corpus Conversion: the ICE-GB

You may have already engaged in some corpus conversion activities above -if you didn't have COCA and COHA that is. Here's another one: In this tiny assignment, we are going (continued) to process the files from the one-million-word British Component of the International Corpus of English (Release 2), a POS-tagged and syntactically parsed corpus, of which a very small section is exemplified in Figure

What are the things we will need to do?

• We load the rChoiceDialogs package, exact.matches.2, and set options so that we get warnings as soon as they arise (e.g., when we load a file whose encoding differs from what we expect to handle). • We define a vector corpus.files; and an output directory (<_qclwr2/_outputfiles/05_18_icegb-output>) for the output files, which will look like Figure

• We paste all words coming from the same sentence together with spaces in between them and output them into a file in the output directory that has the same name as the input corpus file.

What are the functions we will need for that? We use rchoose.dir and dir to define the corpus files and dir.create to create an output directory. We for-loop to load each file with scan, paste it together with collapse="\n", and unlist the result of stringsplitting it on the occurrence of sentence tags. We then use exact.matches.2 to find the words and their locations, and then we tapply paste to all the words in the same sentence (now with collapse="•"). Then we cat the results into a file in the output directory named with paste0, done. Thus, this is the overall structure of the script <_qclwr2/_scripts/05_18_icegb.r>; read this side by side with the actual script in RStudio: clear memory, load the rChoiceDialogs and dplyr packages, and source exact.

matches.2 (1-4) set the warnings option to immediate warning

for-loop (counter = i, over corpus.files): (17, -42) output a progress report: the name of i (18) load the corpus file i (20) and paste it together into one string

output the sentences into an output file in the output directory (39-41)

Which aspects of the script are worth additional comment? None, because this script is very short and simple, but do check out the several excurses at the end: The first one shows how we could change part of the loop by using the argument vectorize=FALSE in exact.matches.2; the second one proposes an alternative to parts of the loop using dplyr's %>%; the third combines those two approaches to replace everything in the loop by some nicer-looking %>% syntax.

Three Indexing Applications

In this section we will play around with something that doesn't really have much corpuslinguistic value per se, but it has a lot of practical value and, more importantly, is a good programming exercise: We will use R for indexing. Two applications will involve a small part of the first edition of QCLWR, the third will involve a paper of mine. Specifically, the first case study consists of (1) finding function names in a text version of a few pages of the proofs of QCLWR (first edition) and (

What are the things we will need to do?

• We source exact.matches.2 and load a text file (that is essentially just a few pages copied from the PDF proofs of the first edition of QCLWR and pasted into a text file). • We paste them together in one long string, but then split that up again into a character vector at each occurrence of a page marker that's repeated on every page of the PDF proofs; this way, that character vector has as many elements as the (part of the) proofs had pages.

• Then we find lines with code in that character vector, namely lines that contain the R prompts ">" or "+" followed later by the line break indicator " ¶". • From those lines with code, we extract strings that are likely to be function names, namely sequences of letters, periods, and/or underscores before opening parentheses. • Then we look for all the potential function names in the character vector with the manuscript text and collect their locations as the page numbers (of this manuscript excerpt). • Finally, we output each function name followed by a tabstop followed by the page numbers (separated with ",•").

What are the functions we will need for that? We use source to load exact.matches.2 and scan to load the text file <_qclwr2/_inputfiles/corp_indexing-1.txt> into R. We use paste to merge all elements into one long character vector and unlist plus strsplit to break it up into a character vector of pages. We then use exact.matches.2 to retrieve lines with code and again to extract strings that might be function names. We then for- loop over the list of function names and use grep to find them in the character vector with the book pages and store them in a list -note how it helps here that grep only returns one position even if a function name is attested on one page multiple times (because an index entry lists each page on which a word occurs just once, too). In a separate second for-loop we then use paste to put page numbers together (separated by commas and spaces) and cat to output all results. Thus, this is the overall structure of the script <_qclwr2/_scripts/05_19a_indexing.r>; read this side by side with the actual script in RStudio: clear memory and source exact.matches.2

for-loop (counter = current.function, over function.candidates):

for-loop (counter = i, over seq(index1)): (48, -58) paste together the name of the index entry with a tabstop before the page numbers, which are in turn pasted together with commas and spaces between them and print them with line breaks between index entries (50-57)

Which aspects of the script are worth additional comment? The script as discussed so far is pretty simple, so no additional comments are required. However, in lines 62-72 and 76-84, I provide alternative solutions to what above is done very R-unlike with forloops in lines 33-46 and 48-58: These alternatives use faster and more elegant functions from the apply family and don't require loops at all. These alternatives are heavily commented so check them out to better understand how one does things 'the R way' and to prepare for the next case study. Also, note that the above way of finding function names (stuff before opening parentheses) is not perfect because: it wouldn't find head in sapply(some.list,•head) or sum in tapply(x,•y,•sum)), so before checking out my first attempt at how to do this in lines 89-103, why don't you try your hand at this?

The second case study is very similar to the previous one -the only differences are that (1) we now don't just index function names but essentially all word types in the manuscript, and (

The third case study is again similar to the previous ones, but adds two little twists. One is that we're now loading a file with Windows's version of Unicode UCS-2LE and so it is particularly useful to load this file with readLines(file(...)) and the encoding argument set to "UCS-2LE".

The other twist is more complex and addresses the fact that if an index term is mentioned on several successive pages, say 1, 2, 3, and 4, then indices usually don't print all of those but sum them up as "1-4". Thus, what we want to do this time is not output index entries like this:

Let's approach this by looking at the index term "(association measure|\\bam\\b)". We assign that to qwe to keep code shorter as I am discussing lines 40-53 -noting that of course (sigh) Linux and Windows use different alphabetical sorting so that index term is the first list element on Windows but the second on Linux . . . :

We want to change that to this:

That is, 3 -2 = 1, 4 -3 = 1, 6 -4 = 2, etc., which is already a nice first step given how this indicates occurrences of adjacent page numbers (with the 1s). The next step is to define an object called ranges, which contains hyphens when the difference between two page numbers was 1, and commas otherwise:

not expected to immediately be able to come up with such exotic solutions and I don't dare claim that my solution is good, elegant, or anything other than functional.

Playing With CELEX

One very useful database for many research purposes is the CELEX database

With these vectors in place, we now do some small searches just to explore the kinds of things that one can now do (in the code file, I also always provide the results you get if you run this code on the real CELEX database, not just the tiny excerpts here). To make this section worth your time, though, most code in this particular code file, <_qclwr2/_ scripts/05_20_celex.r>, will not use the traditional R syntax with nesting of functions, but the %>% operator from the package dplyr (which we then obviously need to load). Here in the book, I will discuss just a few of them, but study the code file to see what else there is for you to explore.

The first example to be discussed involves the number of adjectives that have four syllables (lines 38-47). The traditional nested code is shown in line 40 as well as here, and is ugly and counterintuitive to read because you have to go from the inside out (starting with pronuns[adjectives]):

The nicer way using %>% can be read as "take the pronunciations of the adjectives, strsplit them up at the syllabification code (see Figure

For the last example, we will pretend for a moment that the CELEX database doesn't have segmental information for its lemmas (which it does, in column 7 of <EPL.CD>). We want to find out how frequent different consonant cluster lengths are in English adjectives (ignoring syllabification for now). Since we pretend to not have segmental information, we need to create it ourselves: We create a vector with all consonant phonemes (C.codes) and one with all vowel phonemes (V.codes) and then create equally long (in number of characters) vectors with Cs and Vs, so that we can use chartr for transliteration like here:

If we want to have lengths of consonant clusters, then all we need to do is strsplit on one or more occurrences of "V" (because that will leave only the sequences of "C"s, measure the lengths of each of the remaining list elements (by sapplying nchar to them), and then unlist the consonant cluster length and count them (with table), and that is what lines 83-90 do. Two follow-ups on this: First, lines 98-104 and 106-110 then show you how you find the adjectives with the maximal cluster lengths. Second, recall that the CELEX database has segmental information and we just ignored it: therefore, lines

Match All Numbers

This section is another one of the very few that doesn't have the same structure. This is because this exercise is really only one (complicated) regular expression: The goal is to match all kinds of formats of numbers, which is something that can easily come up when you generate frequency lists of (large) corpora and want to avoid having potentially tens of thousands of frequency list entries that are really just different numbers -in such a situation, you would probably want just one entry "_NUM_" or something similar; thus, this is a realistic situation. Figure

Thus, I encourage you to not immediately look at the solution but go to a website such as

Retrieving Adjective Sequences From Untagged Corpora

In this section we want to do something that may sound trivial given the previous sections: We want to retrieve sequences of two adjectives in the base form, which may not seem like a big deal given, for instance, that we already retrieved sequences of more than two words when we looked for split and non-split infinitives. However, this section adds an additional challenge: The corpus from which we want to retrieve sequences of two adjectives is not tagged. We will do two case studies; one will be based on the Chinese-Hong Kong data from the International Corpus of Learner English (ICLE), the other on the Brown corpus of the ICAME CD-ROM version 2 (as before, see the end of this section if you do not have access to either corpus). Thus, we will pursue the following three-step strategy: We will first retrieve all adjective tokens from the BNC World Edition; because this number is going to be very high, we do not collect them in a vector that's growing in the iterations but use the logic from Section 5.2.8: We insert them into a very large vector defined before the loop using a counter.

After the loop, we'll pick the most frequent n adjectives (something like n = 2,000 for the learner data case study and n = 5,000 for the Brown corpus case study) occurring in it and tag all occurrences of these forms in the untagged corpus files, and then we will retrieve sequences of two adjective tags and whatever they tag from these corpus files; with the ICLE corpus, we will actually save the tagged corpus files before we search them, with the Brown corpus example, we'll tag the files and immediately search them while they are still in memory. One comment here: We could just use Adam Kilgarriff's frequency list file or the results from the word-tag combination exercise in Section 5.2.8 to get adjectives to tag, and one could just use a tagged version of the ICLE or Brown corpus, but we will pretend we don't have access to any such resources and write a script from scratch just so you get some more practice that'll help you do these things when those additional resources are really not available; unlike several scripts above, we will write this one again such that it uses the BNC's XML annotation and, thus, the packages XML and xml2.

What are the things we will need to do?

• We load all required packages (rChoiceDialogs, XML, xml2), source exact. matches.2, and define whitespace. • Following Section 5.2.8, we define a vector with corpus files as well as a long character vector for all adjective tokens with ten million slots (called all.adjectives), and we set a vector counter to 1. • Within the usual kind of for-loop, we use the logic from Section 5.2.5 and let R detect whether it's running on a Windows system or not; depending on that, we load (and process) the file with functions from the package XML (if not on Windows) or xml2 (if on Windows). • Either way, we retrieve all adjective lemmas and insert them into the relevant slots of all.adjectives; note that, while we are only interested in adjective types, we still need their frequencies because we want to pick the n most frequent types -that's why we won't already get the unique types (although we could save a (growing) frequency table). • After the loop, we clean up that vector and create as well as save a sorted frequency list of all adjective lemmas. • We then find the n = 2,000th highest frequency of all adjective frequencies and retain all adjectives with at least that frequency (to tag them in CH-HK ICLE). • We define the CN-HK ICLE corpus files to be tagged and use an outer for-loop to load them, paste them into one string, and, if necessary, clean them up. • With each file, we then use an inner loop over the ≈2,000 adjectives to look for each of them (surrounded by word boundaries and parenthesized for back-referencing) and put them back in with an adjective tag in front of them. • After every one of our ≈2,000 adjectives has been tagged, we save the file in the same directory from which it was loaded (but with a different name!). • We then define those annotated files as the new corpus files and load each of them within a loop. • We look for sequences of two words tagged as adjectives; if we find any, we paste the file name in front of them and save them with 100 characters on each side into a vector. • We output that vector into a .csv file. them. Yes, you would still get all the matches -recall would be perfect -but precision would be bad because your few matches would be buried in many useless rows with file names. So, it is important to think through every step of the process: "What can happen here? A, B, and C. What do I want the script to do when A, what if B, what if C?", etc.

Let us now turn to the second case study in this section, in which we modify this script such that

• the corpus from which adjective doubles are extracted is the Brown corpus of written American English from the 1960s (using the folder BROWN1 from the ICAME2 CD); • the adjectives we are looking for are now many more, namely approximately 5,000 rather than the 2,000 for the learner data; • we are not going to save the tagged Brown corpus files, but perform the search right after the tagging -i.e., immediately in the same loop; • we will add the sentence numbers to the matches and delete all tags before saving the final results file.

The Brown corpus files look like the example shown in Figure

for-loop (counter = i, over corpus.files): (85, -125) output a progress report (86), load the file (88), delete line initial annotation (89-92), paste it together into one long string (94-95), strsplit it again at three spaces, and clean it with whitespace (97-100) (continued) we used here? You may already guess the answer is another assignment for you: Write a corpus-conversion script that reads in the BNC file <HHV.XML> and makes it look like a file of the Brown corpus, i.e., as exemplified in Figure

Type-Token Ratios/Vocabulary Growth: Hamlet vs. Macbeth

The next application is concerned with type-token ratios (as a crude measure of lexical richness/repetitiveness) and vocabulary growth (see

(continued)

Let us begin by discussing how we compute type-token ratios and vocabulary-growth curves using a small vector tokens as a 'corpus', something that I always recommend to get started on a new project: Create a data set realistic enough in its make-up but small enough to be seen on one screen, and start developing your code with that. Here we'll do that together, but that means you should already follow along with the script now. With the functions you already know well, it's very easy to compute a type-token ratio:

But what is a vocabulary-growth curve and how do we compute it? It is a line plot that shows positions of all tokens of a vector on the x-axis (as in Section 5.1.2) and all type frequencies on the y-axis; in other words, each point's y-coordinate divided by each point's x-coordinate provides the point's type token ratio. For the above tokens, it would look like Figure

The x-coordinates of each point are obvious: It's just the numbers from 1 to 10, one for every element of tokens, which means that, given what we said above, the y-coordinates are then the type-token ratios of each token slot multiplied by the number of tokens of each slot. Alternatively, the y-coordinates of all ten points are the number of unique types you have seen at each position if you've read all tokens from slot 1 to that position, i.e., what is here called type.freqs: For instance, note how the rightmost value is 7, i.e., the type-token ratio of the whole vector (0.7) times the length of the vector (10). But how do we get that info? One approach that does the trick is this:

For each position i in tokens, you essentially create a vector that includes all elements of tokens from 1 to i, then store the number of types (in y.coords

Then we determine all unique types . . .

as well as their first occurrences in the vector tokens using match:

That is, "f" shows up in tokens for the first time in the seventh position. So far so good, but now the two lines that do the magic of creating the vector type.freqs we want: Remember that type.freqs only contains 0s so far; the first line now inserts a 1 into each slot that belongs to a new type in tokens, as you can see in the output. Second, if now every new type is represented with a 1 while every re-used, or duplicated type is represented with a 0, we can use cumsum to create a vector that contains, for each position, the cumulative sum of all 1s (i.e., all types that are new till there) and all earlier vector values:

Why, then, is this approach so much faster when applied to a vector tokens with 50,000 sampled (with replacement) letters? Because this approach doesn't have to create 50,000 character vectors and check each of them for the number of unique types -it only determines the number of types (

If you check out the script now, you will see how, after a small excursus, the above code is integrated into a function ttr (defined and heavily commented in lines 67-92); note that its most important output component is the fifth one called TypesCounts, which corresponds to the y-coordinates of a vocabulary-growth curve. This function minimally requires a vector of (word) tokens as its first argument, but if you want a plot such as Figure

The remainder of the discussion below uses this function. But we want to add one little twist to the discussion: A vocabulary-growth curve is dependent -to some degree at least -on the exact order of the words in the corpus. This is unproblematic if you compute vocabulary-growth values on a play or a novel because what other orders of the words that are still the same play or novel would there be?! But if you compute vocabularygrowth values on a corpus consisting of many files, then the order of the files is typically arbitrary and makes the vocabulary-growth curve you plot a bit dependent on the usually unmotivated order of files that the corpus comes in. Thus an interesting approach to deal with this is to plot not just one vocabulary-growth curve for the corpus one is interested in but, say, 100 vocabulary-growth curves, one for each of 100 versions of the corpus in which the words have been randomly reshuffled, which is what we will add here.

What are the things we will need to do? With the function ttr now already defined, this script is relatively straightforward:

• We load the text file containing a Hamlet version from Project Gutenberg (<_qclwr2/_inputfiles/corp_hamlet.txt>); crucially, we need to load it with blank lines left intact (to make sure we can use them to find character names -then we discard them, check out the input file!); after cleaning Hamlet, we split it up into words.

• Then we do the exact same things with Macbeth (<_qclwr2/_inputfiles/corp_macbeth. txt>). • After that, we apply our new function ttr to the vector of all words from Hamlet and add to it the points from the fifth component of the output of ttr applied to the words from Macbeth. • For the resampling approach, create a list collector that has two components -one called collector$HAMLET, the other called collector$MACBETH -each of which is itself a list with 100 elements; each of these 100 elements in both HAMLET and MACBETH will collect the y-axis coordinates for one of the 100 randomly sampled word orders (i.e., what above was called type.freqs). • We then do a for-loop in which we

• We obtain the y-axis values for our plot from the elements of collector.

• We generate the x-axis values for our plot by repeating, for each play separately, the number sequence from 1 to the length of the play in words 100 times. • Then we plot all resampled vocabulary-growth curves into one coordinate system, using different colors (and transparency effects) for the two plays.

What are the functions we will need for that? We use rchoose.files together with tolower and scan (note the argument blank.lines.skip=FALSE) to load each play.

We then identify empty lines to delete with which and nchar as well as the immediately following lines with speaker names; also, we use grep to find lines with stage directions. Then, we apply negative subsetting to discard those lines from each play, before we use unlist(strsplit(...)) to split each play up into words. After that, all we need to do is input the vectors of words into ttr to get our plot (we begin with the plot for Hamlet, then use points to add the one for Macbeth).

For the resampling, we use the functions list and vector(...,•mode="list") to create our list collector. In a for-loop, we randomly reorder each play with sample, apply ttr to it, and collect the y-coordinates from its fifth component. We then unlist the y-coordinates from the two parts of collector, and we use rep(seq(...),•...) to create x-coordinates. Finally, we plot the vocabulary-growth curves using periods as point characters (to minimize overplotting), define the color with rgb(...), and add a grid and a main diagonal abline.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_23_ttrs-vg.r>; read this side by side with the actual script in RStudio. (I am skipping (1) the part of the code that I used above to explain the basics of how to compute vocabulary-growth curves and (2) the definition of ttr because it basically just recycles that code in a function definition and is explained in detail in the code file; we therefore begin around line 100.) load Hamlet (103-104) identify empty lines in Hamlet (107) identify character names in Hamlet (108) identify lines with stage directions

for-loop (counter = i, over seq(no.of.resamples)): (179, -185) reorder the words of Hamlet randomly (182), apply ttr to them (181-183), and save the resulting y-coordinates into the i-th slot of collector$HAMLET (183) do the same for Macbeth (184) retrieve all y-coordinates (i.e., type frequencies) from collector with unlist (187-189) generate all x-coordinates (i.e., token indices)

As you can see, Hamlet is much longer than Macbeth, but the vocabulary-growth curves overlap considerably for as long as Macbeth has words, suggesting that in terms of lexical diversity, the two plays are not that different. Which aspects of the script are worth additional comment? Most of this is not particularly challenging programming-wise, so the only thing maybe worth mentioning is lines 164-166: And sure enough, this will create the same plot. The reason I use the first variant is essentially pedantry: In line 160, we did not just apply ttr to ham.words to plot; no, we also assigned the result of ttr to an object because we might do more with those results for Hamlet later. In line 165 I just make sure that we have a corresponding object mac.ttr the hyphenated forms with (a) spaces instead of hyphens and with (b) nothing instead of hyphens respectively.

• We then load each corpus file again, tolower it, trim it down to just the sentences, and then look for all alternate spellings of the hyphenated forms and add their frequencies to freq.spaces and freq.nothings. • We compile the results in a data frame, save it, and explore a table of percentages of the spelling variants.

What are the functions we will need for that? Not much new here: We use rchoose.dir and dir as nearly always and dir.create to create the interim-results folder. We use a for-loop to scan and tolower the corpus files, and grep to get the sentences. We use exact.matches.2 to find hyphenated forms and sub to clean away the tags and spaces; then we apply table to create a frequency list and then save it.

In the second part, we use character and numeric to create empty collector vectors to merge the hyphenated forms and their frequencies from all over the corpus, a for-loop to load each frequency list file, and, if there are hyphenated forms in the file, we merge them with subsetting, incrementing the vector counter on each iteration (as in Section 5.2.8). We use nzchar to discard all unused slots of the collector vectors and use tapply and sum up all forms' frequencies so we can use subsetting to pick all those with frequencies of 1,000 and more. We create collector vectors for the frequencies of non-hyphenated forms with rep and length and set them all to 0. Then we do a second for-loop over the corpus files, this time using lapply with exact.matches.2 to look for all non-hyphenated equivalents to the most frequent hyphenated forms -see how nicely that avoids another loop? Then, we count their frequencies (with sapply, "[", and length) and add up their frequencies. Finally, we compile all results in a data frame and generate a table of proportions that lists for each form which spelling variant it prefers.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_24_hyphenation.r>; read this side by side with the actual script in RStudio: clear memory and load the packages rChoiceDialogs and dplyr; also, source exact.

matches.2 (1-4) define the vector corpus.files (6-7) create a directory for interim-results files (9-10) for-loop (counter = i, over corpus.files):

save that table into an interim results file (26-29) move into the interim-results directory (34-35) and define long collector vectors all.hyphs and all.hyphs.freqs for all forms and their frequencies respectively; set a vector counter to 1 (37-41)

for-loop (counter = i, over dir()): (43, -55)

output a progress report (44) and load the i-th frequency list file (45) if there were no hyphenated forms in that file, go to the next one (47-49) otherwise, insert the observed forms and their frequencies into the next available slots of all.hyphs and all.hyphs.freqs

the search expressions for the forms without spaces or hyphens and make each one of them the first argument of exact.matches.2 one time, while the next arguments that exact.matches.2 receives are always the same ones: current.sentences becomes the vector to be searched, and the concordance output always gets suppressed. The result of that lapply execution is a list with as many elements as search.expression. nothings has elements. But we are not interested in all the info that exact.matches.2 provides -we are only interested in knowing how many instances were found of each search expression, which is retrievable from the output components 1 and 4, which is why we pass that list on to (%>%) sapply, which extracts ("[") from each element of that list the first component

Lexical Frequency Profiles

As we are approaching the end of this chapter, it is only fitting that this case study is one of the most complex ones. It involves the notion of lexical frequency profiles, which is a corpus-based approach in applied linguistics concerned with the amount and the kind of vocabulary second/foreign-language learners use in their speaking and writing (see Laufer & Nation 1995 for an introduction; Meara 2005 for a critique; and Laufer 2005 for a response). It involves retrieving all words from one or more texts produced by learners and classifying all words as belonging to one of several word families and word frequency bins. The bins that have frequently been used come in the form of base word files that establish word families as well as frequency-based groupings of these word families; consider Figure

• We extract the numbers from each line and add them to NUMBERS.

• After our loop, we compile all this into a data frame that now has a structure that allows us to process things more nicely (which is a technical term meaning 'more R-like'). • We define the paths to the two Wikipedia entry text files and create a list lists.

of.tables to collect results for each file. • In a loop, load each file, split it up into words, and create a sorted frequency list of the words so that you have all types and all their frequencies; from that you already compute Yule's lexical richness measures. • For each word type, find its base word list file number; for instance, the word "an" is in base word list 1. • For each word type, finds its family word; for example, the word "an" has as its family word the word "a". • For the family word of each word token in the file, we sum up the frequencies of all its members; for instance, the word "an" is a member of the family 'headed' by "a" that comprises only "a" and "an". The words "a" and "an" occur 151 and 21 times respectively, which means we need to get 172 here for the family frequency of both "a" and "an". • While we're still in the loop, we compile the results into a data frame that we save into an output file that has the name of the programming language studied and part of which looks like this; note rows 5 and 33:

• Also, we store in lists.of.tables for each programming language the frequencies with which types from different base word lists are attested (for easier plotting later). • Finally, we generate and save in a file for each programming language a bar plot that provides the percentage of types in each Wikipedia entry that are in each base word list file; also, afterwards, we compute a chi-squared test to see whether the lexical frequency profile of the article on Perl is significantly different from the one on Python.

What are the functions we will need for that? The definition of just.matches is the same as multiple times above, and our function yules.measures requires sum, table(table(...)), "*", and list. After that we use rchoose.files to define the base word list files (and later the Wikipedia entries), which we read in with the right encoding. We use gsub, grep, and just.matches to define the list file numbers, words, and numbers, whereas, to define the family words, we use grep (to see which lines contain family words and which don't) and then sapply and "-" (yes, the regular minus) as well as max and which.min to compute the matrix position.differences excerpted above; at the end, we use data.frame and write.table to merge all results and save them.

As we turn to the Wikipedia entries, we scan them, strsplit them into words, and create a sorted frequency list. We then use match to assign base word list numbers and family words to each word type of each Wikipedia entry; the more challenging part of summing the family frequencies and assigning them to each relevant type is done with tapply(...,•...,•sum) as so often before, and then we again use match and subsetting for the assignment part. We use strsplit and unlist to paste together file names for results, save all results per programming language into a data frame with write.table and then generate a barplot, which we annotate with text and save into a file with png and dev.off. At the very end, we use chisq.test for a chi-squared test for goodness of fit.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_25_lfp.r>; read this side by side with the actual script in RStudio: clear memory, load the package rChoiceDialogs

for-loop (counter = i, over seq(baseword.files)): (40, -103) output a progress report (41) load the current base word list file and discard empty lines

the smallest value per column and its corresponding row name (90-96), which we then insert into the collector vector (97-98) use just.matches to find the line-final numbers per line (100-101) and store them in the relevant collector vector (102) compile all collector vectors in a data frame and save it into a .csv file (105-108) define a vector with the paths to the Wikipedia entry text files

split it up into words, discard empty elements, and create a sorted frequency list (121-127); for readability's sake, create short telling names for all word types and their frequencies (129-131) compute measures of lexical richness

paste together an output file name for a barplot (182) and a table list.percentages stating how many word types are from which base word list (183-185), and generate as well as annotate that bar plot and save it into a png file (186-204) compute a chi-squared test for goodness of fit to test whether the lexical frequency profile for the entry on Perl is significantly different from that on Python (209-214); explore how the observed frequencies compare to the expected ones and the residuals (215-218)

Which aspects of the script are worth additional comment? This script has some complex parts but most of them were already dealt with in detail -make sure you understand the explanation of lines 72-97 above (on how to identify family words) and the explanation in lines 155-157 on how we obtain and order the summed family frequencies for each word in the family. However, I do want to mention two small things in the plotting and

(continued)

Case Studies 257 the statistical analysis. First, note how in the generation of the plots the function call par("usr") is helpful

• the frequencies of words in corpus A and corpus B do not differ from each other;

• the frequencies of words in corpus A differ from those in corpus B.

But these two things are not the same! Here's the example I use to teach that difference: made-up frequencies of the words horrible, horrifying, and horrid in the Brown and the LOB corpus (lines 222-230 in the code file): The first chi-squared test tests the first scenario, the second chi-squared test tests the second, because it determines whether the frequencies in the Brown corpus differ from those expected from LOB, but not also vice versa. Note how the two tests return very different p-values so it's important you understand that these two are different hypothesis tests.

CHAT Files 1: Eve's MLUs and ttrs

In this assignment and the next, we turn to a completely different corpus format. After dealing with unannotated files, lots of XML examples, a bit of SGML, tabular versions of the COCA/COHA corpora, the Brown and the ICE-GB formats, we are now turning to the CHAT format that is widely used in language acquisition corpora, but also others. These case studies are interesting because they showcase once more how R allows you to handle data and perform analyses that most normal software cannot handle or do.

Most widespread corpus-linguistic software applications require that all information concerning, say, one particular sentence is on one line. In its most trivial form, this is reflected in the BNC by the fact that an expression such as "Wanted me to." would be annotated in one line as in (13) and not in, say, three as in (

(13) <w c5="VVD" hw="want" pos="VERB">wanted </w> <w c5="PNP" hw="i" pos="PRON">me </w>• <w c5="TO0" hw="to" pos="PREP">To </w> <c c5="PUN">.</c> (14) Wanted•me•to.

VVD PNP TO0 PUN want me to.

The following is an excerpt from the file <eve01.cha> from the Brown data of the CHILDES database

What is the current task? We want to explore data of one of the most widely studied children acquiring English, Eve from the

• splits the argument up at anything that's not a digit, unlists the result, and turns it into a numeric vector; • if that numeric vector has only two parts, it adds a 0 as a third one; and • returns the first part plus the second part divided by 12, plus the third part divided by 365. • As a second plot, one might be interested in exploring the finer resolution of lus and ttrs; for instance, we can plot type-token ratios against utterance lengths in Eve's first and last recording (to see whether things changed), or we could plot ecdf curves of the lengths of utterances.

What are the functions we will need for that? The definition of just.matches is the same as above, and the only new function we need to define age.converter is the very simple function floor: ?floor ¶ (plus, at this point, we only need one of the two capabilities of age.converter, from string to number). After that we use rchoose.files to define Eve's corpus files and the function vector (with and without mode="list") to define collector structures. We then load each corpus file with scan, merge it with paste(...,•collapse=...), gsub line-initial spaces away and replace them by spaces, strsplit the file apart again, unlist, and do further cleaning with multiple gsubs, before we strsplit up all utterances into words and retain (with grep) only those with at least one word character.

After that we store the results we obtain by sapplying functions to the list to count lengths and compute type-token ratios (with an anonymous/inline function); we compute a mean type-token ratio using all types and tokens, and a mean of all lengths of utterances with mean, but to avoid the effect that outliers might have we use the argument trim=0.05 to discard both the smallest and the largest 5 percent of the data. We use just.matches to extract from the file's header the name of the child and her age and paste together names for all vector and list elements. We summarize our collector vectors with sapply and summary.

We then sapply the function shapiro.test to all utterance lengths of each file and subset their p-values to see that, surprise, surprise . . . none are normally distributed. We then use two nested for-loops to compute wilcox.tests on every combination of elements of lus, and store their negative log 10 p-values in a large 20 × 20 matrix called tests (20 because we have 20 corpus files).

Finally, we visualize the data. We apply age.converter to Eve's age strings, use par(mar=...) to make more room for our right y-axis label, and use plot to create a scatterplot of mlus against ages. We define each axis manually and use mtext to create axis labels. We use lines to add the type-token ratio plot and add smoothers with lines(lowess(...)). For the comparison of the first and last recording, we just generate regular scatterplots of type-token ratios against mlus, but we jitter the points to avoid overplotting and add the usual grid. Finally, we use plot(ecdf) for the ecdf comparison plot.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_26_CHILDESmlus.r>; read this side by side with the actual script in RStudio: clear memory, load the packages dplyr and rChoiceDialogs

don't plot mttrs, but 1 + 4 × mttrs (178-180), then we add smoothers and a grid (181-183) restore the normal plotting margins (184) set up a plotting window with one row and three columns (189) into the first panel, plot a scatterplot of jittered type-token ratios against jittered MLU values of the first recording

Which aspects of the script are worth additional comment? Most aspects of this script in terms of loops and text processing were not particularly difficult -there were many operations in the loop, but simple ones. The position of this script here at the end is largely due to the more complex statistical and visualization issues such as the 380 significance tests (lines 146-162) and the code for some of the plots, which involves many different tweaks; make sure you go over all this thoroughly line-by-line so you see what each one does. Two suggestions at the end: First, you should try to write a function right.y.axis; it might take as arguments the tickmarks of the left y-axis (because you want to have corresponding tickmarks on the right side) and the minimum and maximum right y-axis values (because that will determine the range of values you might want to label on the right), and it might then return the right y-axis tickmarks and/or y-axis values that are rescaled from the right y-axis to the left for plotting. Second, we have proceeded in a slightly simplistic manner here because the only way we tried to make sure that 24-word utterances in which Eve repeated two words 3 and 21 times do not distort our data too much is by using a trimmed mean. One way of being more careful about such things could be to include a check in the right place that would either flag or omit such cases -why don't you make that a practice assignment?

CHAT Files 2: Merging Multiple Files

The final case study is again on CHAT files, but this one involves more of a data management task, which then makes it possible to do linguistically interesting searches. Specifically, in order to deal with multiple files at the same time, we want to write a script that reads in multiple CHAT files and outputs a list or a data frame with all utterances in the rows and all annotation tiers in the columns; this will allow us to perform nice searches with regular expressions on multiple tiers to identify the rows where different strsplit the file apart again, unlist, and retain (with grep) only those with utterances or their annotation.

In a second for-loop for each line, we use substr to extract speaker/annotation names etc. and then use an if-conditional to distinguish utterances from annotation. If we have an utterance, we just fill relevant slots of our collector list, but if we have annotation, we use a second if-conditional with length to check whether that annotation has already been used -if not, we store the annotation; if it has, we paste together a newly numbered annotation name and then store the annotation.

After both loops, we use order to re-order the components of chat.files.list and as.data.frame to convert it into a data frame; to make sure all columns have the same length, we apply max to the results of checking all columns' lengths with sapply. Finally, we store the data frame with write.table.

Thus, this is the overall structure of the script <_qclwr2/_scripts/05_27_merging-chat.r>; read this side by side with the actual script in RStudio: clear memory, load rChoiceDialogs (1-2) define a vector chat.files with the corpus files (4-5) create a collector list and set the utterance counter to 0 (7-9)

for-loop (counter = i, over seq(chat.files)):

paste it together in one string with a collapse argument unattested in the string

for-loop (counter = j, over seq(current.chat.file.05)): (40, -67) extract the line type (utterance or annotation) from the current line (42), the identifier (speaker name of annotation type) (43), and the line content

if there isn't, save this line's annotation into chat.files.list

(continued)

I think computer searching of corpora is the most useful tool that has been provided to the grammarian since the invention of writing.

• stringi (

• stringr (

In addition to these packages, I think the following ones may also be useful to learn about: tm, RcmdrPlugin.temis, openNLP, koRpus, hunspell, and wordnet -but again, explore the Task View for other useful tools that may help you in your research. A not so much corpus-linguistically interesting package, but one that provides a lot of useful functionality, is readr (

Index