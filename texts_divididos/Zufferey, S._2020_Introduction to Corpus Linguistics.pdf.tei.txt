Preface

Since the 1990s, linguistics has progressively experienced a fundamental methodological turning point. Following the works of American linguist Noam Chomsky, it changed from the essentially rationalist discipline it had been since the middle of the 20th Century, and gradually (re)opened up the empirical approaches represented by corpus linguistics and experimental linguistics. Over the past decade, this transition has accelerated even more, in such a way that the majority of linguistic works published in international journals currently make use of empirical data. Thus, linguistic corpora have gradually established themselves as fundamental tools for linguists, and their use has spread to other fields in linguistics, including those traditionally favoring a rationalist approach, such as syntax. The development of corpus linguistics has led to the creation of new methods for collecting and analyzing linguistic data, which were made possible thanks to the development of computers and the arrival of the Internet. This new direction in linguistics has encouraged spectacular advances for dealing with the multiple facets of human language in all its complexity from a scientific perspective.

Our book intends to introduce such a wealth to readers who are not particularly used to reading linguistics-oriented literature.

In our times, the ability to quantitatively analyze corpus data has become an integral part of the linguist's toolbox. Nevertheless, the use of such data is based on precise theoretical and methodological principles, which require a thorough understanding. This turning point in linguistics implies the need to introduce the new generations of students to the use of these methods which will help them understand the issues underlying their use in scientific literature, to critically assess the results obtained, and to use them in the context of their academic work. Our book is intended as an educational support for students and, in general, for all those wishing to learn the use of corpora in linguistics.

The material introduced in this book does not presuppose prior skills other than basic linguistic knowledge, as well as a minimum command of the most common computer tools, such as spreadsheet software. This book has been designed as study material for teaching corpus linguistics at university initiatory phases, as well as a tool for students wishing to be trained in the use of corpora. Students will be able to work independently thanks the revision questions presented at the end of each chapter, and the detailed answers provided.

As it is an introductory work, this book is necessarily partial and does not deal with all the questions raised by the use of corpora in different linguistic disciplines. It does not cover certain advanced analysis methods which require a high level of computer and statistical skills for data analysis. However, further readings are suggested at the end of each chapter that will enable those who wish to deepen one or other of the aspects presented to go a step beyond.

Finally, this book places a special emphasis on French as an object of study. While it is true that corpus linguistics has imposed itself in an incontestable manner in the English-speaking world and that a significant proportion of French-speaking researchers currently use these methods, the teaching of corpus linguistics still remains marginalized in France. Therefore, this book also aims to highlight the vitality and richness of corpus studies devoted to French, as well as identify the most important resources which have been developed for this language, in the hope of making a contribution to the rise of this discipline for the study of French. Defining elements

The term corpus has a Latin origin and means "body". A text corpus literally embodies a set of texts, a collection of a certain number of texts for study. For example, it is possible to collect a series of newspaper articles and make a corpus of them in order to study the specificities of the journalistic genre. In the field of language teaching, it is also possible to collect texts written by students having different levels, and to build a corpus of these writings in order to study the typical errors that students produce at different learning stages. A methodology using data from the outside world rather than using one's own knowledge of the language is called an empirical methodology. Corpus linguistics can be defined as an empirical discipline par excellence, since it aims to draw conclusions based on the analysis of external data, rather than on the linguistic knowledge pertaining to researchers.

Working with corpus linguistics therefore implies being in contact with linguistic data in the form of texts, and also in the form of recordings, videos or any other sample containing language. Most of the time, these samples are collected in a computerized format, which makes it possible to study them more effectively than if they were on paper. Let us imagine, for example, we wish to know how many times and in what passages Flaubert evokes the feeling of love in his novel Madame Bovary. If we have a paper version of that book, finding these passages will be a long and tedious task, which will require going through the entire text. However, having a computerized version would make the task much easier. We simply need to look up for the terms love, in love or the verb to love in its different forms with the search function of the word processor so as to locate the appearances and easily count them.

For most of the questions addressed by corpus linguistics, it would be impossible to search through a paper database, and that is why having computerized corpora becomes essential.

The problem of manual tracking and counting of occurrences is all the more acute since corpus linguistics is often based on large amounts of data which have not been drawn from a single book, in view of observing the multiple occurrences of a certain linguistic phenomenon and thus apprehending its specificities. For example, let us suppose that we wish to know whether Flaubert talks about love in his work. In this case, focusing solely on Madame Bovary would induce a bias, because this novel is not representative of the whole of his work. So, in order to be able to answer this question, it is necessary to go through the entirety of his novels, making the task even more complex to perform manually. Let us now imagine that this time we want to know whether the French authors of the 19th Century all deal with the question of love as much as Flaubert does.

In this case, it would be impossible for us to look up the occurrence of terms related to love in all of the novels written by French authors in the 19th Century. In order to avoid this problem, it would be necessary to collect a sample of texts, representative of the works of this period. We will discuss this topic in Chapter 6, which is devoted to the methodological principles underlying the construction of a corpus. For the moment, the important point to bear in mind is that corpus linguistics often resorts to a quantitative methodology (see section 1.5) so as to be able to generalize the conclusions observed on the basis of a linguistic sample to the whole of the language, or belonging to a particular language register. As we will see in the following chapters, corpus linguistics may be of use in all areas of linguistics, for instance in fundamental (see Chapter 2) or applied (see Chapter 3) linguistics.

For example, it is crucial in lexicography, since it makes it possible to make an exhaustive inventory of a language's lexicon. It also makes it easy to find examples of uses in different types of sources (literary, journalistic and others), while bringing to light the expressions in which a word is frequently used. In other words, it makes it possible to establish very useful phraseology elements for dictionaries. For example, it is useful to know what the word "knowledge" means, but it is just as important to know that this word is frequently used in phrases such as "acquire knowledge" or "having good knowledge of", etc. Corpus linguistics is a particularly effective method for establishing the frequent contexts in which a word or an expression is used. But corpus linguistics is also used for conducting research in fundamental areas of linguistics such as the study of syntax, since it makes it possible to identify the types of syntactic structures used in different languages.

For example, by making a corpus study, it is possible to determine in which textual genres the passive voice is most commonly used. Finally, thanks to the existence of a corpus of oral data, corpus linguistics also makes it possible to answer questions related to phonology and sociolinguistics. For instance, it makes it possible to establish the area of geographical distribution of certain pronunciation traits, such as differentiating the short /a/ form in the French word "patte" (paw), from the long /ɑ/ form in the word "pâte" (pastry). Answering these different questions requires the use of different types of corpora, as well as having available data regarding their contents. For example, in order to determine the geographical area of diffusion of a certain pronunciation trait, it is necessary to know where each speaker having contributed to the corpus came from. This type of information is called corpus metadata. We will review the main types of existing corpora at the end of this chapter, and discuss the issue of metadata in Chapter 6.

To sum up, in this section, we have defined corpus linguistics as an empirical discipline, which observes and analyzes quantitative language samples gathered in a computerized format. In the following sections, we will discuss in depth the different central points of the definition, indicated in bold, in order to better understand the theoretical and methodological anchoring of corpus linguistics. Empiricism versus rationalism in linguistics

Corpus linguistics is an empirical discipline, which means that it uses data produced by speakers in order to study language. This methodology is opposed to the rationalist method, which functions by looking for answers by relying on one's own linguistic knowledge, rather than looking for it in external data. Let us take an example. In order to determine whether the phrase "When do you think he will prepare which cake?" is grammatically correct or not, the use of empirical methodology would go through large corpora to find whether this syntactic structure is used by English speakers or not.

If sentences following such a syntactic structure never or almost never appear in the corpus, linguists might conclude that this sentence is only rarely used in English. Rationalist methodology, on the contrary, might respond to the same issue by relying on the intuitions of linguists. In this particular case, they might wonder whether they could produce such a sentence or not, whether it seems correct or incorrect depending on their knowledge of the language and might infer a grammaticality judgment from it. Grammaticality judgments are often classified into three types: correct, incorrect or marked, in the event that a sentence may seem possible, but sounds unnatural. This example illustrates a fundamental difference between empirical and rationalist methodology. While the rationalist methodology leads to the formulation of categorical judgments, the empirical methodology provides a more refined answer to this question, since the observation of corpus data offers a precise indication of frequency, rather than a result in terms of absence or presence.

This is one of the reasons why many linguists currently consider that the empirical methodology better matches a scientific approach (in the sense of confrontation against the facts) than a purely rationalist method for studying language.

Nonetheless, the choice between the use of empirical or rationalist methods is not limited to the field of linguistics. Certain scientific branches such as physics, chemistry, as well as sociology and history are essentially empirical disciplines. In fact, both physicists and historians base their insights on external data, which they collect in the world, in order to build a theory, test it and draw conclusions from it. On the other hand, other disciplines such as mathematics or philosophy are traditionally based on a rationalist approach, since mathematicians and philosophers use their own reasoning to build theories and to draw conclusions, rather than from the collection and observation of external data. Philosophers often resort to thought experiments, but these are not experiments in the empirical sense of the term, because they are based on the reflective abilities of researchers. Chomsky's arguments against empiricism in linguistics

Although corpus linguistics has experienced a strong growth over the past 20 years, the empirical grounding of linguistics is not new. Linguists have long used observational data. In the 19th Century, for example, linguists used to work on the comparison of Indo-European languages in an attempt to reconstruct their common origin. Research was based on existing data about the languages spoken in Europe such as German, French and English. Similarly, in the first half of the 20th Century in the United States, the so-called distributionist approach to syntax focused on the study of sentence formation in syntactic structures as they appeared in text corpora, and from there, tried to infer language's general functioning. Around the late 1950s, the use of corpora in linguistics was almost completely interrupted in certain fields such as syntax, following the works of the American linguist Noam Chomsky. In fact, Chomsky defended a strictly rationalist methodological approach to linguistics, and fiercely opposed any use of external data.

The objections made by Chomsky against the use of external data in linguistics have been numerous. We will briefly review them, to show in what ways most of them have lost their raison d'être in the context of current research.

Chomsky's first objection to the use of corpora, which is also the most fundamental one, is that corpora contain language samples produced by speakers. According to him, linguistics should not focus on the linguistic performance of speakers, but on the competence they have in their mother tongue, something he calls their internal language. Now, here is the problem. When people speak, what they produce (their performance) does not necessarily reflect what they know about their language (their competence). For example, under the effect of stress or fatigue, speakers sometimes produce verbal slip-ups or make language mistakes. From time to time, almost everybody happens to badly conjugate an irregular verb and mistakenly produce the form "he eated" instead of "he ate". However, if the person who produced this wrong form were recorded, and then asked whether he or she thought he or she had spoken correctly or not, we can almost be sure that he or she would realize his or her mistake and would be able to state the correct form, "he ate".

Conversely, a speaker could pronounce a word like "serendipity" after having heard it from somebody else's lips, but without really knowing its meaning. These examples illustrate the fact that the words speakers "utter" are not always a true reflection of their linguistic competence. In this way, according to Chomsky, the fact of studying corpora places linguists on the wrong track, because they lead them to consider language from the point of view of "production", which merely represents a biased reflection of the rules of language.

According to Chomsky, another problem related to corpus linguistics stems from the fact that corpora are not representative of the language as a whole. He illustrates this problem in an extreme way, by picking the case of an aphasic speaker recorded in a corpus. Linguists analyzing this corpus would draw totally incorrect conclusions about the language in question, since this person does not represent the linguistic competence of a typical speaker. Furthermore, even if we were not to include an atypical speaker, a corpus could never represent more than a tiny language sample when compared to all the oral and written productions in any language. It is for this very same reason that it is impossible to conclude that a word simply does not exist in a language just because it is absent from a corpus. It could simply never have been pronounced in such particular context, while it could exist in other language registers or have been mentioned by other speakers not included in the corpus.

This problem is particularly acute in the case of rare linguistic phenomena, such as infrequent words or little used linguistic structures.

This limitation has led to Chomsky's third criticism of corpora, namely the fact that a corpus can never contain the whole of a language and that, therefore, the above-mentioned biases are not solvable. According to him, this problem is all the more serious because even if a corpus were of a very large size and included a representative portion of the language, it would not be fully analyzable by linguists, given the fact that it is impossible to manually analyze the content of billions of sentences.

Chomsky's last two objections have largely become obsolete due to the advances made in computer science. In fact, the size of corpora has increased exponentially over the past 20 years, and corpus analysis tools have also made considerable progress. It has thus become possible to analyze very large amounts of data, which represent a much more accurate mirror of the language than when Chomsky formulated his objections. We will return to this in section 1.4, devoted to the connections between computer science and corpus linguistics. In addition to these technological advances, theoretical and methodological advances have also largely made it possible to eliminate or control the other types of biases mentioned by Chomsky. For example, good practice for building a corpus is to accurately document the type of language it contains. This helps to avoid analyzing the language of a single aphasic subject by mistake, for example, as Chomsky might suggest.

It is nonetheless true that a corpus can only show that which it contains, and therefore the absence of evidence that a word or a structure exists in a corpus cannot constitute definitive proof of their absence from the language. Thus, for certain research questions relating to rare or hardly observable phenomena in a corpus, it might be advisable to complement research with another empirical method, namely with the experimental method. As we will see later in this chapter, this method shares the use of a quantitative methodology with corpus linguistics.

In conclusion, we should point out that the rationalist method suggested by Chomsky is also accompanied by biases and limitations which are not negligible and can be corrected by the use of empirical methods. In particular, this method leaves a large space for the subjectivity of linguists while it overestimates the linguistic skills of speakers. Indeed, the use of grammaticality judgments presupposes that all speakers have a definite and consistent intuition regarding all the sentences in their mother tongue. However, such is not the case. If all English speakers agree that a sentence like "Mary dog her walks" is incorrect in English, whereas the sentence "Mary walks her dog" is correct, judgment will not be so unanimous in the case of complex sentences, as the one mentioned above: "When do you think he will prepare which cake?". These divergences become problematic as soon as these judgments are used for building a linguistic theory.

What is more, while it is likely that many English speakers would reject a sentence such as "He does be working" for being grammatically incorrect, in certain areas of the English-speaking world (such as Ireland), this sentence would be acceptable. By resorting to many different speakers and including them in reference corpora of speakers coming from different geographical areas, corpus linguistics makes it possible to respond to this problem in a much more satisfactory way.

What is more, in many areas of linguistics such as lexicology, language acquisition and sociolinguistics, the idea of relying on the internal judgments of linguists is simply not conceivable. No one can study children's language by remembering how he or she spoke as a child, or make assumptions about language differences between men and women by imagining how he or she would speak if he/she were a man or a woman. In all these fields, the use of text corpora has been obvious for a long time and corpora use was never interrupted as a result of Chomsky's work. The paradigm shift in recent decades has taken place in areas where it is conceivable to use a purely rationalist methodology, for example syntax.

Finally, it is important to remember that the role of linguistic theory and the intuition of researchers is not absent in most corpora studies. Indeed, a majority of linguists consider corpora studies as a tool, making it possible to validate or invalidate hypotheses on language, formulated in advance, on the basis of scientific literature and their linguistic intuitions. We will see many examples of this approach (empirical validation) throughout this book. This corpus-based research approach is opposed to an approach which considers corpus data as the only point of reference, both in a theoretical and a methodological sense. In this approach, linguists begin their research without an a priori and simply let hypotheses emerge from corpus data (this is called a corpus-driven approach). This approach is almost unanimous among linguists working with an empirical methodology. On this point, we agree with Chomsky's metaphorically explained opinion where he states that working with linguistics in this way would be the equivalent for physicists of hoping to discover the physical laws of the universe by looking out of their window.

Observing data without a hypothesis often leads to not being able to make sense of data. It is for this reason that the approach that we will adopt in this book corresponds to a corpus-based approach, considering these as available tools for linguists to be able to test their hypotheses. Corpus linguistics and computer tools

As we have seen above, corpus linguistics, as performed nowadays, cannot do without computers. Even if works related to corpus linguistics have existed for a long time (such as the indexing of the Bible by theologians or the file-based construction of dictionaries by scholars like Antoine Furetière in French or Samuel Johnson in English), this discipline was only able to properly take off after the arrival of computing.

Corpus linguistics depends on computer science for various reasons. The first one, which we have already mentioned above, is related to the need for computerized texts in order to be able to carry out truly quantitative research. Nevertheless, looking for elements in a corpus, even a computerized one, by using a simple word processing tool is rather inconvenient. Going back to the example of the search for terms related to love in Flaubert, which we discussed earlier, we find that the use of the search function of a typical word processor quickly reaches its limits. First of all, in order to verify that all occurrences found when looking for the verb to love correspond to expressions of love as a feeling rather than to modal uses as in the phrase "I would love it that you kept quiet", it is necessary to examine each occurrence and thus browse the entire text.

Second, to find all the occurrences of the verb to love, it is necessary to perform a different search for each verbal form, for example love, loved, etc. It is for this reason that other computing tools, specifically devoted to corpus linguistics, have been developed.

In particular, concordancers are useful for searching all the occurrences of a word, plus their context of use and for displaying the results line by line in a single query. These tools also make it possible to establish the list of words contained in the corpus, together with their frequency, and to generate a list of keywords matching the content of a corpus. In the case of corpora containing texts as well as their translation, certain tools called aligners make it possible to align the content of the corpus sentence by sentence. That being done, bilingual concordancers search directly for the occurrences of a word in one of the two languages of the corpus, and simultaneously extract the matching sentence in the other language. We will learn how to use these tools in Chapter 5, which is devoted to the presentation of the main French corpora, as well as the tools for analyzing them.

Then, in Chapter 7, we will also see that in order to answer certain research questions, it is necessary to annotate the content of a corpus. For example, let us imagine that we wish to study the different contexts in which we can use the causal adverb since. If we only look up the word since in the corpus, we will also find occurrences which do not correspond to the use of this word as a causal adverb, but to its use as a preposition, for example in "I haven't seen Mary since Christmas". So, to be able to correctly look up the uses of since we are interested in, we should only keep those which are adverbs and exclude prepositions. This search can be greatly simplified if the corpus has been annotated by determining, for each word, its grammatical category. This operation, called part-of-speech tagging, can be performed automatically by certain software.

Another problem might arise if we decide to study the use of relative phrases such as "the girl who is intelligent" or "the violin which was left on the bus". For this study, a good starting point would be to look for relative pronouns such as who or which in order to find occurrences of relative sentences in the corpus. The problem is that these pronouns are also used in interrogative sentences such as "Who do you prefer?" or "Which hat is yours?" In this case, looking for the grammatical category of the word will not solve the problem, because they are both pronouns. In order to find only the occurrences of who and which as relative pronouns, we should use a corpus in which the syntactic structure of each sentence has been analyzed in such a way that we can assign a grammatical function to each word and group them into syntactic constituents.

Tools for analyzing the syntactic structure of sentences have also been developed in the context of works for automatic language processing. These automatic analyses still require human checks so as to avoid any form of error, but their performance is continually improving. The arrival of these tools has greatly accelerated research in corpus linguistics. We will discuss this issue in Chapter 7, which is devoted to annotations. But corpus linguistics was not only developed thanks to the creation of such tools. Above all, it is the general development of computers and the digital revolution which have made the greatest advances possible. In fact, the increase in the computing power of machines -as well as in their memory -has made it possible to build ever larger corpora. Until the 1980s, a corpus of a million words was considered to be a very large corpus. For instance, the first reference corpora (such as the Brown corpus developed for American English in the early 1960s) were about this size.

At the same time, the arrival of cassette recorders to the market enabled the first creations of oral corpora containing an exact transcription of spoken speech, rather than a synthesis taken in shorthand.

The marketing of scanners in the 1980s later made it possible to digitize a significant amount of data and corpora began to reach larger sizes, up to 20 million words. Then, with the democratization of computer use, the amount of digitally disseminated texts greatly accelerated the growth of corpora.

Finally, since the beginning of 21st Century, the wide dissemination of documents online via the Internet has given another dimension to the size of corpora available to researchers. At present, the Google Books corpus, for example, contains more than 500 billion words, which represents approximately 4% of all the published books of all time Quantitative versus qualitative methods

We have seen that computers help us to work on very large corpora and automatically count word occurrences, find keywords, etc. The need to use a large amount of data and the desire to quantify the presence of linguistic elements in a corpus corresponds to a quantitative research methodology. This methodology involves observing or manipulating variables, as well as the use of statistical tests. The main objective is to test a limited number of variables, in a highly controlled environment whenever possible and on a language sample that can be representative of the phenomenon studied. This can later make it possible to generalize the results obtained to the whole language or to a part of the target language (e.g. journalistic language). These methods nonetheless imply a certain form of reductionism and a simplification of reality. Ultimately, the addition of studies with well-defined and properly controlled variables may provide a global and realistic picture of a phenomenon.

Let us take an example. Suppose we want to test the hypothesis that women talk more about their feelings than men. To test this hypothesis by means of a corpus study, we should first make sure that we are comparing records of men and women produced in the same context, for example, in the context of friendly discussions around a topic, or a face-to-face interview with a researcher. We will also need to make sure that the corpus collected in this way includes approximately the same speaking time or the same number of words pronounced by men and women. This control over the linguistic context and the duration of interactions helps us to ensure that men and women have had fairly equal motives to pronounce words related to emotions/feelings, and as many chances of doing so. Second, we would have to choose a list of words to search within the corpus, representative of the vocabulary related to emotions, for example verbs such as to annoy, adjectives like furious or nouns like anger.

Then, by comparing the number of times these words have been produced by the two groups and by validating the significance of the differences observed between the groups through statistical tests, we would be able to provide an answer to the research question. In this study, we have sought to reduce the number of confounding variables by controlling the context of production of the statements, as well as by limiting the word choice in the examined vocabulary. It is precisely this limited and reductionist aspect that the opponents to quantitative methods criticize, thinking that the constructed and unnatural context in which structured interviews take place does not reflect the richness of natural and spontaneous exchanges between speakers.

The other major methodological paradigm includes so-called qualitative studies. The main objective of these studies is holistic: they aim to study a phenomenon understanding it as a whole, as detailed and as thoroughly as possible, but in a small number of people. Due to their nature, qualitative studies are interpretative. In linguistics, research paradigms involving a qualitative methodology typically resort to the administration of questionnaires with open questions, interviews, observations or introspective techniques, such as think-aloud protocols. For example, in order to study the differences in the way of expressing emotions between men and women, a qualitative methodology could involve asking a reduced number of speakers, for example three men and three women, to describe the way in which they express their emotions, either by talking freely with the experimenter or by talking to each other. The analysis would then require an in-depth study of some of the examples found interesting during the discussion.

One of the main criticisms aimed at qualitative methods is that they are very subjective in nature, insofar as they are largely based on the interpretations made by linguists and the subjective impressions of a few speakers. Thus, the specific cases they describe cannot often be generalized to a population, which, by the way, is not the aim pursued by such studies. Rather than the generalization of results, these studies are based on the possibility of making a transfer from a particular situation so as to understand another one with which it shares common traits. For example, an in-depth case study on the difficulties of expressing emotions in an aphasic patient may help to highlight similar difficulties existing in other patients with the same disorder.

To summarize, each of the two methodological paradigms introduced in this section has both advantages and disadvantages. Quantitative methods enable the generalization of results to the whole of a population, whereas qualitative methods offer a more detailed and nuanced panorama of a real case. Recently, the complementarity between these approaches has started to be broadly accepted in research and many studies are crossing the two types of methodologies, in order to benefit from their advantages and limit their disadvantages.

For example, if we want to know whether learners of French as a foreign language at an advanced level are able to use collocations as native speakers do (collocations such as "prendre une décision" -to make a decision -or "pleuvoir à verse" -to pour with rain), we can search for occurrences of these expressions in text corpora produced by learners and compare the number of times these expressions appear -and their frequency -in a corpus of similar textual productions made by native speakers. By comparing these frequencies through statistical tests, we will know whether learners actually use these expressions as often as native speakers do, or not. Even if we find a difference between the two groups, something which this study will not tell us is why learners do not use these expressions as often as native speakers do or which expressions they use instead. To find out, we can complete this study with a qualitative analysis, by observing, for example, which words often accompany the occurrences of the noun décision in French, which are not the verb prendre.

If we observe that several times the verb used is faire (make), rather than prendre (take), a decision in English-speaking learners, but not in German-speaking learners, we will conclude that these errors could come from a problem of transfer from their mother tongue and, more specifically, from the expression to make a decision in English.

In summary, a corpus can be analyzed using a quantitative or qualitative methodology. While we acknowledge the use and importance of combining these two approaches, in the rest of the book we will focus on the quantitative approach to corpus linguistics, which poses its own theoretical and methodological challenges. Differences between corpus linguistics and experimental linguistics

Corpus linguistics and experimental linguistics share very important methodological properties, since both are empirical in nature and both generally involve a quantitative rather than a qualitative approach. However, these two types of approaches differ in one very important point. On the one hand, corpus linguistics focuses on data observation as found in collections of texts, recordings, etc. On the other hand, experimental linguistics points to the manipulation of one or more variables in order to study their effect on other variables.

Let us imagine once again that we are interested in the types of language errors produced by learners of French. By means of a corpus study, we will be able to identify all the types of errors produced and then quantify each of them: for example, 30 spelling mistakes, 12 lexicon errors, 20 syntax mistakes, etc., made every 100 words. Then, by applying statistical tests, we will be able to determine whether one of the error categories is significantly more frequent than the others. We will also be able to compare the number of errors produced in each category by students of different levels and, thanks to statistical tests, determine whether students make significant progress faster in certain categories than in others. In contrast, what a corpus study will not help you to do is establish with certainty the factors influencing the number of errors. The corpus only shows you the result of the speakers' production, but not what led to these results.

In order to determine the factors that lead learners to make mistakes or not, we will need to resort to experimental methodology.

When we conduct an experiment, the goal is to manipulate the possible causes and then to observe their effects. Going back to our example research question, we may wonder what makes some students produce more errors than others, and in certain contexts, what makes the same student produce more errors than in other contexts. As regards the difference between students, we may think that one possible cause is the level of general intelligence of each student, the assumption being that overall smarter students should produce fewer errors than less intelligent students. The level of intelligence thus constitutes the cause that we will manipulate in order to observe its effect on the number of errors produced. In order to measure the effect of the intelligence variable, we will first need to measure the students' intelligence, for example by means of an IQ test. We will then use the result of this test to determine whether the students who have a higher IQ are also the ones who make the fewest language errors.

In the case of the second research question, which seeks to determine why the same student makes more mistakes in certain contexts, we may assume that stress promotes the production of errors. In order to test this hypothesis, we will have to conduct an experiment in which half of the students are placed in a stressful situation such as an examination context or, for instance, a test with a limited amount of time to complete the task, whereas the other half of the students are placed in a low-stress situation, for example, without any time constraint, performing a task which does not involve marked assessment, etc. Then, we will compare the number of errors in the two groups so as to determine, by means of a statistical test, whether the students under a stressful situation make significantly more errors than the other students, or not. In the two examples of studies that we have just discussed, the approach is the same: to identify a possible cause and to assess its effect through experimental manipulation.

Conversely, a corpus study focuses on linguistic productions without manipulating the data before collecting them.

The study of linguistic productions in a corpus and the manipulation of experimental variables both have their advantages and disadvantages. On the one hand, corpus linguistics has the advantage of favoring the observation of natural data, that is, those which are not influenced by an experimental context. A corpus of journalistic texts includes real productions by journalists, which are not produced for the purpose of being observed. Likewise, a text produced by a learner is also natural, insofar as it is produced in its usual conditions, without there having been any particular manipulation. In addition, the use of corpora favors the observation of a very large amount of linguistic data, whereas experiments are based on a limited number of linguistic items for the task to remain feasible for participants, who would not be able to read thousands of sentences at a laboratory, for example. Finally, once a corpus has been created, it can be used for numerous research questions without requiring any additional time or financial costs.

On the other hand, experiments require significant time resources as well as the usual obligation of having to financially compensate participants for their cooperation.

Experimental studies also have definite advantages over corpus studies. The first advantage, mentioned above, is that experiments allow us to test the existence of a causal relationship between two variables, such as the fact of being stressed and producing more errors. Corpus studies do not make it possible to draw this type of conclusion. Second, while an experimental paradigm can be developed to test almost any kind of phenomenon, there are some rare linguistic phenomena which may be absent or too little represented in a corpus to be examined in this way. For example, if we want to decide whether learners are fluent in French idioms such as "mettre le feu aux poudres" (to stir up a hornet's nest) or "avoir un poil dans la main" (to be extremely lazy) through a corpus study, we will have to look for them in a corpus of learners' productions. Now, it is quite possible that these expressions are never found there, but this does not necessarily mean that the learners do not know how to use them.

It only means that they did not have an opportunity to produce them in the corpus. Using experimental methodology, we will be able to test whether learners have mastered these expressions. For instance, we can encourage them to read the expressions and then ask them to choose, from among several definitions, the one corresponding to their meaning. Finally, experimental linguistics makes it possible to study the linguistic competence of speakers, through different language comprehension tasks which can be more or less explicit or implicit, such as the conscious evaluation of sentences, their intuitive reading, etc. Corpora can only reflect the linguistic productions of speakers.

To conclude, corpus studies and experimental studies can often be used in a complementary way, and, when put together, they represent powerful tools for answering a good number of research questions. Different types of corpora

As we will see in the following chapters, corpora represent linguistic samples of a very varied nature, and it is precisely this variety that makes it possible to answer diverse research questions in all fields of linguistics. In this last section, we will introduce a first classification of the types of existing corpora, in order to be able to refer back to it in the following chapters.

The first distinction we can make among all the existing corpora is the one that classifies them into a sample corpus and a monitor corpus. Sample corpora are those in which data have been collected once and for all, and which no longer evolve thereafter. For this reason, they are also known as closed corpora in the specialized literature. The advantage of these corpora is that they have been designed to contain a set of texts representative of the language, or a part of the language to be studied, with a balanced representation of the different text genres, for example. Thus, these corpora make it possible to draw conclusions which can be generalized. On the other hand, their main defect is that they age quickly and do not follow changes in the language. Therefore, sample corpora need to be recollected at regular intervals.

On the other hand, monitor corpora are never finished and constantly continue to integrate new elements, which is why they are described as open corpora in the literature. A typical example of this type of data is the corpus that contains newspaper archives or parliamentary debates. Every year, the number of available data increases. It is for this reason that it is difficult to maintain a perfect balance between the different parts of these corpora, whose representativeness cannot be fully guaranteed. We will return to the problem of representativeness in Chapter 6. On the other hand, these corpora remain up to date. In cases where they comprise a period of a few decades, they make it possible to observe the appearance of certain changes in language.

The second major distinction to be made among existing corpora differentiates general language corpora from specialized language corpora. General language corpora aim to offer a panorama of the whole of a language at a given time. It is evidently impossible to collect a sample of the whole language, but in the same way that a general language dictionary aims to describe the common lexicon of a language, the general corpus seeks to offer a global image, including the main textual genres found in language. These corpora are really valuable when it comes to studying a language as a whole, but they cannot offer precise answers on linguistic phenomena present in certain specific communication means, such as mobile texting, social media, medical reports, etc.

In order to study one of these areas specifically, it is preferable to resort to a specialized corpus. In fact, there are corpora especially devoted to texting, social media, etc. In addition, general corpora include productions by adults who are native speakers of the language represented. Other corpora specialize in representing other population categories, regardless of whether they are monolingual children in the process of acquiring their mother tongue, bilingual children, foreign-language learners, or even children with neuro-developmental disorders influencing language acquisition, such as autism and specific language impairment. Finally, by default, a general corpus includes examples of the variety considered as a language standard, or one of its main varieties. In French, it generally refers to the French language from France and, more precisely, from the Parisian region. In English, general corpora can refer to the English language from the UK or to American English. Conversely, some corpora specialize in the productions of speakers of a certain language variety, such as French from Frenchspeaking Switzerland, Belgium, Canada, etc.

General or specialized language corpora can contain either written language or spoken language samples. For a long time, written language corpora were the norm, but analysis of the spoken language has developed broadly since the 2000s. Corpora of spoken language are typically of smaller size than written language ones, since they require manual transcription. As a matter of fact, it is easy to record voices, but what is difficult is to carry out searches directly on an audio file. At the same time, speech recognition software does not always fully allow reliable automatic transcriptions. It is for this reason that the oral data must be transcribed manually, which often limits the size of the spoken corpora. More recently, audio-visual recording corpora (also called "multimodal" corpora) have been created, in order to facilitate, for instance, the study of gestures and facial expressions as well as their role in communication. These corpora still pose many codification and interpretation challenges.

Finally, let us point out that video corpora are also used for the study of sign language.

Another distinction that can be made regarding the types of existing corpora relates to the type of processing carried out on the linguistic data of the corpus. On the one hand, raw corpora contain nothing but language samples. This scenario represents the majority of the French corpora. On the other hand, some annotated corpora contain specific linguistic information, apart from the language samples. The most common type of annotation is the assignment of a grammatical category to each word in the corpus, as we have already mentioned. More rarely, certain corpora contain a syntactic analysis of all of their sentences, as well as other types of information, such as an annotation of the discourse relations (cause, condition, etc.) which interconnect the sentences within the text corpora. Finally, certain corpora, which have been transcribed with the aim of studying phonological phenomena, may end up being transcribed using the International Phonetic Alphabet.

So far, all the types of corpora we have considered are monolingual. Another distinction that we can make is to differentiate these corpora from multilingual corpora. There are two types of multilingual corpora. On the one hand, we have comparable corpora, which contain similar samples produced by native speakers in two or more languages. For example, it is possible to build a comparable corpus of parliamentary debates in France and the UK. Such a corpus would make it possible to compare the ways of speaking in a similar context in two languages and two different cultures. On the other hand, so-called parallel corpora contain texts produced in one language and their translation into one or more other languages. These corpora make it possible to study the linguistic correspondences between languages, as well as the linguistic phenomena linked to the translation process. Parallel corpora can also be annotated with exact matches between sentences.

This process is called alignment and gives rise to so-called aligned corpora.

Finally, many corpora are drawn from contemporary written or spoken data. However, there are archives that make it possible to study the history of a language, going back to ancient French, for example. Contemporary corpora are used for studying language in a synchronic way, that is, at a given moment during its evolution, whereas historical corpora make it possible to carry out studies from a diachronic point of view, that is, on the evolution of language. Conclusion

In this chapter, we have defined corpus linguistics as an empirical discipline, that is, based on the observation of real data. We have also seen that corpus linguistics often resorts to a quantitative methodology, studying a large sample of data which is representative of the phenomenon studied, with the aim of generalizing the observations to the whole of the language or to a language's register. We have shown that the main difference between corpus linguistics and experimental linguistics is the way in which empirical data are collected. In the case of corpus linguistics, data are collected in a natural context and then observed, whereas in the case of experimental linguistics, one or more causes are manipulated within a controlled context in order to observe their effects. Finally, we have seen that corpora can be very diverse in nature, depending on whether they are made up once and for all or incremental, general or specialized, annotated or not, monolingual or multilingual, synchronous or diachronic.

1.9. Revision questions and answer key 1.9.1. Questions 1) Which of the following disciplines traditionally involves a rationalist methodology, and which disciplines are based on an empirical methodology? Can we think of any situation in which a discipline of a rather empirical nature could have recourse to a rationalist methodology and vice versa? chemistry -ethics -medicine -law -anthropology 2) Among Chomsky's objections to corpus linguistics, which of them can also be applied to the experimental methodology?

3) In the research projects mentioned below, which one seems to use corpora as a methodological tool (corpus-based) and which seems to use corpora as a theoretical tool (corpus-driven)? a) Search in a corpus for all passive voice sentences in order to formulate the rules governing the use of this construction in French. b) Search in a corpus for all passive voice sentences in order to determine whether they are used more with state verbs than with activity verbs. 4) Why have computing tools especially devoted to corpus linguistics been developed? What are their main functions? 5) Look for an example of a quantitative study and another qualitative study that could be done so as to determine the most common types of spelling mistakes made by children. Which would be the specific contributions of each of these studies? 6) How could we use a corpus and carry out an experiment to study the question of the different types of spelling errors in a complementary way?

1.9.2. Answer key 1) First of all, let us recall that the rationalist methodology interrogates the knowledge of the researcher by means of introspection and reasoning, whereas the empirical methodology looks for answers by observing or experimenting on data that is external to the researcher. Chemistry is typically an empirical science, which makes extensive use of experimentation and observation. Ethics is a philosophical discipline that involves reflections on moral questions. These reflections are, by nature, introspective and involve a rationalist methodology. Law is a science that studies the rules and laws that govern social relationships. Many aspects of the law involve the interpretation of existing rules or the creation of rules based on reasoning and common sense. Thus, introspection plays a big role. That being said, in certain cases, law also deals with external data. For example, a search can be performed throughout previous decisions (case law), in order to find a similar case that could apply to a certain situation.

The role of case law is very different in different legal systems. In Englishspeaking countries, which apply the common law, previous cases play a fundamental role, because they become binding rules for solving the following cases. We can therefore say that in these countries, the part of empiricism when applying the law is also very important. Anthropology is a discipline that studies humanity in its various aspects

2) Chomsky notably criticized corpus linguistics for offering only a partial vision of language, insofar as a corpus includes the productions of a limited number of speakers, at a given situation. This same observation also applies to the experimental methodology, which tests a small number of speakers along a limited number of linguistic stimuli. The main response to such criticism is that these areas are based on the use of quantitative methods (namely inferential statistics), which make it possible to draw conclusions from a sample and to extrapolate them to an entire population. The criticism of the potentially problematic choice of subjects who could be aphasic and not represent the normal use of language also applies to experimental methodology. In theory, though, such subjects could also be recruited for an experiment by mistake. That being said, good practices in corpus linguistics and experimental linguistics require obtaining information about participants beforehand, which can eventually eliminate this type of bias.

Typically, researchers verify that the people who contribute to a French corpus are native French speakers. Likewise, they test the language skills of speakers before considering them by default as French-speaking, bilingual, etc.

3) a) This type of research is corpus-driven, because the starting point is not hypotheses which have to be verified throughout the corpus. The starting point for research is the corpus itself, in order to be able to infer usage rules from its content. b) On the other hand, this type of research is corpus-based, because it starts from a hypothesis (e.g. "passive sentences tend to be used more frequently with state verbs"), and seeks to verify it in the corpus, which, in that way, only works as an analysis tool.

4) These tools have been developed for simplifying searches within a corpus. Otherwise, it would be very inconvenient to use the standard tools that are present in a word processor, for example. In particular, concordancers make it very easy to extract all the occurrences of a word or an expression with its left and right context, as well as to determine its main collocations. These tools also help us create a list of all the words in the corpus, sorted by frequency. While one corpus can be compared to another reference corpus, these tools also make it possible to extract a list of keywords that are specific to the corpus studied. In the field of multilingual corpora, aligners make it possible to align parallel corpora sentence by sentence, and then to extract a sentence and its translation by means of a bilingual concordancer.

5) A quantitative study on this question could focus on the creation of categories for classifying spelling mistakes, for example, agreement errors, redoubling of consonants, dumb letters, etc., and then counting all the occurrences of errors belonging to each category. By applying a statistical test, this study would then make it possible to know whether students tend to make certain types of mistakes more often (e.g. grammatical errors) rather than other mistakes (e.g. lexicon errors). A qualitative study on this same question would identify some examples of spelling mistakes and analyze in detail the contexts in which they occur, for example the grammatical category of the words concerned, whether they are rare or frequent words, occurring in a long or a short sentence, what type of phonemes is poorly transcribed, etc. This study would make it possible to identify linguistic contexts that tend to be conducive to spelling mistakes.

6) The results of the quantitative corpus study summarized above, namely the quantification of the different types of spelling mistakes, could be considered as a kick off for an experimental study. For example, the corpus study could help identify one type of common error, and one type of rare mistake. An experiment could then help to determine whether being in a stressful situation or not has a different impact on the two types of error. 7) a)

In order to study a phonological phenomenon like this, a spoken corpus is essential. This corpus should be specific to the population of French-speaking Switzerland. A large type of corpus comprising a large number of different speakers would be desirable. Finally, this corpus should contain a synchronic type of data, corresponding to the current pronunciation, rather than to its diachronic evolution. b) In order to study the evolution of a language, a diachronic corpus is essential. This constraint implies the use of written data, since oral data only go back to the middle of the 20th Century. Finally, the chosen corpus should include productions made by adult native speakers. c) In order to study translation, a parallel corpus is required. This corpus should contain original texts in French and their translation in English. It should be a synchronic corpus, corresponding to current uses of the language. Further reading For a discussion regarding the main defining elements of corpus linguistics, the works by

The corpora containing spontaneous interactions in a conversational context have also made it possible to study the various ways in which speakers attune their speech to their conversational partner, as well as the study of prosodic phenomena in various communicative contexts. These studies have shown that, in the context of spontaneous speech, speakers tend to stress those words that are not foreseeable in conversation and to choose a prosodic contour reflecting the syntactic organization of their sentences

Finally, the use of corpora makes it possible to perform a quantitative analysis of word frequency, as well as its influence on their pronunciation. Frequency studies have shown that words that are more frequently used have a greater impact on consonant lenition phenomena. For example, the pronunciation of [t] evolves towards

These phenomena are important for understanding how phonological changes occur in a language over time. In summary, studying phonology using corpus data makes it possible to bring to light the interfaces between syntax and speech. These are not so evident when we work with data drawn from reading words or texts (these are less extensive and not so spontaneous). The first study that we will introduce in this section concerns the question of speech articulation rhythm in different French varieties. -Paris and Lyon (France); -Tournai and Liège (Belgium); -Neuchâtel, Nyon and Geneva (Switzerland).

Each person was recorded both during a reading task and throughout a conversation. This study thus made it possible to compare two kinds of speech. Before proceeding with the analysis, the data were transcribed and the texts were aligned with the sound signal. For each speech sample, the authors counted the number of syllables spoken between two pauses. The articulation speed was then calculated in milliseconds per syllable for each segment between two pauses. The data obtained were analyzed using a statistical model which made it possible to test the influence of several variables (the causes we mentioned in Chapter 1; see also Chapter 8 for a more detailed explanation of this notion) on other variables (the observed effect). In this study, the observed effect was the rhythm of speech articulation. The possible causes tested in the model included social variables such as age, geographic origin of the speakers and their gender, as well as speech style (reading or conversation).

The results, which we will only partially report here, indicate that the geographic origin of the speakers does have an influence on articulation speed. More specifically, Swiss speakers have a slower speech articulation rhythm than French speakers (particularly Parisian speakers). Belgian speakers also speak more quickly than Swiss speakers, especially those from Neuchâtel and Nyon. This study tends to confirm the idea that Swiss speakers have a slower articulation than other French speakers. Furthermore, speech style may also have an effect on articulation speed, syllable duration being shorter in conversations than in reading. This result confirms the importance of studying not only reading texts but also, more importantly, spontaneous spoken speech excerpts in order to study a phenomenon such as articulation speed.

The second case study that we will discuss in order to illustrate the role of corpora in phonology concerns the phenomenon of liaisons. In French, many liaisons are considered optional, insofar as speakers can choose whether to make them or not. For example, in (1), it is possible to either pronounce the latent-word final consonant [z] as connected to the final -s of the verb allons (let's go) or not. (1) Nous allons au cinéma. (We are going to the movies.) Several studies have used corpus data to study the factors that lead speakers to pronounce the liaison or not. For example, Results showed that optional liaisons were mostly found in consonants [t], [z] and Morphology

In order to study the rules for assembling morphemes into words, as well as the productivity of different morphemes (the number of words that a rule makes it possible to create), morphology needs to rely on external data, since frequency data cannot be inferred through introspection. Therefore, morphologists have long resorted to word lists. At present, these lists can be retrieved automatically from computerized dictionaries. Some morphologists also consider these lists as a form of corpus, because they make it possible to gather large amounts of data. Working with word lists can certainly be considered as an empirical method. Nevertheless, these data do not necessarily represent a corpus stricto sensu, since they do not contain extracts of language produced naturally. For some research questions, these lists can be very useful, whereas in other cases, it is necessary to resort to a real corpus containing linguistic productions in their context of use. The usefulness of word lists has been clearly illustrated by a study by -the final sounds of words; -their last letters, defined as the spelling reproducing the last vowel and, if applicable, its corresponding coda, for example, -one in the word trombone; -the suffix, in the cases where there was one.

This study made it possible to show that the grammatical gender of a noun can be predicted in a large number of cases, contrary to the claims of many French theoretical studies in grammar. Furthermore, the spelling's ending seems to be the best of the three predictors. In the French language, certain sequences of letters are strongly associated with a certain grammatical genre, for example, -ette is associated with the feminine form, regardless of whether this is a suffix, as in the word statuette (small statue), or not, as in the word devinette (riddle). For this study, the use of a word list offers major advantages compared to a corpus: the grammatical category of words is already known, which simplifies noun retrieval: every word is already associated with its grammatical gender. These two pieces of information are missing from raw corpora. What is more, since all nouns in French have a grammatical gender (or in rare cases, even two), a dictionary offers an extensive list of examples which help us look for regularities.

The use of natural data from corpora rather than simple word lists is nonetheless essential to answer other research questions in morphology. First, in order to assess the productivity of a morphological rule, or even to assess word formation, that is, whether certain words made up from derivation or morphological composition exist, a corpus can offer much more information than the one found in a dictionary. Indeed, these corpora, and in particular, the large corpora available via the Internet, make it possible to find occurrences even for very rare words, as well as to provide very recent examples of language uses. This is all the more significant at a moment in the history of language when these uses have not yet been listed in dictionaries. The use of corpora also offers the possibility of finding out the context in which a certain word was produced, with the aim of checking, for example, whether the meaning of the derived word in such a context was the one expected.

Corpus-based research has also made it possible to show that certain derivations deemed impossible from the point of view of theoretical morphology did nonetheless exist. For example, in theoretical analyses of French morphology, it was deemed impossible to attach the prefix anti-to a morphological pattern such as verb + -able Second, in their study focusing on the derivation -able,

Finally, the use of corpora makes it possible to identify whether certain morphological derivations are more or less productive in certain language registers, or if they are produced more by certain types of speakers (e.g. within a determined age group) or in geographical areas. However, word lists do not make it possible to answer such questions, since they represent a normative use, corresponding to a standard variety. Corpora also help to trace how the productivity of a morphological rule evolves over several decades by means of monitor corpora, or even over several centuries, by comparing corpora from different periods The study by Syntax

The use of corpora in the field of syntax has been controversial for a long time, since this field has relied on an essentially rationalist methodology for several decades, as a result of Chomsky's works (see Chapter 1). However, the use of corpora in the field of syntax has grown considerably. In particular, the use of corpora makes it possible to compare the productions of various speakers, of different language varieties as well as different registers, providing a much more nuanced and realistic vision of the structures underlying language uses, rather than the intuitions of a single speaker. In addition, the syntactic analyses of corpora have made it possible to obtain a fine measurement of the frequency of so-called grammatical sentences, compared to those considered as ungrammatical, and thus making it possible to overcome such binary opposition. This frequency analysis can also be completed by an analysis of other factors (lexical, grammatical, discursive, etc.), making the uses of certain types of constructions more or less likely to occur in various discourse genres.

The large amount of data provided by corpora, combined with the use of automatic analysis tools, have also made it possible to uncover trends which could not have been observed with the naked eye on the basis of just a few occurrences. The study by (2) Dans la rue, il y a des femmes qui discutent. (In the street, women are talking.)

In the literature, this type of structure is associated with the presentation of new events in discourse. By performing a corpus study on spoken French using the Corpus de français parlé parisien des années 2000, the authors identified all the occurrences of "il y a" or "y a" (both forms meaning "there is") and then manually chose only the cases (98 in total) in which (il) y a was followed by a definite noun phrase and a pseudo-relative. Among these sentences, only 16 occurrences corresponded to cases where a new event was being introduced in the discourse, as the literature claims. Analyzing examples helped the authors identify other types of functions for this structure, notably the introduction of a new entity in discourse, as in (3). (3) Il y a Sophie qui veut te parler. (Sophie wants to talk to you.)

These different functions were more easily identified thanks to the large availability of contextual language in corpus data. This study illustrates how a corpus study can combine quantitative elements (the prevalence of different functions for a structure) with qualitative ones (the identification of semantic functions).

Verwimp and Lahousse's study could be carried out on a transcribed spoken corpus in the absence of any form of syntactic notation, since the structure the authors were looking for matched a clearly identified lexical pattern. Nevertheless, the same does not apply to the study of other structures that are not associated with such a pattern. For this type of case, the use of a syntactically annotated corpus becomes compulsory. This is illustrated by the second study that we will discuss in this section.

Another large-scale annotated corpus study was carried out to explore the question of the placement of the attributive adjective, either before or after the noun it modifies. In French, this is considered a complex question, because the factors leading to one placement or the other are linked to phonology, morphology, syntax, semantics and discourse. Among all these factors, the identification of those playing the most important role cannot be carried out without a quantitative analysis of large-scale data.

In conclusion, the analysis of syntactic phenomena using large annotated corpora helps us go beyond traditional qualitative and rationalist analyses. However, these analyses are only possible on corpora that have been parsed syntactically, and these are still rare due to the complexity of applying automatic parsers. Spoken data are particularly difficult to automatically annotate syntactically due to the presence of numerous repetitions, hesitations, etc., which ruins parsing. The results from studies on syntactically parsed corpora cannot therefore be generalized to all language registers for the moment. However, it is likely that these studies will increase in the future insomuch as the quality of syntactic parsers will keep on improving. Lexicon

Unlike syntax, lexicon is ideally suited for corpus analysis, and it is also the field that has been approached from that perspective for the longest time. In fact, most of the time, the study of lexicon can be done directly on a raw corpus, or in some cases, on a corpus that has been annotated with part-ofspeech tagging.

Corpora make it possible to identify all the words used in a language as thoroughly as possible. It is evidently impossible to list all the words existing in any language, but using large corpora, it has become possible to get a much more realistic idea of the number of words in circulation compared to the lists that could be drawn from dictionary databases, which list only part of it. For example, based on the Google Books corpus, which contains more than 500 billion words (including 361 billion in English), drawn from literary sources from the 16th Century to the present day,

Corpora also have another great advantage for lexicon study: they make it possible to identify the sequences of words that frequently appear together, which are called collocations, for example prendre un douche (to take a shower) or forte pluie (pouring rain). We can thus identify fixed sequences like idioms, as well as determine their degree of fixedness. For example, these studies help us to establish whether it is possible for a certain idiom to be used in the passive form or not, with different verbal tenses, or whether the idiom's topic is fixed, as in j'en mettrais ma main au feu (I would stake my life on it), or free, as in mettre le feu aux poudres (to stir up a hornets' nest).

From the point of view of lexical meaning, corpora are valuable resources for determining the vast array of meanings that a word may take on in different contexts. By means of corpus studies, it is also possible to define lexical fields and to study meaning relations between words, such as synonymy and anonymity. For example, in order to determine whether two words are synonymous or not, the analysis of corpus data makes it possible to determine whether these two words can appear in the same linguistic context or not. Finally, the analysis of corpora from different textual and spoken genres makes it possible to determine the situational contexts in which the words are used, in a much more nuanced way than register indications, such as soutenu (formal) or populaire (informal), found in dictionaries (see Chapter 3, section 3.5).

The first example of a study that we will introduce in this section is an antonym analysis and, more specifically, antonyms which are used together within the same utterance, as in (4), where the use of antonyms in bold serves to reinforce the contrast with another pair of opposites in the utterance, in this case, between students and workers. (4) The initiative was very popular with the students and very unpopular among workers.

The second lexicon study that we will introduce deals with the question of neologisms and, more specifically, how they get into the language or not. In France, the question of neologisms is a sensitive point, especially when the new words have been borrowed from English. The rejection of Anglicisms gave rise to several decrees, and then to the law concerning the use of the French language (known as the Toubon law), which prohibits the use of foreign terms in public administration texts. One of the consequences of these decrees was the creation of a Commission for the enrichment of the French language (formerly known as the Terminology and Neology Commission), which was responsible for suggesting French terms that could help avoid Anglicisms. In order to study whether these suggestions are actually used by French speakers,

Finally, we will mention another study concerning the French lexicon and its connections with English, which was conducted in the context of Frenchspeaking Canada. In Canada, the question of Anglicisms is also sensitive, insofar as French is a minority language and struggles to keep its importance alongside English. Discourse analysis

The field of discourse analysis is an empirical field par excellence, insofar as the objective is to study the structure and content of different types of interactions. However, for a long time, these studies were carried out in an almost exclusively qualitative manner, with the aim of analyzing certain particular situations in detail. These studies are very useful for understanding the nature of interactions. For example, Traverso (2019) studied the structure of requests from people asking for help in an office with access to social rights. This study provided a very detailed analysis of the linguistic, paralinguistic and gestural resources used in this particular situation. All these criteria cannot be contemplated simultaneously by a quantitative analysis. Discourse analysis studies also triggered the development of the first multimodal corpora, which make it possible to study interactions in very rich contexts. In addition to these qualitative studies, it is also possible to analyze certain aspects of discourse from a quantitative perspective.

We will focus on this type of analysis in the following section.

The discourse elements that are best suited for a corpus quantitative analysis are those easily identified on the basis of raw data. For example, many studies have focused on how discourse markers such as bon, ben and alors (well, so, I mean, etc.) are used in different kinds of speech and with what functions (e.g.

The above-mentioned studies mainly apply to spoken and interactive language. However, other studies have also focused on written discursive genres. In particular, these studies aim to study the way in which discourses are constructed from the point of view of their cohesion, by analyzing discourse connectives such as parce que, donc and quand (because, therefore and when), for example in Quantitative discourse studies have also sought to study the differences between discursive genres The study by (5) Emma est arrivée en retard parce qu'elle a raté son train. (Emma arrived late because she missed her train). (6) Emma est très désorganisée, car elle perd tout le temps ses affaires. (Emma is very disorganized because she loses her stuff all the time).

Simon and Degand further hypothesized that the difference between car and parce que is not stable between the spoken and written modes, due to the fact that the connective car no longer seems to be widely used in spontaneous spoken discourse. In order to verify these hypotheses, they used a written corpus (Le Soir newspaper) and a spoken corpus (Valibel database), both representing the variety of French spoken in Belgium. First, they retrieved all the occurrences of car and parce que. They found that in written language, the two connectives have a very similar frequency (approximately one occurrence every 300 words for parce que and every 250 words for car), whereas their frequency is quite different in spoken language. On the one hand, parce que is 10 times more frequent in spoken language than in written language, whereas it is the opposite for car, which is 10 times less frequent than in written language.

In spoken language, parce que is more than 185 times more frequent than car.

The authors then manually classified 50 occurrences of each connective found in the spoken and written modes as either objective or subjective, that is, a total of 200 occurrences, randomly chosen from the corpus. Then, for each connective, they were able to compare the differences in distribution between the types of relations both in the written and spoken modes. The results indicated that in written language, car is a more subjective connective than parce que. However, in spoken language, parce que is used for communicating all types of causal relations, replacing car. This study shows the semantic criteria for making a distinction between two connectives that have a similar meaning and reveals important differences in their use between the spoken and written modes.

The second study we will discuss in detail in this section deals with a particular textual genre, namely the SMS language and its influence on the young generation's command of other written genres.

The authors analyzed the types of spelling mistakes produced by the students. For the dictation exercise, they found that the grammatical errors were the highest (agreement, etc.). In Facebook conversations, spelling alterations to typical written words in social media represented less than one in three words, which contradicts the idea that social media language is entirely different from other language registers. Next, the authors showed that there is no link between the propensity of students to use alterations on social networks and their spelling skills in the two dictations. The authors also investigated whether the formal alterations found in the language of social networks had repercussions in other discursive genres. Unfortunately, only a list of 30 words had occurrences in the different genres, which is insufficient for carrying out a truly quantitative analysis. However, they could see that the altered forms in social network conversations (bcp instead of beaucoup, c instead of c'est, etc.) were not found in the spelling of these words in the other register nor were they misspelled in the dictation exercises.

This study thus offers arguments against the misconception that the writing style in social networks degrades young people's spelling skills and makes them unable to differentiate textual genres. Pragmatics

Pragmatics studies language use in context. This definition brings together a wide range of heterogeneous phenomena such as speech acts, implicatures, politeness phenomena and conversation analysis. Pragmatics has many points of contact with both discourse analysis and sociolinguistics. As in the case of these two disciplines, corpora represent valuable tools in pragmatics, because they make it possible to study the use of language in real communication situations.

Certain pragmatic phenomena such as turn-taking in conversations or the use of discourse markers discussed in the previous section can be studied by looking for certain linguistic forms, for example discourse markers such as bon, ben, voilà (well, so, actually) or certain specific parts of corpora, such as the first utterances in conversations, depending on whose turn it is to speak. However, for other pragmatic phenomena such as speech acts and implicatures, as well as for expressing politeness, there is no systematic relationship between linguistic forms and pragmatic functions.

For these phenomena, it is necessary to use data annotation and this annotation must be done manually on the basis of an observation of the entire corpus. This process may become time-consuming, and annotations are not always easy to carry out, insofar as the speech acts that speakers communicate by means of their utterances in many cases are not communicated transparently. For example, it is possible to ask someone to open a window with a formulation such as Could you open the window? which explicitly mentions the subject of the request. However, this can also be done by means of much more indirect formulations such as It's hot in here! or It's hard to breathe. The role of this type of formulation largely depends on the context and cannot be automatically inferred from the linguistic form employed. As soon as a speech act is associated with a certain type of utterance (e.g.

interrogative sentences) or frequently associated with certain words (such as sorry or oops for excuses), a corpus search becomes greatly simplified. This research should nonetheless be subject to manual verification (see

We have already illustrated the use of corpora for the study of discourse markers and connectives in the previous section. In this section, we will introduce a study illustrating the usefulness of corpus data for the study of scalar implicatures. These implicatures are communicated through the use of a weak scalar term which contextually excludes the affirmation of the stronger term. For example, the quantifier certains (some) as in (7) pragmatically excludes the interpretation, although logically possible, according to which all of Laura's friends are nice. (7) Certains amis de Laura sont sympathiques. (Some of Laura's friends are nice).

Thus, the use of quantifiers generates a pragmatically enriched interpretation in the form of an implicature: certains mais pas tous (some friends, but not all of them, are nice). Some pragmatists (e.g. Sociolinguistics

In order to study social variations in language use, sociolinguistics necessarily resorts to external data. The use of corpora has therefore long been a fundamental tool in sociolinguistics. In particular, it makes it possible to compare how different social groups such as women, men and people in the cities, the suburbs or in the countryside use language in natural situations. Another major concern of sociolinguistics is to document regional variations in the use of languages, and for this, corpora are also an essential resource. Moreover, numerous corpora have been developed with the aim of documenting the use of certain regional varieties such as French-speaking Switzerland, Belgium and Canada (see Chapter 4). Sociolinguists also believe that variation is a sign of an ongoing change in language, and in many cases, corpora represent an effective tool for uncovering this kind of variation, particularly in the different registers of the spoken language.

The first study we will introduce deals with the issue of linguistic changes. Gardner-Chloros and Secova (2018) studied the formulation of indirect questions in Parisian French and, more specifically, the existence of indirect questions in which the interrogative word is positioned in situ in embedded structures as in (8), rather than the standard variant (9). ( (literally: I don't know he does what). (9) Je ne sais pas ce qu'il fait. (I don't know what he is doing).

In order to study the distribution and prevalence of this type of structure in the French spoken in Parisian suburbs, all the occurrences of indirect questions were collected in a small oral corpus of approximately 350,000 words. The identity of the person who produced the occurrence was then classified into different categories:

-age group; -ethnicity (French parents, mixed origin, two immigrant parents from the same culture); -the diversity of friends' networks (in terms of percentage of friends in the same ethnic group); -degree of bilingualism (French and another language). This coding enabled the authors to count how many occurrences were produced following the different criteria identified and to prove that young people from bilingual backgrounds use the post-verbal structure (8) significantly more when compared to young people from monolingual French-speaking families. The ethnic group also plays an important role, since young people with two immigrant parents also use this structure significantly more than young people from French families. Gender also plays a salient role, since boys tend to use such structures more than girls. Finally, having a culturally diverse network of friends is also correlated with the use of these structures: young people with a group of friends mixed at 80% tend to use them more than young people with an unmixed group, or a group mixed at 20%.

The authors also tested which of all the factors mentioned above best predicted the type of indirect question used (pre-verbal as in (8) or post-verbal as in (

The authors finally compared the results of their corpus with the use of these same structures in the Corpus de français parlé parisien des années 2000. They observed that this structure was much less used in this second corpus, with only two occurrences by male speakers of Moroccan origin. Thus, this study showed that the use of post-verbal indirect questions represents a case of language change initiated by the less privileged social strata of the population, rather than a prestige change (as is the case with other sociolinguistic changes). This change seems motivated by the desire to give more weight to the interrogative word by placing it at the end of the utterance, while keeping the same word order in situ in direct questions and in embedded questions.

-age (15-16 or 17-18 years); -gender; -the level of experience in writing text messages, the latter variable being coded in a binary way (experienced or inexperienced), from the number of messages sent each day.

Each of the messages in the corpus was coded according to the three linguistic variables to be studied, namely the message's length, the presence of opening and closing elements and its main function. The results indicated that text messages differ from other text genres in that 73% of them do not contain an opening and/or closing expression. As regards the effect of the authors' gender, the messages written by girls tend to be longer than those of boys, but only among the age group of 15-16 years. The level of experience also has an influence on the type of message produced. Inexperienced writers most frequently produce text messages with opening and closing expressions. Finally, SMS mainly has a relational function more than an informative one (56% of messages), and this trend tends to be more pronounced in the younger group and among girls. This study illustrates the way in which different social factors influence textual production within the context of a particular discursive genre. Diachronic linguistics

The quantitative study of the linguistic changes that languages experience over the centuries, is based on the study of corpus data. Such data makes it possible to document the different stages of these changes, as well as the different periods in the history of a language in which these changes took place. The main limitation to the use of corpus data for studying linguistic changes from a diachronic perspective lies in the fact that linguistic changes generally first take place in the spoken language. However, the first spoken corpora date from the second half of the 20th Century. For previous periods, data is limited to written records and, more specifically, to certain registers such as literature and legislation. Due to the lack of spoken records, linguists sometimes resort to the speaking attributes of fictional characters in dialogue as a source. While these dialogue excerpts reveal a less formal language than in other written genres, they do not reproduce certain essential characteristics of the spontaneous spoken language, such as hesitations and errors.

Certainly, these data offer no indications concerning phonology or prosody, which are nonetheless important elements in the processes of linguistic change. The first study that we will introduce in this section deals with the emergence of the counterfactual conditional in structures as shown in (10). (10) Si j'avais su, je ne serais pas venu. (If I had known, I would not have come). In order to trace the emergence of this function of the conditional in French,

The results indicated that the first evidence of the past conditional goes back to the 12th Century but that the frequency of this structure was fairly low until the 16th Century, with around 10 occurrences every 100,000 words. The frequency then sharply increased from the 17th Century and reached 70 occurrences every 100,000 words from the 18th Century onwards. The moment when the uses of the past conditional increased in the 17th Century matched a strong progression in its counterfactual uses, which became the most broadly employed function of this construction. Linguistic theories on the evolution of perfect forms predict that evolution can develop from a previous interpretation (e.g. when the process described by the verb, namely the action, the state or the result described, precedes another process in the sentence) to a past interpretation (when the process described by the verb represents a bygone situation at the time of the utterance).

The authors therefore coded every occurrence according to the type of process described: previous or past. Their results showed that the past conditional had an essentially previous interpretation until the 17th Century, when the past interpretation began to increase. This change matched a strong growth in the counterfactual uses of the past conditional. The study thus illustrates how corpora make it possible to document the emergence and the later evolution of a certain language trait. Thanks to the annotation performed on data themselves, it also shows that certain frequency changes can be associated with the emergence of new functions. The second study that we will discuss concerns the pair of causal connectives car and parce que, which we have already mentioned above (section 2.5). Conclusion

In this chapter, we have shown that the use of corpora can prove to be a valuable tool in all areas of theoretical linguistics. One of the main advantages of corpora is that they contain natural data providing a glimpse of different forms of language use, which can thus be studied while taking into account a rich linguistic context. We have seen that certain studies can be carried out on the basis of raw data, particularly in the field of lexicon, whereas in other fields, such as syntax or discourse analysis, usually data has to be annotated, something which can be done manually or partially automated. We will discuss the topic of manual annotations in Chapter 7. The studies introduced in this chapter have also shown that it is possible to quantitatively analyze different types of observations and data. We will introduce the most accessible methods in detail in Chapter 8.

Finally, all the corpus studies illustrated in this chapter were carried out on corpora of very diverse shape and size. In Chapter 5, we will discuss in more depth the data that are available in French to the public for carrying out corpus studies. Revision questions and answer key 2.10.1. Questions 1) What data are necessary to be able to conduct a corpus study on the way the pronunciation of vowels like the [a] in patte and the 2) Why is the use of word lists not always enough for studying morphological phenomena in a language?

3) Why is syntax a field of study in linguistics where the use of corpora is still limited? 4) Which are the lexicon studies that cannot be fully performed on raw data? 5) What are the areas of discourse analysis properly suited to carry out quantitative studies? 6) What are the constraints posed by the study of pragmatic phenomena such as speech acts and implicatures on a corpus study? 7) Which are the sociological factors suitable for carrying out the study of linguistic phenomena on corpora? What types of data do we need to perform these analyses? 8) What are the limitations to the use of corpora for studying the evolution of languages diachronically? 2.10.2. Answer key 1) In order to study this question, it is necessary to use two spoken corpora respectively representing a variety of French spoken in France, for example in Paris, and that of French-speaking Switzerland.

These corpora must also contain diachronic information, for example regarding the evolution of pronunciation from the second half of the 20th Century to the present day. A selection of occurrences of the vowels to be studied, representing the different periods, should then be looked up in the corpus. For each occurrence, the pronunciation should be noted, either by a phonetic analysis of the production of the vowel or by a manual annotation carried out by two native speakers. Then, the various pronunciations should be quantitatively compared between the two corpora and the periods involved.

2) First, when using word lists, morphologists do not have access to the linguistic context in which these words were produced. For a number of research questions, this type of information is crucial, for example, when it comes to determining the meaning of a derived word. Second, by using a word list from a dictionary rather than a corpus, morphologists only have access to a limited subset of the speakers' linguistic productions. In fact, dictionaries provide a list of well-established uses in language, which correspond to the language's standard variety. However, new uses are constantly emerging and these are not listed in dictionaries. Likewise, corpora also represent less formal varieties of language than the written standard found in dictionaries, which makes it possible to diversify the language uses identified in this way. Finally, large corpora make it possible to have access to much more data than word lists, which makes it possible to identify rare phenomena which do not appear in dictionaries.

3) Syntax has been the area of language most influenced by Chomsky's work and has followed for decades the rationalist methodology that this author advocated for linguistics. Indeed, formulating grammaticality judgments is one of the areas in linguistic studies for which native speakers can trust their intuitions, albeit partially. At present, this objection in principle is largely over, but the main remaining barrier is of a methodological nature, since the study of many syntactic phenomena requires the use of syntactically annotated corpora. However, syntactic annotation tools are still imperfect and sometimes too complex to implement or use. As a result, few corpora have been the subject of such annotations and their complexity discourages certain linguists from using them. 4) Unlike syntax, in many cases, the lexicon can be studied on raw data, that is, non-annotated data. However, due to the existence of many polysemic and homonymic words in French, many lexical searches must be done on the basis of morphosyntaxically annotated data in order to avoid identifying parasitic occurrences.

For instance, when looking up the word "orange" as a color, it is necessary to separate its adjectival uses from its nominal uses. In addition, research on the metaphorical uses of a word must be subject to manual annotation on the basis of their context, since metaphors have no lexical markers that make it possible to differentiate them from the literal uses of the same words. 5) In general, all the elements that can be identified on the basis of surface lexical features are well-suited for quantitative analysis, since it is possible to identify numerous occurrences by means of a computerized search. We should nonetheless observe that this first identification often requires a later manual sorting of the data. In the case of discourse, the phenomena which are well-suited for quantitative analysis are the lexical cohesion markers. These are the use of anaphoric pronouns (he, she, etc.), discourse connectives indicating coherence relations between discursive units (because, when, if, etc.) and the use of discourse markers (well, I mean, you see, etc.), which index the interpersonal relationships between the speakers and offer clues to the discursive planning of the speakers.

Conversely, phenomena such as the structuring of information within discourse and the analysis of global coherence are much more difficult to annotate on a large scale since they require an entirely manual processing of the corpus.

6) The main constraint posed by many pragmatic phenomena such as the study of speech acts and implicatures is related to the fact that the occurrences of these phenomena cannot be identified on the basis of lexical markers which can be automatically searched for in the corpus. For example, an indirect speech act such as a request can take many different superficial forms as a question (could you shut up?) or an assertion (I'd like you to stop talking). The same goes for implicatures, which are not linked to the use of specific words, with the exception of generalized scalar implicatures, which are typically associated with the use of quantifiers such as some and a few, logical connectives like or and some telic verbs such as to start. In order to identify the occurrences of pragmatic phenomena which are not related to words, it is therefore necessary to scan the entire corpus manually.

What is more, the speech act that the speaker intended to produce is sometimes ambiguous and even when having access to a linguistic production: it is not always possible to clearly identify the speaker's intention. This annotation therefore involves an interpretation on the part of the researcher, which is, in part, necessarily subjective.

7) When the sociological information about the speakers in a corpus is known, it is very easy to use this for comparing the productions of different categories of speakers such as men and women, people from different age groups, from different socio-economic backgrounds, etc. These data are typically not part of the corpus itself, but are listed in the metadata (see Chapter 6).

8) The main limitation concerns the availability of diachronic data. Spoken corpora in particular do not go back further than the second half of the 20th Century, which considerably limits the generalizations that can be drawn regarding the evolution of languages, since this primarily takes place in speech. In addition, diachronic corpora only offer a list of certain textual genres, such as literature and legal texts, which also limits the generalization of the conclusions that can be drawn from them. Further reading The usefulness of corpora for studying lexicon, syntax, pragmatics, sociolinguistics and discourse is introduced with English examples in the book by O' How to Use Corpora in Applied Linguistics

In this chapter, we will continue to explore the multiple uses of corpora in the different areas of applied linguistics, in order to complement the presentation of theoretical linguistics in Chapter 2. In particular, we will see how corpus data can be used for studying the language of specific groups such as children, individuals with language impairments and foreign language learners. We will then illustrate the role of corpora as a tool for teaching languages, as well as for creating dictionaries. Finally, we will discuss the uses of corpora outside the language sciences, in order to study literary texts, and also within the legal framework. Language acquisition

In order to study language development in children through observation, the use of empirical methods is essential. It is for this reason that this field has been a pioneer in the development and sharing of corpus data. At the beginning of the 20th Century, several researchers systematically studied the language of their children by means of notebooks, where they recorded their observations (e.g.

Language acquisition corpora show some specificities when compared to other corpora. First, children's language develops mainly during the preschool years. The development of spoken language largely precedes the start of the written language learning process. This is why language acquisition corpora are by nature spoken corpora, which require a written transcript in order to be analyzed. The transcription itself poses certain challenges, since children who acquire language produce mistakes (see Chapter 7 for a discussion regarding the different ways of annotating such mistakes). Studying these mistakes provides valuable clues for understanding the acquisition process. It is therefore advisable not to erase them in the transcriptions, but to keep a log of them so that they can be easily identified (e.g. using categories such as "consonant substitution", or "truncated word"). Finally, in the recordings, children interact within their environment (often at home), with people they know well, most often, their parents.

For this reason, language acquisition corpora frequently include language samples produced by children as well as by adults. This configuration makes it possible to analyze the way in which adults respond to children's language, as well as the connections between the language that children hear from their parents and their own productions. This information represents valuable clues for studying acquisition mechanisms, and these are particularly valuable for theoretical frameworks which attribute a key role to social interactions as the source of language acquisition (e.g.

Two types of corpora can be considered when studying children's language. On the one hand, we have so-called longitudinal corpora, in which one or more children are regularly recorded over a period of several years, and on the other hand, we have cross-sectional corpora, in which groups of children of different ages are recorded only once. Each corpus type has its own advantages and disadvantages. The great advantage of longitudinal corpora is that they make it possible to study the evolution of language much more precisely than cross-sectional corpora, since these contain samples collected at close intervals. Their disadvantage is that they include samples from a very limited number of children. Given the importance of individual differences in language acquisition

The main limitation to the use of corpora (whether longitudinal or crosssectional) for studying children's language is that they contain only a small portion of the language that children produce at a given time. Typically, a longitudinal corpus includes a recording of a few hours, gathered every one to three weeks.

A second limitation inherent to the use of corpora is that even very dense corpora can only be used to study language productions. However, it is well known that a comprehension and production of language do not develop in a totally synchronous manner (e.g.

The first study that we present to illustrate the use of corpora for the study of language acquisition concerns the acquisition of the French verbal system for expressing temporal references. Language impairments

In the same way as corpora make it possible to study language acquisition in children experiencing typical development, they also provide important information about language acquisition in atypical populations such as children with autism spectrum disorder or specific language impairment. The main advantage of using corpora to study such populations is that they provide an overview of the communicative strategies they deploy in order to compensate for certain language and communication difficulties. For example, corpus data make it possible to analyze the way in which a child with limited syntactic skills manages to ask for an object or to ask a question. These also provide an overview of a child's linguistic competence when they find themselves in a familiar situation, offering them better chances of displaying all of their skills. This feature becomes of crucial importance when studying the speech of children with autism spectrum disorders, who suffer from a high level of anxiety when facing unknown situations.

Finally, corpora are also useful for comparing the skills of children and patients in different and natural discursive situations (Da Silva Genest and Masson 2019). Corpora are becoming increasingly used for identifying linguistic markers charactering certain language disorders in adult populations, for example Alzheimer's disease

The main limitation to the use of corpora for studying atypical populations (which is also valid for the study of normally developing children) is that corpora only provide information on spontaneous linguistic productions. However, when a certain linguistic phenomenon is not found in the corpus, it does not necessarily mean that the child or patient recorded in the corpus does not have the competence to produce such types of words or sentences. It only means that they did not employ them during the recorded sessions, either because they did not have the opportunity to produce such an element, or because they deployed an avoidance strategy due to the fact that they could not master such constructions. As we have already seen, this limitation is particularly acute for the study of rare linguistic phenomena. As we will later explore in the case studies, when studying children with atypical development, it may be advisable to analyze a corpus of children with typical development (or suffering from a different pathology which does not involve the same linguistic deficits) in parallel, so as to favor the comparison between different linguistic productions in similar contexts.

In the same way, when it comes to studying the language of adult patients, it is necessary to compare it with the language used by healthy adults, produced in similar contexts.

Another limitation to the use of corpora is that the latter do not make it possible to measure the connections between verbal productions and other cognitive skills, such as working memory or non-verbal intelligence. Gathering data about these non-linguistic skills is often important in order to understand the nature and causes of the language deficits observed. It is for this reason that the study of language production in patients is often carried out via constrained production tasks, such as the ability to name images or to repeat non-words or sentences (see, for example, Seiger-Gardner and Almodovar 2017), rather than based on corpus data alone. Performed in an experimental context, these tasks make it possible to control many linguistic parameters that influence production, as well as to limit the impact of avoidance strategies and to test the existence of connections with other individual differences in working memory or non-verbal intelligence.

The first study that we will discuss in this section compared the linguistic productions of six children with autism spectrum disorder (ASD) with those of six children with Down syndrome. The aim was to determine whether the syntactic development of children with ASD followed a different trajectory compared to another population who also suffers from language impairments. The second study concerns the production of noun phrases by Frenchspeaking children with specific language impairment (SLI). Second language acquisition

The field of second language acquisition is one of those in which the use of corpora has grown exponentially in recent decades. The creation of numerous learner corpora, as well as the development of new methods and annotation tools, has largely contributed to this evolution. While linguists working on the question of second language acquisition have long used learners' productions as a source to build their theories, these data were limited to very small samples or even to single-person studies. Therefore, the generalization potential linked to these data was highly questionable. This led to the creation of real learner corpora, aiming to provide representative samples of this population.

There are different types of corpora containing language produced by learners. The first learner corpora produced at the end of the 1980s were limited to written productions, and these corpora are still the most numerous today

As is the case with other types of corpora, learner corpora have mainly been compiled in English, but there are also resources for other frequently taught languages such as French (see Chapter 5, section 5.5). Some corpora contain language samples produced by learners of different mother tongues, and this information can be found in the metadata (see Chapter 6, section 6.4). These corpora are valuable tools for measuring the role of different mother tongues in the process of acquiring the same foreign language, as we will see below.

Finally, most learner corpora are cross-sectional corpora, including one sample per participant and representing a given moment during the acquisition process, since most of the time learners included in a corpus have a homogeneous level of competence in the foreign language. These corpora are very useful for determining learner competence at a certain level, but considered individually, they do not make it possible to study different acquisition stages. For this, longitudinal corpora are necessary, but these are rare due to the difficulty of sampling the same learners across several years. One way to get around this problem is to compare several cross-sectional corpora including learners from different levels of competence.

The first study we will discuss focused on whether learners at a very advanced proficiency level keep on improving, which would justify the need to define different development stages for learners beyond the so-called advanced stage of acquisition. To do this, in the spoken corpus InterFra, Forsberg Lundell et al. (2014) defined three groups of French non-native speakers whose mother tongue was Swedish. Each group included 10 speakers. The first group was made up of speakers aged 19 to 34 years who had lived for one to two years in France. The second group included speakers aged 25 to 30 who had lived between 5 and 15 years in France, and the third group included speakers aged from 45 to 60 years who had lived between 15 and 30 years in France. These groups were compared to two groups of 10 native speakers each, who were between 15 and 30 years old and 45 and 60 years old respectively, chosen to match the ages of learners. The groups of learners were then compared on the basis of five linguistic indicators:

-the number of non-native morphosyntactic forms produced, for example, gender or plural agreement mistakes; -the number of left dislocations, in sentences such as "moi, si tu me demandes, il a tort" (literally: me, if you ask, he is wrong), which are typical of spoken French; -the number of formulaic sequences such as collocations; -lexical richness, calculated based on the number of words with high, middle and low frequency in corpus data that are used; -fluency, measured by articulation speed and utterance length between two pauses.

The results indicated that the group of learners with 5-15 years of residence differed from the 1-2 years of residence group on the following criteria: use of formulaic sequences, lexical richness and fluency. The group having lived longer than 15 years in France did not differ from the 5-15 years of residence group on any of these indicators. However, speakers who had lived the longest in France managed to pass for natives in a listening discrimination test administered to French native speakers, unlike the speakers from the 5-15 years of residence group. This indicates that some form of progression must have taken place between these two groups, but which could not be measured through the tests chosen for this study. Furthermore, all the groups of learners differed from native speakers (but did not differ from each other) on the assessment of morphosyntax, which seems to indicate that this is an area which can remain beyond the reach of even the most advanced learners.

This study thus showed that language continues to develop beyond the so-called advanced acquisition stage and that this progression is not uniform among the different dimensions of language. As the lexicon continues to progress, certain aspects of the language system such as morphosyntax remain at a non-native level, even at truly advanced acquisition stages.

The second study that we will introduce compared the use of two English discourse markers, in fact and actually, by learners of two different mother tongues and by native speakers. Buysse (2020) compared the oral productions of French-speaking and Dutch-speaking learners of English in the LINDSEI corpus with the productions of native English speakers in the LOCNEC corpus, a corpus which was compiled to be comparable with the LINSDEI (see Chapter 4 for a definition of the concept of comparability). The interest in comparing French and Dutch speakers is that there are different translation equivalents for the English markers in both languages. While Dutch has two markers which closely resemble those in English (eigenlijk for actually and in feite for in fact), French only has one close marker, which is en fait. So, in French, actually has no translation equivalent of its own. In her study, the author first performed a frequency analysis regarding these two markers in the three sub-corpora.

The results indicated that actually is significantly more common than in fact among Dutch native speakers in comparison to French speakers. Conversely, in fact is significantly more common than actually in the speech of French speakers compared to Dutch speakers. On the basis of the literature, she then identified all the possible functions for these markers in English, such as introducing an elaboration or a contrast, and then annotated all the occurrences of these markers according to one of these functions. This analysis allowed her to show that learners use all the possible functions that the markers offer, even if their respective frequency varies a little between French speakers and the other two groups. That being said, the low number of occurrences of the marker actually among French speakers (56 in all) prevents a quantitative analysis of the differences between its different functions. In summary, this study demonstrated the influence of speakers' mother tongue on the use of discourse markers in a foreign language, and in particular, the importance of having a similar marker in L1 to help learners use markers appropriately in a foreign language.

Indeed, Dutch speakers, who have two very similar markers in their mother tongue, use in fact and actually in the same way and in the same proportions as natives. On the other hand, French speakers tend to under-use the marker, which has no direct equivalent in their mother tongue (actually) and to overuse the other marker (in fact), to perform the same functions, as they would do in French. This study thus indicates that negative transfer effects occur even among advanced learners. Language teaching In addition to collecting learner corpora as we discussed earlier, the area of language teaching currently makes an extensive use of corpora produced by native speakers (see, for example,

In the field of vocabulary in particular, the use of corpora makes it possible to empirically provide lists of the most frequent words in a certain field, which should therefore be taught as a priority. Another key point for mastering a foreign language is to know, apart from the meaning of isolated words, certain elements of phraseology, in other words the typical linguistic sequences in which a word occurs, for example for the word "knowledge", "to acquire knowledge", "knowledge gain" or "prior knowledge". Some researchers even think that these elements should be taught as lexical units

In addition, corpora provide examples of spoken language, which are clearly more realistic than the constructed dialogues contained in most language methods. Given that they include natural interactions, corpora include reformulations, hesitation markers, turn-taking devices, etc., which are not reproduced in artificial dialogues, but which are important for learners to master since they are an integral part of language uses among native speakers. Finally, the creation of learner corpora has also made it possible to bring a new dimension to language teaching, by allowing learners to consult non-native productions and to compare them with native productions. Access to such data enables learners to become aware of the differences between their productions and those of natives. In addition, these corpora often contain an annotation of errors, which favors an explicit learning process and enables learners to become conscious of typical errors and to avoid them.

An important question for language teaching is to determine to what extent the corpora developed for linguistic research can be reused as such in the classroom. On the one hand, there are many advantages to letting learners use corpora by themselves, for example, by teaching them to search for word occurrences using a concordancer. This practice induces active reflection on the language, when it comes to determining what to look for and how to look for it, which enhances learner autonomy and stimulates students to become involved in the learning process

Limitations on the use of corpora created for research also apply to the use of raw corpus data for creating language methods. In order to base a language method on corpus data, it is imperative that the corpus chosen is adapted to the target audience, in particular from the point of view of the variety of the language represented, discourse genres, the age of the speakers, etc. According to

In this section, we will introduce two studies which show the usefulness of corpora for language teaching. Each of them compared corpus data with the presentation of the same phenomenon using different language methods. -the grammar points discussed; -the order in which they were presented; -the vocabulary used in the examples to introduce such points. Then, they compared the examples with frequency data drawn from a 20 million word corpus, corresponding to four different language registers.

In each of the three areas studied, significant differences were observed between corpus data and the presentation of the same phenomenon in language methods. For example, in the section introducing the forms that noun phrases may take in English, most of the methods only indicate a pre-nominal modifiers can be an adjective (a nice man), a present participle (an exciting game) or a past participle (stolen goods). However, in written corpora, nouns are also common modifiers of other nouns (e.g. metal seat and tomato sauce) and the relationships they express are diverse and complex. This syntactic pattern should also be included in language methods. In addition, the order of presentation for the different grammatical features does not correspond to the uses observed in corpora, especially in the case of verbal tenses. Most methods strongly emphasize progressive forms and represent them as the default form in conversations. However, corpus analysis shows that the most frequent case in many language registers is, on the contrary, the simple aspect.

Finally, the authors observed that the verbs used for illustrating different grammatical properties in language methods are not necessarily the most common verbs in real-life language. Although introducing less frequent verbs may be useful for broadening learner vocabulary, it is nevertheless surprising that the most common words are not used, at least for beginner learners. This study showed that the intuitions of language method designers often do not reflect actual language uses. Corpus data make it possible to produce better-suited educational materials to match the realities encountered by learners.

Racine and Detey (2017) also compared the information given in language methods with corpus data, focusing on the question of liaisons in French. Producing liaisons is a particularly difficult aspect of spoken French for learners. In fact, producing liaisons correctly requires mastering production constraints (such as identifying the consonant involved in the liaison, taking into account possible modifications to the phonological environment, etc.), as well as the syntactic environment making the liaison either required, optional or forbidden. Most methods of French as a foreign language focus only on the compulsory, optional or forbidden nature of the liaison, which they present in a normative and simplified manner Lexicography

Writing a dictionary requires the use of textual data in order to identify the words that should be included and to illustrate their contexts of use. Since the beginnings of dictionaries, lexicographers have manually collected examples from various sources, mainly literary ones. This focus on literary texts is particularly visible in the case of French, a language for which lexicographers have focused on a formal register of the language. For example, the Trésor de la Langue Française has drawn its 430,000 examples from "two centuries of French literary productions"

Regardless of the stylistic genre targeted, the use of quantitative methods linked to corpus linguistics has led to many advances in lexicography and has been one of its main application areas. Indeed, searching for occurrences of words corpora rather than registering them manually over readings has permitted lexicographers to list examples much more easily and to use the frequency information provided by the corpus data, in order to decide which meanings to include and the order in which to present them in the dictionary entry.

Since the 1970s, the first dictionaries making use of corpus data became available in English. The first large-scale lexicographic project involving the massive use of corpus data was the COBUILD dictionary, dating from 1987. Following this project, many lexicographers quickly decided to use corpus data by insisting that word census and meaning classification based on a qualitative approach were unreliable To be sure, corpora offer many uses for lexicography (see, in particular,

The first study that we will introduce for illustrative purposes stresses the importance of using a corpus adapted to the target audience of a dictionary, rather than a general language corpus. As we said above, in the Englishspeaking lexicographic tradition, the use of corpora is now the norm, but reference corpora most often refer to the same stylistic genres.

The authors then retrieved certain keywords from the Oxford Children Corpus in order to compare them with those in the Oxford English Corpus, containing texts intended for adults. These lists were retrieved automatically, then manually compared by sorting the keywords into thematic groups for each textual genre. This analysis indicated that the two corpora had different themes. While the authors who write children's fiction talk more about nature and the physical world (including body parts, buildings, objects and time), the authors who write adult fiction mainly deal with politics, religion, work, education, human relations and death. From the point of view of functional words, texts intended for children in the corpus mainly refer to the question of space, whereas the texts intended for adults focus primarily on the temporal dimension. Many differences between corpora are also present in non-fiction genres. In addition, this comparison reveals differences in the way of addressing readers, since children's writers have a more direct and informal style.

The authors argued that these differences should be implemented in children's dictionaries in several ways. First, the differences in themes should lead to a choice of words included in the dictionary based on books written for children, rather than producing dictionaries for children that are only abridged versions of adult dictionaries. In particular, the words chosen should reflect their frequency in the books designed for the age group targeted by the dictionary. A comparative analysis of collocations in the two corpora also indicated that words are used differently in children's literature and that these differences should be reflected in dictionaries. For example, the English word play is frequently used with musical instruments and sports nouns in the two corpora.

By contrast, it is more rarely found in collocation with words indicating a role, as in he played an important role in his failure, in texts destined for children than in texts for adults. Finally, the authors argued that corpora intended for children provide excellent sources of examples, because they use words in a context which is familiar to them and involve books that they partly recognize and often appreciate.

For all the words studied, the results indicated significant differences between the results of the corpus study and the entries of different French dictionaries. For example, one of the words analyzed was mec (guy). The corpus analysis revealed four types of use for this word: -a man as opposed to a woman (1); -any male individual (2); -a boyfriend (3); -a virile man (4).

However, most dictionaries covered only a part of these meanings. On the other hand, when a meaning was included in the dictionary, it was often done incorrectly. For example, for meaning (3), the Nouveau Petit Robert mentioned "a woman's companion", whereas the corpus showed that this use also extends to homosexual couples. Likewise, the Trésor de la Langue Française indicated that meaning (2) of the word mec listed above is often used in a derogatory way, as in certain collocations like pauvre mec or petit mec (a poor guy). However, corpus analysis has revealed that the predominant collocations are rather neutral, as in jeune mec (a young guy) or positive, as in mec bien (a good guy), beau mec (a handsome guy). Dictionary examples do not properly reflect the most frequent connotation of the word mec in the sense of the male individual. This study showed that the use of spoken corpora makes it possible to offer a more complete and appropriate treatment of frequently spoken words which are hardly ever (or not even) included in dictionaries. Stylistics

The stylistic study of literary texts is traditionally based on a qualitative analysis of chosen text excerpts. However, the choice of excerpts is often complex, since the identification of interesting characteristics to be studied in a text is not always visible at a first reading. This is why literary scholars working in stylistics have started to acknowledge the interest of incorporating quantitative methods from corpus linguistics into the analysis of literary texts, so as to have a more objective starting point for identifying interesting themes and relevant excerpts, which should be the subject of a book's qualitative analysis. As we will see below, in the field of stylistics, corpus linguistics tools do not seek to replace qualitative analysis, but only aim to guide the choice of themes and excerpts to analyze.

Another advantage of the quantitative analysis of literary texts is that it provides clues for identifying the author of a text when the latter is unknown or controversial. In this case, a series of linguistic indicators are used for measuring the formal similarities between this text and other reference works by different possible authors. The problem then is to find out the best indicators for identifying the similarities between an author's text and its differences with the texts of other authors and to later apply these measures to the controversial text in order to determine which author this text shares more similarities with. These indicators are, for example, words and sentence length, the list of frequently used words, vocabulary richness, frequent collocations, the position of words in sentences and frequent syntactic structures

In order to illustrate the role of quantitative methods in corpus linguistics for the stylistic analysis of literary texts, we will first introduce a study on theme and keyword identification in the novel Casino Royale by Ian Fleming. For this study,

In addition to this keyword analysis, the authors classified the semantic areas of the novel, which WMatrix software can perform automatically. By combining these two methods, the authors were able to identify important words from the novel. Then, they analyzed the occurrences of these words one by one using the concordance file (see Chapter 5, section 5.7). This analysis made it possible to show how the different uses of these words contribute to build upon the themes of the novel, in a much more detailed way than through the analysis of selected excerpts, because all the occurrences involved in the development of a theme could be identified and analyzed. This study revealed the importance of supplementing the analysis of excerpts with automatic analyses covering the whole of the novel. The second example that we will discuss in this section deals with the analysis of blockbusters, big budget movie film scripts, as a fictional genre.

McIntyre and Walker (2010) built a corpus of 200,000 words from blockbuster film scripts. Literary critics of this cinematographic genre noted that male and female characters are given unequal treatment, with a clear bias in favor of men. Gender distinctions are also reflected in the stereotypical roles occupied by the different characters. In particular, physical prowess is a characteristic trait of the male hero, and most of the time, the latter operates at the margins of society, whose established power he rejects. The authors wanted to verify whether the characteristics identified by literary critics were reflected in the language of film scripts.

The results indicated that male characters have a much longer speaking time than the female characters, more than four times more words in the corpus. Likewise, the topics men and women talk about (identified from the keywords of the corpus) also differ. Words linked to power were dominant in men's discourse, which again confirms one of the gender stereotypes. On the other hand, the analysis did not show a tendency to reject authority on the part of male protagonists, as illustrated by the frequent use of terms of address such as sir. Nevertheless, the authors added that a qualitative analysis would be essential to learn more about this last point. In short, this study confirmed that corpus analysis can help us to study not only the style of an author or a text genre but also the way in which the different characters in a written production are represented. Legal linguistics

In recent decades, the expertise of linguists has been increasingly sought in the context of legal cases, for questions relating to all language areas. For example, linguists are sometimes required to authenticate a person's voice on a recording or to determine their geographic origin. Linguists also analyze complaints related to the linguistic complexity of certain public information documents, when users argue that they have misused a product or misunderstood their rights due to such excessive difficulty

While some of the questions mentioned above can be answered using theoretical knowledge, as in the case of Gricean maxims, many others require the analysis of corpus data. This is particularly the case of requests concerning the attribution of a text to one or more alleged authors. We have already mentioned the question of textual attribution in the context of literary stylistic analyses. This problem arises somewhat differently in the case of judicial inquiries, in particular because of the type of linguistic material involved. In this context, linguists are most often required to examine threatening or blackmail letters, suicide notes, ransom demands and police statements. However, dealing with this type of material poses a certain number of methodological challenges

The results indicated that the word then appeared once every 930 words in witness statements, as opposed to once every 78 words in police statements. Thus, frequency of this word in Bentley's statement, corresponding to one occurrence every 53 words, was much closer to police language rather than to that of the witnesses. A search for this same connective in the COBUILD corpus of spoken English showed a frequency of one occurrence every 500 words, which matches the use by witnesses rather than the police. Even more strikingly, the structure of the type then I appeared 10 times more frequently in the COBUILD corpus that the I then construction, repeatedly employed in the declaration. This result confirmed that it is very unlikely that Bentley spontaneously could have produced those sentences and that the police certainly added elements to his story. Based on this new analysis, the case was re-evaluated and Bentley was posthumously acquitted in 1998. Conclusion

In this chapter, we have shown how corpus linguistics can be of use in different areas of applied linguistics. We have seen that language acquisition corpora have helped us to understand the different stages of this process, in particular regarding the study of the associations between the language that children hear in their environment and their own productions. We have seen that corpus analysis makes it possible to better characterize the language specificities of people suffering from language and communication impairments, by studying the different ways in which these patients interact in a natural environment. We then reviewed the multiple applications of learner corpora to better grasp second language acquisition processes and showed how these corpora can be integrated into teaching materials. We also discussed the increasingly widespread use of corpora as a basis for the creation of dictionaries and showed that these data help us to overcome many inherent limitations of a purely qualitative approach to writing dictionaries.

Finally, we discussed the different ways in which the corpus linguistics methodology makes it possible to provide valuable tools for the stylistic analysis of texts, as well as for author identification in a legal framework. 3.9.

Revision questions and answer key 3.9.1. Questions 1) In addition to morphosyntax discussed in this chapter, what are the other aspects of language acquisition that are well suited for corpus-based research? 2) What are the methodological aspects that should be considered when carrying out a corpus study with children with autism spectrum disorders?

3) What type of learner corpora should be used for tackling the research questions below? a) Study of the development of negation in French during the process of acquiring French as a foreign language. b) Study of the role of culture in the ability of learners to produce speech acts of requesting in French.

4) Can we use corpora to teach the pragmatic aspects of language, such as politeness, to learners? 5) What are the advantages and disadvantages of using real examples drawn directly from corpora in dictionaries? 6) Imagine a research question in literary stylistics, which could be appropriate for carrying out a quantitative corpus study. 7) Imagine another situation apart from the ones described in this chapter in which corpus analysis would make it possible to detect language uses condemned by the law. Answer key

1) As is always the case with corpus research, the more easily an element is searchable using surface features of the language like unannotated words and sentences, the easier it is to include in a corpus quantitative analysis using raw data. For example, this type of analysis is quite suitable for studying the vocabulary growth of a young child or, more specifically, the emergence of certain words in their lexicon. If the corpus has been annotated, as is the case of many corpora in the CHILDES database, other analyses become possible. For instance, it is possible to study the type of lexical errors made during various developmental stages or the diversification in the repertoire of speech acts which are available to the child.

2) To begin with, we should bear in mind that the use of corpora for studying the productions of children with atypical development meets the same limitations as for typical children: only the aspects linked to production can be studied and they do not make it possible to ensure that an element which is absent from the corpus actually means an inability to master it. This second point can be particularly problematic in the case of children with atypical development, because they develop compensation strategies that allow them to avoid the confrontation of elements which they find problematic. In the case of children with ASD, it is important to write down the different indications regarding the linguistic profile of the recorded children and to include them in the metadata, since autism represents a broad spectrum of skills and deficits which could lead to comparing children with very different linguistic and cognitive profiles.

Above all, it is fundamental that any child recorded in the corpus has been diagnosed with ASD according to the diagnostic tests recognized in the literature, rather than following the mere indication of the pediatrician. Next, information on cognitive skills (non-verbal IQ, working memory) is also important, ideally at different time frames in the case of longitudinal corpora. Finally, it is important that data collection takes place in a context that is familiar to the child and involves as many familiar activities as possible in order to limit any blockages due to anxiety issues.

3) a) In order to study the development of negation in French during the process of acquiring French as a foreign language, it is advisable to use a longitudinal corpus covering different acquisition stages or, else, various cross-sectional corpora of learners at different levels. Negation takes different forms in spoken and in written French (optional use of "ne" in spoken data). This type of study should also compare acquisition processes in spoken and written data. b) In order to study the role of culture on the ability of learners to produce speech acts of requesting in French, it is advisable to use a corpus comprising learners of various languages and cultures. Comparing spoken and written data could also be useful. In order to determine the causes of possible differences between learners and native speakers, the use of comparable corpora produced by native speakers of French would be advisable.

4) Yes, it is possible to teach the use of politeness through corpora. That being said, as we discussed in Chapter 2, it is difficult to identify certain pragmatic uses automatically by means of a corpus search, because these uses are not transparently linked with the linguistic form used. In many cases, it is necessary to analyze the corpus manually so as to find interesting occurrences. However, automatic corpus analysis provides many examples of politeness routines, as the ones related to the opening of a conversation, to its closure or, to speech acts such as apologizing. This could be achieved by searching for specific locations in the interactions (the first or the last lines of exchanges), or through keywords like sorry or excuse me.

5) The main advantage of using real examples is that they appropriately match the language as it is used by speakers. These examples also reveal the phraseological constructions into which words are frequently grouped, making it possible to provide a rich illustration of their uses. On the other hand, these examples are often too long to be inserted into dictionaries without making any changes. Furthermore, the actual uses of words do not always make it possible for their meaning to be inferred, and therefore do not necessarily accomplish their illustrative role as intended in dictionaries. For example, one of the occurrences of the word colline (hill) in the Le Monde corpus (year 2012) is "Sur la colline, tout le monde est allé voter" (On the hill, everyone went voting). This sentence would be a poor example for understanding the meaning of this word. There are many other similar cases in the corpus.

Word occurrences should be carefully sorted in order to keep the examples which are sufficiently concise and which offer an illustration making it possible to infer the meaning of the word. 6) Corpus linguistics methodology makes it possible to make contributions on many aspects of literary stylistics, since the search can be based on a keyword analysis of the book in question. For instance, such an analysis could try to identify the important themes in Molière's different plays, as well as to assess whether there are differences in the way in which male and female characters approach such themes. 7) A very good example of the use of corpus linguistics concerns the problem of plagiarism. Today, plagiarism has become increasingly easier to achieve thanks to the wide availability of texts online. By analyzing very large corpora, this crime has now become more easily identifiable by automatic means. Indeed, on a sentence longer than seven to eight words, the probability of producing exactly the same construction as somebody elseby chance -is close to 0 Further reading The role of corpora for studying language acquisition in children is described in a very accessible way by How to Use Multilingual Corpora

In this chapter, we will discuss the main characteristics of multilingual corpora, as well as their different uses. First, we will discuss the advantages and disadvantages of two types of multilingual corpora, namely comparable corpora and parallel corpora. We will see that one of the great difficulties inherent in the use of comparable corpora is the need to define a neutral term of comparison, called tertium comparationis, which enables us to measure similarities and differences between languages. We will discuss the different possible terms of comparison, depending on the type of research question being considered. Parallel corpora make it possible to compare texts in their original language, with the corresponding translation into one or more languages. We will discuss the particularities of translations as a text genre and show that, due to these particularities, they cannot be used as if they were original language texts. In the rest of the chapter, we will illustrate the use of multilingual corpora in the fields of contrastive linguistics, translation and bilingual lexicography. Comparable corpora and parallel corpora

Multilingual studies can be based on two types of corpus data. First of all, comparable corpora contain original texts in different languages. These corpora are built so as to make samples as similar as possible between languages, and to prevent comparison bias. For example, it would be inappropriate to compare French editorials with English dispatches, even though these two types of texts belong to the journalistic genre. Indeed, their many differences in communicational aims and content make them different in nature, and such disparities could mask differences between languages. It is necessary to neutralize the differences in the type of data used in order to bring out the differences between languages. According to -the time when the texts were written; -their discursive genre (descriptive, argumentative, etc.); -the type of audience targeted and their field (law, science, etc.).

For example, in order to study the linguistic differences between French and English, one possibility would be to create a comparable corpus of leading articles from journalistic sources with a similar political orientation, published during the same years.

Parallel corpora containing texts in one or more original languages, and their translations into one or more languages, represent the second type of multilingual corpora. It sometimes happens that parallel corpora contain only texts translated into different languages from another language that has not been included in the corpus, or it may occur that the original text cannot be identified among all the texts. As we will see later, the use of corpora in which source languages and target languages remain unidentified poses major problems for contrastive linguistics, due to the special status of translations as a discursive genre (see also

Both comparable and parallel corpora have many advantages, and also some disadvantages, which we will discuss in the rest of this section. First of all, we should point out that the use of these two types of corpora is not mutually exclusive. On the contrary, the disadvantages of one type can often be counterbalanced, at least partly, by the advantages of the other, and vice versa. This is why many authors are in favor of carrying out contrastive studies on the basis of both comparable and parallel data, when available corpora and time allow for it. We will see examples of such studies later in this chapter.

The main advantage of comparable corpora is their great simplicity of access. A priori, it is possible to create a comparable corpus for any language pair, provided that digitized texts of a comparable nature are available in each language. In the case of languages that have already been the subject of numerous corpora, as is the case for European languages,

The major drawback of comparable corpora is that researchers have to find data that are highly similar in different languages, in order to avoid blurring comparisons, as we have already discussed. In addition, usage conventions may vary considerably between languages even when the same text genre exists in both, which makes them difficult to compare.

In addition to the difficulty of identifying suitable corpora, from a linguistic point of view, the main limitation regarding the use of comparable corpora is the need to find a neutral term of comparison, undeformed by the prism of either language. Finally, we should point out that while linguistic features can be identified when comparing words or syntactic structures in different languages, these traits are nonetheless difficult to annotate systematically. Indeed, they require a complex type of linguistic interpretation and analysis on the part of the annotator, which, in many cases, implies that the results of the annotation may differ when performed by several annotators (see

Unlike comparable corpora, the main advantage of parallel corpora is that they guarantee excellent comparability between languages, since the texts they contain are the same. These corpora make it possible to look for equivalences between words, syntactic structures and discursive phenomena, without having to set points of comparison. As a result, comparing languages through the use of parallel corpora is greatly simplified in contrast to comparable corpora because annotators can keep a track of translation equivalents without having to annotate syntactic or semantic features. This method is called translation spotting in the literature

Another practical problem associated with the use of such corpora is their limited availability. In fact, not all languages or discursive genres are regularly translated. In most cases, translations correspond to written genres, often related to the administrative or the literary field

In addition, language pairs that are regularly the subject of direct translations from one into the other are also limited. What is more, these corpora often include a single source language and a single target language, which makes it impossible to generalize results beyond that particular language pair.

In order to overcome certain limitations pertaining to parallel corpora, the ideal would be to work with a bi-directional corpus, where both languages are alternately source and target, since these corpora make it possible to combine the two types of multilingual data discussed above (comparable and parallel). Bi-directional corpora offer the possibility of studying equivalences in both translation directions through the use of parallel corpora. In addition, these corpora can be used as comparable corpora, produced in very similar situations, when analyzing only the original language portions of the corpus, as illustrated in Figure

Certain corpora fulfill these conditions, such as the Europarl Corpus, a corpus of debates at the European Parliament, where each member employs their own language and whose exchanges are later transcribed and translated (see Chapter 5 for a list of these corpora). Looking for a tertium comparationis

One of the main difficulties inherent in contrastive studies is to find a suitable point of comparison between languages. The problem is that, by nature, comparing two languages implies comparing systems that are partly incommensurable. Therefore, linguists are confronted with the challenge of finding common elements around which languages are close enough so as to be comparable. In fact, relevant differences between languages can only be observed insofar as the latter are compared on the basis of a similar concept or structure. If the objects compared differ in nature, then the differences observed will not be relevant. Let us take a practical example. Observing that mice are smaller than elephants is irrelevant to understanding the morphology of mice or elephants, since these are different animals. On the other hand, observing the differences in size between a Chihuahua and a Saint Bernard is relevant for understanding the different morphologies of dogs.

Contrastivists call this point of comparison between languages tertium comparationis. Such a point of comparison should be determined in a neutral manner in relation to the functioning of one language or the other, in order not to bias comparisons. For example, comparing the phonological system of French and German using a list of German phonemes as a starting point would provide a biased comparison, since the comparison platform is not neutral but established on the basis of one of the language's categories. It is therefore necessary to choose a point of comparison that can be applied to both languages and, which is, as far as possible, neutral. For example, when trying to compare tense categories between German and English, Gast (2012) selected different time spheres along a time axis, including the Past Tense, the Present Perfect, the Present Tense and the (will) Future, independently from both languages. He then drew a line corresponding to the time interval that each tense category covered in each language.

Through this comparison, he showed that the English Past Tense and the German Präteritum seem to cover similar time intervals, whereas the English Present Perfect and the German Perfekt do not have the same function. Thus, while the English Present Perfect only applies to events in the near past, the German Perfekt covers a wider range which also includes the distant past, as illustrated in Figure

The suitable tertium comparationis type for carrying out a study depends on the kind of linguistic elements compared (phonemes, syntactic structures, speech acts, etc.). A distinction can be made between the tertium comparationis based on linguistic forms and those based on linguistic function

A tertium comparationis determined exclusively by formal criteria, however, is not appropriate, not even for comparing structural elements from different languages. As we have seen previously, English and German have two verbal tenses to refer to the past. Thus, from a structural point of view, we could say that these two languages are similar. However, uses between the two languages are quite different. Conversely, a language may lack a certain linguistic form but still express it through other means. For example, in some languages, speakers verbalize the source of information (which they have acquired either directly by their own perception or indirectly by inference or hearsay) by means of a verbal suffix. These languages have what is called an evidential verbal system. This is not the case in French, which does not have such suffixes in its verbal morphology. However, French speakers have other means of indicating sources of information in their statements, in particular by adding phrases such as il paraît que (it seems that), j'en conclus que (I conclude that) or je vois que (I see that).

So, to infer from the absence of a suffix that the French language does not make it possible to express belief sources would therefore be wrong. That being said, the fact that languages express certain concepts by different means can, in certain cases, give rise to interesting differences, particularly at the age when these elements are acquired by children and the way in which speakers encode this information. The potential impact of such encoding differences on speaker's cognition is known as linguistic relativism (see In many cases, a tertium comparationis based on semantic equivalence appears to be preferable to a tertium comparationis based on formal criteria. However,

In summary, in addition to being based on corpora with high comparability, contrastive studies should use neutral points of comparison that make it possible to establish comparisons between linguistic phenomena across languages, which are as relevant and adequate as possible. Depending on the research question, the appropriate equivalence levels will be different. Translations as a discursive genre

The main question raised by the use of parallel corpora concerns the status of translations and, more specifically, the possibility of using them as language samples. An important amount of research carried out since the 2000s has shown that translations represent a discursive genre in their own right, and that translations do not fully share the same properties as texts written in original language. This discursive genre is also sufficiently stable and different from others so as to be identifiable using machine learning algorithms for automatic text classification

One of the reasons why translations represent a stylistic genre different from original texts is that these keep a certain imprint of the source language. Even if translators are language professionals, their lexical, grammatical and stylistic choices are still influenced by what they have to translate. For example,

In addition to these influences, which vary from one source language to another, some authors have hypothesized that translations are so similar (to the point of making up a stylistic genre of its own) due to certain effects related to the translation process itself. These effects might reflect translation universals rather than the variable effects pertaining to the languages involved

To conclude, in order to limit the bias introduced by the use of translations, it is desirable to use bi-directional corpora as far as possible, as well as to study language equivalences in the two directions of translation. We will see examples of such corpora later in this chapter. We will illustrate the fact that these corpora can help us to work simultaneously on comparable and parallel data, and thus exploit the advantages of each, while limiting their bias. Multilingual corpora and contrastive linguistics

In its beginnings in the 1950s, contrastive linguistics emerged as a discipline aiming to compare two or more languages with the aim of improving language teaching methods. Indeed, linguists working on language teaching had long observed that mistakes made by learners were often linked to transfers from their mother tongue. This observation justified the systematic study of differences between languages in order to better understand the risk of making mistakes in different learner populations (see, in particular,

These new data led to a relative abandonment of contrastive studies for several decades. The situation has changed a great deal since the 1990s, thanks to the arrival of corpus linguistics, which made it possible to empirically compare linguistic systems. The data provided by these contrastive corpus-based studies are not only useful in theoretical linguistics for understanding how languages work, but may be helpful for other applications, notably for the development of tools such as bilingual dictionaries (see section 4.6). In this section, we will present a sample of studies which illustrate the usefulness of corpora for carrying out contrastive linguistic studies.

The first case study that we will discuss concerns the French-English language pair and, more specifically, how the verbs faire in French and make in English work, both of which can be used in causative constructions such as faire rire or make believe. On an intuitive level, it may seem that these verbs share a similar meaning and perform equivalent functions in both languages. However, by means of an empirical study of both comparable and parallel data,

Gilquin's study is based on the PLECI bi-directional parallel corpus, which contains newspaper articles and fictional texts in English and French. This corpus can be useful both as a comparable corpus and as a parallel corpus, as illustrated in Figure

The results revealed some similarities between the two languages. First, the distribution of occurrences between nominal and pronominal subjects was very similar. Second, the two verbs were mainly complemented by verbs describing concrete actions such as partir rather than existential verbs like exist. Despite these similarities, significant differences were also observed. To begin with, in terms of frequency, the verb faire appeared four times more frequently in texts in original French compared to make in original English texts, and this was a first indicator that the role of each is not the same in both languages. Furthermore, verbs used with make were much more limited than those used with faire. The four most frequent verbs in English (feel, look, work and think) represented 25% of the occurrences. By contrast, in French, 12 different verbs were needed to reach this same proportion of occurrences. Conversely, some of the uses of the verb make seem much more atypical than the verb faire.

For example, the verb make was mainly used in relation to inanimate subjects, which was not the case with faire. In sum, although the two verbs have a partly convergent semantic profile, each of them also has frequent uses that are not found in the other language in a similar proportion. These semantic differences indicate that the verb make might not be the best translation choice for faire, and the other way around. In order to empirically determine the percentage of correspondences between two words, a mutual correspondence (MC) value can be calculated. This value takes into account the number of translations by the supposed equivalent word compared to the total number of occurrences, in both directions of translation As + Bs

In the case of the pair made of faire/make, the MC value was 15.4%. Such a low value tends to confirm that these two words are not equivalent. In most cases, the causative construction faire + infinitif in French is translated by an English verb carrying the notion of causality, also called the synthetic causative. For example, the expression faire taire is often translated using the verb to silence. In the case of the verb make, its most frequent translations are the verb make as well as paraphrases, for English expressions that cannot be literally translated into French. For example, the sentence "it was the very intensity of her devotion that had made her give him a softness of upbringing…" was translated by adding "Par un excès de tendresse, Lady O'Connell l'éleva avec une faiblesse…".

In a nutshell, this study made it possible to show that two words which may seem close, and which are often described as translation equivalents in reference tools such as bilingual dictionaries, are in fact partially different from each other. Furthermore, these differences can only emerge on the basis of a quantitative corpus study, which highlights the differences in frequency and context of use.

The second study we present in this section was devoted to the analysis of the different factors that influence translations in parallel corpora. To do this, Dupont and Zufferey (2017) compared the way in which concessive connectives are often treated as translation equivalents in bilingual dictionaries, namely: however, yet, nevertheless and nonetheless in English and respectively pourtant, toutefois, néanmoins and cependant in French. The authors specifically studied the role of three factors in the observed equivalences: the translation direction (French-English or English-French), the stylistic genre (journalistic texts or parliamentary debates) and the translators' degree of expertise (non-professional volunteers, journalists or qualified translators). For this study, the occurrences of the eight abovementioned connectives were drawn from three parallel corpora (Europarl for the parliamentary debate genre, a corpus of newspaper articles and the TED corpus of online conferences; see Chapter 5 for a description of these corpora). These occurrences were then manually disambiguated in order to remove occurrences which had not been used as a concessive connective, for example when the connective yet was used to indicate a temporal relation.

The results showed that in original texts, the frequency of connectives often vary depending on language register, particularly in English, where the four connectives vary significantly. In French, only the connective pourtant varied significantly between journalistic texts and parliamentary debates. An analysis of translations also showed differences between the two genres. For French connectives, the typical translations in the journalistic genre were either the generic connective but, or there was an outright absence of a connective in the translation. In the parliamentary debate genre, more specific connectives were used: the connective however was a frequent translation for the four French connectives, not to mention yet as the translation of pourtant and nevertheless for néanmoins. Such a tendency to omit connectives in the journalistic genre can also be found in English. This observation can no doubt be explained by the concern for efficacy in this genre, which tends to limit the amount of words used.

The other translations were more variable than in the French-English direction.

We can see that the direction of translation is an important factor to take into account when establishing equivalences between languages. Differences between stylistic genres were also visible in the MC values between connectives. Indeed, these values were very low in the journalistic genre, oscillating between 14% and 27%, against 33% and 57% in the parliamentary debate genre, which reflected the above-mentioned more specific translation choices.

The last variation factor analyzed in this study referred to the translator's level of expertise. On the one hand, European Union translators are qualified professionals. On the other hand, the translations provided for TED conferences are carried out by volunteers. Finally, journalistic text translations are generally carried out by journalists, who are language professionals but not translation professionals. For the English-French pair (remember that the TED corpus is unidirectional), these variations enabled the authors to compare the impact of this variable on the translations under scrutiny. The comparison revealed that translation choices were systematically less varied in the TED corpus than in other corpora. The number of zero translations was also significantly lower. This trend reflected the fact that amateur translators are more likely than others to use the source text as a guide and to avoid structural changes as much as possible (see also

In summary, this study showed that the type of equivalences observed between languages can be variable across discourse genres. However, contrary to what happens in monolingual studies, contrastive studies are often performed on data from a single genre -due to the scarcity of multilingual corpora -which does not always make it possible to compare different genres. This study also showed that equivalences between languages should be considered separately for the two translation directions. Finally, the degree of expertise of translators also plays a role in their translation choices, and this factor should therefore be taken into account in the study of parallel corpora. Parallel corpora and translation studies

Translation studies is the scientific study of the processes at work in translation, as well as the factors that influence their realization. While translation is a practical and applied discipline, translation studies (translatology or traductology) is a theoretical science. As in the case of contrastive studies, translation studies has benefited from the availability of multilingual corpora, as well as theoretical and methodological advances in corpus linguistics. As we will see in this section, the use of large multilingual corpora makes it possible to carry out quantitative studies on different language pairs simultaneously and, therefore, go beyond the isolated observations that can be made on the basis of individual practice. Later, we will see that the use of the empirical methodology ingrained in corpus analysis can also work as a guide for the translator when it comes to making certain translation choices.

The first study that we present in this section looked into the existence of translation universals. As discussed previously, translations differ in several ways from original texts produced in one language. Translation studies specialists have suggested that a portion of these specificities can stem from the existence of translation universals, that is, from phenomena specifically pertaining to the translation process. One of these universals concerns the supposed propensity of translations to be more explicit (explicitation phenomenon), in terms of cohesion markers, than original texts. This hypothesis has been partly confirmed through corpus studies, performed on a single language pair and limited to one translation direction. Due to these limitations, these studies cannot be generalized to all translations. In order to overcome this limitation,

The explicitation hypothesis relates to the number of cohesion markers present in translations, which is assumed to be higher than in original texts. Among these, Zufferey and Cartoni chose to focus on the category of causal connectives. Indeed, their use is very frequent, and often optional. In other words, they can be omitted without creating comprehension difficulties

Furthermore, the authors were able to observe that the explicitation rate varied significantly depending on the causal connective in question. On the one hand, some connectives like parce que in French and because in English gave rise to very few explicitation cases. On the other hand, causal connectives like puisque in French and given that in English gave rise to many explicitation cases. The authors attributed this gap to the different semantic profiles of connectives. Those that give rise to explicitation are typically used for introducing a cause presented as already known or easily inferred by the interlocutor, unlike the other connectives which are used for announcing a new cause for the interlocutor (see

The second study that we will discuss does not deal with the analysis of translations themselves but with the stylistic analysis of the source text, namely the search for recurring patterns and monitoring how these patterns are translated.

By analyzing the recurring sequences and the keywords they contained, the author was able to show that these repetitions played a particularly important stylistic role in the novel (which also contains many more repeated sequences than other works by the author), and that these repetitions should be maintained in the translation in order to preserve the spirit of the text. In fact, these sequences made reference in part to the titles of other literary works, and helped to grasp certain intertextuality elements. However, an analysis of the translations of these 27 recurring sequences, both by the Finnish translator and by the Czech translator, showed that they were mostly neutralized by stylistic choices avoiding repetitions. The tendency of translators to avoid repetition is also one of the recurring trends identified in translations, and some translation theorists point to various techniques for achieving that result (Ben-Ari 1998). However, as in the case of Irving's novel, the presence of repetitions may be an integral part of the work's style and erasing them would certainly involve a form of stylistic loss.

In this way, we can see how corpus linguistics can provide translators with tools that may help them adapt their translation choices on the basis of a better identification of the recurrent linguistic properties at work in a text. Parallel corpora and bilingual dictionaries

We have already discussed the importance of corpora for monolingual lexicography in Chapter 3 (section 3.5). In this section, we will refer more specifically to the role of parallel corpora in the creation of bilingual dictionaries. Bilingual dictionaries are essential for foreign-language learners but they are also controversial among language professionals, especially translators. The latter, in particular, criticize bilingual dictionaries for the limited list of equivalences that they provide and the lack of context, which often prevents users from making an appropriate distinction between the different meanings of a word or expression. Finally, as monolingual dictionaries, these dictionaries do not provide any indication regarding the frequency of the different meanings, apart from the order in which they are listed.

To some extent, equivalences between languages obtained through the use of parallel corpora respond to such criticisms. Corpora provide access to a broad context and offer a greater variety of equivalences than dictionaries. What is more, these can be easily classified by frequency, and differentiated according to the textual genres under consideration. In addition, computerized word alignment techniques make it possible to automatically produce bilingual dictionaries (see, for example,

In order to illustrate the importance of corpus data for providing more suitable translation equivalents than those of bilingual dictionaries, in this section, we will discuss a study concerning partially equivalent word pairs in French and in English.

At a second stage of the study, the authors looked for occurrences of these words in French-English comparable corpora. They chose 100 occurrences in each language and annotated them with the different meanings listed in monolingual dictionaries. They confirmed that some of the meanings frequently found in the corpus could not be adequately translated by their "equivalent". For example, 75% of the occurrences of plus au moins in the corpus should have been translated using expressions such as pretty much or somewhat in English, rather than using the expression more or less. The authors concluded that bilingual dictionaries do not provide enough information for helping users access the correct translation equivalents.

Many other studies have compared the translation equivalents provided by bilingual dictionaries with equivalents observed in parallel corpora. These studies invariably highlight a discrepancy between the translation equivalents found in dictionaries and in corpus data. In most cases, the equivalents provided by dictionaries are much more limited than the equivalents found empirically, or vice versa, dictionaries sometimes list equivalents that are completely absent from corpus data. We will work on two examples by way of illustration. Conclusion

In this chapter, we have discussed the different uses of comparable and parallel multilingual corpora. We have seen that their advantages and disadvantages are often complementary, and that it is useful to combine these two types of resources in contrastive linguistics. The study of translation often relies on parallel corpora, but can also make use of comparable corpora of texts translated into different languages, without considering the source language. In the field of translation studies, one of the major aims of such studies is to analyze the features of the translated language, with the purpose of looking for translation universals. We have also shown that corpus analysis methods can be useful for uncovering recurring patterns in a source text and to better adapt the strategies used for its translation. Finally, we argued that parallel corpora have become indispensable resources for the creation of bilingual dictionaries, since they provide rich lists of translation equivalents accompanied by their contexts of use, as well as information concerning their frequency in various genres.

3) Why can we say that translations are a full-fledged text genre? 4) What are the parameters to take into account in order to carry out a contrastive study on the use of the indefinite pronouns on in French and man in German? 5) How could we test the supposed translation universal according to which translations are simpler than original texts by means of a parallel corpora study? 6) What types of equivalences are most likely to be insufficiently dealt with in bilingual dictionaries? Answer key

1) a) In order to study the similarities and differences between the causal connectives porque in Spanish, parce que in French and perché in Italian, the use of a parallel corpus offers great advantages. Indeed, such a corpus makes it possible to establish the degree of mutual correspondences between these connectives, by counting the number of times that they can be translated by each other. Nonetheless, the use of this method also involves the risk of having a distorted vision of the functioning of these connectives, due to the translation prism. This study should therefore be supplemented by a semantic and pragmatic analysis on how these connectives work in the original language, by means of comparable corpora. For instance, the use of these connectives could be compared only in the source language section of the parallel corpus. b) Conversely, to study the way in which European elections are reported in the press in Germany, France and England, the use of comparable corpora seems the most judicious choice.

Indeed, for this study, it is important to have access to texts that were originally produced in each language, in such a way that they reflect both the linguistic structures of each language and bring out potentially different discourses regarding the same event. A parallel corpus, containing translations, would not be able to meet these two objectives.

2) a) The comparison of the consonant system in German and French can be done on the basis of formal rather than functional equivalences. In particular, consonants can be compared on the basis of their articulatory features.

b) In order to compare speech acts of thanking in French and Chinese, a tertium comparationis based on pragmatic equivalence is necessary. It is not only the words or expressions that should be compared, but also their illocutionary force, that is, the communicative intention of the speaker.

3) Several reasons have been given in the literature for explaining the linguistic specificities of translations. The first type of explanation concerns the influence of the source language, which inevitably leaves traces in translations. Even if translators are language professionals, they are inevitably influenced by the words and linguistic structures they have to translate, which leads them to make different lexical and syntactic choices than those of a speaker writing in their mother tongue. The second category for explaining translation specificities is of a general nature and is based on the supposed existence of translation universals (linguistic phenomena resulting from the very process of translation), regardless of the source and target languages involved. These universals include simplification, explicitation and standardization. All these processes reflect the pedagogical role of translators, who (unconsciously) try to improve the readability of texts.

4) First of all, this study should be carried out by means of a parallel corpus, in order to determine to what extent these two pronouns are translation equivalents or not. More specifically, a bi-directional parallel corpus should be used, since the equivalences are often variable depending on the direction of translation. This analysis of translations should be supplemented by a study on comparable corpora, made up of the two original language sections from the parallel corpus. For this analysis, the important point would be to establish which comparison factors would best highlight their common points and their differences. In this case, the possible factors could be the tense and aspect of the verb following the pronoun, etc. Finally, this study should, wherever possible, include two different discourse genres, in order to measure the extent of the variations between them.

5) The simplification universal implies that translations should be simpler linguistically than the original texts of the same discursive genre. Various lexical and syntactic factors, easily measurable, could contribute to this simplicity. For example, lexical simplicity implies that the number of different words should be smaller than in an original text. This can be measured thanks to the type/token ratio (see Chapter 8). Syntactic simplicity is measured, for example, by the average length of sentences. The number of words used per sentence can also be calculated, even on a corpus that has not been subjected to syntactic annotations.

6) The most problematic equivalence cases for bilingual dictionaries are partial equivalences, just as those we discussed in this chapter for expressions such as plus au moins and more or less. In these cases, the formal proximity and the identification of certain cases in which these expressions are equivalent may suggest that these words are completely equivalent, when actually they are not. Conversely, false cognates, where meanings are completely different between languages despite a formal resemblance, are easier to identify, since their meaning clearly appears to be different. Further reading How to Find and Analyze Corpora in French This chapter has two aims. Firstly, we will introduce the main existing corpora in French. These corpora can be divided into four categories: -written corpora; -spoken corpora; -corpora devoted to specific demographics, such as children or learners; -multilingual corpora where one of the languages included is French.

Secondly, we will present a set of concordancers, which are corpus analysis tools, and discuss their main functionalities. Links to websites providing online access to corpora, as well as corpus consultation tools, are listed at the end of the chapter. Corpora formats and their availability

Thanks to the Internet, in recent decades sharing corpus data has become far simpler. For example, it is very common for research teams to offer the corpus compiled during their research projects as a tool available to the general public, once the project is finished. The rights of access and use of this data may vary depending on the content and project in question. In some cases, corpora available to the public can be downloaded directly from a website. In other cases they are not downloadable, but instead are only available online, via a dedicated search interface. We will discuss the advantages and disadvantages of these different formats in this section. It should be noted that use for commercial purposes may be subject to specific additional conditions.

The main advantage of downloadable corpora is their great flexibility for carrying out word or structure searches using a concordancer (see section 5.6). These corpora can also be annotated manually or automatically. An important element to take into account when downloading a corpus from the Internet is its encoding format (this not to be confused with the compression format, when applicable, that needs to be handled by means of specific archiving tools). When several formats are available, it is important to choose a format which is compatible with the research tools that will be used for the study. For example, the AntConc concordancer that we discuss can only process text format files (or files with XML or HTML tags which can also be treated as text files). Another element to take into consideration before deciding to download an entire corpus is its size. Indeed, some current corpora like the Google Books corpus

Many corpora can only be viewed online using a dedicated research interface. The advantage of this format is its great simplicity of use. In fact, most interfaces offer user-friendly methods specifying the choice criteria, such as gender, type of speaker, time period, etc., as well as fields for typing in the element(s) to be looked for in a full text search, sometimes enabling the use of search patterns (called regular expressions, see section 5.6). The major drawback of these interfaces is that they do not authorize any type of search. Some are limited to a continuous character string, something which prevents the search for compound words, like chemin de fer (railway) in French, which includes three separate strings of characters. If the search patterns are not usable, this further complicates the search. Let us take a look at an example: it is possible to look for all the occurrences of a regular verb like aimer using a single query looking for the root aim, followed by a wildcard replacing an unspecified number of characters, for example aim*.

If this type of search is not enabled by the interface, all the verbal forms must be looked up one by one with their exact forms.

Another limitation to the use of online corpora available for consultation is that they only offer limited access to their metadata (see Chapter 6). Thus, important information for certain research questions is regularly missing. For example, the online interface on the OFROM corpus (see section 5.3) only offers the possibility of looking for productions by women or men, but for the moment does not indicate the total number of participants of each genre, or the number of words produced by each of them. This type of information is nonetheless crucial for the quantitative comparison of linguistic productions between the two genders. In the same way, the consultation interface for the CLAPI corpus (see section 5.3 for a description) does not currently provide information about the total number of words included in the portion of the corpus that is available for online consultation. In many cases, this piece of information can be obtained by contacting the corpus creators.

However, when this type of information remains unavailable, these gaps lead to serious limitations in data analysis (see Chapter 8). The problem of sources is even more acute in the case of databases grouping different types of corpora, such as the Lextutor database, which contains both spoken and written data, retrieved from different genres, but unevenly distributed. These databases are useful for quickly finding concordance examples but cannot be seriously considered as representative corpora (see Chapter 6).

In addition to the two above-mentioned distribution formats, some corpora that are available online require prior user registration, as well as explicitly stating the research purpose for which the data will be used. For example, this is the case of the Belgian Valibel database of spoken French (see section 5.3) or the SMS corpus in Switzerland's national languages, collected by the universities of Zurich and Neuchâtel (see section 5.5).

Other corpora are still not distributed for free but can be obtained by paying a varying fee, depending on whether the intended use is for research or for commercial purposes. After purchase, Le Monde newspaper corpus (see section 5.2) can be downloaded via corpora distribution sites such as the European Language Resources Association or ELRA, the Linguistic Data Consortium or LDC, or distributed in a CD-Rom format, such as the French SMS corpus collected in Belgium. Other corpora such as the new version of the Frantext literary text corpus (see section 5.2) are accessible via an annual renewable subscription. Many corpora are also available via the Sketch Engine online platform (see Chapter 6), which is free to access for many institutions in European countries. In addition, institutions often finance the purchase of corpora for their members, so it is advisable to check with one's institution before engaging in any individual purchases.

Finally, we can mention that it is possible to build new corpora from websites that distribute royalty-free data, for example, literary texts now available in the public domain, government data such as parliamentary debates, or texts from participative sites like Wikipedia. In all cases, it is necessary to study the rights of use indicated by the respective sites before starting to compile the corpus. We will discuss this issue in more detail in Chapter 6, which is devoted to presenting the basic principles of corpus creation.

To conclude, it is important to emphasize that, regardless of the format in which a corpus can be accessed, reusing corpus data amounts to benefiting from the often long and costly work carried out by other research teams. That is why, when existing corpora are used, their source must be explicitly mentioned. More specifically, when researchers reuse a corpus created by other teams, they must mention in their publication the Internet link where the data were downloaded or retrieved from. Very often, the authors of a corpus provide a bibliographic reference where they describe their corpus or the name of the team who compiled it. These references must be quoted in any paper making use of the data. Reference corpora

Unlike many European languages, French still does not have a reference corpus, a representative sample of the French language in general, similar to the British National Corpus that exists for British English, one of the pioneers in the genre. For the time being, it is not possible for linguists to observe how the French language works through the study of a single corpus. However, a multi-genre French corpus is currently in development and should offer an open access phase to the general public in the coming years For the time being, the closest to a reference corpus for French is the corpus of contemporary French created within the framework of the Orféo project The Sketch Engine corpus management system

In addition to the big generic corpora, for certain types of research, using corpora belonging to a specific type of genre may prove to be a wise choice. The results obtained from different specific corpora can also be combined in order to improve the generalization of results. We will describe these corpora in the following sections. Written French corpora

In the field of journalism, the most exhaustive resource is undoubtedly the Le Monde corpus, which contains the newspaper's archives for the period 1987-2012, representing a total of nearly 1,200,000 articles, corresponding to almost 20 million words per year. The Le Monde corpus is a valuable tool not only for exploring the French journalistic style but also for studying recent developments in the language, thanks to its data spanning 25 years. Unfortunately, this corpus is not available for free and must be purchased via the ELRA platform. On the other hand, the newspaper's articles for the year 1998 are available for free online consultation via the Lextutor platform. Another newspaper is also a good reference for the French journalistic style. The Corpus Journalistique issu de l'Est Républicain includes articles from this regional newspaper for the periods

In the more specialized journalistic genre, the Sciences Humaines corpus produced by ATILF (Analyse et Traitement Informatique de la Langue Française [Computer Processing and Analysis of the French Language]) in Nancy includes 125 linguistic articles from the Sciences Humaines journal. Despite its modest size, this corpus makes it possible to study the specificities of the journalistic style when applied to a particular field. The corpus is now available via the Ortolang platform, where it can be downloaded for free.

As for the literary genre, the Frantext corpus brings together many literary texts ranging from ancient to modern French, in a corpus which totals more than 250 million words. Since 2018, a new version has made it possible to consult the corpus by means of an improved interface, facilitating the search for regular expressions. The corpus has been lemmatized and tagged into grammatical categories, which also helps in refining the search criteria. This version of the corpus is available online but requires a paid subscription. A portion of the corpus, including works from the 18th to the 20th Century, can be downloaded for free from the Ortolang website. The site's interface makes it possible to choose works based on different criteria, such as the time period or the author.

The Base du français médiéval (Guillot-Barbance et al. 2017) offers access to different diachronic corpora. The main corpus, BFM 2016, includes 153 texts, corresponding to more than 4 million words. This database also provides access to the Corpus représentatif des premiers texts français or CORPTEF, which brings together texts from the 9th to the 12th Centuries, and to the Passage du latin au français corpus or PALAFRALAT, which aims to document the linguistic transitions between Latin and French. These data are available free of charge and can be viewed through an online interface. Most of the texts can also be downloaded in PDF format. The Google Books corpus

In the field of new media, the CoMeRe database includes communication corpora mediated by networks, such as SMS/text messages, tweets, blogs, etc. These data are accessible via the Orféo platform. Also in the field of new media, the Belgian sms4science corpus

Finally, the Corpus Français de l'université de Leipzig, which is not actually a corpus stricto sensu as it contains a set of isolated sentences rather than whole texts, brings together different sources such as newspapers and web pages, as well as entries from the participative encyclopedia, Wikipedia. The data collection mode makes this corpus unsuitable for many types of research but provides a very useful interface for lexical searches, offering the possibility of looking for simple or compound words and having access to all the occurrences within the context, with an indication of the source for each occurrence. The interface also automatically generates a list of the most frequent co-occurrences for each word, as well as the most frequent words to the left and to the right of the word in the search. For each request, the interface returns an indication of the frequency rank of the word looked up in the corpus.

This piece of information makes it possible to estimate the potential difficulty of a word, for example, in the context of language teaching or for preparing experimental material, by controlling the frequency of the words used in the experimental materials. Spoken French corpora

Numerous spoken corpora have emerged since the 2010s. Here, we limit our presentation to resources of a general nature, which are at least partly publicly available. However, many other more specific resources can also be downloaded for free from the Ortolang platform. The corpus of spoken languages in interactions or CLAPI The Corpus oral de français de Suisse romande or OFROM corpus was collected at the University of Neuchâtel The Corpus de français parlé parisien dans les années 2000 or CFPP2000 The Corpus de français parlé au Québec or CFPQ was collected at the University of Sherbrooke The Belgian French Valibel corpus

The Traitement des corpus oraux en français project or TCOF from the ATILF laboratory brings together corpora collected between the 1980s and 1990s, and later enriched in the 2000s. The portion of the corpus available to the public not only includes interactions between adults and children, but also interactions between adults only. It contains 124 transcripts of dialogues, ranging from 5 to 45 minutes aligned with the sound, representing a total duration of 124 hours. The CID corpus

Finally, the Backbone Project contains many videos of interviews with young speakers of different languages, including French. This corpus was designed to document less commonly taught languages or regional varieties of widespread languages. In the case of French, the interviews include young people from the Guadeloupe and Montpellier regions, in particular. This corpus also has an educational purpose in the area of language teaching. This is why it has incorporated grammar, lexicon and language register annotations, which can be looked up through the online interface. Children and learner corpora Many language acquisition corpora can be found in the Child Data Exchange System or CHILDES database

A specific section of the CHILDES database is dedicated to French corpora. In 2019, this section amounted to a total of 16 corpora. The majority of them (12 out of 16) are longitudinal corpora, comprising between one and six children. Half of the corpora focus on very early childhood, with speakers between 1 and 3 years of age, a period during which many elements of the spoken language are acquired. The other corpora include children up to the age of 7, and only one of them (VionColas corpus) includes children up to 11 years old. Six corpora are only available as written transcriptions, five others also have access to sound, and five of them include a video recording.

The CHILDES database also offers a section on bilingualism, including five corpora for which one of the languages is French, the other language is Portuguese (Almeida corpus), Dutch (Amsterdam corpus), Russian (Bailleul corpus) and English (GNP and Watkins corpora). These corpora contain recordings from one to seven children, ranging from 1 to 7 years old. Most of them are longitudinal corpora.

A section of the CHILDES database is dedicated to children with atypical language development. Some of the corpora in this section include data in French. The FoudonReboul corpus is a longitudinal corpus of eight children with autism spectrum disorder (ASD), recorded between the ages of 4 and 9 years. The Nadig corpus also includes 28 French-speaking children diagnosed with ASD, aged between 3 and 7 years. Le Normand corpus includes seven children diagnosed with epilepsy and specific language impairment (SLI), recorded between the ages of 4 and 5 years.

Apart from the CHILDES database, some recent corpora have aimed to study the development of written language in older children. This is the case of the EMA écrits scolaires corpus (Boré and Elalouf 2017), a longitudinal corpus bringing together the text productions of primary and middle school children. A portion of the corpus contains narrative texts produced by CP and CE1 students (6-7 years old), while the other section is made up of a series of argumentative texts produced by CE2 and CM1 students (8-9 years old). The corpus includes images of handwritten texts, raw transcriptions in text format and an annotated transcription, also in text format. This corpus can be downloaded from the Ortolang platform.

In the field of written French language acquisition, the Littéracie avancée corpus produced by the Laboratoire linguistique et didactique des langues étrangères et maternelles (LIDILEM) of Grenoble Alpes University is made up of writings by undergraduate and master's degree students, covering the entire span of study. It contains academic writings such as dissertations, book synopses and reports, as well as motivational letters. The corpus is made up of 11 sub-sections containing at least 10 texts each, produced under similar conditions, namely by students of the same level. This corpus can be downloaded from the Ortolang platform.

In the area of French as a foreign language, numerous learner corpora have been collected. Here, we will only discuss those that are at least partly available to the general public. A more exhaustive list of learner corpora in many languages is provided on the Center for English Corpus Linguistics (CECL) website, from UCLouvain in Belgium. Many learner corpora are also available on the TalkBank online database.

The Corpus écrit de français langue étrangère or Lund CEFLE Corpus brings together texts produced by Swedish learners of French, aged between 16 and 19 years with varying skill levels. The texts are compositions of a descriptive or narrative nature, as well as stories created on the basis of images for description. The corpus amounts to approximately 100,000 words but only part is publicly available. This portion of the corpus comprises a longitudinal section, where each learner has produced four texts. Learner levels range from initial to very advanced, with three to four students for each level. The cross-sectional portion of the corpus includes 136 texts written based on the same image description task, by learners from the initial level to the advanced level, and by a control group of native speakers. All of these can be downloaded in text format.

The Dire autrement corpus, created in Canada by Marie-Josée Hamel and Jasmina Milicevic, contains texts mainly produced by English-speaking learners. It totals approximately 50,000 words and gathers material from different textual genres, either of a narrative or an argumentative nature. The corpus is available on request from the authors.

The French Learner Language Spoken Corpora created by Florence Myles and Rosamund Mitchell brings together seven spoken corpora by French language learners. Six of them were collected at English universities and include English-speaking learners of French, who often studied at university level or during high school. The last one (Brussels' project) includes Dutch-speaking learners. Learner levels vary depending on the corpus. Some of the corpora are longitudinal and others are cross-sectional. These corpora can be downloaded or viewed via an online interface. All the corpora have been transcribed in CHAT format and can be explored with the CLAN tool (see section 5.6.3). The Phonologie du français contemporain corpus

The Interfra corpus created by Inge Bartning and Fanny Forsberg Lundell focuses on Swedish learners of French at different levels. The corpus contains interviews, narrations based on videos, and images. The first part of the corpus includes French learners who have been exposed to the language within the context of schooling. On the one hand, it comprises high school students and, on the other hand, university students from beginner to advanced level. A second portion of the corpus focuses on advanced learners who have all lived in France, for a period ranging from 1-2 years to more than 30 years (see Chapter 3, section 3.3 for a study based on these data). Control groups of native speakers were also recorded. The corpus is fully available to the public and can be viewed free of charge via an online interface.

The University of West Indies Learner Corpus or UWI L2 Corpus created by Hughes Peters includes material spoken by adult French learners (16 in total) who were also speakers of English and Jamaican Creole, and who had studied French at university. The corpus contains conversations during spoken exams and in informal contexts, and amounts to approximately 15,000 words, 9,500 of which were produced by learners. The corpus has been transcribed in CHAT format and can be downloaded or viewed online. Multilingual corpora including French

Most of the time, comparable corpora are assembled by researchers for the needs of their projects from existing monolingual corpora. However, comparable corpora are sometimes already publicly available. This is the case, for example, for the three corpora of parliamentary debates collected by

The number of parallel corpora available is constantly increasing. The OPUS database includes many free access parallel corpora, including the Europarl corpus, described below, as well as corpora with subtitles and multilingual data collected from the Internet, such as Wikipedia. These data have been automatically annotated with part-of-speech taggers; however, these annotations have not been verified manually so there remain a small percentage of errors.

The Multilingual Corpora for Cooperation (MLCC) project aims to bring together both parallel and comparable corpora. The parallel portion of the corpus, Multilingual Parallel Corpus, contains texts translated into nine European languages: German, English, Danish, Spanish, French, Italian, Greek, Dutch and Portuguese. The data in this corpus have been drawn from two sources:

-the Official Journal of the European Commission C series, Written Questions from 1993, which corresponds to more than 10 million words; -the Official Journal of the European Union, Annex: Debates of the European Parliament 1992-1994 and which amounts to a total of 5-8 million words per language. The comparable portion of the corpus contains articles from financial newspapers of the early 1990s, in six languages: German, English, Spanish, French, Italian and Dutch. This resource is available for free via the ELRA website. The parallel portion of the MLCC has been rendered somewhat obsolete by the development of the Europarl corpus

However, these corpora include not only the speeches produced originally in each language, but also their translations. Given the fact that the Europarl corpus was mainly compiled to serve as training material for machine translation systems, the difference in status between the original and translated languages is of little importance. Nevertheless, to carry out contrastive studies, it is very important to have access to this information (see Chapter 4, section 4.4). This is why

The Hansard corpus is also made up of parliamentary debates, more specifically, from the Canadian Parliament. Therefore, debates are in French and English, and accompanied by translations into the other language. This corpus includes spontaneous language, prepared speeches and written texts. The version of the corpus which is available free of charge online includes 1,300,000 aligned sentences or fragments, amounting to approximately 2 million words per language.

Representative of another type of language register, TED conferences have recently made it possible to create a large parallel corpus. Indeed, the presentations in English which can be viewed on the TED website have been transcribed and then translated into many languages (for subtitling purposes) by voluntary users. These translations make it possible to study translation equivalents in a completely different register from the legal and institutional style of the Europarl and Hansard corpora. However, these translations are generally not the product of professional translators, as they are made by volunteers. Taking this into account, the presence of errors should not be discarded, nor should the fact that style may not fully reflect that of professional translations (see Chapter 4, section 4.4, for a study comparing the translations from the Europarl and the TED corpora). Another limitation of this corpus is that it is unidirectional. Actually, TED Talks are always made in English; as a result, English is the only source language, contrary to the Europarl corpus, where all languages are alternately source and target.

In the TED Talks corpus, the only variations concern the numerous target languages. Regarding yet another genre, the CRATER corpus In the literary field, the ParCoGLiJe corpus In the area of new media, the Swiss portion of the sms4science corpus collected by the universities of Zurich and Neuchâtel

The English-French Cabal2 parallel corpus, produced by Poitiers University, at the laboratory Formes et representation en linguistique et littérature or FORELL includes journalistic texts, most of which have been drawn from Le Monde Diplomatique between 1998 and 2003. The other sources are Courrier International, Time Magazine, National Geographic and some chapters from Jules Verne's novels. In total, this corpus includes 200 articles which correspond to approximately 400,000 words. The corpus can be queried online. The results provide the sentence in which the looked-up word appears, together with its translation. This tool is very useful for quickly finding examples of word translations but it cannot be used to perform a truly quantitative contrastive analysis, as the total number of occurrences of the word is not mentioned, nor is the translation's direction. In addition, the tool is not suitable for complex queries. Corpus consultation tools

Concordancers are tools specifically designed for corpus analysis. In this section, we will begin by briefly introducing their main features. We will then focus on the freely available AntConc concordancer, and introduce some of its functionalities. Finally, we will discuss briefly the features of the CLAN concordancer which makes it possible to explore data coded in CHAT format, the annotation standard used in the CHILDES database. Concordancers

Above all, a concordancer is a tool that makes it possible to look up words in their context of use. For instance, in the Littéracie avancée corpus described above, a search for the French word avis (opinion) by means of a concordancer indicates that the students have used this word 61 times. It also helps visualize the sentences in which it was used, aligned per occurrence of the word retrieved, as we can see in the search results reproduced in Figure

In order to analyze the typical environment of a word, concordancers help determine which words co-occur most frequently with the word looked-up. Although the list provided above gives us an approximate idea of the frequent co-occurrences, it does not let us quantify such associations. For example, in the Littéracie avancée corpus, the five most frequent cooccurrences to the right of the word avis are avis sur, which appears 11 times and then avis et (six times), avis de (five times), avis divergent (four times) and avis des (three times). This list also makes it possible to identify other modifiers of the noun avis apart from divergent, on the basis of the observation of the list of occurrences. Frequent modifiers are différent, particulier, défini, général, mitigé, personnel and respectif. The search for co-occurrences to the left indicates that the most frequent elements found in the corpus are: leur avis (18 times), les avis (seven times), d'avis (six times), l'avis (six times) and son avis (five times).

Some concordancers can calculate the probabilities of collocations between certain words, rather than simply establishing the list of words which co-occur in the corpus. In the case of the word avis, the most likely collocations calculated by Antconc are: subsistés, respectifs, émettre, défavorable, réponses and divergentes.

Finally, some concordancers can be used to extract a list of keywords in a corpus by comparing them with a reference corpus (see Chapter 6). More specifically, the concordancer determines which of the words are used significantly more (or even less used in the case of negative keywords) in the search corpus compared to the reference corpus. Another possibility is to take a portion of a corpus and compare it to the rest of the corpus, or to compare a corpus with another similar corpus. For example, in the Littéracie avancée corpus, we can identify the keywords which specifically match student reports, compared with other types of academic work such as dissertations, by comparing this sub-section of the corpus to the others. The resulting list of keywords includes common nouns such as réflexivité, résumé, généralisation, portfolio, globalisation, article, stagiaires, etc., as well as proper nouns like Salaün. The presence of proper nouns in the keyword lists is very frequent because these words often specifically refer to a particular person, which is not used equally in different corpora.

In the case of the noun Salaün, its presence in the keywords of the corpus can be explained by the fact that Salaün was a general delegate of a road prevention association who had taken part in an interview that students had to discuss in one of their assignments.

Setting up a list of keywords also makes it possible to compare the themes of different works by the same author. For example, if we compare the novel by Jules Verne Le Tour du monde en 80 jours with his other novel Vingt Mille Lieues sous les mers, the specific keywords of the first one are common nouns such as train, gentlemen, voyageurs and paquebot, as well as proper nouns like Fogg, Passepartout, Phileas, Hong Kong, etc. In another register, a comparison of the Le Monde corpus from the year 2011 with that of the year 1987 generates keywords in the form of common nouns such as euros, economy, Internet and international and proper nouns like Sarkozy, Hollande, Obama, Aubry and Merkel. This list is a good reflection of the important topics and personalities discussed in 2011 whom we did not yet talk about in 1987. When we compare the years 1987-1995 of the Le Monde newspaper, keywords change to: serbes, ETA, Bosnie, Sarajevo, Croatie, Jospin, Balladur, etc.

In a nutshell, the keyword list of a corpus is very useful to identify its main topics, provided that the comparison with the reference corpus is appropriate. Indeed, the latter is of paramount importance in establishing the list of keywords. If we compare the reports of university students with a year of the Le Monde newspaper rather than the rest of their academic work, the keyword list includes élèves, activité, formation, réflexivité, écriture, évaluation, résumé, pensée, enseignants, savoir, etc., because the topics covered vary more widely between the two corpora than between the different types of university work. This is why the topics emerging from this second comparison are those related to the field of education in general, rather than those addressed in the reports in particular.

In sum, concordancers make it possible to analyze recurrent properties in a corpus, such as its frequent words, its collocations and its keywords from a quantitative point of view, something which is not possible to infer from simply reading texts. This is why they represent essential tools for grasping the quantitative properties of a corpus. Focus on the AntConc concordancer

The AntConc concordancer, developed by Laurence Anthony, is available for free online. AntConc can be used to perform all the analyses described above. This concordancer is compatible with the various current operating systems (Linux, MacOS or Windows). In this section, we will describe its basic principles of use. For a start, AntConc can only read text format files. So, to begin with, it is necessary to convert the files included in the corpus to text format. Depending on their origin, this can be rather easy or more difficult: usual word processors generally have a text format saving functionality, but extracting text from a PDF file can be difficult. AntConc can also read XML files, since these contain text which is accompanied by tags. Before opening one or more files for them to be processed with AntConc, we have to make sure that the encoding chosen in AntConc for reading the characters is suitable for reading the file correctly.

By default, AntConc uses UTF-8 encoding. However, this encoding does not correspond to text files containing French characters, because of accented characters.

When the text of a file is not displayed correctly in AntConc, the encoding used can be changed to make it match the file's encoding, for example, the ISO 8859-1 format (Latin.1).

AntConc can be used for looking up words in context and sorting their occurrences depending on the words that appear to the left or to the right of the search word. To look up certain words, the use of wildcards can be of great help. All the wildcards recognized by AntConc for defining the search pattern can be easily viewed in the software. These wildcards are mainly used for looking up all the possible endings of a regular verb in a single request, by searching for the radical of the verb followed by any number of characters (through the use of an asterisk), such as donn*. To look up a singular and a plural word in a single query, it is possible to replace zero or one character exactly with another wildcard, for example, homme+.

AntConc also offers the possibility of visualizing the places where various occurrences can be found in a file by means of a graph, as well as going through the entire file until we find where the occurrence originated. This functionality is very useful to obtain the maximum amount of contextual information and thus disambiguate certain occurrences. The Clusters and Collocates tabs help you to identify the collocations spotted in the corpus, as well as the most likely collocations.

AntConc has another feature which offers the possibility of generating a list of all the words in the corpus sorted by frequency via the Word List tab. This same tab also shows the number of word types and word occurrences in the corpus (see Chapter 8, section 8.2, for a discussion of these concepts).

These figures are essential for performing lexical diversity calculations on corpus data, such as the type/token ratio (see Chapter 8). Finally, AntConc makes it possible to create a list of keywords from the corpus based on the comparison with a reference corpus. Focus on the CLAN concordancer

The CLAN concordancer works on files encoded in CHAT format, which corresponds to all of the data in the CHILDES database, as well as a number of learner corpora. CLAN can be installed on Mac and Windows operating systems. CLAN commands can also be used with the online version of the corpora.

CLAN offers the possibility of formulating queries on the CHILDES corpus for studying many aspects of children's language. Although query syntax may seem complex at first, it is actually easy to master. A request in CLAN should always start by specifying the name of the command to be performed, followed by the search elements, the file or file's name where the information should be retrieved from, and whether it should be related to the whole corpus or only narrowed to some files. Finally, if applicable, the command should specify the type of speaker whose words have to be analyzed. This specification is often very useful, since the interactions in acquisition corpora most often take place between children and one or more adults and it is necessary to analyze the speech produced by each of them separately.

One of the most useful CLAN commands is the combo command, which helps you to look up words or word sequences produced by specific speakers in the corpus. If the corpus has been annotated, this command also makes it possible to search for grammatical categories, speech acts or even errors. In CHAT format, annotations always take the form of an additional line below the transcription, identified as %mor, for example, when referring to a grammatical category or the morphological representation of a word. The coding of speech acts is identified with a line called %spa. Relevant nonverbal actions are coded with a line called %act. Finally, error coding is identified with a line called %err and has a standardized format. For instance, the $LEX reference indicates a lexical error. In the transcription itself, incorrect words are followed by an asterisk in square brackets so that they can be identified. Let us take a look at an utterance from the York corpus (De Cat and Plunkett 2002): *CHI:tu me l'as donné.

%mor:pro:subj|tupro:obj|me pro:subj|il$v:aux|avoir&PRES&2spart|donner-PP&m. % act: takes a book Going back to the combo command, to find all the occurrences of the word pourquoi produced by the child, the syntax of the command should be formulated as follows: combo +spourquoi +t*CHI CLAN also helps you to determine the frequency of words in a CHAT file using the freq command. This command makes it possible to obtain the list of words sorted by frequency, in the same way as the list of words generated by AntConc. The command also helps you to calculate the type/token ratio (see Chapter 8), which represents a measure of lexical diversity. The syntax for such a command is as follows: freqFILE NAME +t*CHI +o Finally, the complexity of children's language is often measured at the start of their development by their mean length of utterance (MLU) (see Chapter 3, section 3.1). The MLU can be calculated automatically in CLAN using the MLU command.

To do this, the syntax is very similar to the other commands: mluFILE NAME +t*CHI Conclusion

In this chapter, we have presented the main corpora available in French. We have observed that, despite the absence of a reference corpus, numerous more specific corpora are available, which can be combined to carry out research in many areas of linguistics, as we will see in the subsequent exercises offered.

The main limitation for using these corpora is their availability, which is often limited and requires going through an online interface, in which only some functionalities can be used. When corpora can be downloaded, a concordancer should be used in order to explore them systematically. Finally, we reviewed the main features of concordancers and presented two of them succinctly: AntConc and CLAN. 5.9.

Revision questions and answer key 5.9.1. Questions 1) Using the interface provided on the website of the Corpus français de l'université de Leipzig, what is the frequency order in this corpus of the words maison, chalet, immeuble, bungalow. What are their most frequent collocations? What can you conclude from these observations?

2) Using the AntConc concordancer, find the 10 most frequent content words (defined as nouns, lexical verbs, adjectives and adverbs) used by the undergraduate students in the Littéracie avancée corpus. What can you conclude? In this same corpus, what are the five most frequently observed co-occurrences and the five most probable collocations for the word élève(s)?

3) Using the Google Books corpus online interface, find: a) when the new spelling of the word clé started replacing the old spelling clef; b) which researcher is more popular, Ferdinand de Saussure or Noam Chomsky; c) whether the nominal use of the word orange preceded or followed its adjectival use in the history of French. 4) Use the online interfaces of the OFROM corpus and the CFPQ corpus. How often do men and women use the verb détester in Frenchspeaking Switzerland and Quebec? What remarks can you make about the possibilities offered by these interfaces? 5) In the York language acquisition corpus on the CHILDES database, what is the most frequent word produced by Anne in the first recording at the age of 1 year and 10 months old? What about the last recording, at 3 years and 5 months old? How did its type/token ratio change in these two files, and what is the MLU in both files?

Compare with the results for Max in the same corpus. Based on these clues, who seemed to acquire language the fastest? 6) From the TED corpus, identify the different possible translations of the word issue into French. Answer key

1) The word maison has 282,802 occurrences in the corpus. It is the 474th most frequent word and takes frequency class number 8. It is the most frequent word in the group. The word immeuble is the second most frequent word, with 20,133 occurrences, making it the 6,633th most frequent word in the corpus and corresponding to frequency class number 12. The third most frequent word is chalet, with 8,184 occurrences in the corpus, corresponding to frequency rank number 13,609 and frequency class number 13. The word bungalow is the least frequent word, with 1,100 occurrences in the corpus, corresponding to frequency rank number 53,542 and frequency class number 16. As shown in the graph provided on the website, the main collocations of maison are the nouns mère, retraite, disque, famile and jardin, the adjective familiale and prepositional phrases such as d'édition and d'arrêt. The collocations for the word immeuble are the nouns quartier, logement, appartement, étages, incendie and bureaux, the prepositional phrase d'habitation, the past participle situé and the demonstrative cet.

The collocations for the word chalet are the nouns bois, ski, montagne, location, résidence and vacances, the prepositional phrase d'alpage, the past participles situé and assigné, as well as the proper nouns Roman Polanski and Gstaad. Finally, the collocations for the word bungalow are the nouns plage, villa, location, chambre, chalet, vacances, camping, maison, mobil-home, as well as the adjective petit and the prepositional phrase d'accessibilité. We can see from this list that the meaning of the words can, at least in part, be inferred from their collocations. We can also observe that the word maison is the most generic of the four, and the only one that takes figurative meanings as in maison d'édition, maison d'arrêt or maison de disques. When compared with immeuble, we can see that the word maison also has its own attributes, like jardin and famille. Conversely, maison is associated with appartements and étages, which specifies the type of housing in question, as well as bureaux which shows a different use from that of maison.

Finally, chalet and bungalow are both associated with vacation homes, but of a different kind. While chalet is associated with ski, montagne and bois, bungalow is associated with plage and camping. The proper nouns associated with chalet in the corpus show one of the limitations of the collocation analysis. Indeed, in the corpus, several articles referred to Roman Polanski's residence, which made these associations very strong, but these words do not obviously collocate in everyday language.

2) In order to answer this question, it is necessary to open AntConc, and there, to open the files in the L2_DOS_SORB and L3_RS_BOCH directories, which correspond to material by undergraduate students. Then, we have to generate the word list under the Word List tab. The 10 most frequent content words are the following: This list illustrates the fact that the most frequent words in a corpus are those belonging to functional categories such as prepositions and determiners. Indeed, the first content word only appears at the 32nd frequency rank! We can also observe that the frequency of words in a corpus decreases rapidly. The most frequent word in the corpus, that is, de, has 4,461 occurrences, whereas the 32nd word has only 499 occurrences, representing almost 10 times fewer occurrences. In addition, from frequency rank number 4,077 onwards, the words in the corpus only have one occurrence. This distribution reflects Zipf's law (see Chapter 6).

The five most frequent co-occurrences to the right of the word élève(s) are: de, ont, et, en and avaient. To the left, these are the words: les, des, l', aux and un. The five most likely collocations are: répartie, accompagné, onze, évaluerai and équitablement.

3) a) To answer this question, we have to type "clé, clef" on the online corpus interface. We should also be careful to choose the French corpus and to determine a sufficiently long time period, for example from 1800 to 2000.

The results obtained indicate that the spelling clé became as frequent as clef in 1963 and has made strong progress since then, to the detriment of the old spelling.    4) In the French corpus of French-speaking Switzerland, the verb détester is used five times by men and 15 times by women. This search can be done very easily by looking for the infinitive détester in its lemmatized form, which helps us to find all the inflected forms in a single request. In the corpus of spoken French from Quebec, there is no occurrence of the verb détester produced by men, versus 16 occurrences produced by women. The search is much more complicated in this interface, since the search by means of lemmatized forms is not possible. All verb forms must be looked up separately.

5) The most frequent word produced by Anne at 1 year and 10 months old was no, with 32 occurrences. At this age, her type/token ratio was 0.37. At 3 years and 5 months old, the most frequent word was ça with 54 occurrences, and her type/token ratio was 0.21. In addition, her MLU increased from 1.82 to 4.83 from the first to the last recordings. The most frequent word produced by Max at the start of the corpus at 1 year and 9 months old was also no, with 24 occurrences. At this age, his type/token ratio was 0.38. At 3 years and 2 months old, at the end of the corpus, the most frequent word was I, with 48 occurrences. The type/token ratio was 0.24. His MLU ranged from 1.17 at the start of the corpus to 3.78 at the end. The comparison of MLU between Anne and Max seems to indicate that Anne developed her language faster than Max.

The fact that the type/token ratio decreases with age in the two children reflects that the total number of occurrences they produce increases a lot as recordings progress (e.g. ranging from 298 occurrences to 1,092 occurrences for Anne), which implies a poorer lexical diversity in proportion to the total number of words produced. The type/token ratio cannot therefore be considered as a reliable measure of linguistic development. A better appraisal can be obtained by comparing the number of different words, known as word types, produced by each child. At the end of the corpus, Anne produced 230 word types against 182 for Max, which tends to confirm that her language was more advanced.

6) The English noun issue was used 1,957 times in the TED conference corpus. It is therefore difficult to study all of these occurrences. To quickly determine frequent translations, 100 occurrences can be randomly chosen. This observation of translations gave the following French translations for the first 100 occurrences of the word issue: We can observe that the main translation of the noun issue in to French was problème. We can also reason that the word issue does not always have an exact equivalent in French, which might explain the high number of untranslated occurrences, or translations by means of a paraphrase. In particular, this word is used in expressions such as this issue, often translated by pronouns like cela in French. This research project also revealed the difficulties inherent in the observation of translations. In fact, the TED interface does not currently let us specify the search for words or character strings.

Thus, the search also generates irrelevant occurrences of words like tissue, as well as many cases of sentences which have not been translated into French. For this search, it was necessary to consider 175 English sentences in order to find 100 translations of the noun issue into French. Further reading A list of existing corpora in many different languages can be found in

In this chapter, we will present the best practices for creating a corpus. First, we will discuss some facts that need to be considered before deciding to create a new corpus and highlight the advantages of reusing existing data whenever possible. Then, we will address various important methodological concerns for creating a corpus, in particular questions related to the size and representativeness of samples, and will explain simple methods for data sampling and coding. We will also briefly discuss the challenges posed by the creation of the spoken corpora. We will finally see that the task of creating a corpus carries with it a certain number of ethical and legal issues which must be dealt with. Before deciding to build a corpus

The first element to check before starting to compile a new corpus is whether existing data can be used for the planned study. As we will see throughout the chapter, creating a corpus is a challenging task and presents many difficulties. It is actually not always easy to find texts available in a digitized format for all text genres, and even when such texts exist, they might not all be usable due to copyright issues. Choosing the right texts to be included in a corpus should also be the object of careful reflection, since any kind of analysis carried out on data that are not representative of the target genre (see section 6.2) could be largely invalid. When it comes to creating a reference corpus, the data collection phase is so time-consuming that it can only be tackled by a group of experts. Becoming involved in a corpus creation project individually is realistic only in the case of specialized corpora, for example, if the task is narrowed to a specific language register or a regional variety, that is, a project of a smaller size.

Even for this type of corpus, several months of work are often necessary for collecting the data, and may take even longer if the latter are enriched with linguistic annotations (see Chapter 7).

The problems are even more complex and numerous when it comes to spoken data. These need to be collected in the form of audio files which are later transcribed to become analyzable with corpus searching tools. The transcription process itself is very time-consuming and its complexity depends on the exact type of annotation that is added to the data (prosodic contours, etc.). To get an idea of the magnitude of the task, up to 15 hours of work are necessary to transcribe one hour of recording

In Chapter 5, we saw that many corpora in French have already been created, and that some of them are available free of charge to the public. Some other European languages, not only English but also German, Dutch, Spanish and others, have an even broader choice of corpora than French. So, when formulating an empirical research question, it is advisable to consider whether these resources could not be used for the study. If necessary, existing data can be supplemented with a smaller portion of new data, and thus significantly simplify the data collection phase. For example, an empirical study on the regional differences in the way questions are formulated in spoken French could reuse data collected from different spoken corpora, including France, French-speaking Switzerland, Quebec and Belgium. If the study were to be extended to other regional French varieties, for example, the French spoken in the Caribbean islands, existing data could be supplemented by samples of such variety.

In Chapter 4, we also saw that comparable corpora can often be assembled from existing data. For example,

If, after research, it turns out that the existing corpora are not suitable, then the creation of a new corpus might be considered. In this case, it is essential to properly outline the research question that will be studied on the basis of new data, since the latter will have a crucial influence on the whole process, both during the data collection and the annotation phases. In the field of corpus linguistics, it is very common to hear that there are no good or bad corpora, rather there is only corpora which are more or less suitable to address a certain research question. For example, for investigating the expression of subjectivity in journalistic discourse, a corpus entirely made up of editorials would not be appropriate, since this is only a sub-section of the genre, which incidentally is more likely to contain markers of subjectivity than other sub-genres, as dispatches for instance.

In this case, two scenarios are possible: either the conclusions of the study will be limited to the editorial style, or the corpus should be diversified in view of including other types of journalistic texts. The problem we have just mentioned involves a key methodological point for corpus studies, which is the representativeness of data. We will discuss this point in the next section. Establishing the size and representativeness of data

Let us begin by repeating that there is no ideal size for a corpus, in the same way as there are no intrinsically good or bad corpora. Suffice it to say that the characteristics of a corpus may be more or less appropriate for answering a research question. As the technical capacities of computers have evolved, it has become possible to collect ever larger corpora. Currently, some corpora such as the Google Books corpus (see Chapter 5) and the FrenchWeb 2012 corpus (available on Sketch Engine), collected from the Internet, contain several billion words. For a long time, the rule of thumb for collecting a corpus was that it should be as large as possible. The logic behind this principle was that the larger the corpus, the more likely it would contain occurrences of rare linguistic phenomena. Indeed, when the words of a corpus are listed following their frequency order, as in Table

However, more recently, some researchers have defended the idea that maximum size should not always be the goal in the creation of a corpus, since smaller-sized corpora may prove to be adequate for many research questions which do not involve rare words, as we will see in this chapter. In fact, a large corpus is not always suitable for addressing all kinds of research questions. The question of the optimal size for a corpus primarily depends on the nature of the linguistic phenomenon to be studied. The more frequent a linguistic phenomenon, the better it can be studied on the basis of a small corpus. Rarer linguistic phenomena, on the other hand, require larger corpora. This question is also related to the degree of generalization targeted. A corpus representing a specific genre can be relatively small, whereas general corpora need to be much larger.

As we discussed in Chapter 1, a corpus does not simply represent a collection of randomly chosen texts. In fact, a corpus is a collection of texts or recordings specifically chosen in order to be representative of a language, of a certain register or even a language variety. The question of representativeness is therefore essential so that a corpus can be used for answering a research question. In order to fully understand what this notion represents, we will draw an analogy with opinion polls. Let us imagine that we wish to find out which candidate is more likely to be elected in the next presidential elections. In order to find out, it is not possible to ask all the citizens who they intend to vote for. It is therefore necessary to prepare a sample of the population of a more modest size, to whom it might be possible to ask such a question.

Later, the results obtained on the basis of this sample can be extrapolated to the entire population. But in order for this technique to work, it is crucial to carefully choose the sample of respondents, in such a way that it represents the whole population. For example, if the sample chosen includes 500 students met at the exit of a university building, the sample obtained will most likely not correspond to the actual result of the election, since this sample is not representative. In fact, students represent only a small portion of the population. In order for the sample to be representative, it should also include people with other types of occupations, different age ranges and from different regions. The same applies to the compilation of a corpus. In order to be a representative, a reference corpus should contain a balanced set of samples covering the main stylistic genres, both in the spoken and written modes.

The main issue is to determine the criteria according to which it is advisable to classify the elements included in the corpus to ensure its representativeness. We can be sure about one thing: these criteria should not be related to the linguistic content of the samples, but rather to a classification made on the basis of external criteria, such as text genres and language registers. For example, it would be rather inappropriate to try to study the production of speech acts in the legal context by choosing a corpus exclusively based on a number of performative verbs such as demand, condemn, order that it contains, since this criterion would influence the results found in the analysis afterwards. However, this study would require the assembly of a corpus which tangibly represents the legal language, such as court decisions, because these writings properly match the targeted field of study, that is, the legal language.

To sum up,

In the case of reference corpora, sample distribution between different genres is a complex problem in itself, due to the lack of an existing typology of spoken and written genres that is unanimously accepted. To simplify, let us say that written corpora should contain both public texts (published works) and private ones (letters, emails, etc.), collected from different fields such as the press, the sciences or the literature. The spoken section of a corpus should reconcile a variety of choices. It should include both planned and spontaneous spoken speech, monologues and dialogues, drawn from contexts with various degrees of formality. Very often, the creators of new corpora solve the problem of representativeness by following the criteria used in existing reference corpora, such as the British National Corpus, a pioneer of the genre.

In cases where researchers need to compile specialized corpora, the question of representativeness is posed a little differently. To continue developing the analogy with polls, if the goal is to know who students will vote for in the presidential elections, it will be enough to interview a sample of students, since such a sample is representative of that population. In the same way, the question of the representativeness of specialized corpora is clearly simplified, because this can be achieved by choosing texts or recordings belonging to a specific genre. However, we should keep in mind that there may be sub-genres within a genre, such as novels, short stories or children's stories within the literary genre, and that these may vary from each other.

Even when working within a text genre, we should aim to diversify its sources as much as possible. For a literary corpus, for example, works from different authors should be included. From a lexical perspective, the representativeness of data in specialized corpora, such as corpora devoted to newspaper or legal articles, can be measured using the concept of saturation

As a matter of fact, the representativeness of a corpus cannot be ascertained once and for all. In the case of closed corpora (see Chapter 1), data aging implies that they are no longer representative of the most recent developments in the language. In the case of monitor corpora, while the new data added at regular intervals may improve their representativeness, they nonetheless pose balancing problems between the different portions of the corpus, an aspect we will discuss in the following section. In summary, Choosing language samples

To achieve the representativeness aim discussed above, a corpus should include a sampling of different types of texts or recordings. Unless we are working on a very specific corpus like the Bible or the complete works of an author, most of the time, it is actually impossible to include all the texts or all the recordings belonging to the genre to be studied in a corpus. This is why it is necessary to prepare samples which, once assembled, can work as a representative sub-section of the genre to be studied. The preparation of samples to be included in the corpus poses two important methodological questions: on the one hand, the appropriate size for each sample, and, on the other hand, how to balance the portions of the corpus in such a way that the result is truly representative of the genre.

In order to understand the difficulties of corpus balancing, we will give an example. To be representative, a spoken French corpus should include speakers from different regions, different ages, both male and female. If 95% of the corpus is made up of Parisian speakers aged between 20 and 30 years, such a corpus will not be a representative sample of the population, even if this means including the other speakers among the remaining 5%. While it is true that a sample is expected to bring together a rather restricted version of the overall population, it should also reproduce its main features so that the results obtained on this sample can be extended to the entire population. A corpus of spoken French should therefore include a similar proportion of male/female speakers, from different age groups and different regions. Many other criteria could be included in this selection, such as the socio-economic level of the participants, for instance.

As with the question of corpus size, the balancing criterion largely depends on the question the corpus will help to study. In general, a corpus should not be used for establishing a contrast between elements which have not been balanced during the corpus compilation phase. For example, a corpus created for studying the pronunciation of vowels in Paris and Marseille and which has not been compiled representing a balanced sample of different age groups cannot be used carelessly for studying the evolution in the pronunciation of vowels between generations.

In the case of written language general corpora, it is important for the chosen samples to represent different genres, including both published and unpublished texts. In the case of the British National Corpus

-the field, that is, the topic explored in the text; -the time when the text was produced; -the distribution mode, depending on whether it was a book, a newspaper or an unpublished text.

The spoken samples were chosen on the basis of demographic criteria such as age, gender, geographic region and social class, as well as contextual criteria. However, the creators of large corpora have agreed that it is very difficult to fulfill all the criteria to achieve a perfectly balanced corpus. One of the major problems is the difficulty of incorporating new data, an aspect which tends to create bias around the choice in favor of more readily available data. Such difficulty is largely due to copyright issues, which we will address in section 6.6. This issue prevents the inclusion of recently published texts in a corpus that have not yet fallen into the public domain. In addition, published texts are generally more easily accessible than unpublished texts, such as emails or personal letters. Finally, texts published on the Internet are much easier to access than texts published on paper. These differences inevitably induce a certain bias towards specific text categories.

In the end, balancing a corpus is never a perfect task. As

In concrete terms, balancing the portions of a corpus can be achieved by defining a sampling frame which delimits the population to be sampled and lists its relevant properties. Each of these properties should then be mirrored by the corpus sample proportionally to its prevalence in the population. For example, in order to create a corpus of French, speakers from different regions should be included in the sample. Therefore, geographic region becomes a relevant trait for the sampling frame of a spoken French corpus. The number of French speakers to be included for each region can be determined proportionally to the number of French speakers living in the different regions sampled. However, in many cases, these proportions are difficult to determine accurately. For example, it is difficult to determine exactly what proportion of the texts published every year belong to the fiction genre and how many are non-fiction. In this case, obtaining the exact figures is undoubtedly possible, but highly complex.

The problem becomes even more challenging for the categories of unpublished texts, for which there are no existing figures. In these cases, the classification is often based on common sense or on the pragmatism of the corpus designer, depending on the importance of subcategories for addressing the questions that the corpus is supposed to help study. Now, let us move on to the question of which samples to include in the corpus. The first important question is how these samples should be chosen. A first technique involves choosing the samples completely at random, the idea being that out of the total number of samples in the corpus, the most frequent characteristics will eventually stand out on their own. However, we cannot take for granted that this method yields a balanced sampling, especially in the case of a small corpus. As previously mentioned, a better method might be to define a sampling frame and to divide the samples to be collected depending on the important properties of such a frame.

For example, if the sampling frame for a corpus of French spoken in Switzerland includes different criteria such as gender, age, socio-economic level or place of residence, an equivalent number of samples should be chosen to match each selected criterion. Within each criterion (e.g. 20-to 30-year-old middleclass men living in the canton of Geneva), participants can be chosen at random. According to

The next question to consider is related to the number of samples required in the corpus, as well as the ideal size for each sample. Once again, the answers to these questions depend on the type of corpus the researcher has in mind. The more generic the corpus, the more samples will be needed, whereas for a more specialized corpus, fewer samples are necessary in order to provide representative data. On the basis of corpus studies on the differences between genres and between language registers, Biber

Finally, another important question concerns sampling units. Should we include whole texts or only excerpts, or even isolated sentences? The answer to this question often depends on the accessibility of data. On the one hand, it is preferable to create a corpus including language samples which represent a coherent whole, rather than isolated sentences. However, this is not always possible due to copyright reasons. The correct size of samples also depends on the type of text considered. For example, if the goal is to reach a sample of 200,000 words per text genre, this number of words can almost be instantly reached by including one or two entire books in the sample. In this case, it would be a better idea to choose excerpts (e.g. chapters) from different books, instead of a longer portion of a single book. For other types of text such as letters, text messages, etc., the units are so small that it makes no sense to not fully include them.

In all cases, it is important to systematically avoid including the same portions of text, for example, always the beginnings or the endings. Indeed, Preparing and coding corpus files

In order to include language samples in a corpus, first we have to obtain them. In the case of spoken corpora, data acquisition first requires them to be transcribed. This is a very complex process, and we will discuss it in detail in the next section. In the case of written corpora, the situation is not always simple, either. The most favorable scenario is clearly the one in which data are readily available in digital format, which is progressively becoming more frequent, especially when it comes to data gathered from the Internet. However, retrieving text from digital formats may have varying degrees of difficulty depending on the original format; for example, it can be very difficult to isolate the texts of different articles in a journal page formatted as a PDF document, or even impossible when the PDF file includes text images. So, even when we are working with data in a digital format, the task of converting the original format into a usable format, based on the text, is still necessary, as we will see below.

In some contexts, however, written data are not available in digital format. In this case, we can either work with printed texts available on paper or with handwritten texts, such as student essays or private letters. Printed texts can be scanned and then processed thanks to optical character recognition (OCR) software, but these always require a manual check made by a human in order to provide a completely reliable result. There might be a high number of errors if the original print is of a poor quality. Finally, for handwritten data, there is no solution other than to manually type it on the computer. Data transcription also raises many questions related to the way in which some of their original features might be preserved. For example, student essays often contain spelling mistakes, which should be left untouched, since they can be very informative for many research questions. But in this case, a version without misspellings should also be included so that the words can be found by a concordancer.

In Chapter 7, we will see that errors can be systematically annotated in a corpus, in the same way as syntactic or semantic information is provided.

No matter the way of acquiring data, an important point is to save the corpus files into a format which can then be used by a concordancer. As we saw in Chapter 5, most concordancers (like AntConc) only read files in text format, which can include texts tagged in XML format. Therefore, all newly created files for a corpus should be directly saved into text format. Files which have already been scanned are rarely saved in this format, since this format does not make it possible to include formatting marks, and this makes documents difficult to read. In the case of corpus studies, this is not a problem, however, since the files will not be read by humans, but processed by a concordancer. Files processed with OCR software or word processing tools are often saved in proprietary formats (such as Microsoft Word, for example, DOC or RTF). Files saved in these formats can be easily transformed into text files thanks to specific options such as the "save as" command found in word processors.

Web pages are available in HTML format. This format contains many formatting marks in the form of tags, which are interpreted for creating the various graphic effects which are necessary for a browser to display a web page. These later become visible when a file is opened with an editor in pure text format. Despite their lack of linguistic relevance, these tags are interpreted as textual elements by concordancers.

In order to avoid this problem, software should be used for retrieving text from these files and eliminating unnecessary markings (HTML tags). Word processing software often include this feature, simply by choosing the "text format" option from the "save as" command. The only problem with this option is that it is necessary to open every file one after the other in the word processor so as to perform the operation. This might eventually become a problem with a corpus, including thousands of different files. An alternative solution is to use the AntFile Converter, which is a file conversion software developed by Laurence Antony, the creator of AntConc (see the URL at the end of this book for AntFile Converter). This software can be downloaded for free and used for converting any number of XML, HTML or sometimes even PDF files into text format.

The Sketch Engine corpus creation and management platform, discussed in Chapter 5, also automatically transforms the format of files downloaded from the Web. The advantage of this platform is that it offers the possibility of automatically downloading large amounts of data from the Internet in a single operation (web crawling). The corpus created in this way can be directly analyzed using the tools provided by the platform, for example, for retrieving concordances, word lists or keywords. In its recent versions, the WordSmith concordancer also offers a similar function. This type of tool has made the collection of web-based corpora extremely easy. We should nonetheless bear in mind that the texts found on the Internet are of a highly variable quality and are not representative of the whole language.

If the corpus has been created manually rather than through the use of a platform enabling automatic data download, a practical but important question concerns the number of files that should be created. More specifically, should we create a single file for the whole corpus? Or should we create one file per corpus sub-section? What about a file per text included in the corpus? In general, it is preferable to store every language sample in a separate file. In this way, it is later easier to combine them in different ways for creating sub-corpora, rather than having to retrieve text portions from a larger file. For example, if we collect data on the language used by young people in France, we might then want to compare data depending on different criteria such as gender, geographic region or age group. This comparison can be done in a relatively simple way by grouping all the men's files and all the women's files or, for the same purpose, all the Paris files and all of the Marseille files.

But if all men are included in a single file and women in another, then the geographic comparison data needs to be reprocessed.

In order to be able to easily group the files into sub-corpora, it is necessary to represent the features of each sample in an easily accessible manner. A practical solution is to code these characteristics directly onto the file names: this is why these names are another important element that should be taken into account when creating the corpus. For example, it is possible to name files only by using a number, each representing a criterion used when compiling the corpus. Going back to the example of young speakers, one possibility would be to identify all the files from Paris with the number "1", those from Marseille with the number "2", etc. Then, the second digit could be used for coding gender, "1" for women and "2" for men, then the third reference could be for coding the age group, for example, "1" for 16-to 19-year-olds, "2" for 19-to 22-year-olds, etc.

Finally, several digits can be used for coding the participant's number. Following this procedure, the sample corresponding to the first 18-year-old male participant from Marseille registered in the corpus would be saved in a file called "221001.txt". The disadvantage of this method is that the coding is opaque for a user who does not have a precise vision of the system used.

A more transparent way to achieve the same result is to use abbreviations. For example, the same file could be coded using abbreviations such as Mar for Marseille, h for men (homme), ado for the 16-to 19-year-old group, which would result in a file called "mar_h_ado_001.txt", if we use the underscore symbol as a separator for the abbreviations. If this system is used, abbreviations should be kept short in order to avoid generating excessively long file names, which might not be readable. We should also avoid inserting spaces or other punctuation marks, since these could interfere with the programs used for opening the files on different platforms (typically concordancers). Finally, if word abbreviations are used, it is desirable that each abbreviation of a category contains the same number of characters (e.g. three letters for all the names of cities), in order to make reading in columns of lists of files easier.

We have pointed out that corpus files should contain plain text, in order to facilitate data analysis. However, for a corpus file to be used as a sample representing a certain type of language, metalinguistic information (which is not part of the text or of the dialogue) should be accessible to the researchers who will analyze it. For example, this type of information includes the date of a newspaper article, the place where a conversation was recorded or the characteristics of the speakers taking part in the dialogue. This "piece of extra information concerning the data" included in the corpus is what we call metadata.

For this information to be made available for future users of the corpus without separating it from the rest of each sample portion, it should be possible to include it in the files, but in such a way that it is not taken into account by a concordancer when counting the words of the corpus. A possible solution could be to insert these marking elements inside tags, something that the concordancer will be able to ignore. Most often, these tags are delimited by chevrons (the less-than and greater-than signs < >).

In this way, the metadata of a corpus sample can be added at the beginning of each document as follows: <texttype: newspaper article> <publication: Le Monde> <author: Jean Dupont> <date: 1 April, 2019> <subject: April Fool's Day> In the AntConc concordancer, discussed in Chapter 5, it is possible to inform the program about the existence of tags and not to consider the information they contain.

In some cases, the abundance of metadata requires the use of a precise syntax for tags, based on the conventions of computer languages for XML or SGML coding. The conventions used can be explained in the corpus documentation or may follow a more widely recognized standard. Indeed, some sophisticated marking formats have already been developed for corpus data. One of the best known is the TEI format (Text Encoding Initiative), which makes it possible to encode many markings in a standardized way and make corpora sharing easier. Large reference corpora such as the British National Corpus are tagged following the TEI conventions. Without going into details, a TEI-tagged document always contains two types of elements: -the header; -the body of the text.

And these two elements are respectively made up of other elements. The header section contains metadata, a description of the file, the encoding, the text profile, mainly the language, the context or participants, and even a history of its revisions. All these elements, except for the file's description, are optional. The body of the text mainly contains tags, which are destined to delimit text units such as paragraphs or even sentences. Following XML conventions, TEI tags always begin with chevrons < > and close with </ >. As we will see in Chapter 7, TEI tagging can also be used for making more detailed annotations than dividing the text into sentences.

In addition to indicating the metadata by means of tags inside each file, it is also very useful to provide a summary table in the corpus documentation, as illustrated in the simplified Table Recording and transcribing spoken data

The collection of spoken corpora poses certain additional challenges compared to written corpora. One of the main difficulties stems from the need to transform spoken data into a written format. As we have seen in Chapter 5, for corpus data to be analyzed, they should be in written format, since concordancers cannot search for words or expressions in audio files. In this section, we will briefly discuss some of the problems related to the representation of spoken data, as well as some possible solutions to sort them out.

The first step we can take to work with spoken corpora concerns the mode of acquiring data. Spoken data need to be recorded and the recording process itself requires special preparation. For data to be as representative and informative as possible, it is essential to properly define the research questions that these data will help answer well in advance. In particular, these questions will determine the type of interactions that should be recorded, the constraints regarding the context as well as the information contained in the transcript. If, for example, the aim of a spoken corpus is to study the lexical specificities of a language variety, the prosodic information contained in the interactions will be of little use. If, on the other hand, the research question concerns information structure in discourse, more specifically the introduction of new and given information in different spoken genres, then prosody will play an important role in studying the interface with the utterance structure and therefore requires a transcription.

An important point to establish before carrying out the recordings is the nature and the amount of contextual information that will need to be added to the transcripts. Spoken conversations are naturally more ambiguous and less precise than written communication, since speakers can use the immediate context to make themselves understood. Audio recordings make it almost impossible to grasp this type of information, which later have to be added to the corpus so that the interactions can be understood and analyzed by the experts who will listen to these recordings. In the case of audiovisual recordings, a larger share of contextual information will be captured and should not be explicitly added to the corpus (although some kind of codification may later be required to perform specific analyses).

Due to the difficulty of collecting and transcribing spoken data, the question of the amount of data needed for creating a corpus is even more acute than for written data. But in this case too, there is no ideal size for a spoken corpus. The amount of data required for studying a certain phenomenon primarily depends on how frequently it occurs. For example,

As we have already mentioned, providing metadata details is particularly important in the case of spoken corpora. Among other things, the metadata appearing in the header should offer information about the main features of the participants: degree of relatedness (parents, friends, colleagues, strangers, etc.), the context in which the conversation took place, the manner in which the recording was captured, etc. The importance of this information depends on the questions that the corpus is expected to answer. For studies on how interpersonal relationships influence interactions, it will be necessary to have as much information as possible about the degree of relatedness between participants, whereas for studies on the use of discourse markers such as bon or ben, this type of information is not so relevant.

In the same way, contextual information may be added to the statements inside the transcripts. Let us insist on the fact that the context of an interaction is so rich that it would be illusory to try to account for all the aspects involved in a transcript. Choices will have to be made depending on the importance of this information for the research question. At least, the transcripts should contain enough contextual information for the meaning of the utterances to be reconstructed if this became necessary in the absence of context. For example, if a person passes by and this event invites a comment from the participants, this piece of information should be mentioned in the transcript, indicated between tags so as not to be confused with the transcription itself.

Finally, the last difficulty related to transcription that we will mention concerns the presentation of the transcripts. In a dialogue, the participants do not always speak one after the other as it happens in the dialogues of a novel or a play. On the contrary, there are many overlaps between speaking turns, as well as pauses. The analysis of overlaps and pauses can be important for certain studies, so the question arises on how to best account for these phenomena. If a transcript is presented in a purely linear fashion, one intervention above the other, valuable information might be lost. This is why other types of presentation are often used. For example, in the CLAPI corpus, the contributions from the different participants are presented one below the other. Overlaps are indicated by green square brackets, making it easy to see where and when they occur, as shown in Figure

In summary, the transcription of spoken data requires many decisions to be made concerning the nature and the amount of information to be added, not only to the dialogues themselves, but also on how to communicate such information on the files and visually. These decisions should be made even before the data collection process begins, since an important portion of contextual information could be lost if it is not recorded during the interactions. Ethical and legal issues

Creating a corpus involves using (or even sharing with other researchers) language samples produced by third parties. Those persons having contributed to a corpus through their language productions have rights that need to be respected. In the case of a spoken corpus in particular, it is essential for participants to know that they are being recorded and that their data will later be used for linguistic analyses. For this, the creators of a corpus must hand a document to their future participants clearly explaining who the data will be accessible to and how it will be used. The participants can then freely decide whether or not to sign a form stating their consent to take part or not in the study. However, such consent to participate does not suffice to share the data with other people afterwards unless this usage has been explicitly mentioned in the form. In fact, a participant may agree with the idea of being recorded by a researcher and then having such data used for research, but not necessarily agree with having their data being shared with a large number of people, perhaps even with web-free access.

In order to be able to share corpus data, it is imperative to ask participants both for their authorization to use and to distribute the data before collecting them. If a participant later refuses to have their data distributed, removing them from the corpus may pose many difficulties. In the case of dialogues, all of the data of the events that the participant was involved in will have to be removed.

The right to anonymity of the persons mentioned in the corpus represents another important ethical problem. Often, people interacting in recorded conversations refer to third parties by naming them. These people did not provide their consent to being talked about in public documents, so their names should be removed before publishing such data. This anonymization process is not always easy, however. For example,

In the case of written corpora, the situation is simpler, especially when it comes to published data. It is reasonable to think that the public figures mentioned in the articles agree to waive their right to anonymity. The responsibility of the corpus compiler is involved when it comes to texts with potentially defamatory content. In the case of articles found on the Internet, in particular, source verification is necessary before indiscriminately including texts collected automatically, following the web crawling processes described earlier in this chapter. In general, we should also be aware of the fact that distributing a corpus implicitly amounts to disseminating the ideas contained inside its texts. In some cases, this may pose ethical problems for researchers. For example,

Written corpora containing published texts are confronted with copyright issues. While it is true that laws differ from country to country, it is not possible to distribute the texts of an author during his/her lifetime without demanding some type of compensation. In France, this period is valid until 70 years after the author's death. However, contrary to what many people think, data accessible on the Internet are also subject to copyright. Their use can be softened, providing that they are accompanied by sufficiently permissive user licenses, such as the Creative Common license which concerns the contents of the collaborative encyclopedia Wikipedia. Due to copyright restrictions, in the case of less permissive licenses, corpora creators encounter many restrictions for including data. There are several possible strategies for properly addressing the copyright problem.

First, we can limit our choice to works that have fallen into the public domain and/or coming from websites where data have been declared free of rights. This solution is the safest one from a legal point of view, though it is not the most satisfactory one from a linguistic point of view. As a matter of fact, this selection method hinders the collection of data that truly mirror contemporary language and certain stylistic genres which are poorly represented on the Internet.

A second solution would be to negotiate the right to use data with their owners. This solution can be realistic when creating a corpus drawn from a limited number of sources. Rights holders often agree to authorize a single researcher to use a reasonable amount of their data for research, but this type of corpus often cannot be later redistributed. This limitation poses a problem for research replicability, which is an important scientific element in order to grant its validity.

Another way of dealing with the copyright problem during corpus distribution would be to allow users to search for concordances in the corpus, but not to visualize it in its entirety. This is the case for many corpora that are only available via an online consultation interface. These interfaces only enable occurrence searches for words or expressions within a certain context. This solution effectively preserves copyright, since the works remain inaccessible. For users, these interfaces make it possible to answer a certain number of research questions related to the lexicon. However, they are unsuitable for research questions that require data processing, for example, some kind of annotation or those that have to take into account a large context, in order to identify certain linguistic phenomena such as speech acts or discursive phenomena. Conclusion

In this chapter, we have discussed the main elements to consider when creating a new corpus. For a start, we mentioned that corpus creation is a long and complicated process. This is why reusing the already existing data should be prioritized as far as possible. Then, we saw that the important methodological trait to be respected when creating a corpus is datarepresentativeness. The latter can only be defined in relation to a specific research question. The representativeness of a corpus also depends on its balance and the choice of samples it contains. We also introduced some basic principles regarding sample collection and balancing. We then addressed some concrete problems, related to data coding and transcription into a corpus, and concluded that these questions needed to be resolved before starting the data collection phase. Finally, we saw that the creation of a corpus poses several ethical and legal questions which should be carefully considered, since distributing data amounts to disseminating information that belongs to and concerns third parties, whose rights must be respected. Revision questions and answer key 6.8.1. Questions 1) What types of data should be collected to conduct a representative study of how young people use the discourse marker genre in French? 2) How could we balance the different parts of a corpus aiming to study the French literature of the 18th Century?

3) What are the main questions to consider when choosing the samples to be included in a corpus? 4) Using the Sketch Engine, choose five keywords in order to create a corpus on the French cinema. What are the characteristics of the corpus thus created? Which are these keywords? 5) What transcription information would it be important to add to a corpus of spoken conversations to study the language of the suburbs in France? 6) What are the ethical issues to consider in the following cases: a) a collection of texts produced in class by children; b) a recording of spontaneous conversations of a group of friends at a bar; c) a recording of a teacher's course for a spoken corpus. 7) Which of the following actions do you find problematic from a copyright perspective: a) using a digital version of the novel series Harry Potter for compiling a corpus stored exclusively on your computer; b) sharing this corpus with your partners as part of a corpus linguistics course in order to do joint homework; c) distributing this corpus on the Internet; d) including an entire chapter drawn from this corpus in a publication with the aim of illustrating certain linguistic phenomena that you have annotated. Answer key

1) In order to have representative data for this research question, the corpus chosen should evidently contain language produced by young speakers. This concept would need to be clarified to be operational, for example, by deciding to include an age group ranging from 15 to 25 years. This study does not specify a geographic region. One way of delimiting research for such a project would be to compare young people living in large cities in four French-speaking regions from different countries, for example, Paris, Brussels, Geneva and Quebec. The young speakers included in the corpus should proportionally represent the two genders and have different socio-economic profiles. Finally, the corpus should contain young speakers recorded under similar conditions in order to avoid any context-related bias. Another study could aim to compare these uses across different speech styles, which should then be represented in the corpus in a balanced manner.

2) This research question is fraught with different constraints. First, the corpus should contain French literature, which restricts the literary subject to works written in original French, rather than translations. It should also span a specific period, which could be defined, for example, as works published between 1800 and 1900. The difficult point to assemble this corpus relates to the way of balancing its content between different literary genres. It should therefore contain novels, short stories, plays and poetry. Let us assume that the corpus targets a size of 200,000 words per genre, in order to have a representative sample of each of them. Since novels and plays are long texts, it would be a good idea to include excerpts (e.g. a chapter or an act) from many different works, rather than two or three texts in their entirety. Conversely, since poems are a very short genre and more marginal in terms of the amount of texts published, a possible decision would be to limit the share of poems to a smaller percentage of the corpus, for example, to limit poetry to 50,000 words.

3) The first question to ask is whether a sample is representative of the genre it embodies in the corpus. For example, before deciding to include an interview with a young Brussels resident in a corpus of French spoken in Belgium, we should make sure that this person has not recently moved to Belgium from another country, and that they properly reflect the linguistic specificities that the corpus is supposed to embody. The second important question concerns the size of the sample that will be included in the corpus. As we recalled above, it is not always optimal to include entire texts in a corpus when these are very long. Depending on the target size for each genre, it is necessary to determine the appropriate size for each sample. A third question to consider concerns the way in which the samples are acquired, depending on whether these are digitized texts, texts to be scanned or transcribed.

Besides, it is also necessary to determine which metadata will be associated with each corpus sample. All of these decisions need to be made based on the research question being considered. A final and very important point is to ensure that copyright is respected, either because the text is copyright-free or because the author has provided written consent granting permission to include their text in the corpus. Finally, we should make sure that the sample is acceptable from an ethical point of view, in the sense that it respects the right to anonymity of the person involved and that it does not contain inappropriate content. 4) By creating a corpus with keywords such as cinéma, films and acteurs and using the default parameters offered on the site, Sketch Engine produces a corpus of 90,441 word occurrences, including 2,680 word types retrieved from 35 different pages. The most frequent content word is cinéma, at the 20th frequency rank.

The keywords in the corpus include proper nouns such as Edison, Funès, Fernandel, Gabin and Reynaud and also content words like cinéphile, cinéma and crédits. Collocations include elements like cinéma français, cinéma muet, art dramatique, histoire du cinéma, grand écran, carrière cinématographique, mise en scène, film français, actrice américaine, etc. These collocations make perfect sense in view of the search terms used for creating the corpus. 5) Transcription information should include both metadata and indications inside the transcripts themselves. The metadata of such a corpus should at least include information of where the recording took place, its date, the context in which it happened, the conversation topic, the number of participants, the gender of the person recorded, his/her age and profession. Within the transcripts, it might be useful to include an annotation of the non-standard words used, for example, in verlan, with their equivalent standard so that they can be found in a concordancer.

An indication of pauses, overlaps and certain prosodic phenomena may also be useful. 6) a) Above all, a collection of texts produced in class by children for assembling a corpus requires protecting the children's right to anonymity. No element in the corpus should make it possible to identify any participant. Depending on the nature of the texts, it is necessary for the content to exclude any element making it possible to identify any other person. b) In the case of a recording of spontaneous conversations of a group of friends at the bar, everyone involved should be notified that the conversation is being recorded and that he/she agrees. Depending on the purpose of the corpus, this agreement should also consider data sharing with third parties. Then, depending on the conversation topics, it would be necessary to ensure that the content is neither defamatory nor offensive, and that it does not enable third party identification.

c) Finally, in case we decide to record a lesson from one of the professors for a spoken corpus, we should make sure that the professor has been informed about the recording and has given his/her consent, both for the use and for the possible sharing of the data. 7) a) Using a digital version of the Harry Potter series for assembling a corpus to be stored only on your computer and searching for elements in the text is not problematic a priori, insofar as this digital version has been legally acquired, for example, by buying an e-book from an online bookstore. b) However, sharing this corpus with your classmates within the framework of a corpus linguistics course with the aim of carrying out joint homework is a bit more delicate an issue, because the fact of buying a book does not entitle you to duplicate it or to transmit it free of charge to others.

This practice may, however, be considered as a tolerated use of the material if the aim is to carry out joint homework on the data, which are not being used in any other way. c) Distributing this corpus on the Internet is completely illegal under copyright rules which can be enforced for decades after the author's death (70 years in France). In this case, the Harry Potter series will still be protected for many years, and any form of distribution is currently prohibited. d) Including an entire chapter of this corpus within a publication to illustrate certain linguistic phenomena that you have annotated can also be problematic from the point of view of copyright. Though it is acceptable to quote portions of a text while indicating its source, these quotations should not exceed a clearly defined size limit. This size varies from country to country, but generally does not exceed a few hundred words.

Thus, publishing an entire chapter of a book is not acceptable. 6.9. Further reading How to Annotate a Corpus

In Chapter 6, we discussed the importance of associating metadata with corpus files. In this chapter, we will explore how to insert other types of information into a corpus as linguistic annotations. To begin with, we will see that annotating a corpus is a way of adding value to it by widening the field of questions that it will make it possible to investigate. We will then review the different types of annotations we can add to a corpus, briefly present some tools for performing some annotations automatically or for making manual annotations easier. We will also discuss best practices to follow when making a manual annotation and present the different ways to assess the reliability of such annotations. Finally, we will present the principles to be respected in order to make annotation sharing easier. Corpus annotations

Raw data which are collected in a corpus are not always adequate for answering many research questions. Let us imagine, for example, that we wish to know how often French nouns such as ferme and car appear in a corpus. A simple search for their occurrences in a raw data corpus will provide a biased answer, since these words also have other uses apart from being nouns, and these will be included among the search results (ferme is also a conjugated form of the verb fermer and car is also a coordinating conjunction). In order to keep only the relevant occurrences, should we filter all the relevant occurrences by reviewing them one by one? Such manual sorting tasks can actually be avoided if the words in the corpus have been previously annotated into grammatical categories. This annotation makes it possible to exclusively look for the noun occurrences of ferme, for example.

The great advantage of part-of-speech tagging is that it can be done automatically with almost the accuracy of a manual annotation, regardless of the amount of text to be annotated. As a matter of fact, part-of-speech tagging has been performed on the Google Books corpus (see Chapter 5), which contains billions of words from different languages and has made it possible to refine research on language evolution. For example,

Finally, the main objection often raised against annotations is that any form of annotation is necessarily subjective, at least in part. This implies making choices about the categories to be annotated (see section 7.3), which are never completely neutral. Furthermore, the annotation itself is accompanied by a hermeneutic dimension, which reflects the annotators' point of view about the text. This is due to the fact that all the categories include borderline cases, for which different annotations can be justified. In these cases, annotators must make decisions while simultaneously interpreting data.

An example drawn from recent research on language acquisition will better illustrate the impact of annotation choices. Different authors have studied the early production of causal connectives in children, in different languages. (1) Grand-mère: pourquoi ça? (Grandmother: why that?) Léa: parce que j'en avais envie. (Léa: because I felt like it.) Conversely, Evers-Vermeul and Sanders (2011), as well as (2) Léa: je prends ma gourde parce que j'ai soif. (Léa: I'm taking my bottle because I'm thirsty.)

(3) Léa: venez parce qu'il est très tard ! (Léa: come because it is getting very late!) This example illustrates the influence of the methodological choices associated with data annotation and the conclusions that can be drawn from a corpus study. While it is true that annotation processes involve choices that are always partly subjective, many researchers (e.g. Different types of annotations

All levels of linguistic analysis can be annotated in a corpus. To begin with, some annotations are related to phonology, particularly in the case of transcriptions for spoken corpora. For example, these annotations indicate prosodic phenomena like pauses, hesitations and prosodic phrasing. In this way, they make it possible to look for such phenomena automatically, without having to listen to all the audio files in the corpus again. In addition to the study of prosody, these annotations can be useful for works of a very diverse nature, for example, for studying the notion of fluency or the interface between syntax and discourse, so as to better understand information structure in discourse.

Words are often the linguistic element the most subjected to annotations in a corpus. The most basic of these annotations is the division into words or occurrences (known as tokenization), which includes punctuation identification, elision processing (in French) or the identification of numbers and dates. However, lemmatization refers to the act of associating every word occurrence in a corpus with its basic morphological form. For example, adjectives such as gentil or gentille (in French) are the masculine and feminine variants of the same word (or type) from a morphological point of view. This canonical form of the word is called its lemma. In the case of adjectives, their lemma is by convention the singular masculine form. Similarly, the words mouse and mice are the singular and plural variants of the same lemma (mouse), whereas eat, eaten and eating are the conjugated forms of the lemma eat. Lemmatizing a corpus is an essential process for studying lexicon, since annotation makes it possible to look for occurrences without having to detail all the morphological variants it may adopt.

Lemmatization is one of the types of annotations which can be done automatically, with considerable accuracy (see section 7.4).

The notion of the lemma should be differentiated from the concept of the lexeme, which is used for defining a word from a semantic point of view. So, for example, bat is a lemma which can correspond to two different lexemes, as this word is polysemic and may either refer to a flying mamal or an object used to hit a ball. Finally, we should bear in mind that the usefulness of lemmatization may change significantly from language to language. In languages making little use of inflectional morphology such as English, this process is not much useful. It is nonetheless more useful in French, and even more in languages containing an abundance of morphological inflections, such as Finnish or Russian. In addition to lemmatization, words can be annotated into grammatical categories thanks to part-of-speech tagging, as we previously mentioned in relation to word annotations such as ferme and car. We will come back to this later in the chapter.

Finally, words can be annotated into semantic categories, for example, the word tennis can be annotated as a part of the sports category, later making it easier to quickly go through the content of a corpus, and to disambiguate polysemic words in context. Annotation also provides training and testing data for automatic word sense disambiguation. Indeed, this type of annotation is more difficult to perform automatically than part-of-speech tagging, since it requires conceptual knowledge in context and this is still a major challenge for artificial intelligence. For example, depending on the context, the word mouse may belong to the category of computers or animals.

In addition to words, sentences are also regularly annotated in corpora. The most common annotation is syntactic parsing. As soon as a corpus has undergone a part-of-speech tagging process, it is possible to parse it, and thus reveal how grammatical categories can be grouped into smaller phrases within a sentence. In traditional grammar, these syntactic representations often adopt the form of constituent analyses, as illustrated in Figure

(4) The teacher congratulates the student Due to their tree representations, parsed corpora are often called "treebanks". The most famous corpus containing such an annotation is the Penn Treebank corpus (see (

Apart from indicating constituent structures, some parsing analyses show the dependency relations between words or syntactic constituents. A dependency relation takes place when one word governs another. For example, in a noun phrase, the noun governs the adjective, which depends on it for receiving agreement marks. Dependency analysis was performed automatically on the Google Books corpus

Sentences can also be analyzed from the point of view of semantic relations between their constituents, as well as the thematic role of each of them. These roles include the agent, the patient and the cause. Finally, sentences may contain a pragmatic annotation of the speech act involved (e.g. a question, a request or a confirmation).

At a higher level, some annotations indicate the relations holding between sentences, or between discourse segments. In this case, it is rare to be able to rely on automatic annotation tools. For this reason, discursive annotations are most often the result of human annotators. These annotations include, for example, coreference relations, associating pronouns or noun phrases with their antecedent in discourse, as illustrated in (6). In this example, the pronoun he in the second sentence is coreferential with the noun phrase Fred. In other words, it is used for designating the same person. This annotation is useful since a pronoun like he is not an autonomous referential expression, meaning that it does not by itself make it possible to identify the referent in question if we ignore the context. ( Other discursive annotations can explain discourse relations holding between phrases or propositions within complex sentences, such as causality as in ( (7) Fred was very happy because he had won his tennis match. In French, the DEDE corpus

Finally, corpora including children's language or productions of foreign language learners may contain an annotation of errors. These annotations provide valuable clues to assess the nature and extent of the problems associated with each acquisition stage. Error annotations often include categories such as: incorrect word or structure, missing element, superfluous element or inflection/derivation mistake (see Standardization of annotation schemes

For all the annotations described in this section, international standards have emerged in the literature. When starting an annotation project, it is advisable to gather information about the existence of such standards and to consult previous studies to decide on the best way to annotate data. Whenever possible, it is preferable to reuse existing annotation schemes and to adapt them to the needs of the study rather than creating new schemes from scratch, for which comparisons with literature could be difficult to draw. One of the main initiatives, within the formal framework of the International Organization for Standardization (ISO), more specifically in technical committee no. 37 dedicated to language resource management (see the URL at the end of this book), has enabled around 20 standard drafts for linguistic annotation, be it lexical, syntactic, semantic or discursive in nature. For example,

In many cases, standards are established de facto, when a big project is successful, rather than through the creation of a formal framework. This is the case, for example, in the field of discourse relations. The taxonomy of relations developed for annotating the Penn Discourse Treebank

Finally, we should point out that the annotations described in this section are performed on entire corpora. Indeed, each word belongs to a grammatical category and every sentence communicates a speech act. These annotations imply a global processing of the corpus. Conversely, many other annotations made on corpus studies are devoted to a single type of element to be studied. For example, The stages of the annotation process

In this section, we will detail the different steps which outline the process for annotating corpus data. Before starting the annotation process itself, we first have to define the categories which will be annotated in the corpus. In practice, these will be represented by a list of tags or sometimes, by dependency relations. For example, the annotation of speech acts requires setting up a list of the acts to be annotated, for example, requesting, promising, asserting, threatening, etc. However, a semantic annotation of verb types could differentiate their aspect (state or event verbs). Finally, an annotation of the grammatical categories associated with each word requires establishing a list of these categories. Defining categories is not as simple a process as it may seem at first glance.

One of the problems is the need to find a good balance between tags which are specific enough so as to avoid unwanted amalgams, and categories which are sufficiently generic so that they can be reliably identified by annotators (see section 7.5). For example, should we make a generic category including all the determiners or should we use specific tags for articles (the, a, etc.), on the one hand, and the possessive pronouns (my, his, etc.), on the other hand? Should we make a generic category for conjunctions or a category for coordinating conjunctions and another one for subordinating conjunctions? There is no single answer to these questions, as the right level of granularity of a (more or less precise or generic) annotation depends, to a large extent, on the objectives of the annotation and its feasibility. It is possible that some sub-distinctions are not useful and can therefore be omitted for a certain research question.

For example, a corpus that is annotated into grammatical categories with the aim of being used as a reference tool for beginner language learners will have to use a simplified categorization of grammatical categories. Contrary to this, a categorization intended to be used as an entry point for a syntactic parser software should contain sufficiently precise tags in order to avoid incorrect syntactic analyses. Regardless of whether the annotations are made by software or by a human, it may also happen that some useful distinctions cannot be annotated in a reliable way, because they imply that many cases are ambiguous (see section 7.5). In this case, simplification becomes necessary.

While preparing the instructions for the annotation process, it is important for each category to be clearly defined so that the annotators know how to use them, in cases where the annotation is performed by humans and not automatically. For example, for annotating speech acts in a corpus, a tag like question could be interpreted very differently depending on the annotators, if no explanation is added. Some will only annotate interrogative syntactic forms as in (8) with this tag, whereas others will also include indirect interrogatives as in ( (8) Who will come to the party? (9) I wonder who will come to the party. (10) I am dying to know the guest list.

In order to avoid these problems, a set of tags should be accompanied by a list of criteria specifying in which contexts each tag should be used (see section 7.6). For example, the category question could be defined as follows for an annotation of speech acts: "Any utterance used in context as a request for information, whether a direct or indirect one". This definition would invite annotators to include examples (

In sum, the definition of annotation instructions largely depends on the linguistic phenomenon studied. In general, it is preferable to choose an annotation scheme as neutral as possible from a theoretical point of view and, in any case, to stick to categories clearly identified and widely accepted in the literature. The results of the annotation will be easily understood and it will be possible to reuse it in future work. As far as possible, the chosen tag sets should be based on annotation standards which have been broadly accepted in the literature. Should these standards be lacking, it would be a good idea to consider the annotation schemes used in previous studies. We should not rule out the point where innovations are desirable or even necessary, but these need to be clearly justified in relation to existing schemes.

Once the tag set has been defined, the corpus processing phase can begin. The first step is to identify which occurrences will be annotated in the corpus. For some phenomena such as the annotation of morphosyntactic categories or speech acts, every word or sentence in the corpus will be involved. However, for other phenomena, the annotation will only refer to very precise elements in the corpus. For example, an annotation of discourse markers such as well, you know and I mean will only relate to these elements.

Annotations covering the entire corpus are often made using computing tools, which we will describe in the next section. In the rest of this section, we will focus on a detailed annotation of specific elements in the corpus. For these annotations, a strategy should be deployed to identify relevant elements. In most cases, identification must be done on raw data, sowhenever possible -it is important to find one or more lexical elements that can be automatically identified by a concordancer. For example, discourse markers can be identified by looking for lexical elements encoding them as you know, I mean, etc. An annotation of cleft structures in French can start by looking up structures containing the verb form c'est. We have offered examples of this type of research in Chapter 2. In other cases, such research will benefit from a preliminary part-of-speech tagging or even from a parsing analysis performed automatically.

For example, in order to study causal relations in French, part-of-speech tagging makes it possible to only look for occurrences of car working as conjunctions and eliminating those which are nouns (a type of vehicle).

Whatever the strategy used for retrieving data, the results obtained will in most cases require validation and manual sorting in order to eliminate irrelevant occurrences. For example, occurrences of expressions like you know and I mean do not always correspond to a discourse markers. Sometimes, these expressions are also used in a compositional manner, as in (

Once the corpus to be annotated contains only the relevant occurrences, the annotation process itself can then begin. Whatever the annotation considered and the tag set chosen, the annotation of the first occurrences is generally difficult and many problems and borderline cases arise. It is often impossible to anticipate which dubious cases will appear since corpus data are always much more complex and ambiguous than the reference sentences found in dictionaries. Let us take an example to illustrate this difficulty. According to the Lexconn database (13) Je suis d'accord de le voir, dans la mesure où il vient seul. (I agree to see him, provided that he comes alone.) (

(15) Tel que je le connais -et je le connais bien -je lui fais confiance: ce mouvement ne va certainement pas s'arrêter et, dans la mesure où il ne s'arrêtera pas, il sera conduit avec habilité, M. Pujol et les autres ministres-présidents n'en manquent pas. (As much I know him -and I know him well -I trust him: this movement will certainly not stop and, as long as it does not stop, it will be led with skill, Mr. Pujol and the other Prime Ministers will see to it.

(16) Mes chers collègues, cette énumération résulte de vos propres interventions. Dans la mesure où on a même demandé l'insertion des familles "homosexuelles", je ne vois pas pourquoi vous protestez. (My dear colleagues, this list results from your own speeches. Insofar as we have even asked for the insertion of "homosexual" families, I don't see why you should protest.) (17) Dans la mesure où nous nous dirigeons vers l'adoption d'un nouveau traité, qui plus est constitutionnel, le dessein d'une Europe élargie et sa finalité doivent être mis en débat dans nos sociétés, sans quoi nous ne serions pas à l'abri d'un incident majeur. (Inasmuch as we are moving towards the adoption of a new treaty, which is a constitutional one furthermore, the aim of an enlarged Europe and its purpose must be debated in our societies, without which we would not be immune from major incident.)

These examples illustrate the complexity and the ambiguity of the real corpus data when compared to invented examples. For example, in (17), we may wonder whether the speaker takes it for granted that Europe is moving towards a new treaty (and therefore presents it as a cause) or whether, on the contrary, it is a condition for a treaty to be adopted. These difficult cases must be resolved by following systematic criteria guiding all decisions. For example, in (

Annotation criteria can only be fully determined on the basis of the cases encountered in the corpus, since an exhaustive list of problems is often difficult to anticipate. This is why it is wise to test the categories to be annotated on a small portion of occurrences, ranging, for example, from 50 to 100 depending on the difficulty of the annotation scheme and the total number of occurrences to annotate, then refine the criteria or even redesign the categories on the basis of this first annotation. For example, if an initial distinction proves impossible to discern sensibly in the corpus by the annotators, it should be abandoned. Conversely, if a category seems to cover too many disparate cases, it will have to be refined. Annotation should therefore start from theoretical criteria, but will be progressively modified depending on the nature of the data to be annotated, as illustrated in Figure Annotation tools

We can classify annotation tools depending on the automatic or manual processing that they bring into play. On the one hand, there are the tools making it possible to carry out annotations in an automatic way, for example, by means of part-of-speech tagging or parsing. On the other hand, there are tools that facilitate the process of manual annotation of data by providing an interface to perform the annotation, as well as a format for representing such annotations. In this section, we will briefly introduce some of these tools. We should beware that there are a very large number of them and that their more or less suitable character depends on the type of annotation the researcher has in mind. It will therefore be up to each researcher to identify the tool best suited to his corpus and his/her research question. Very often, a good starting point is to identify which tools have been used in similar studies in the literature.

The usefulness of part-of-speech tagging is such for corpus studies that this annotation is now directly embedded into some corpus creation tools. The Sketch Engine platform, which we presented in Chapter 6, automatically performs this tagging process when a new corpus is created and this can be done for several languages (see below). Since this platform enables you to create new corpora both from the Internet and from manually inserted files, it is very convenient to use it for carrying out part-of-speech tagging. The set of part-of-speech tags used by this program is presented in the documentation provided on the site. Let us also note that systems for annotating part-of-speech tags (POS taggers) should be carefully developed for each language and that the sets of tags corresponding to grammatical categories often vary from one language to another (in concrete terms, their notation may vary even more). However, there are also initiatives there to build up sets of grammatical categories, which if not identical, are at least compatible between languages in terms of fundamental categories (see, for example, the Universal Dependencies project).

The tags used may also vary depending on the corpora consulted in different languages, since the annotated corpora available on the Sketch Engine have not all been annotated in the same way. The list of tags used for a corpus is presented on the site. Let us also remember that due to these differences between languages, some of the tools offered online may only work for English. However, Sketch Engine makes it possible to tag a corpus in French as well as in many other languages. If necessary, this annotation can be corrected manually.

Once annotated, the elements of the corpus can be searched by their POS tags by means of a concordancer. For example, Sketch Engine provides the option to search by lemma or by grammatical category. In order to combine search criteria, the CQL query option (Cassandra Query Language) must be chosen. For example, to look for all the adjectives that are used with a form of the word acteur, the CQL query should take the following form: [lemma = "acteur"] [tag = "ADJ"]. This research specifies that all forms of the lemma acteur will be retrieved from the corpus when a word tagged as an adjective appears to its right. To find the adjectives used to the left of this word, the element order of the query must be reversed [tag = "ADJ"] [lemma = "acteur"]. The CQL syntax needed for formulating queries is documented in the search interface.

The annotation of syntactic dependencies can also be done automatically using computer tools, but so far these have not generally been included in the interfaces for creating corpora, and their use requires natural language processing (NLP) skills that go beyond this book. Interested readers will be able to use toolbox components designed for NLP such as GATE, NLTK or spaCy to make these annotations (see URLs at the end of the chapter).

Manual annotations can be done using tools that help processing and reusing annotations, without replacing the effort of human reflection when categorizing each occurrence. For example, Brat is an online tool that makes it possible to annotate not only entities in a corpus, but also the relations between them. We can thus annotate events described in a corpus, as well as the links between the various participants in these events. For example, a verb like invite in ( (18) Max invited Lili to his house for a meal.

All of these are part of the event (invite) and can be linked to it by an annotation describing the named entities involved and the semantic links between them. Being able to annotate relations is also essential for associating anaphoric relations in a text. The annotations made with Brat can later be used for research in the Sketch Engine.

Another example is the EXMARaLDA tool, which has been specifically developed to assist in the transcription and annotation of spoken corpora, and later be able to manage them. This tool makes it possible to perform a time-aligned annotation with audio or video files, to insert metadata and to have access to the results of the annotation in different formats.

Many tools, among which we can mention EXMARaLDA, use the XML format, which is a coding language (eXtensible Markup Language) for defining, and then storing and transmitting structured data. In this language, the elements of a text are marked up using named tags including one or more attributes. These elements can be embedded into each other depending on the needs of the structure.

For example, an element like word can be embedded into a sentence type of element. Taggers in XML thus provide a good way for associating annotations with corpus data. However, in order to be interpreted, a document marked up in XML coding must follow certain syntax, in the form of a document type definition (DTD), which specifies the list of tags with their potential attributes, as well as the possible order of appearance for tags. The most widely used XML schema for coding corpus data is the one provided by the TEI (Text Encoding Initiative). XML can also be used for encoding the metadata of a corpus (see Chapter 6, section 6.4). In those cases standards like the Dublin Core can be used. Tools have been developed in order to make data annotation in XML format easier, such as the Nite XML tool, for example.

For annotations of a specific linguistic phenomenon (in the situation presented at the end of section 7.2), one solution would be to retrieve the relevant data from the corpus, and then to annotate them separately. For example, we can retrieve all the discourse markers using a concordancer, and then annotate their function in a file. Concordancers generally make it possible to export the data retrieved in text format. However, it is afterwards essential to import the data into spreadsheet software rather than to annotate them in a simple word processor. Actually, a spreadsheet makes it possible to view the data as one occurrence per row in a single column. The following columns will be used for inserting the annotations. Thanks to the functionalities offered by the spreadsheet, the different annotations can then be counted, compared by means of a crosstab and whenever necessary, they can be inserted into statistical analysis software (see Chapter 8, section 8.6). Measuring the quality and reliability of an annotation

The notion of quality of an annotation is particularly useful for annotations made automatically, such as part-of-speech tagging. In this case, the quality of the automatic annotation is measured in comparison with a reference annotation produced by human annotators. This annotation is presumed to be completely correct, that is, it matches what has been deemed appropriate in the annotation scheme. It was by means of this type of comparison that the annotations offered for the Google Books corpus in section 7.2 were considered as suitable tagging and parsing systems. This definition of the accuracy of an annotation is often subdivided into two separate criteria. On the one hand, the recall measures the number of occurrences of each category correctly found by the system. For example, we can report the number of noun occurrences in the reference annotation that the system correctly identified as such, weighing the total number of nouns in the reference.

On the other hand, precision measures the number of occurrences properly tagged as nouns, from among all the ones tagged by the system as such. For every category, we can combine the recall and precision scores by calculating their harmonic mean (called F1 score), with identical or different weights. Finally, we can express the overall quality of the automatic annotation by calculating a mean of F1 scores per category, by weighing every category according to its number of occurrences in the reference (micro versus macro mean).

The reason for considering recall and precision at the same time (e.g. with the F1 score) is as follows, illustrated with the example of noun tagging. If a system had a strategy for annotating all ambiguous words as nouns (e.g. rain, break or close), its recall would certainly be excellent, but its precision would be very low, since the tag noun would probably be mistakenly assigned in many cases. Conversely, if a system annotates as noun only the elements that are preceded by a determiner, its precision would be much better this time, since the number of wrong taggers would be lower. However, its recall would be weak because it would fail to correctly tag all the proper nouns. Overall, part-of-speech taggers currently produce results with more than 95% accuracy in the most studied languages such as English and French. Residual errors are either corrected manually or in the case of very large corpora such as the Google Books corpus, they are left as such, because their prevalence is low enough not to significantly bias the results of a research.

While calibrating the performance of automatic tools is ultimately based on a reference human annotation, the question arises whether the annotations made by a human are necessarily 100% accurate. Most of the time, this is not the case. As we have seen, annotation always requires a certain type of interpretation and this may vary slightly from one annotator to another. This is why it is crucial to have annotations carried out by several annotators and to calculate the degree of agreement among them, as we will see below. In addition, humans can also make mistakes due to fatigue, lack of attention or a poor understanding of the annotation instructions. Finally, some examples are really ambiguous because the boundaries between certain categories may be blurred. To understand the ambiguity of such examples, one possibility is to enable the use of two tags simultaneously. The examples annotated in that way can later be treated separately in data analysis and can offer new insights on the nature of the categories thus defined.

In order to improve the reliability of annotations made by humans and to help define clear categories, a commonly used method is to have the same annotation made by two different annotators. The more convergent their annotation, the more it can be considered reliable and the divergent case can be resolved through a discussion a posteriori. Furthermore, this double annotation process will make it possible to indicate whether the categories have been poorly defined. As a matter of fact, if a certain annotation turns out to be impossible to achieve in a convergent manner for human annotators, this indicates that the phenomenon may be poorly defined or that the categories have been poorly delimited.

In order to measure the agreement between two annotators, a first solution is to count the number of times they choose the same tag and to deduce a percentage of convergent annotations. To do this, the two annotations must be compared as in Table

As an example, this table summarizes the annotation of the two possible connective functions for dans la mesure où we mentioned above, carried out by two annotators. Numbers indicate that the two annotators agree that the connective communicates a condition relation in 20 out of the 100 sentences they examined, and a causal relation for 60 other sentences. Conversely, there are 14 sentences where annotator 1 considered that the connective indicated a cause, whereas annotator 2 thought it indicated a condition, and 6 sentences in the opposite case (thus, the total of the sentences in the four situations is 100). In total, the annotators agreed in 80% of the cases. The problem with this estimate is that it does not indicate whether this agreement should be judged as satisfactory or not. What is more, it does not take into account the probability of an agreement which may be the result of chance in some cases.

To illustrate this problem, let us imagine that a first annotator always chooses the tag condition to annotate data that actually has 50% condition relations and 50% causal relations. His agreement with a second annotator who would actually analyze the sentences and annotate them correctly would still be 50%. If, however, the real proportion of condition relations in the corpus is 80%, the agreement obtained by chance would be 80%! A better approach is to use the kappa coefficient

In this formula, P(O) corresponds to the proportion of agreement observed during the classification task and P(E) corresponds to the statistical expectation of agreement. The value of P(O) is obtained by dividing the number of matching responses (classifications) by the total number of responses. The value of P(E) is obtained by calculating probabilities, by estimating the average of concordant classifications when the proportion of each class is fixed to the value observed for each annotator. Thus, if each annotator divides their responses into two classes of comparable sizes, P(E) will be close to 50%, whereas if the sizes are unbalanced in the two annotators, P(E) will also increase, since they will have more chances of annotating the most frequent class at the same time

The kappa coefficient may have a value oscillating between -1 and 1. A coefficient equal to zero means that agreement between the two annotators does not exceed the one obtained by chance. A negative value indicates an opposite correlation, obtained when annotators disagree the most. The level of agreement is then lower than the one which could be obtained by chance and the value of P(O) is close to 0. Conversely, the maximum value K = 1 indicates that annotators always agree, the value of P(O) then being 1. So, in practice (for a task that is somewhat coherent), the kappa coefficient generally varies between 0 and 1, that is, between results that can be obtained by chance and total agreement. This does not mean, however, that any coefficient above zero is a good result. According to

The kappa coefficient can be calculated automatically on online statistical calculation sites like VassarStats, which we will discuss in further detail in Chapter 8. In the case we presented in Table

Finally, we should point out that for some research projects, it is not possible to engage two annotators for the whole annotation task. Several solutions are then possible. First, to double annotate a small part of the data to verify that the scheme can be applied convergently, and then to have the rest of the data annotated by one person. Another method is to test the agreement of an annotator with themselves over time. For this, it is possible to re-annotate the same portion of data after a certain time interval, in order to measure the degree of convergence between the two annotations. Sharing your annotations

Given the importance of the effort invested in an annotation process, many researchers choose to share their annotations, which may benefit other researchers. As we have seen in this chapter, in order to make sharing and reusing of annotations easier, it is important to use categories that are as theoretically neutral as possible. It is also important to be able to export annotations in a standardized format, based on the XML language, for example.

Sharing annotated data also implies that the annotation process and the categories used are clearly documented in an annotation manual, which will be provided to future users together with the data. This manual should allow them to understand and to reuse annotations. For this to be possible, such a manual should include two types of information. First, a list of the tags used with their definition, in the way of a mini-glossary. For example, such a list for an annotation of speech acts could take the form illustrated in (

The annotation manual should also list the rules that have been applied to certain borderline or ambiguous cases in the corpus in order to deal with them systematically. It should also provide information on the corpus processing that preceded the annotation process. For example, it should specify the manner in which the corpus has been segmented into words, sentences, utterances or discourse segments. A form of segmentation is actually a necessary step for any analysis relating to a certain linguistic level and such a segmentation process also poses difficulties which must be resolved. For example, does segmentation into words also include compound words? Does the definition of discourse segments include sentences without verbs? As we saw at the beginning of the chapter, these decisions influence the annotation process and must be clearly documented. Finally, when automatic processing tools have been used for preparing and (helping to) annotating the data in the corpus, they must be clearly indicated.

Likewise, revisions that have been made at different stages of the annotation process must be documented, as well as successive versions of the corpus that have been produced, if applicable. Many examples of annotation manuals can be found online and are provided with all the corpora made available to the public. Conclusion

In this chapter, we introduced the notion of linguistic annotations and discussed the different ways in which these annotations can be incorporated into a corpus. We first reviewed the different types of annotations, covering very diverse elements ranging from phonemes to discourse relations. We then detailed the different stages that make up an annotation process and stressed the importance of good methodological practices, so that the annotation is as valid and reusable whenever possible. We then briefly presented some tools that let you to automatically make annotations or guide manual annotations. We have also seen that these tools often work in standardized formats like XML, which makes it easy to share annotations. We have also addressed the problem of the quality and reliability of annotations and argued that in the case of human annotations, it is difficult to define a quality standard, since each annotator partially interprets the data while annotating them.

However, an annotation can be tested from the point of view of its reliability. An annotation is reliable if two annotators produce convergent annotations or if the same annotator produces convergent annotations at two different times. Finally, we presented some recommendations for the creation of an annotation manual, which documents both the content and the annotation process itself in order to enable other researchers to reuse it. 7.9. Revision questions and answer key 7.9.1. Questions 1) Explain three advantages of annotating corpus data. 2) Imagine three examples of corpus studies for which part-of-speech tagging is necessary.

3) In the Frantext corpus powered by Sketch Engine, look for the nouns orange and nage, using annotation into grammatical categories. How many unwanted occurrences (adjectives and verbs) does this method allow you to eliminate? 4) Go back to the French cinema corpus that you created in Chapter 6 using Sketch Engine. Use the CQL option to search for adjectives that are used with the words actrice and film. 5) Using the AntConc, retrieve the occurrences of the connective "dans la mesure où" in the Advanced Literacy corpus available on Ortolang (see Chapter 5). Export the first 20 occurrences to a spreadsheet and annotate them with the "cause" or "condition" tags. Ask a second person to make this annotation and calculate the percentage of agreement and the kappa coefficient. If there is no second annotator, annotate occurrences a second time a few days later. 6) Create a mini manual to document the annotations made in 5.

What should it contain? Answer key

1) The first advantage of annotated corpora is that they make it possible to answer many more research questions than raw corpora. Indeed, whenever the words to be looked up are ambiguous or the phenomenon to be investigated cannot be approached, by surface features such as words, raw corpora reach their limits. Annotated corpora, however, make it possible to look for elements in the corpus related to syntactic or semantic annotations, for example. The second advantage of annotated corpora is that annotations can be reused later and thus add value to a corpus. This advantage is all the more evident inasmuch the same annotation can often be used for answering different research questions. Finally, the annotation of a corpus represents a transparent process when it is well documented, which enables an objective evaluation of the data by the scientific community, contrary to other methodological choices which are often less transparent.

2) Part-of-speech tagging is useful for almost all corpus studies, be they lexicon, syntax, semantics, pragmatics questions, etc. A first example in relation to lexicon concerns the study of all polysemic words, for which part-of-speech tagging makes it possible to sort a good part of the irrelevant occurrences (e.g. to differentiate the adjectival uses of past participles). Since most of the words in the non-specialized lexicon are polysemic, this annotation is essential. In the field of syntax, every analysis should start at classifying words into grammatical categories. It is precisely from these categories that syntactic trees are created. Finally, a search for speech acts communicated by performative verbs such as ask, request and promise can be made easier through the use of POS taggers (as well as lemmatization), since it makes it possible to perform searches for first-person occurrences of the present, which are used for communicating performatives.

3) In Frantext, looking for the noun orange yields a result of 139 occurrences, whereas the same search but without specifying the grammatical category results in 174 occurrences. Searching by category therefore makes it possible to eliminate 35 adjectival occurrences, or 20% of errors. The noun nage results in 154 occurrences, compared to 252 occurrences if classification into categories is not performed. This research therefore eliminates the 98 verbal occurrences, corresponding to 39% of errors.

4) In order to look for the adjectives that are used with the word actrice, it is necessary to make a search for occurrences to the right and another one to the left. The syntax for these queries is: [word = "actrice"] [tag = "ADJ"]. This research focuses on the word rather than on the lemma in order to exclude the masculine occurrences, acteur. The adjectives to the right mainly include nationalities such as française, américaine, italienne, australienne and allemande. The search for adjectives to the left of the word actrice must use the following syntax [tag = "ADJ"] [word = "actrice"]. These adjectives are narrower and list modifiers like meilleure, grande and pire. For the word film, the search can focus on the lemma rather than on the word, in order to identify singulars and plurals. The syntax is therefore: [lemma = "film"] [tag = "ADJ"]. To the right, the results are varied and include adjectives such as super, romantique, dramatique, important, érotique and indépendent.

The search for adjectives to the left, with the syntax [tag = "ADJ"] [lemma = "film"] results in the adjectives meilleur, prochain, nouveau and seul. 5) Here in the table below is a list of 20 occurrences drawn from the corpus with two possible annotations. This writing is original insofar as the child owns the character of Perla, the blue macaw of the animated film Rio, as well as one of the places of the film, Rio's forests. Cause Cause

La consigne est respectée, mais Adam se démarque de ses camarades et de l'auteur dans la mesure où l'enfant et l'animal ne restent ni ensemble ni amis à la fin.

The instructions were followed, but Adam stood out from his comrades and the author insofar as the child and the animal did not stay together or friends at the end. Cause Cause Cet exercice semble donc favoriser le statut d'auteur de l'élève, dans la mesure où celui-ci est libre de choisir ses personnages et les événements de son récit. This exercise seems to favor the student's author status, insofar as the latter is free to choose his characters and the events of his story. Cause Condition Il est nécessaire d'en passer par ce genre d'exercice, dans la mesure où celui-ci est important dans l'apprentissage littéraire des élèves. It is necessary to go through this kind of exercise, insofar as it is important for students' literary learning. Condition Cause

Le travail de groupe est un choix pédagogique difficile dans la mesure où l'enseignante a dû faire attention à ce qu'aucun élève ne soit laissé pour compte au sein du groupe. Group work is a difficult pedagogical choice insofar as the teacher had to take care that no student was left behind in the group. Cause Cause

La contrainte peut certes freiner dans certains cas l'imagination, mais elle est utile dans la mesure où elle permet une progression, puisqu'elle place le travail de l'écriture au premier plan, avant l'inspiration réelle.

In some cases, the constraint can certainly slow down the imagination, but it is useful insofar as it favors progression, since it sets the work of writing in the foreground, before actual inspiration. Condition Cause En effet, l'imagination créatrice apparaît comme synonyme d'audace ou de tentation, dans la mesure où elle embellit le quotidien et entretient l'espérance. Indeed, the creative imagination appears as synonymous with daring or temptation, insofar as it embellishes everyday life and maintains hope. Cause Cause

Il y a une impossibilité de l'approfondissement de l'imaginaire, dans la mesure où l'école attend de l'élève qu'il évolue dans une certaine direction, au niveau du langage tout autant qu'au niveau psychique.

There is an impossibility of deepening of the imaginary, insofar as the school expects the student to evolve in a certain direction, at the language level as well as at the psychic level. Cause Cause Le travail de groupe constitue un choix pédagogique difficile dans la mesure où il est compliqué de faire en sorte que chaque élève soit auteur. Group work is a difficult pedagogical choice since it is complicated to ensure that each student is an author. Cause Condition

En conclusion, nous pouvons dire qu'il est possible de considérer que l'élève est auteur dans la mesure où les écrits l'impliquent en tant qu'enfant, faisant ainsi appel à ses sentiments, son vécu, mais aussi à des choix d'écriture et à son expérience personnelle de l'écrit.

In conclusion, we can say that it is possible to consider that the student is an author insofar as the writings engage him as a child, thus appealing to his feelings, his experience, but also to writing choices and his personal writing experience. Writing in young students can also be a source of reading insofar as the response to a writing project generates new targeted readings. Condition Condition

In order to be able to measure agreement, we must create a crosstab with the two annotations. In the Excel spreadsheet, this can be done automatically by choosing the data and choosing the pivot table option. In this table, annotator 1 should be recorded on the rows and annotator 2 in the columns. For the annotation above, this table looks as follows: The two annotators converge on 13 of the 20 sentences, that is, there is 65% of agreement. This agreement corresponds to a kappa coefficient of 0.2. This value is very low and shows that the annotation is not reliable and should be revised. 6) A manual documenting of this annotation should contain the list of tags as Condition and Cause, together with their definition. It should also contain the rules to follow in ambiguous cases, for example: "The condition relation will only be chosen if the verbal tense used is the conditional tense".

This manual should also indicate how the annotation was carried out (by one or two people, throughout how many sessions) and whether successive versions were produced. Further reading McEnery How to Analyze Corpus Data

Quantitative corpus studies produce numerical results such as word frequency or the number of occurrences of a given syntactic structure across different text genres. By using statistical tests, it is possible to analyze these numerical data and reveal tendencies that are not always visible to the naked eye, and to evaluate whether or not the observed trends are statistically significant. The purpose of this chapter is to introduce some simple statistical methods that are commonly used for processing corpus data. To begin with, we will introduce the concept of descriptive statistics. Then, we will present two types of descriptive statistics that, respectively, make it possible to measure lexical diversity (the type/token ratio), and to calculate lexical dispersion in a corpus. We will later describe the principles underlying inferential statistics. In addition, we will see that data from corpora correspond to different types of variables, and that such categorization is important in view of deciding which statistical test should be used.

Finally, we will illustrate the use of inferential statistics by presenting a commonly used test in corpus linguistics, namely the chi-square test, which determines whether frequency differences between categories are significant. Descriptive statistics for corpus data

The first step for analyzing quantitative data is to describe the content of a corpus. For example, this step could involve calculating the number of sentences in the passive voice used in a corpus of journal articles. Imagine that, in this study, the passive sentences were retrieved from 10 texts from different newspapers, resulting in the figures reported in Table

homogeneity. The standard deviation can be calculated automatically thanks to a dedicated function in spreadsheet software (e.g. Excel). This measurement can also be automatically calculated in software devoted to statistical tests (see URLs at the end of this book). Another measure of central tendency is the median, placed at the middle of the different values found in the corpus, so that the data set is divided into the lower half and the upper half. In the case of our example, which includes 10 values ranked in ascending order, the median is equidistant from the fifth and the sixth value, that is, between 78 and 89. Its value is 83.5, the mean of the two intermediate values. When the number of values is an odd number, the median is simply the value in the middle. What is interesting here is that even if the value of 120 in text no. 10 was replaced by 1,200, the median would be the same as before.

We can therefore see that the median is more appropriate than the mean for summarizing data in the presence of extreme values.

Before performing descriptive statistics on the data obtained in corpus studies, it is necessary to make the number of occurrences comparable, and this can be achieved through the use of different sources. For example, in order to compare the number of passive sentences in the above-mentioned 10 texts, it would have been necessary to ensure that all the texts had a comparable number of words. Indeed, we can observe that text no. 10 contains almost three times more passive sentences than text no. 1. If text no. 10 were roughly three times longer than text no. 1, the two texts would be equivalent in terms of the proportion of passive sentences. If this were not the case, then the two texts would be different. As a result, when the sources are of variable length, which is generally the case, it is necessary to transform the number of occurrences into relative frequencies, so that they can be easily compared.

Let us take another example to illustrate this point. Imagine a study aiming to compare the use of passive sentences in spoken and written modes, and finding 67 occurrences of passive sentences in the spoken corpus versus 3,372 in the written corpus. If these figures come from corpora of different sizes, for example a written corpus of 3,179,546 words and a spoken corpus of 573,484 words, they cannot be compared directly. As a matter of fact, a larger corpus increases the probability of finding more occurrences of a phenomenon. Thus, in order to determine whether passive sentences are used more frequently in the written than in the spoken form, it is necessary to transform the data according to the same base of normalization, for example the number of occurrences per 10,000 words, per 100,000 words or even per million words. To turn a number of occurrences into a relative frequency, we need to apply a rule of three, by dividing the number of occurrences found in the corpus by the total number of words in the corpus, then multiplying by the base of normalization, as shown in the example below, which has a base of normalization equal to 10,000.

This (fictitious) comparison indicates that passive sentences are in fact almost 10 times more frequent in written than in spoken discourse: Normalization by means of relative frequencies makes it possible to compare data from different corpora. The choice of the appropriate base of normalization depends on the size of the two corpora. For small corpora of a few tens of thousands of words, a base of normalization set at 1 million words would not be appropriate. If a word appears five times in a corpus of 15,000 words, it would not be wise to conclude that this word would have a frequency of 500 occurrences in a corpus of 1,500,000 words. In fact, the behavior of rare words varies a great deal according to the genre, and even from author to author. For this reason, extrapolating a frequency, obtained on the basis of a small corpus, to a much larger scale does not offer a correct representation of what was actually observed.

In general, it is appropriate to choose a base of normalization close to the size of the smallest corpus examined. However, even if it is necessary to normalize data in order to be able to compare corpora, such normalization cannot completely replace raw data, which should also be reported in order to communicate what has actually been observed. For example, raw data can be presented in brackets in a table next to the relative frequencies, as shown in In the text of an article, the two frequency elements can be reported as follows: "There were 94 occurrences of parce que in the corpus, corresponding to a relative frequency of 3.61 occurrences every 10,000 words". Finally, we should observe that, in other cases, normalizing data may involve other methods, such as transformation into percentages. For example, Measuring the lexical richness of a corpus

A useful application of descriptive statistics for corpus data is to calculate the lexical richness of a corpus. In Chapter 7, we saw that the notion of word encompasses different realities, depending on whether we adopt the point of view of morphology (grouping up lemmas) or semantics (which focuses on lexemes). There is yet another distinction that needs to be applied to the concept of word, since it is fundamental for corpus linguistics. This notion differentiates word types from word occurrences. For example, think of a teacher asking you to hand in an assignment of "10,000 words". To complete this task, you will count each character string one after the other to verify that you have reached the total requested. For example, according to this definition of "word", there are 12 words in the French sentence in (1). This definition corresponds to what we call word occurrences. (1) La mère de Jacques est plus jeune que la mère de Pierre. (Jacques' mother is younger than Pierre's mother.)

Other times, however, we refer to a word not to designate the total number of character strings, but the number of different words that a corpus contains. For example, if your teacher informs you that you have to learn a vocabulary of 500 words in a foreign language by the end of the year, these are obviously different words. So, according to this second definition of word, there are only nine different words (or word types) in sentence (1), namely: la, mère, de, Jacques, est, plus, jeune, que, Pierre.

Notions such as word type and word occurrence are both very important in corpus linguistics. The size of a corpus is calculated in word occurrences, whereas the number of word types indicates the diversity of the vocabulary used in the corpus. In order to measure the lexical richness of a text, it is customary to calculate the ratio between the number of word types and the number of word occurrences, according to the following formula: number of word types in the corpus type/token ratio = number of word occurrences in the corpus

The greater the type/token ratio, the more lexically diverse the text is. Generally speaking, written genres have a higher ratio than spoken genres. For example, in the spoken corpus of French from French-speaking Switzerland OFROM (see Chapter 5), the type/token ratio is 0.02, whereas in the French section of the children's literature corpus ByCoGliJe, it is equal to 0.04.

The problem with the type/token ratio is that it is very sensitive to the size of a corpus (for this reason, the type/token value 9/12 = 0.25 obtained for sentence (1) is in no way representative). The comparison between the two corpora that we have just presented could be reliably established since they are similar in size (around a million words), but this ratio may offer biased results for corpora of very different sizes. Indeed, the larger the corpus, the more the words end up repeating themselves and, therefore, the ratio decreases all the more. For example, the type/token ratio in the Sciences Humaines written corpus that contains around 260,000 word occurrences is 0.06, but it drops to 0.007 for the 2012 installment of the Le Monde corpus, which has over 26 million word occurrences. We must therefore avoid using this type of measurement on corpora of different sizes. An alternative solution would be to divide the corpora into segments of equivalent size (e.g.

1,000 words), then to measure the ratio for each segment and, after that, the mean corresponding to the different values. Measuring lexical dispersion in a corpus

So far, we have discussed how to calculate and report word frequency in a corpus. However, in order to estimate the importance of a word in a corpus, frequency is not the only element to take into account. For example, in a corpus containing scientific articles, the word linguistics may appear relatively frequently, due to the fact that a portion of the corpus is devoted to this field. However, this word will probably never be used in other texts in the corpus covering different fields, unlike words such as analysis, hypothesis or conclusion, which will probably be used in all scientific fields. Let us consider another example. In a small corpus of texts produced by 20 French-speaking students, the word nonobstant appears more frequently than in other language registers such as journalistic texts, but this higher frequency is due to the propensity of a particular student to use this word very frequently.

In this case, it would be inappropriate to conclude that the word nonobstant is used more frequently in student texts than in newspaper articles, as this high frequency does not reflect a common practice among students. In order to avoid this type of bias, in addition to word frequency, it can be useful to calculate lexical dispersion in a corpus. This parameter reflects the way in which word occurrences are distributed across the different portions of the corpus.

There are different ways to calculate lexical dispersion in a corpus. One of the simplest ones is to count the number of portions of the corpus in which the word is present. For example, for a corpus made up of texts written by 20 students, if the word nonobstant is used by three of them, we can say that its dispersion is 3/20, or that this word is found in 15% of the corpus. Although this measure may be useful in some cases, it is not entirely satisfactory, since it does not reflect the proportion in which this word has been used throughout the three portions of the corpus. If, in this case, one of the students produces 80% of the occurrences and the other two students produce 10% each, it would be inappropriate to conclude that the distribution is homogeneous in the 15% of the corpus where this word appears.

For this reason, other, more sophisticated, measures have been developed. Here, we will discuss only one of them, known as deviation of proportions (DP), which was proposed by

In order to determine the expected proportion, we must count the total number of word occurrences in each portion of the corpus separately, and then divide this number by the total number of word occurrences of the corpus. The proportions observed are calculated by taking into account the total number of occurrences of the word, whose distribution we are trying to determine in each portion of the corpus, and dividing it by the total number of occurrences of the word in the corpus.

In order to find the difference in proportions, we then have to subtract the expected proportion from the observed proportion for each portion of the corpus. All of the figures obtained for each portion of the corpus are then added, and divided by 2. The procedure that we have just described is summarized in the following formula, where  refers to the sum: values observed proportions proportions Deviation of proportions

Let us work with an example to illustrate this procedure. Suppose we want to know whether the connective car was used consistently by the 10 students who compiled a dossier in the Littéracie avancée corpus (L2_DOS_SORB sub-corpus). In order to find out, first, it will be necessary to start by counting the number of word occurrences produced by each of the 10 students separately, as in the column "Number of word occurrences" in Table Student Number of word occurrences Expected proportion of car Absolute frequency of car Observed proportion of car Difference in proportions

However, the dispersion measure only becomes truly informative when it is compared to the one obtained for other words. For example, the same calculation indicates that puisque has a deviation of proportions equal to 0.45 and parce que a deviation of proportions equal to 0.43. Car seems to be the connective that is used most consistently by students. The homogeneity of puisque and parce que is also very similar.

In summary, in addition to frequency indications, it is important to provide information about lexical dispersion in a corpus, either by simply calculating the percentage of texts in which a word is used, or by reporting a dispersion measurement, such as the deviation of proportions we have just described. Basics of inferential statistics

So far, we have seen that corpora compile linguistic data collected from texts or recordings. These corpora do not contain all possible linguistic data but represent their samples. The primary goal of research in corpus linguistics is to be able to draw conclusions in relation to a linguistic phenomenon, not only at the level of the observed data sample, but also to generalize it to all data of the same type. To do this, it is necessary to use inferential statistics, a tool that makes it possible to know whether it is correct to generalize the results found, at the level of a sample, to a population.

Before going further in the description of the logic of inferential statistics, we should focus for a moment on the notion of hypothesis, which is the basis for statistics. There are different ways of making hypotheses. We could imagine the following hypotheses in [1] and [2]: [1] French is a more beautiful language than English. [2] Children produce few passive sentences.

These hypotheses are interesting but cannot be tested empirically, since it is impossible to collect data that would allow them to be refuted. In fact, hypothesis no. [1] is a subjective value judgment, whereas hypothesis no. [2] has been formulated too vaguely to be empirically testable.

Empirically testable hypotheses must clearly define the variables observed, the relationship between these variables, as well as the measures used for describing them. For example, hypothesis no. [2] could be transformed into no. [3] H 1 : Children aged between 5 and 8 years produce fewer passive sentences than those aged between 9 and 12 years.

In an inferential statistics test, it is not research hypotheses like [3] that are tested but alternative assumptions called null hypotheses. A null hypothesis focuses on the opposite fact to the one we wish to prove. The null hypothesis for [3], written as H 0 , is [4] H 0 : Passive sentences are used with a similar frequency by children aged between 5 and 8 years and those aged between 9 and 12 years.

The reason why statistical tests assess the null hypothesis and not the research hypothesis itself arises from the philosophical argument that it is not possible to prove that a hypothesis is true in all cases (be they observed or not), whereas it is possible to prove that it is false by presenting a single case when this exists (counterexample). An inferential statistics test provides two important values:

-the numerical result of the test, such as the chi-square value (see section 8.5); -the probability value associated with the result, denoted as p. The value of p can oscillate between 0 and 1. It expresses the probability of the sample data being observed if the null hypothesis were true in the population. Let us go back to the example of the translations of néanmoins and toutefois, presented in Table

The null hypothesis can be rejected when the value of p is smaller than a certain value. In some fields such as medicine, only a margin of error of 1% is tolerated, and p is only considered significant when it is smaller than 0.01. In other fields, some researchers choose a higher confidence threshold and accept a 10% margin of error (which corresponds to a p value equal to 0.1). In this chapter, we will stick to the 5% (0.05) value, which is the most frequently used in the social sciences.

In summary, the result of a test is said to be significant when p is smaller than 0.05. This amounts to saying that in the case where the null hypothesis is correct, the observed difference (or one greater) might be obtained less than 5% of the time. In the case where the value of p is greater than 0.05, the results obtained do not make it possible to confidently reject this conclusion and the null hypothesis. However, this does not mean that the data prove the null hypothesis (following the logic that which has not been proven to be false must necessarily be true). An insignificant test does not mean that the research hypothesis is false, which in itself would be an interesting result. The only adequate conclusion for an insignificant result is that there are not sufficient elements present in the data to be able to reject the null hypothesis.

Finally, an important distinction that must be made in order to understand inferential statistical tests is the difference between one and two-tailed tests. Two-tailed tests are used when the direction of the difference between two groups or two categories has not been specified by the research hypothesis. For example, if the research hypothesis tested is that the connectives néanmoins and toutefois are not used with the same frequency in two text genres, but we do not know which type of text is supposed to contain more connectives than the other, then a two-tailed test should be carried out. However, if the way in which the groups differ is specified in the research hypothesis, as in no. [3], stating that older children use more passive sentences than younger children, then a one-tailed test is preferable. Typical variables in corpus studies

There are different inferential statistical tests, whose application depends on the type of variable observed. Before describing these tests in depth, we should succinctly refresh the concept of variable and the different forms a variable may take. A variable simply designates something that can display different values. For example, the number of adjectives found in different corpora is a variable. Likewise, the gender of participants in a corpus, or their geographical origin, are also variables. In the literature, types of variable may receive different names. Here, we will follow the approach proposed by

Linguistic variables are related to frequency elements measured within the corpus, such as the number of adjectives or the number of speech acts like requests or orders, or the number of negative prefixes such as un-or disfound in a corpus, to mention just a few examples.

Explanatory variables are related to the context in which linguistic data are produced. These variables can include gender and geographical region, as we have already seen, and also the textual genre or form (spoken or written discourse), the language in the case of comparable corpora, and the age or language proficiency level in the case of children and learner corpora. In short, these variables gather all the necessary elements for analysis, which are not the linguistic data themselves. The two categories of variable we have just introduced can be measured through different scales: -nominal; -ordinal; -scalar.

Nominal variables involve values corresponding to different categories, which have no numerical value. For example, a word's grammatical category is a nominal variable that may acquire values such as noun or adjective, among others. The function we assigned to the connective dans la mesure où in Chapter 7 is another example of a nominal variable having two possible values, namely cause and condition. In these examples, nominal variables fall into the category of linguistic variables. Another example of a nominal variable, this time an explanatory one, could be the region of origin of the speakers of a spoken corpus, for example, Paris, Geneva, Brussels, Montreal. In some cases, nominal variables are coded using numbers, for the sake of simplicity, for example 1 for Paris, 2 for Geneva. However, these numbers are simply category codes and their numerical values have no particular meaning.

Ordinal variables are also used for grouping values into different categories, but they have a natural order. For example, if the participants in a corpus are classified into age groups such as group 1 (18-25 years), group 2 (26-40 years) and group 3 (41-60 years) for sociolinguistic analyses, the age variable is an ordinal one. So, here we can see that it is possible to order the different groups, since the participants in group 3 are older than those in group 2, who in turn are older than those in group 1.

In the previous example, while age groups can be ordered as such, it is not possible to do so with the different participants within each group. The variables that can be ordered in this way are called scalar variables, measured on a digital scale, whose points are distributed at equal distance and with a zero point. The property of these variables is that operations such as addition, subtraction, multiplication and division can be performed on them, since they represent measurable quantities, rather than a simple ordered list, as ordinal variables are. For example, if measured continuously rather than in groups, the age of participants can be considered a scalar variable, since the distance between 12 and 13 is the same as between 13 and 14. We can also say that a 28-year-old is four times older than a 7-year-old child. Linguistic variables involving the relative frequency of some kind of phenomenon in a corpus are also scalar variables.

Since different types of variables have different properties, they cannot be processed in the same way. For example, let us imagine that we have collected information concerning the nominal variable mother tongue from the speakers in a corpus, and that we have 36 French-speaking people and 40 German speakers. As such, the only type of information we could report is the frequency with which every variable condition appeared in the data. However, if we had 20 speakers aged 30 years and 20 speakers aged 32 years, we could say, more than on average, the participants are 31 years old.

A detailed description of the suitability of the different types of variables for specific statistical tests is beyond the scope of this chapter, but it is important to know that statistical tests can be classified into two main categories: -parametric tests; -non-parametric tests.

Parametric tests can be used when the linguistic variable under consideration is measured on a scale, and the explanatory variable is measured following a nominal or an ordinal criterion. These tests include T-tests, used when the explanatory variable has two categories, and ANOVA -Analysis of Variance -used when the explanatory variable has more than two categories. For example, this type of test can be used in studies seeking to find out whether advanced learners produce significantly more discourse connectives than intermediate learners, or whether speakers produce more fallacious arguments when speaking at political party meetings or at electoral campaigns. Such tests will not be discussed in this chapter since they are rarely used in corpus linguistics, but you can refer to

When the linguistic variable is measured on a nominal or ordinal scale, it is necessary to turn to non-parametric tests, such as the chi-square test (or chi 2 ). For example, in order to determine whether type of speech acts varies between two text genres, it is necessary to carry out a chi-square test. It is this test that we will present and illustrate in the rest of this chapter. Scalar Relative frequency of passive sentences in a corpus Age of speakers (measured along a continuum) Measuring the differences between categories

In this section, we will focus on how to report frequency differences between categories in order to determine whether these differences are significant, and then to establish a link between two nominal or ordinal variables, for example, whether the value of a nominal or an ordinal variable depends on another nominal or ordinal variable.

Let us imagine that we are conducting research on the use of regionalisms in French. For example, it is widely known that some French-speaking regions such as Switzerland and Belgium use different words, from those used in France, for designating numbers such as 70 and 90, in this case septante and nonante. The situation seems a little more complicated in the case of 80, which is expressed as huitante in some Swiss cantons and quatrevingts in others. According to a recent linguistic atlas of regional French

Different questions can be studied concerning this situation. First, we should check with corpus data whether the use of huitante varies from one group of cantons to another. To do this, we have to look for the number of occurrences of the word huitante in each of these six cantons by means of the OFROM corpus, which compiles the French spoken in Switzerland (see Chapter 5). Then, we have to group the results in a table such as Table

In order to test whether this difference is significant, we use a statistical test called the chi-square goodness-of-fit test, represented by the symbol  2 due to the notation denoted by the Greek letter that gave it its name.

The  2 is actually a value calculated according to the following formula, where  denotes the sum of all categories considered: This value makes it possible to know the probability of such results actually being observed, if the occurrences were randomly distributed along the variable categories (which would then represent the null hypothesis). The higher the value of  2 , the more it suggests that the distribution observed moves away from the expected one if the distribution is uniform. The value obtained should be compared with a critical value of  2 , which in this case depends not only on the field (which establishes the significance threshold) but also on the number of degrees of freedom (often indicated as "df"). The number of degrees of freedom is calculated by subtracting 1 from the number of categories in the explanatory variable. For a variable with two categories, as we have here, the  2 test is significant with a threshold placed at 0.05 if the value of  2 is greater than 3.84.

Thanks to the corpus data considered, we can deduce that (at 5%) the use of huitante is almost certainly associated with a group of cantons, which corroborates the assertion made on the atlas we quoted.

To apply the  2 test, we have to calculate the  2 coefficient value, which can be done manually, as we did here, or by using statistical software. The value can also be calculated online using on the VassarStats site, by choosing the option "Chi square goodness of fit" accessible in the "Frequency data" tab. To do this, in this example, the frequencies observed (Table

The  2 can also be calculated for variables with more than two categories. For example, we might want to determine whether the use of huitante varies between the three cantons that are supposed to use this word, namely Vaud, Valais and Freiburg. This time, we should type the observed values (Table

However, through the observation of the data described above, we cannot yet know whether the cantons vary in their frequency of use of the two regional words huitante and quatre-vingts. To find out, we should draw a comparison between the use of huitante and quatre-vingts in the two groups of cantons. To do this, we have to count the occurrences of huitante and quatre-vingts and relate these to the two cantonal categories in a crosstab.

To build a crosstab, we only have to place one of the variables in columns (often the explanatory variable) and the other variable in rows (the linguistic variable). For the question we are focusing on here, Table

A representation of percentages in relation to the grand total implies a total transformation of the data, in which each cell is divided by the grand total (132). The data thus transformed are not intrinsically more informative than those in Table

The expected value for a cell in the table is calculated by multiplying the total of its row and that of its column, and then dividing the result by the grand total. For example, the expected value for the first cell in Table

As in the  2 goodness-of-fit test, this value should be compared to a critical  2 for a threshold conventionally fixed at 0.05, and the appropriate number of degrees of freedom. In the case of the crosstab, the number of degrees of freedom is calculated according to the (L-1) (C-1) formula, where L corresponds to the number of rows of the table and C corresponds to the number of columns.

In this example, the number of degrees of freedom is equal to (2-1) (2-1), that is, 1. The critical value is 3.84 once again, so the result of the test is significant. This indicates that the distribution of the words quatre-vingts and huitante between the two groups of cantons is not random, but clearly reflects a real difference in linguistic practices between them.

The value of the  2 test of independence can also be calculated using statistical software, or directly online using the same site mentioned previously. This time, on VassarStats, choose the option "Chi square, Cramer's V and Lambda" which is accessible on the "Frequency data" tab, enter the values observed in the table and choose the option "Calculate". Here is what the VassarStats site retrieves for the values in Table

The indication "Cramer's V" is a measurement of the size of the observed effect. This is an indication of the degree of relation between two variables. The result of the  2 test provides only one answer to the existence of a link between two variables, but not to the importance of this association. For a crosstab with one degree of freedom (2x2), we consider that the effect is small when the value of Cramer's V reaches 0.1, moderate when the value reaches 0.3 and large when the value is 0.5 or more

Let us then imagine that we wish to know whether the use of the words huitante and quatre-vingts varies specifically between the three cantons that use the word huitante significantly less, namely Neuchâtel, Jura and Geneva. In order to find this out, if we enter the figures in Table

The suitable test is called Fisher's exact test, which can be calculated using the same online statistical toolboxes as the  2 test. This test offers only a probability and not a statistical value because it is an exact test. In this case, the value of Fisher's exact test is p = 0.29, which indicates that the distribution between these two words does not vary significantly between these three cantons. For this test, the sentence reporting this result could be as follows: "The speakers of the cantons of Neuchâtel, Jura and Geneva do not vary significantly in their use of the words huitante and quatre-vingts (two-sided Fisher's exact test, p = 0.29)".

Finally, note that the chi-square test of independence also makes it possible to know whether all the cantons differ from one another or not. To carry out this test, let us set aside the values of the canton of Geneva, which is under-represented in the corpus, with only 75,598 words (against more than 100,000 in all the other cantons). This poses problems for carrying out an  2 test, as we have just seen. The result of the  2 test of independence for the five cantons of Vaud, Valais, Freiburg, Neuchâtel and Jura is as follows:  2 (4) = 19.63, p = 0.001. We can see that the difference in the distribution of the two words between the cantons is significant.

However, this result is not entirely informative. Indeed, the only thing that this test lets us conclude is that the distribution between cantons differs significantly in some way, or, in other words, that it is unlikely to observe such a biased distribution under the assumption that huitante and quatrevingts are used uniformly across the five cantons. Nevertheless, the test does not tell us whether all the cantons differ from one another or if only some of them are different from the others, nor whether this difference is linked to the use of the two words or only to one of them.

To determine where a significant difference stems from when an  2 test comprises variables from more than two categories, we have to observe standardized residual values of the test. A standardized residual is a ratio calculated from the observed frequency and the expected frequency for each category, according to the following formula: observed frequency expected frequency standardized residual = expected frequency 

The more the residual moves away from 0, the more it means that the category contributes to the significant test result. According to a rule of thumb, we consider that any value of a standardized residual greater than +2 or smaller than -2 shows a significant difference compared to the mean, because this figure means that the value obtained deviates by more than two standard deviations from the mean. A positive value reflects an overuse compared to the other categories, whereas a negative value reflects an underuse. In VassarStats, standardized residuals are calculated automatically and reported below the test results table. In the case of the test that we have to perform, the standardized residuals obtained are shown in Table Conclusion

In this chapter, we have discussed some simple procedures that help to report and analyze the numerical values obtained in a corpus study. We have made an important distinction between descriptive statistics (which relate only to the sample tested) and inferential statistics (which aim to generalize the results to an entire population). We have also described the different forms that variables in a corpus may adopt, and highlighted the fact that statistical tests must be adapted to the nature of the variables. In terms of descriptive statistics, we started by presenting the advantages and disadvantages of the mean and the median as a way of synthesizing data and concluded that the use of the mean also requires indicating the dispersion of the values around it (notion of standard deviation). We then tackled the question of data normalization, which makes it possible to compare values from different corpora, transforming them into relative frequencies through the use of bases of normalization.

We have emphasized that frequency is not the only indicator of the prevalence of a word in a corpus and that its dispersion across the different portions of the corpus is also very important. We therefore introduced a simple and a more sophisticated way (deviation of proportions) to calculate this dispersion. We also showed how to measure the lexical richness of a corpus using the type/token ratio, and presented the limitations of such a measurement. To conclude, we gave two variants of the chi-square test, an inferential statistics test which makes it possible to determine whether the differences observed between the distributions of several categories are significant.

In the case of the chi-square test of independence, we have shown how to represent data using a crosstab. We introduced the notion of standardized residual, which makes it possible to determine where the significant result of the test comes from, and introduced Fisher's exact test as an alternative to the chi-square, when the conditions posed by the latter are not met. Revision questions and answer key

8.8.1. Questions 1) The tables below show the number of occurrences of the (lemmatized) words bateau and je, as well as their English equivalent boat and I, in the bilingual corpus of children's literature ParCoGLiJe (see Chapter 5). This corpus includes four original works in French and their translation into English, as well as four original works in English and their translation into French. Raw data (number of occurrences of the words bateau, je, boat, I) drawn from these works are presented in the four tables below, as well as number of word types and word occurrences. Normalize the data so as to be able to compare the frequency of these words throughout the texts in each table, and then between the sub-corpora (original texts vs. translated texts, French texts vs. English texts). 2) On the basis of the four above-mentioned sub-corpora, what hypotheses could be formulated concerning: Original texts in -the difference in distribution between the words bateau and je in French; -the difference between original texts and translated texts; -the difference between French and English. What are the corresponding null hypotheses? What are the types of variables corresponding to:

-the number of occurrences of bateau, je, boat, I; -the type of text: translated or original; -the novel: Le jardin secret, L'île au trésor, Oliver Twist, Le livre de la jungle; -the number of word types in each corpus; -the language: French or English.

3) Calculate the type/token ratio for each text. In which case is it methodologically correct to apply this measure? 4) Calculate lexical dispersion for the words boat, I, bateau and je in each of the four sub-corpora using the deviation of proportions method. What can you notice? 5) Determine whether the distribution of different translations of néanmoins and toutefois found by

Relative frequency every 10,000 words of lemmas bateau and je in French texts (originals and translations) The normalized data enables us to observe that the pronouns je and I have higher frequency than the nouns bateau and boat. This result is widely expected, insofar as closed-class words usually have higher frequencies than open-class words, as is the case of nouns. We can also observe that the use of je and I is highly variable between novels. This gap reflects the different narrative perspectives used. First-person novels make much more use of it than third-person novels. There are also great differences between certain texts and their translation, especially in the case of Mémoires d'un âne, where I appears three times less frequently in the English translation than in the French. Finally, it is interesting to note that the frequency of the words bateau and boat also varies between the original texts and their translation.

This seems to indicate that the semantic fields of these words are not the same at all. For example, in French, the word sous-marin appears regularly in the novel Vingt mille lieues sous les mers and this word is translated as underwater boat in English. So, in English, the word boat is used for qualifying this means of transport, but this is not the case in French. A more in-depth analysis would reveal other interesting differences in the use of these words between languages, as we argued in Chapter 4.

2) Here are some examples of hypotheses, but many others could be possible: H1: the word je has a higher frequency than bateau since it is a function word; H0: there is no difference in frequency between the words je and bateau; H1: translated texts are lexically less diversified than original texts due to the simplification universal; H0: translated texts and original texts have the same level of lexical diversity; H1: French texts are longer (have more word occurrences) than English texts; H0: texts in French and English have a similar length. 3) a) The number of occurrences of the words bateau and je is a scalartype linguistic variable. b) The type of text, either translated or original, is a nominal-type explanatory variable. c) The novel is a nominal-type explanatory variable.

d) The number of word types in each corpus is a scalar-type linguistic variable. e) Language is a nominal-type explanatory variable. The type/token ratio can only be used for comparing texts of similar length. For example, it is suitable for comparing Lettres de mon moulin and Mémoires d'un âne, but not Lettres de mon moulin and Les trois mousquetaires, since the latter is more than four times longer. In fact, this ratio decreases as the texts lengthen, which is true for the case of these two novels. The type/token ratio can also be used for comparing texts and their translations, only in cases of similar length.

Proportional differences for the words bateau and boat: We can observe that the word bateau in the original texts of the four novels in French and in their English translations is used more homogeneously (0.44 and 0.33 respectively) for the word boat than in the original English texts and their French translations (0.73 and 0.61 respectively). This difference is partly due to the fact that the word bateau is found in the four novels written in French, whereas it is only found in three of the four novels written in English.

Proportional differences for the words je and I: We can see that the dispersion of pronouns je and I is much more homogeneous than that of the noun bateau, as we could expect, given the high frequency of this word. The great homogeneity of this distribution in the texts written in French can also be found in the translations of these texts into English. However, while the texts written in English make homogeneous use of I, their translation into French presents a much less homogeneous use of je.

5) The application of a chi-square test of independence indicates that the distribution of the different translations is not homogeneous between the two connectives ( 2 (4) = 14.06, p <0.01). The standardized residuals also indicate that this difference is due to an overuse of nevertheless/nonetheless when it comes to translating néanmoins (+ 2.41). The other differences are not significant. The effect size is moderate (Cramer's V = 0.22). 8.9. Further reading Conclusion The Stages for Carrying Out a Corpus Study

Throughout this work, we have presented the different facets of corpus linguistics, both from the point of view of the theoretical questions to which this discipline provides answers and of its methodological foundations. By way of conclusion, we would like to offer a list of ordered stages, making it possible to implement the concepts discussed in this book step by step, and to carry out a corpus study. C.1. Stage 0: wanting to know more

Before starting any project, something that is important is the desire to learn more about it. It is actually this initial curiosity that gives birth to the best research ideas. Before starting to work on a project, it is essential to try to find a question you are interested in, or at least which arouses your curiosity. Often, this first idea is intuitive and rather vague.

For example, some are fascinated by the question of the differences between men and women and how they are reflected in language. Others are enthusiastic about children and everything related to them. Still others like politics, history or sport. The great advantage of linguistics is that the study of language has interfaces with very many disciplines, and that it is possible to find study subjects in very varied fields. Take the time to question your interests and listen to your intuitions, even if the latter do not (yet) look like a research question that can be studied empirically. C.2. Stage 1: identify relevant literature

Once you have found the subject you are interested in, the first real research step will be to find relevant documentation. Indeed, corpus-based studies aim to test empirical hypotheses that are often formulated from the results of other studies, either more theoretical or empirical in nature, which have already been published in the scientific literature.

Thus, the first step in a corpus study is to identify relevant sources that have so far explored the research subject under consideration. Most of these sources can be found in scientific journals. For example, journals such as Corpus Linguistics and Linguistic Theory, International Journal of Corpus Linguistics and Corpora are all three specialized in the publication of corpus studies, whereas the Journal of Language Resources and Evaluation publishes articles on methodological aspects related to the compiling and processing of corpus data. In addition, many journals specializing in different areas of linguistics, such as Journal of Pragmatics, Journal of Sociolinguistics and Journal of French Language Studies, regularly publish corpus studies. We have offered many examples of such studies in Chapters 2-4. The names of the journals or books in which these studies have been published can easily be found by consulting the reference list at the end of this book.

The journal sites mentioned above will help you to search for articles using keywords. Another way to identify relevant literature is to use the Google Scholar search engine, which indexes most of the available scientific articles. Other databases that are available for a fee, such as the Scopus database, make it possible to search for articles from many different journals using a single query.

In scientific journals, in most cases, access to articles is not free, although the alternative model called Open Access is gaining popularity. University libraries generally offer access to many online journals and should be checked for availability via the university computer network. When an article comes from a journal that is not freely available, it can still be accessible in certain cases. In fact, more and more researchers are making a version of their articles available to the public on sites such as ResearchGate and Academia, or on the site of their institution or on personal web pages. It is therefore useful to look for the article title directly in a search engine to find out whether such a version is available online. C.3. Stage 2: formulating research hypotheses

Once the relevant studies have been identified in the scientific literature, the second stage in a corpus study is to formulate specific hypotheses to study the research question under consideration. These hypotheses emerge from the results of previous studies, and also aim to supplement them, for example, by testing new variables.

Let us recall that the hypotheses must be empirically testable (see Chapter 8, section 8.4). In other words, they should clearly identify variables and suggest relations between them. For every research hypothesis, it is also important to formulate the corresponding null hypothesis, since it is the latter that will be evaluated by means of statistical tests. C.4. Stage 3: operationalizing your hypotheses and choosing data

The operationalization phase of research hypotheses is crucial for the success of an empirical study. This phase consists of determining how the variables will be measured. For example, if the question to be investigated is the assertion that women talk more about their emotions than men, the operationalization of this question will require building or obtaining a lexicon of words related to emotions that can be searched in a corpus, chosen to make it possible to determine whether women actually use these words more than men.

As from the operationalization phase, it is also important to choose the corpus on which the study will be carried out. If the study makes use of existing corpora, the possibility of investigating a certain question or not, or the manner in which it can be operationalized, depends on the characteristics of the corpus.

For example, to be able to compare the language of women and that of men, this variable must be clearly identifiable in the metadata of a corpus. Likewise, the stylistic genre of the corpus should be compatible with the question under investigation. People are more likely to talk about their emotions in spontaneous conversations or when they relate a memory than when they are giving a scientific presentation, for example. The operationalization phase must therefore lead to the decision to use an existing resource, for example among those described in Chapter 5 or, to create a new corpus, according to the principles introduced in Chapter 6. C.5. Stage 4: extracting and annotating corpus data

Once the corpus has been identified or created, the next step is to determine the best strategy for retrieving the relevant data. In order to automatically search for elements in a corpus, a surface feature such as a word or list of words should be associated with it. In the case of the research question mentioned above, it would be the vocabulary of emotions.

In many cases, the automatic extraction includes data which are irrelevant for analysis and which require manual sorting. For example, not all the uses of the verb like correspond to the expression of an emotion, because in certain cases this verb is used with a modal value (I would like you to eat your ice cream). Data extraction can also be based on a prior automatic analysis of the chosen corpus, such as lemmatization or part-of-speech tagging (see Chapter 7, section 7.4).

For many questions, the raw data retrieved from a corpus will not be sufficient. It will therefore be necessary to annotate data before obtaining answers to these questions. The annotation process requires the prior identification of clear categories (see Chapter 7, section 7.3). A first pilot annotation phase should be used for testing these categories, as well as for identifying problematic cases and leading to the preparation of a more or less detailed annotation manual. In order to be reliable, an annotation should ideally be carried out by several annotators independently, and their measured agreement should be placed above a certain threshold (see Chapter 7, section 7.5). C.6. Stage 5: analyzing data

The data analysis stage must begin with any transformations in order to make the data comparable across corpora. This stage notably involves the transformation of raw figures corresponding to the number of occurrences observed in the corpus into figures reporting relative frequencies, following a base of normalization. Then, different descriptive statistics should be carried out. In order to be able to draw quantitative conclusions, it is necessary to carry out inferential statistical tests, which will indicate whether the results obtained on the corpus sample can be generalized to the entire population.

We should recall that the possibility of using different tests depends on the way in which the hypotheses have been operationalized and, more specifically, on the types of variables involved (see section 8.4). C.7. Stage 6: presenting your study in a report or an article

Corpus studies examine empirical data in order to draw quantitative conclusions, which are to be interpreted in the light of the hypotheses formulated in stage 2. The reports or articles presenting this type of results generally follow a very precise structure. In the introduction to the document, the research question is briefly introduced and contextualized. The second section contains a review of relevant previous studies (also called the state of the art), which served as a theoretical basis for the study, or which are inspired on other corpus studies that the current research project will supplement or sometimes call into question. The third section contains a presentation of the hypotheses resulting from it and which will be the subject of the empirical study. The rest of the document aims to present the results of the study. The detailed presentation of the study begins by describing the corpus used and explaining how data was retrieved from it, as well as their annotation.

Then, the results are presented in detail, as well as the statistical analyses that were carried out, with the results of the statistical tests reported in accordance with a very precise format (see section 8.6). These results should finally be discussed in a critical manner, indicating, in particular, the extent to which the initial hypothesis is verified. A brief conclusion can summarize the main results obtained, or even provide perspectives for further studies. This order of presentation can be summarized in a diagram as follows: 1) introduction; C.8. Conclusion

Once all these steps have been completed, you will have the satisfaction of having contributed to research by providing the scientific community with new empirical data, which in turn can serve as a starting point for other research. Your results could be replicated and reassessed by other researchers. Very often, the results of empirical studies also serve to modify and improve existing theories, and thus contribute to make linguistics a scientific study of language. This is one of the key objectives of the empirical approach that we presented in Chapter 1: to provide a scientific perspective on language, using a rigorous methodological approach based on the quantitative analysis of linguistic data.


