Tables

1.1 Common right collocates of "equal" and "identical" in the Corpus of Contemporary American English (COCA) 2.1 Key situational differences between an email to a friend and an email to a superior (boss) 2.2 Situational differences between news writing and news talk 2.3 Key situational differences between student presentations at a symposium and teacher presentations in the classroom 2.4 Texts for news writing and a news talk (COCA) 2.5 Texts from a classroom presentation and a symposium presentation 2.6 Texts from an email to a friend and an email to a boss 3.1 Distribution of part of speech categories following the word "say" 3.2 Distribution of part of speech categories following the word "say" in spoken discourse and in academic prose 3.3 Raw and normed frequency counts for "said" and "stated" in three registers 3.4 Distribution of "said" and "stated" in two registers 3.5 Distribution of the sentence position of "on the other hand" in spoken and written discourse 4.1 Information on corpora used for projects in this chapter 4.2 Passive voice 5.1 Examples of corpus projects 5.2 Coding scheme 6.1 Frequency of complement clauses 6.2 Complement clauses 6.3 Descriptive statistics for "hedges" in three disciplines through SPSS 7.1 First-person pronoun data in SPSS data view format 7.2 Calculating the mean score for each group 7.3 Calculating sum of squares within groups Figures 2.1 Situational variables 3.1 Distributional patterns of the word "say" in COCA 3.2 Concordance lines for the word "say" in COCA 3.3 Results from the keyword analysis for British and American press reporting 3.4 Icons on the top to choose from for the next project 3.5 Text analysis based on vocabulary frequency in Word and Phrase 3.6 More information about the word "thief" 3.7 Concordance lines for the word "thief" 3.8 Part of speech search in COCA 3.9 Distributional patterns of the 4-gram "on the other hand" in COCA 3.10 Concordance lines for the 4-gram "on the other hand" in COCA 3.11 Frequency search in Word and Phrase 5.1 Example of a text file with header information 5.2 Embedding header tags in AntConc 5.3 AntConc using three-word lists for vocabulary frequency comparison 5.4 Searching your own corpus: Concordance lines in AntConc for the word "and" 5.5 Sorting in AntConc 5.6 File view in AntConc 5.7 Search term distribution in full texts 5.8 The word "and" and its collocates 5.9 Collocates in concordance lines 5.10 Running your own n-grams in AntConc 5.11 Running a word list in your corpus in AntConc 6.1 Boxplot of the use of nouns by teachers and students 6.2 Variable view in SPSS

Preface

In our experiences teaching introductory corpus linguistics classes, we have found that undergraduate and graduate students gain both confidence and ability doing corpus analysis when they are given the opportunity to work with corpora and are exposed to hands-on experience with corpus tools and corpus analysis. While an understanding of the principles, approaches, and advantages of using corpora provides the necessary foundational knowledge of this approach to language analysis, there is a technical side to corpus linguistics that is best acquired through practice and experience with corpora. We have found that students are sympathetic to the benefits and advantages of using language corpora, but the real challenge is teaching them how to work with corpora. When "doing" corpus linguistics, students need to gain experience searching a corpus and interpreting the results of their corpus searches so that they can use this information to explain why their analysis and findings are important or relevant. In this book, we offer multiple opportunities to work on corpus projects by first including an entire chapter dedicated to smaller corpus projects (Chapter 4) and then providing students with information on how to build and analyze their own corpora (Part III). We have found that offering students the opportunity to build and analyze their own corpora gives them valuable experience in corpus building and sometimes even encourages them to build other corpora for projects outside of the class.

In order to allow the students to gain experience in "doing" corpus linguistics, we have intentionally limited the corpora and software used in the corpus projects. There are many different corpora available with different levels of public availability and numerous software programs that can be used for analysis (some free and some not). An extensive list of English corpora around the world is at the end of the book.

Each corpus and software program has its own idiosyncrasies and we have found that these different corpora and software programs are sometimes confusing to students who do not have access to the corpora and/ or struggle to learn one program or search interface in a corpus and then have to learn another. To address this issue, all of the projects in this book use the suite of corpora created by Mark Davies at Brigham Young University (

Because a good deal of corpus work involves quantitative data analysis, we also included some elementary statistical information (Chapter 6) and tests (Chapter 7). Keeping with one of the guiding principles of this book, we see this introductory information as a way to have students learn the basics of analysis with the hope that they may apply this knowledge in other projects or learn more about more advanced statistical techniques that they can use in the future.

There are many different descriptive and theoretical frameworks that are used in corpus linguistics. We have selected one particular framework to guide the students in their interpretation of their corpus findings. Register analysis has strongly influenced our work and we believe that this approach to understanding language use is broad enough to encompass the various types of projects that students choose to do. In Chapter 2, we outline the basics of a register approach to language analysis and then ask students to refer to this same framework when building their corpus and doing their corpus study. We recognize the relevance of other descriptive and theoretical agendas but feel that focusing on a single approach provides students with more extended experience interpreting their corpus results and motivating the significance of their findings. Without knowledge and practice using a specific framework, we have found that students can be quite enamored with the "button-pushing" aspects of corpus linguistics at the expense of interpreting the results of their searches.

We also recognize the importance of reporting on research in a cohesive way. To this end, we have included material dedicated to the specifics of writing a research paper and presenting research (Chapter 8). Our goal in this chapter is to provide both students and teachers with some guidelines for how to demonstrate and present their specific research projects.

In the final chapter (Chapter 9), we ask students to consider more advanced types of corpus research with the hope that this book will serve as an introduction to the field and encourage students to pursue these ideas at a more advanced level and perhaps even impact the field in significant ways.

Introduction to Doing

Corpus Linguistics and Register Analysis DOI: 10.4324/9781003363309-2

Language and Rules/Systems

While all humans use language to communicate, the ability to describe language is not nearly as advanced as our ability to actually use language. One defining component of the scientific study of language (i.e., linguistics) includes a description of how language works. Speakers of English are able to produce plural nouns that end in different sounds -we say batS and bagZ, not batZ and bagS. These same speakers can also produce plurals of nonsense words that we have never heard before -we would say bligZ and not bligS. Speakers of English also know that We worked out the problem and We worked the problem out are both acceptable sentences but We worked it out and *We worked out it may not be equally acceptable (the latter is likely to sound strange to many native speakers of English). The fact that we can agree on these aspects of English related to the pronunciation of plurals and word order points to the fact that language, in many respects, is predictable (i.e., systematic). Such aspects are not only related to sounds and the order of words, but they are also related to how we might use language in different contexts and for different purposes. For example, we would not be likely to ask a professor for an extension on an assignment by saying: "Hey, man. Gimme an extension". Instead, we are more likely to make such a request by saying: "Would you please allow me to hand in that assignment tomorrow? I have experienced some personal issues and have not been able to fully complete it yet".

While it may be difficult to explain these particular aspects of the English language, native speakers apply these "rules" of language flawlessly.

Chapter 1

Linguistics, Corpus Linguistics, and Language Variation

1.1 Language and Rules/Systems 1.2 What Is Corpus Linguistics? 1.3 Register, Genre, and Style -Is There a Difference? 1.4 Outline of the Book

In other words, one important component of linguistic description is to make implicit "rules" or patterns of language (knowledge we use) explicit (knowledge we can describe). It is safe to say that language users follow rules (and sometimes choose not to follow rules) for specific reasons even though they may not be able to explain the rules themselves (or even if we cannot agree on what a "rule" actually is). An important part of linguistic study focuses on analyzing language, describing, and in some cases explaining, what may seem on the surface to be a confusing circumstance of facts that may not make much sense.

When many people think of language rules, they may think of the grammar and spelling rules that they learned in school. Rules such as "don't end a sentence with a preposition" or "don't start a sentence with the word and" are rules that many people remember learning in school. Very often people have strong opinions about these types of rules. For example, consider the excerpt below taken from a grammar website on whether to follow the grammar rule of "don't split an infinitive".

Even if you buy the sales pitch for language being descriptive rather than prescriptive, splitting infinitives is at the very best inelegant and most certainly hound-dog lazy. It is so incredibly simple to avoid doing it with a second or two of thought that one wonders why it is so common. There are two simple solutions.

(1) "The President decided to not attend the caucus" can be fixed as easily as moving the infinitive: "The President decided not to attend the caucus". I'd argue that works fine, and not using that simple fix is about as hound-dog lazy as a writer can get, but split infinitives can be avoided entirely with just a bit more thought. How about: (2) "The President has decided he will not attend the caucus". What the heck is wrong with that?

It's hound-dog lazy, I say. Where has the sense of pride gone in writers?

(

Examples such as these are not uncommon. One would only have to look at responses to social media comments or blog posts to find many more instances of people who have very strong opinions about the importance of following particular grammar rules.

So far, we have looked at "rules" as doing two different things: 1) describing implicit, naturally occurring language patterns and 2) prescribing specific, socially accepted forms of language. Although both descriptive and prescriptive perspectives refer to language rules, prescriptive rules attempt to dictate language use while descriptive rules provide judgment-free statements about language patterns. Both prescriptive and descriptive aspects of language are useful. When writing an academic paper or formal letter, certain language conventions are expected. A prescriptive rule can provide useful guidelines for effective communication. However, descriptive approaches can be useful in uncovering patterns of language that are implicit (as in the examples described above). Descriptive approaches can also be used to see how prescriptive rules are followed by language users.

The concept of language rules raises another interesting question: Why are these rules sometimes followed and sometimes not followed? Consider the prescriptive infinitive rule described above. Is it accurate to say that those who write to not attend are not following a rule? In some respects, this may be the case, but there is another -perhaps somewhat misunderstoodissue related to language that deserves some attention and serves as a basis for this book: the role of language variation. It is an incontrovertible fact that language varies and changes. The type of English used in Britain is quite different from the type of English used in the United States. The type of English used in Britain or the United States also varies from region to region or among people from different socio-economic classes. The type of English used 150 years ago in the United States is quite different from the type of English used in the United States today. Language even changes and varies in a single person. The study of language variation seeks to understand how language changes and varies for different reasons and in different contexts. There are different perspectives on how to investigate and understand language variation. The perspective that we will take is, as you can tell from the title of the book, related to an area of language study called corpus linguistics.

What Is Corpus Linguistics?

One way to understand linguistic analysis and language is through corpus linguistics, which looks at how language is used in certain contexts and how it can vary from context to context. While understanding variation and contextual differences is a goal shared by researchers in other areas of linguistic research, corpus linguistics describes language variation and use by looking at large amounts of texts that have been produced in similar circumstances. The concept of a "circumstance" or "context" or "situation" depends on how each researcher defines it. Corpus linguistic studies have frequently noted the general distinction between two different modes of language production -written language and spoken language. From a written perspective, one may be interested in contexts such as news writing, text messaging or academic writing. From an oral perspective, one may be interested in language such as news reporting, face-to-face conversation or academic lectures. Although text messaging and academic writing are both written, the purpose of text messaging is quite different from the purpose of academic writing and we would likely expect some degree of language variation in these different written contexts. The same may be said with face-to-face conversation and academic lectures: both are spoken but they have different purposes and consequently have different linguistic characteristics. More generally, we might also expect that spoken language (in all its various purposes and contexts) would likely differ from written forms of language. Spoken language does not generally have the same type of planning and opportunities for revision that we find in many types of written language. We will consider how different circumstances (or situational variables) can affect language use in the following chapter. But, before we do, we would like to briefly describe what we mean by a corpus.

A corpus is a representative collection of language that can be used to make statements about language use. Corpus linguistics is concerned with understanding how people use language in various contexts. A corpus is a collection of a fairly large number of examples (or, in corpus terms, texts) that share similar contextual or situational characteristics. These texts are then analyzed collectively in order to understand how language is used in these different contexts. The result of this analysis is a collection of language patterns that are recurrent in the corpus and either provide an explanation of language use or serve as the basis for further language analysis. One common method used in corpus research is to look at the environment of a particular word or phrase to see what other words are found (i.e., "collocate") with the reference word. While there are many different corpora available (see a selection at the end of the book), as an example, we will use the Corpus of Contemporary American English (COCA), which is a publicly available collection of over 1 billion words of text of American English (freely available at www.english-corpora.org/coca/), to investigate the use of two words: equal and identical.

In many respects, equal and identical can mean the same thing (two things that are similar to each other), and they are often taken as synonyms of one another. For example, we can use both of these words in a sentence such as: These two students are equal/identical in their performance on the exam with the same general meaning. If we were asked to define the word equal, we may use the word identical in our definition

Additionally, the corpus can inform us about frequency differences between equal and identical as shown in Table

In other words, we can see that the word equal is more frequent than the word identical because the frequency of collocates shows a large difference between the two words. In fact, the word equal occurs 22,480 times in the corpus, and the word identical occurs 8,080 times.

In addition to information on collocation and frequency, a corpus will also allow us to examine the extent to which certain types of prescriptive rules are followed. Let us look at what a corpus might tell us about splitting infinitives. Earlier in this chapter, we saw that this rule can raise the ire of some people -to the point of associating some serious character flaws in those writers who do not follow it. The Corpus of Contemporary American English shows that we have examples such as to better understand (874 times in the corpus) compared with to understand better (94 times) and to really get (349 times) compared with to get really (151 times). Additionally, the sequence of words to better understand is most commonly found in academic writing while the sequence to get really is most commonly found in spoken language contexts. This evidence suggests that a strong prescriptive statement such as "don't ever split an infinitive" runs into serious problems when looking at actual language use. In fact, some examples of split infinitives are more frequent in formal academic writing; others are more frequent in spoken language. In other words, even though there are strong views on certain usage rules of English grammar, many of these rules may run counter to authentic examples of how language is used by reference to corpora. That is to say, the "rule" against splitting an infinitive is not always followed (i.e., there is variation in the application of the rule).

We have already seen a few examples of what corpus information can tell us. Now we will consider the defining characteristics of corpus linguistics as they will be used in this book. In a general sense, a corpus can refer to any collection of texts that serve as the basis for analysis. A person might, for example, collect examples of news editorials that are on a particular topic and refer to this collection as a "corpus". However, would we say that this person is "doing" corpus linguistics? In their 1998 book, Corpus Linguistics: Investigating Language, Structure and Use, Biber, Conrad, and Reppen define corpus research as having the following characteristics:

• it is empirical, analyzing the actual patterns of use in natural language texts • it utilizes a large and principled collection of natural texts, known as a "corpus", as the basis for analysis • it makes extensive use of computers for analysis, using both automatic and interactive techniques • it depends on both quantitative and qualitative analytical techniques

The third and fourth characteristics of corpus linguistics make reference to the importance of computers in the analysis of language as well as different analytical approaches. It would be difficult to imagine how one might use a 450-million-word corpus such as COCA without using a computer to help identify certain language features. Despite the large number of texts and the relative ease of obtaining numerous examples, a corpus analysis does not only involve counting things (quantitative analysis); it also depends on finding reasons or explanations for the quantitative findings. In Chapters 3 and 4, we will cover some of the specific directions we can explore in the corpus through software programs that allow for different types of analysis. It is important to remember that corpus methods do not just involve using computers to find relevant examples; these methods also focus on analyzing and characterizing the examples for a qualitative interpretation.

In addition to these four characteristics of a corpus, Elena Tognini-Bonelli, in her book Corpus Linguistics at

A final point to consider when looking at corpus research relates to various views that researchers may have about corpus linguistics. Elena

Researchers then analyze them grammatically

Register, Genre, and Style -Is There a Difference?

Researchers in the field of applied linguistics, and more specifically, in discourse analysis, have defined and distinguished notions of register, genre, and style when it comes to text analysis.

In contrast, genre studies differ from both register studies and studies of style in all of the four dimensions mentioned above. As Biber and Conrad in their Table

In this book, we take a register perspective to describe linguistic variation whether it is lexical, grammatical, or textual.

Outline of the Book

This book is divided into three main parts. In Part I, we introduce the concept of a corpus and locate corpus linguistics as an approach to language study that is concerned with the analysis of authentic language, and a focus on language variation, using large amounts of texts (Chapter 1). We then provide a register analytical framework for interpreting corpus findings (Chapter 2). In Part II of this book we focus on how to use existing corpora. We introduce a set of search tools and a set of language units that could serve as the basis for the analysis of already existing corpora (Chapter 3). We then provide 12 different projects that use existing online corpora in order to introduce the basics of how to work with corpora, interpret data, and present findings (Chapter 4). Once these basics of corpus analysis and an analytical framework have been addressed, readers will be ready to build their own corpora and conduct their own research study. Part III of the book takes you through the steps of building a corpus (Chapter 5) and then covers some statistical procedures that can be used when interpreting data (Chapters 6 and 7). Chapter 8 then provides a stepby-step process for writing up and presenting your research. Because this introductory book contains some of the basics of how to conduct a corpus research project, we do not cover many of the relevant issues that corpus linguistics is presently addressing in its research. In Chapter 9, we discuss some of these issues with the hope that this book has taught you enough about corpus research to pursue a more advanced study of the field.

As we have discussed in Chapter 1, language variation is a prevalent characteristic of human language. There are, however, many different ways to investigate variation. We could choose to look at how language varies in different areas around the world (for example, the differences between British and American English). We could also investigate how language varies by identity or social class. Another way of looking at variation would be to consider the differences among individual writers or speakers. We could, for example, study the speeches of Martin Luther King Jr. in order to understand how his "style" might differ from speeches given by other people such as Winston Churchill. We could also take a different perspective and examine variation in language use by reference to the different contexts in which language is used. This approach can be done on both large and small scale depending on a specific research goal; however, at the foundation of this approach to language analysis is the assumption that language variation is functionally motivated by reference to clear descriptions of context. This perspective on language variation is referred to as register analysis which uses a series of steps to describe and interpret linguistic differences across relatively general contexts such as written versus spoken language or face-to-face conversation versus academic lectures. These same steps can also be used to describe variation in more specific contexts. Large-scale investigations require a representative (and usually quite large) sample of language and a method to determine linguistic features that are frequent in a given context. The research goal of this broad approach to register analysis seeks to identify the linguistic characteristics of language used in general contexts such as face-to-face conversation or academic writing. More recently, similar methods used in the analysis of registers have been used to identify and interpret language variations that are not concerned with identifying and describing registers but are instead concerned with describing and interpreting language variation. This latter approach has been called a register-functional approach

Why Register?

As noted above, linguists have taken different approaches to investigate language variation. When traveling to different parts of the world, one may notice that there are different words for the fossil fuel that people put into their cars (gas, petrol, motor spirit). Not only do people have different words for things, but also the way that people say certain words can sound very different from region to region. In some parts of the United States, the words pin and pen both sound like pen. In some parts of the United States, people say caught and cot with different vowel sounds; in other parts of the United States, the words sound the same. The role that geographic location plays in lexical and phonological variation in these examples is generally covered in a field of linguistics known as sociolinguistics. Researchers in this field seek to understand how language variation is related to factors such as geographic region, identity, ethnicity, age, and socio-economic status. The traditional sociolinguistic approach frequently considers variation to be present when the same concept is expressed in different ways. From this standpoint, what counts as a variable must be a concept that is similar in meaning but different in the words used to describe it or in the phonological form of the word, or, in some cases, a different grammatical structure that describes the same concept (for example, the comparative structure more better).

Traditional sociolinguistic researchers will likely acknowledge that language doesn't just vary in the lexical, phonological, or syntactic form of similar concepts. Linguistic variation is not always the result of region or age differences but instead can be attributed to differences in mode (spoken versus written) or communicative purpose (informing versus persuading). Even within a specific mode or context, we find variation in specific written and spoken forms of language. Academic writing, for example, occurs in a different context than newspaper writing or letter writing. Viewing language variation in this way essentially "predicts" that contextual differences will result in the variation of linguistic features. In basic terms, a register is a variety of language that is characterized by both a specific context and the language used in the context. Variables in register analysis are not restricted to linguistic characteristics that are not meaning-changing; register analysis considers the context as a variable and looks at the different linguistic features that are found in specific situations.

In some respects, a register perspective is similar to traditional sociolinguistic approaches. Both sociolinguistic variation and register variation studies are interested in how social or situational characteristics relate to language use; however, register analysis considers a wider range of factors that are not only due to what are traditionally viewed as "social" factors (e.g., age, identity, socio-economic status). For example, when looking at potential differences between speaking and writing, the communicative purpose and topic are likely not as socially conditioned as are other components accounted for in register variation such as the relationship between participants. Seen from this perspective, register analysis takes into consideration a wider range of factors that may include social factors but may also include other factors, for example, topic, purpose of communication, and mode of communication. Another difference between sociolinguistics and register analysis relates to the linguistic features under investigation. Sociolinguistic studies are generally focused on a small number of language features that vary for purely social reasons. This approach allows us to understand why some people use the word gas and others use the word petrol. Register analysis takes a different view of language variation by using corpora to identify and interpret linguistic features. A register approach also uses a different type of analysis to investigate the extent to which linguistic features co-occur in given situations of use. From this perspective, the focus can either be on specific linguistic features or on the co-occurrence of multiple features found in particular situations of language use. Because register variation considers how linguistic features co-occur in a given context, a corpus linguistic approach is well-suited to register analysis because corpora provide large amounts of authentic texts for analysis. In fact, it would be hard to see how a register analysis could be achieved without the use of corpora. Looking at a smaller number of texts would likely not provide a representative sample of language use to allow for a characterization of a given register. However, as discussed above, the tools used in register analysis are also well-suited to identifying and interpreting variation in texts. For example, it is possible to look at two different types of writing tasks that vary in communicative purpose or in the amount of time provided to complete the task. The written text produced in two different conditions can be analyzed for variation in linguistic features and interpreted functionally. This is not to say that texts that vary in communicative purpose or time allotted to write a text are of different registers, but these contextual differences are likely to result in different linguistic features and that can be functionally interpreted. The number of texts needed for the latter approach does not necessarily have to be representative because the goal is not to identify a register but to provide a functional account of variation. The relevance of both applications of register analysis relates closely to the definition of corpus linguistics discussed in Chapter 1. Recall that corpus linguistics includes both quantitative and qualitative analysis. While the quantitative information can sometimes be fairly easy to obtain (after all, many times all one has to do is push a few buttons to obtain results!), proposing reasons for the quantitative information can be more challenging.

What Is a Register (Functional) Analysis?

If we see register as a variety of language, then we can describe register analysis as a framework to understand language variation. Register analysis is most readily associated with the work of Douglas Biber and his colleagues and students. According to

As we touched on in Chapter 1 and will explore further in Chapter 9, scholars are typically interested in taking one of two approaches to study variation using corpus methods. On the one hand, they focus on variation in the use of one individual lexical unit (e.g., words, collocations, ngrams, or lexical bundles) or in the use of an individual grammatical unit (e.g., subordinate clauses, modals, pronouns). They use a corpus to find out how these individual features vary across contexts/registers. On the other hand, scholars such as Biber and his colleagues are interested in describing language variation from another point of view. Instead of focusing on individual linguistic features, they are interested in characterizing texts from a comprehensive linguistic perspective. To do this, they search multiple linguistic variables at the same time in a corpus. These studies report on how these multiple linguistic features work together (i.e., how they cooccur) in texts, and then examine how their co-occurrence patterns vary in the different registers/contexts. This approach provides a comprehensive linguistic picture of the text under consideration and allows for a functional interpretation of the relevant linguistic features.

Following the three components of register analysis described above, we focus on describing situational variables in this chapter. In Chapter 3, we will show you how to search an existing corpus for the linguistic variables you may be interested in (whether lexical or grammatical), and in Chapter 4, we will take you through several projects that will help you learn how to do studies of this kind.

Describing Situational Characteristics and Identifying Variables

Prior to any linguistic analyses, register studies examine multiple aspects of the communicative context (often referred to as the "speech situation") that the sample texts are taken from. During the past few decades, a number of scholars (e.g.,

In addition to participant information, a situational analysis also requires a description of the environment and conditions in each context. Relevant in this aspect are the channel and the production circumstances. Channel refers to both the mode and medium of the language. Mode refers to the way the language is transmitted: speaking and writing are generally the two main modes of using language, but there are also gestural systems such as signing that can convey meaning. Medium refers to the relative permanence of the language. We may compare many forms of written language as being more permanent than many forms of spoken language. Written forms of language can be preserved for multiple readers or multiple opportunities for reference while spoken language is generally more short-lived. This is not to say that all written forms of language are permanent, or all spoken forms of language are transient. We need to differentiate between mode and medium to distinguish the differences between a written grocery list and a recorded speech. The grocery list may be written out, but it is not as permanent as a recorded speech. In addition to channel, a situational analysis also characterizes the conditions in which the language has been constructed. We may also want to use mode and medium when referring to different types of written language found on the internet or cellular phones (such as social media posts, tweets, and text messages), which vary in grades of permanence depending on the topic and potential effect they have. We likely can think of examples when a writer has either deleted or regretted a particular tweet, social media post, or text message. The production circumstances of language may also relate to the process of planning, drafting, and revising. Some types of written or spoken language require extensive planning, drafting, or revising while other forms do not. In the written mode, an academic research article has gone through an extensive planning, drafting, and revising process; in a social media post or text message, generally this is not the case. Even in the spoken mode, we can acknowledge the differences in planning between an academic lecture or business presentation and face-to-face conversation. We can see the effect of production circumstances in spoken language where it has been shown that planned discourse contains fewer filled pauses (uh, um) and hesitations than unplanned discourse

Next, we will take a closer look at the variables of setting and communicative purpose. Setting describes the time and place of the communicative events. A face-to-face conversation involves a shared physical space but may take place in a private or public setting. A telephone conversation may also be in a private or public setting but generally does not take place in a shared physical space. Another relevant variable related to setting includes whether the language has been produced in the present or in the past. For example, newspaper articles written in the 21st century are very different from those written in the 19th century. In addition to setting, variation also can be the result of the communicative purpose. In some contexts, the purpose is to inform, persuade, or tell a story while in other contexts the purpose may be to just interact and share thoughts, ideas, or feelings. There are also cases where we would expect that the language would be more or less factual. We would hope that a newspaper article or academic textbook would contain factual information. We generally do not expect facts in a fairy tale or work of fiction. Communicative purpose also includes the extent to which the speaker or writer uses language that expresses their attitude about the topic (something we might not expect in news reporting but might expect in a news editorial).

A final aspect of the situational analysis relates to topic. This is a very broad situational variable that has not been investigated in much detail. A conversation about where to find a suitable place to eat will likely have very different linguistic characteristics than a conversation about how to fix a broken refrigerator. In a similar way, an introductory psychology textbook will likely have very different linguistic characteristics than an introductory music theory textbook. However, the situational variable of communicative purpose sometimes is also relevant in relation to topic. One might argue that the communicative purpose of a conversation on finding a place to eat and fixing a refrigerator are quite different, but the two different textbook types share the same communicative purpose. Thus, topic and communicative purpose sometimes "overlap" or share relevance, but other times they do not.

Although we have covered seven different situational variables, there are cases in which all seven variables are not involved in a situational analysis and there are cases where additional aspects of the situation will need to be added. In a register analysis of potential differences between different types of news writing, many situational variables associated with participants, mode, channel, and production circumstances may not differ although the communicative purpose may. Editorial news writing often seeks to persuade readers to adopt or at least consider a specific viewpoint; news reporting does not share this same communicative purpose but instead seeks to inform readers. To examine the language of university classrooms as a context for a speech situation (or domain), we might need to consider discipline, level of instruction, or other aspects such as interactivity, as part of the situational analysis.

In Chapter 3, you will have the opportunity to do situational analyses in some of the projects, and in Chapter 5 you will do a situational analysis of your own corpus. To understand how to apply the situational variables to different text types, we provide three examples of situational analyses below.

Email to a Friend and Email to a Boss

There are at least three situational differences between a letter to a friend and a letter to one's boss: certain aspects of the relationship between participants and certain aspects of the production circumstances (see Table

News Writing and News Talk

In this section, we will look at two related contexts that also differ in some situational respects: news writing and news talk show language. While both of these texts share the situational characteristic of conveying (potentially new) information to an audience (under communicative purpose), there are many differences in other areas of situational analysis. Even the communicative purpose could be argued to be different in these situations in that the news talk show involves not only providing information but also presenting an opinion.

Classroom versus Symposium Presentations

Second, the most pertinent differences are apparent as we examine the relationship among participants. For example, while classroom settings most likely allow questions at any point in time during the teacher's presentation, presentation settings have set routines and questions can only be asked after the presentation has been delivered. In terms of the presenter's familiarity with the participants, it is clear that the teacher knows (or is at least familiar with) most of the participants in the classroom because the same set of people would meet weekly for 12 or more weeks at a time in the same physical space. In contrast, the presenter at the student symposium may or may not know the audience. Yet another difference in this area is the presenter's social/academic status in the group they are presenting. The teacher (addressor) in the classroom setting is superior to the audience (addressee) and has a high social/academic status in the community. In contrast, the presenter at the student symposium plays a subordinate role in the academic community (especially if teachers are sitting in the audience) and has a relatively low social status in the community. If the student is an undergraduate, s/he has an even lower status than that of graduate students. If the student is a master's student, s/he will have a lower status than a doctoral student, and so on. Finally, the teacher holds the power in the classroom while the audience holds the power at this particular symposium because the presenters are being judged for their performance.

Third, the production circumstances are also different in the two settings. Primarily, this difference is attributed to the ability of instantaneous revisions of the text. In the classroom, there is plenty of room to negotiate and revise the text on the spot since questions can be asked at any time and clarifications can be requested. At the symposium presentation, there is a lack of spontaneous interaction, or immediate requests for clarification; there is no room for immediate negotiation of the text. Clarification questions can be made after the presentation has been delivered. Table

In sum, there are enough situational differences in some of the aspects of this context to predict that language would be used differently.

Providing a Functional Interpretation

The third step in a register analysis requires the researcher to provide a functional reason for the linguistic features in a given text. Because register analysis seeks to describe the relationship between situational variables and linguistic variables, the occurrence of linguistic features requires a description of the context. In fact, from this perspective, language features occur because they are fitting to a specific context. In other words, the situational variables in a sense "lead" language users to adopt particular linguistic features. As we discussed in the previous section, these linguistic features can be single features, but they can also be a set of co-occurring features.

News Writing and News Talk

In this example, we will show a single feature analysis focusing on first person pronouns in the subject position.

In Table

Classroom Versus Symposium Presentation

In Table

In these segments, we would like you to see differences in terms of the use of nouns and grammatical features specific to conversation such as non-clausal units, tags, and so on

The question is: "Why are these two texts so different in their noun and conversational features and in their lexical density?" In other words, what may account for the differences? If we look at the situational differences between the two texts, there are many. However, perhaps most pertinent to these two segments is the production circumstances and the communicative purpose. In the classroom, there is no pre-set script to follow; that is, there is always room for asking questions, and the potential for interaction is always present. In contrast, at a symposium presentation, the rules are strict, and questions may be asked only at the end of the talk. Therefore, the presenter is expected to talk continuously for a period of time, after which the questions from the audience may be asked. And we found that an immature cynofields resides in the kidney that's where we found the most cells with those characteristics and I interpreted that we found also oh . . . oh . . . a relationship for those cynofields but those were more mature. We can say that because . . . The electro-microscopy results with that we can see the morphology and chronology and this is a human cynofield with a transmission electronic microscopy of the human cynofield and we did with a zebrafish we found very similar morphology that granules are round as same as the human ones and the nucleus is big at this stage so we found the cell that looks like cynofields so now we want to study its function we study the function by migration of recommendation to the infection and then we see they change their morphology. So we know that cycles-sum in human cynofields includes information response and we inject the fish with the cycles-sum we let them live for 6 hours in order to provide an order response and then to (syll) we sacrifice the single cell suspension and within the facts analysis of photometry and those are our results. We found we use a control also and we can see in the control the populations of cynofields are in not increase as dramatically with the one that we injected we cycle-sum and it was 20% more of population of those cell that we found in this gate.

In terms of communicative purpose, there are two major areas that may account for the differences. On the one hand, in the classroom, the purpose is to explain concepts and methods; at the symposium, the purpose is to report on the processes and results of a research project. In addition, in the classroom, the addressor (the teacher) periodically checks for comprehension to see whether the students are understanding the material. In contrast, at the symposium presentation, the addressor (the presenter) assumes comprehension and expects questions to be asked afterwards. For these reasons, there seems to be quite a difference in the frequency of the conversational features between the two texts. However, the same is not true for the use of nouns. Because the communicative purpose in both contexts is to convey information that is potentially new to the audience, the actual information seems to be packaged in similar ways; that is, the information is delivered through nouns either embedded in noun-noun sequences or in a series of prepositional phrases. In terms of how the information is conveyed, we see differences in the type-token ratio. The teacher uses more repetitions (hence the lower ratio number) while the presenter is conveying the information without that many repetitions.

Units of Analysis and Register Studies

As we mentioned at the beginning of the chapter, many corpus researchers choose to investigate a single linguistic feature to see its variation in multiple registers. Among the lexico-grammatical studies is

Another example of single feature analysis in corpus studies is found in studies focusing on two-or multi-word sequences (collocations, and ngrams or lexical bundles, respectively). Lexical bundles are the most frequently occurring word combinations in a register; that is, in situational language use. The most often investigated bundles are four-word combinations.

These approaches to studying language use in registers provide detailed analyses of these individual features and their individual patterns. Therefore, we can learn a good deal about the use of that one feature. While these studies are interesting and very informative for these features separately, as Csomay indicates, "comprehensive descriptions of variation in language use cannot be based on investigating a single linguistic form or a single linguistic feature in isolation" (2015: 5). When describing the linguistic characteristics of texts, relying on one feature at a time is difficult mostly because a) an a priori selection of that one feature is hard to predict, since we would not really know which feature will mark the difference in the situations we are comparing, and because, as mentioned above, b) language features are typically in relationship with each other and do not occur in a vacuum. In order to characterize a register, we would need to provide a comprehensive analysis. For a comprehensive analysis, we need to look at all linguistic features in texts. In addition, we need to examine their distributional patterns to gain a "full" picture as to what linguistic features registers may be made up of. While this is possible, we would still lack the understanding of how various language features relate to one another.

However, as mentioned above, in this book, we will mainly focus on individual linguistic features and their variation in registers for two main reasons. On the one hand, these kinds of studies are relatively easy to carry out without the necessary background knowledge for a more sophisticated type of study. On the other hand, and tied to the previous point, our book is for a relatively novice audience. It is written for someone doing corpus linguistics perhaps even for the first time and for someone who has limited or no basic statistical knowledge. In contrast, to carry out comprehensive linguistic analyses from the start, the researcher must have a solid background in computer programming and in multivariate statistical methods. While such methodology is not the focus of this book, based on the results of previous comprehensive studies, we will point to ways a cluster of already identified features could be analyzed and discuss how they could be useful for our own analyses.

End of Chapter Exercises

1. In this chapter, we have discussed some benefits of investigating language from a register-functional perspective.

Notes

1 Since these text segments are uneven in length (one is 157 words long and the other is 241 words long), we had to scale the raw frequency counts as if both texts were 100 words long. To do this, we need to norm the feature count with a simple statistic: (raw feature count / actual text length) * desired text length. We will discuss this technique more in subsequent chapters. 2 To calculate type-token ratios, we take the number of words in a text and divide it by the number of word types. If a word is repeated, it counts as a new token but not as a new type. For example, in the following two sentences, there are ten tokens (i.e., number of words) and eight types (because "the" and "cat" are repeated): He saw the cat. The cat was in the garage.

Part II

Searches in Available Corpora

DOI: 10.4324/9781003363309-5

When researchers use corpora for their analyses, they are interested in exploring the use of lexical items, or certain grammatical constructions. They may also investigate lexical or grammatical patterns to see how variation in those patterns may relate to different contexts. In register studies, as we saw in the previous chapters, contextual differences refer to differences in particular aspects of the situational characteristics in the construction and production of a given text.

In this chapter, we will use the Corpus of Contemporary American English (COCA) to illustrate the most commonly identified units of language that researchers use for their analyses: words, collocations, n-grams/lexical bundles for lexical patterns, and part of speech (POS) tags for grammatical patterns. We will illustrate how to identify these units of language by providing different tasks that will give you practice in searching and analyzing these units of language. In addition, we will suggest that you access other corpora to carry out further projects in this area, for example, the Michigan Corpus of Academic Spoken English (MICASE). We will also recommend a software tool, AntConc, to carry out a keyword analysis (further details on the software is in Chapter 5). This chapter is divided into four main sections: 1) Words with two subsections: KWIC (keyword in context) and keyword analysis (based on word frequency); 2) Collocations; 3) Ngrams; 4) POS tags.

Before we can start, you will need to do two things: 1) register with COCA as a student, so you can have extended searches in that database and 2) download the latest version of AntConc (currently 4.2.0), so you Chapter 3

Searching a Corpus

3.1 Words (KWIC and Keyword analysis) 3.2 Collocates 3.3 N-Grams 3.4 POS Tags can run that program for your keyword analysis. Both COCA and Ant-Conc are free of charge.

To register with COCA, go to COCA (www.english-corpora.org/coca/) and, on the top right, click on the icon with a person's head in a box (marked yellow) and then click on "Register". This site will periodically ask you to donate money; however, as we mentioned in the previous chapter, it is not a requirement for your registration. Once you sign in, the icon with the figurehead turns green instead of yellow.

To access MICASE you can go to the address below. It is free with no sign-up, but they do ask that you make appropriate references when you carry out research with the texts available through that corpus (

To download AntConc, go to Laurence Anthony's software page, and download the latest version (currently 4.2.0) to your system (Mac, Windows, or Linux) knowing that we are using the Windows version in our discussions (

Words

Keyword in Context (KWIC)

Let's say that we are interested in searching COCA to see how often the word "say" is used across three registers: spoken discourse, newspaper, and academic prose. As we described in the previous chapter, after logging into the latest version of COCA (currently 2023), you have different choices, namely, choose "list", which will provide you with concordance lines, "chart" which will provide a chart of the frequency of your selected keyword in different registers, or "word" which will give you a plethora of information about the word you are searching (e.g., collocations, synonyms, clusters, related words, and much more). Try your hand with the different options by looking up any keyword you wish so you can see what kinds of information is available to you!

The keyword that we are searching for here and now is "say". If you click on "chart", you get a summary of the frequency distribution for this word across registers. As we see on the chart in Figure

If we were interested in how our keyword is used in the immediate textual context, we would select "context" from the choices on top as shown in Figure

When we use concordance lines, we look for individual (or a group of) pre-selected keywords to see them in their immediate context. The most common form to display a keyword in context (KWIC) is through concordance lines. As mentioned above, concordance lines highlight the word you pick and provide additional text around it. You can set the size of the text by selecting the number of characters (see our discussion on AntConc in Chapter 5) in the window around the keyword, and the output lists the examples selected from the text. Each time you run the search, even with the same keyword (by choosing the button "KWIC" in the top left corner of COCA), it will display a different set of randomly selected examples.

Why would the textual environment be interesting? Because you can see the word in context now, you will be able to see patterns surrounding the When you access COCA, the different colors denote different part of speech categories (here everything is black and white only). You can get the full list of what the colors mean via the website, but for now, let's pick a few: verbs are pink, prepositions, as in "say in the paper", are marked as yellow; nouns are turquoise, as in "I heard him say abstinence"; and adverbs and adverbials are in orange, as in "They all say basically the same".

Project 3.1: "Say" Followed by What Part of Speech?

Given the 100 randomly selected examples that you see, answer the research question "What is the distribution of the different part of speech categories following the keyword 'say' when it is a verb?". To answer this question, create a table (see Table

"SAY" NOUN PRONOUN ADJECTIVE PUNCTUATION "THAT" OTHER Total Project 3.2: "Say", POS, and Register

For this project, your new research questions are: "What is the distribution of the different part of speech categories following the keyword 'say' (when it is a verb) in spoken discourse versus academic prose?" and "Is there a difference in use?"

In this case, since you are more interested in how your keyword is used in different registers, you would want 100 sample concordance lines randomly selected from spoken discourse, and 100 concordance lines randomly selected from academic prose. To get those concordance lines for your sample, go to the main page of COCA, click on "chart" and type in "sa*_v" in the search box. This will select only instances of "say" as a verb and show you the distributional pattern across several registers. Click on the bar for spoken discourse (SPOK) and that will give you 100 sample cases when the verb "say" occurs in spoken discourse. Type in "say_spoken" under "Create new list" above the sample concordance lines and save the list. Then click on the academic prose bar to get your second 100 samples just from that register; save it as "say_academic". Create a table like Table

Project 3.3: "Say" and "State"

You also know that "say" and "state" are synonyms, so they could be used interchangeably when they are verbs -at least in terms of syntax and semantics. For simplicity, let's just pick the past tense/participle forms of these two verbs: "stated" and "said". You may predict that different registers probably use these two verbs differently because one is considered more "formal" than the other. Your research question is: "What is the frequency and distributional pattern of 'say' and 'state' (as verbs in past tense and participial forms) in three registers: spoken discourse, newspaper, and academic prose?" Create a table suggested in Table

• To state an action in the past, as in "She said that he had gone to the party" • To state a completed action with a result relevant in the present (present perfect), as in "He has stated his rights already" • To state action completed in the past prior to a past action (past perfect), as in "I had said that before" • To modify a noun (adjectival function), as in "the stated example"? Fill in Table

Keyword Analysis

Through a keyword analysis, you can identify words that exist in one text but not in another text. In other words, a keyword analysis helps you identify unique words in a text (keywords) when you compare that text with another text. You can also compare two corpora that are made up of several texts. In this case, you compare words in one corpus, called target corpus, with words in another, called reference corpus. Keyness defined this way was first promoted by

We list a few studies next where scholars have used keyword analysis, so you can have an idea of what kinds of questions they were interested in:

1. Words used by different characters in classic literary works have been a very popular topic for keyword analyses. For example,

Another example of tracking keywords in telecinematic discourse

leads to conclusions about changing gender roles through time. This work is exemplified by

With these studies in mind, the next project will compare two corpora already available through AntConc. Let's say, we are interested in finding out how American press reporting is different from British press reports in terms of vocabulary. You probably know the answer already that there are many words that would be different, but this project will look for some specific examples to show how you can support a perspective with empirical data. Our research question is: "How does vocabulary in American press reporting differ from words used in British press reporting?" Before we start, please watch the tutorial on how to use the keyword feature in AntConc: www.youtube.com/watch?v=SludW4FHatI&list=PL iRIDpYmiC0SjJeT2FuysOkLa45HG_tIu&index=10. Since we are using already existing corpora in AntConc, there is no need to upload any texts from your own corpus, but as the tutorial says, you are more than welcome to do that as well for other projects (including the related projects described below).

Step 1. Following the online tutorial on YouTube referenced above, identify the press reports sub-corpus in both the American and the British collections (AmE06_PressRep and BE-06_PressRep) and feed them into the tool. The British press reporting is your target corpus and the American press reporting is your reference corpus.

a) Go to File → Open corpus manager b) Download the required sub-corpora into the program; you have successfully downloaded the texts if the little diamond shape becomes green c) Highlight the British press reporting text (the line becomes blue) and press "choose", making sure that the tab on the right-hand window is on "target corpus"; then repeat the same for the American press reporting but now your tab will be on "reference corpus".

Step 2. Once you have downloaded the appropriate sub-corpora, run the analysis by clicking on the "start" button. The higher the keyness value for the words, the more likely that they appear in the target versus the reference corpus. The words in Figure

Another example is the word "blair" ranked as sixth on the list. This word clearly refers to Tony Blair, a former Prime Minister of Great Britain, and will be more prominent in British than in American press reporting (64 times in nine texts versus one time in one text, respectively).

5: Keywords in British versus American Press Reporting

After running the texts as described above, what other words can you identify on the list that are clearly more likely to be used in the British press? For example, UK, Britain, London … Can you group them based on some category that is contextually driven (like the analysis above may suggest)? For example, references to Great Britain as a context (its institutions, cities, etc.).

Can you think of any other groupings that you could come up with? (Hint: Do you see anything that may be special from a linguistic point of view?) Now click twice on the word "labour". Let's predict why this word is more likely to be used in British over American press reportage. What is the meaning/synonym of "labour"? Yes, it has something to do with work. Click on the word twice to see how it is used in context. What do you notice? Read some of the concordance lines and try to answer the question: Why do you think it is used with a capital "L"?

Project 3.7: Keywords in Your Corpora

If you want to work with your own texts, for example, from the Michigan Corpus of Academic Spoken Discourse (MICASE), you will need to download the texts first, save them as text files, and then feed them into AntConc (see YouTube tutorial).

If you need to convert webtext, i.e., from html to text, follow the steps on this website: www.computerhope.com/issues/ch001877.htm#text. If you need to convert pdf files to a text format, the best tool to use is another free program by Laurence Anthony, and it is called AntFileConverter: www.laurenceanthony.net/software/antfileconverter/. Once you have your texts in .txt format, you can go back to the tutorial and learn how to feed them into the AntConc program.

The one thing to keep in mind when you compare corpora for a keyword analysis is to choose corpora that are approximately the same size in terms of the number of words in them. A corpus with more or longer texts will allow more words in them. With more words, the frequency of each word increases and since the keyword analysis is based on frequency in one corpus over another (see tutorial), this may be problematic if you have different-sized corpora.

If you use MICASE, you can compare two different disciplinary areas to see whether they use certain words in one discipline versus another. To do so, select two disciplinary areas you are interested in -for us, it is history and natural sciences (natural resources). If you select these areas, you will be given multiple texts to choose from. Select the file titled "History Review Discussion Section" (DIS315JU101) and "Ecological Agriculture Colloquium" (COL 425MX075). We will use two areas because they have about the same number of words in the files. Download the files and make sure they are saved in text format (see the website referenced above on how to do that). Run the keyword analysis and then determine what sort of groupings you can identify for the types of words one session is using over the other.

Collocates

As mentioned in Chapter 1, collocates are two words that occur together more often than we would expect by chance. More specifically, Webster's dictionary defines collocate as a word that is "habitually juxtaposed with another with a frequency greater than chance".

Project 3.8: Collocates of "Spend"

Go to the main page of COCA, type in "spend", and click on "Word" among the list of options. On this page, you have all kinds of information about the word spend, including what words they collocate with. Look at what collocates this word has. You will see that the collocates are, for example, spend time, money, day, etc. If you click on the word "time" from this collocate listing and scroll down, you will be able to bring up all the examples where these two words occur together in texts. Click on "money", and examine the first 100 samples provided to you, answering the following research question: "In which register does the collocate spend (v) + time occur most frequently (in your sample)?" Another way of looking at collocates is if you click on the link called "Collocates" in the top right, you will see all the collocates with the word "spend" and for each, you can see their frequency counts as well.

N-Grams

Most often, n-grams in linguistics are sequences of words explored as a unit, where the value of n denotes how many words there are in that unit. If the basic unit of analysis is a word, then we can call that a uni-gram (1-gram). If we have two words to consider as a unit, they are bi-grams (2-grams), and if we have three words as a sequence in a unit, it will be a tri-gram (3-gram), and so on.

A special computer program is often designed to process the text and to look at sequences of words in a window of text as they emerge in a corpus. Depending on how "big" your unit is (i.e., how many words in a sequence you want to trace at a given point in time), the window size is set accordingly. That is, if you want to identify bi-grams, you will capture each two-word sequence in the corpus. If you are looking for tri-grams, you will capture each three-word sequence, and so on. As you are doing so, the already identified sequences are tracked and the new sequences are constantly checked against what the program has already found. Each time the same word sequence is found, the program counts the frequencies of that sequence. We can explore how frequently particular word combinations (n-grams) occur together in a corpus or how they are distributed across different registers.

If you know ahead of time what sequences you are looking for, you can just type the sequence in the search engine. In this case, you are not looking for the kinds of n-grams that may be emerging in that corpus; instead, you are just looking for a pre-defined sequence of words (that may, or may not, have been extracted from a previous corpus). For example, if you type in the word baby, you will see that it occurs over 60,000 times in the COCA corpus. But you picked the word ahead of time, so you knew what to look for. If you are interested in the sequence a baby, it occurs more than 10,000 times in the same corpus, and if you are interested in the sequence like a baby, you will see that it occurs more than 600 times in COCA. At the same time, the four-word sequence sleep like a baby only appears 25 times in the same corpus. In all of these instances, however, you have typed in the words that you were interested in.

In contrast, if you don't know what you want to look for ahead of time, but you are interested in the possibilities a corpus may have, you can either design a new computer program for yourself and run your own data, or run the n-gram program already available to you (e.g., through AntConc, a software that we discuss more in Chapter 5). The COCA site actually provides the lists for you, including bi-, tri-, four-, and five-grams, and their frequencies in COCA.

One-Grams or Uni-Grams and Individual Words

When you search for uni-grams, you are basically interested in individual words. When you know ahead of time what word you are interested in, they are often referred to as "keywords". As we have discussed earlier in this chapter, in the KWIC section, you could then see your keyword in a textual context. The purpose of your search for keywords could be that you analyze a) what kinds of words surround your keyword, b) how they are positioned in the sentence or in the phrase, or c) what the frequencies are of your keywords. We have illustrated how this may work in the previous chapter.

Project 3.9: "WAY"

Go to COCA and search for the word (or 1-gram) way. Click on the chart button. Report on the overall frequency of the keyword "way", its normed frequency, and its distributional patterns across all seven registers (as well as their use through time) noted in COCA -namely, spoken discourse, fiction, magazine, TV shows and movies, newspaper, and academic prose.

Project 3.10: Frequency Rank of "A" Versus "Juxtaposition"

Let's have a look at the vocabulary characteristics of a text. This way we can investigate patterns in larger units such as a text. Each word in the COCA corpus is classified into frequency bands. That is, each word is ranked depending on how often it occurs in COCA. For example, the third person pronoun he is ranked as the 15th most frequently occurring word in the corpus with a frequency of 6,468,335. The word way, when a noun is ranked 82nd in COCA with a total frequency of 1,260,011. The most frequently occurring words in general are function words -for example, articles (a, an, and the) or prepositions or particles (in, on, etc.). In fact, the definite article the is ranked #1 in the corpus with a frequency of 50,017,617. Now compare the rank and frequency of the word juxtaposition (as a noun) to the indefinite article that you have just searched. What is the rank and what is the frequency for this word? Is it a frequent word in the corpus?

Project 3.11: Vocabulary Characteristics of a Text

Go to COCA's main page and click on the icon that looks like a page of writing or a page of typed text (Figure

As you see, there are three frequency bands. Words marked with the color blue are among the top 500 most frequently occurring words in the corpus. The green ones rank as the top 501-3,000 most frequently occurring words in the corpus, and the yellow ones are marked for those that are in the rank of less commonly used vocabulary in the corpus (beyond the rank of 3,000). Be careful not to interpret these as frequencies because these are rank numbers. You can see the actual words from the text in these three bands on the right-hand side. Their frequency in that text is also shown. For example, the word "thief" is a low-frequency word (in band three marked with yellow) and it occurs three times in this text. If we jump to the frequency band, we see that the word "the" occurs 22 times in this text sample.

While Figure

If you click on any of the words in the list, it gives you information about that one word in terms of its distributional patterns across the different registers, provides a definition of the word and its collocates, and also provides examples from the COCA corpus in concordance lines. For example, if you select "thief", you will see the window shown in Figure

Project 3.12: Vocabulary and Academic Prose

In our example above, we used a sample from fiction as a register and looked at different kinds of words in the text. Following the same steps outlined above, choose a sample from another register -for example, academic prose (marked as ACADEMIC on the list). Follow the same steps to look at how many and what kinds of words you see in this text sample. Do you see any difference between the two registers in terms of the percentage of frequent (or not frequent) words in the text? Report your results.

If you want to check any text of your choice, all you have to do is copy and paste the text in the textbox and the words will be marked up for your text in the same way as it is done in the examples. The frequencies, once again, were identified in COCA. It is a great baseline corpus for your own research as well. This is a very powerful and useful tool to determine the vocabulary characteristics of a text. Can you think of ways you may be able to use this tool for your own writing?

Bi-Grams and Collocates

Bi-grams are two words in sequence. The difference between bi-grams and collocations is the fact that bi-grams are identified based on two words that happen to be next to each other in a corpus while collocations are two words co-occurring more frequently than by chance. Collocates are always two-word combinations, are statistically determined, and are also called 2-grams (or bi-grams), as mentioned before. All collocates are bi-grams but not all bi-grams are collocates.

Project 3.13: "Way" and Its Collocates

Look for the prepositions that are found with the word way. In COCA, type in way, click on "collocates" and type in the following:

As part of your qualitative analysis, you may want to see some of the meaning differences when you use different types of prepositions, or how a second particle or preposition is attached.

Tri-Grams

Any three words co-occurring together in the same sequence are known as tri-grams. Some are complete structural or semantic units such as by the way or in a way, and can be treated as fixed expressions with a specific meaning. Others may be a semi-structural unit such as by way of, which is

Project 3.14: "By the Way" in Registers

Let's look at COCA again. It is a common belief that by the way is used only in spoken discourse, and never in academic prose. First, let's search for by the way in two registers: spoken and academic prose. Make a frequency table as we did before and report on their distributional patterns. Do you see anything interesting to report or discuss?

Project 3.15: "By the Way" and "In a Way"

It is believed that both by the way and in a way are used as sentence adverbials. Make a general search for each and see whether this could be supported by the 100 examples that you have found. Another aspect of this would be to look at where these tri-grams appear in a sentence or utterance. Look at your 100-item sample in each of the two registers and determine whether they are always at the beginning of a sentence or utterance. Is there a difference between the two registers from this perspective?

Project 3.16: "By Way of " What?

Type by way of into the top and the command [nn*]. This string by way of [nn*] will allow you to search for nouns that come after the tri-gram by way of. This command will give you the strings and will list the nouns following the string.

Step 1: Click on "chart" and the button below where you typed in your string above. This will give you the distributional patterns of this construction (with any nouns) following the tri-gram. Then click on the SPOK (spoken discourse) bar. Is there a difference between spoken and academic discourse in the kinds of nouns that are used after by way of? When tri-grams occur with a particular frequency (e.g., 20 times) and in one specific register in a corpus, they are often called lexical bundles. However, four-grams are most generally investigated as lexical bundles and are discussed in the following section.

Four-Grams and Lexical Bundles

Four-grams are sequences of four words occurring together in a corpus. When we call them 4-grams, it does not matter how often they occur in a corpus; they are still called 4-grams. That is, every four-word sequence in a corpus is a 4-gram. However, similar to tri-grams, when these four-word combinations occur at least 10 or 20 or more times in a million words (depending on how conservative we want to be) and appear in at least five different texts (to avoid idiosyncratic -that is, individualistic -use) in a register, they are referred to as "lexical bundles" in the literature (see the background on lexical bundles in

Lexical bundles, a special type of four-word sequences, are defined by the number of times they occur in a million words in a register. As mentioned above, they are not necessarily structurally complete units, e.g., in the case of. But sometimes, they happen to be units that we recognize and know quite well, such as if you look at in classroom discourse or on the other hand in academic prose. The latter is a semantically and functionally complete unit even though that is not a typical characteristic of bundles by definition, and, therefore, it is not common to find these among bundles. Given that earlier studies (especially

Report on what you see in terms of spoken/academic usage of the lexical bundle on the other hand in the COCA corpus. Then click on the column under spoken. This way all your examples will be from the spoken subregisters in COCA. (See Figure

Take each one of the 100 samples you get this way and classify each bundle according to its position in the sentence. You can use the following categories: a) sentence initially; that is, when the bundle is the first four words in the sentence or utterance (for spoken); b) sentence finally; that is, when the bundle is the last four words in the sentence or utterance (for spoken); and c) neither sentence initially nor sentence finally; that is, when neither a) or b) applies. Use Table

As a next step, do the same with academic prose to get your samples from that register. Then classify each of the 100 examples as one of the three categories above. Finally, calculate the percent value for each. (Note: Since you had 100 observations for each register, the percent value and the raw counts are the same.) Reset the sample size to 200, run it again, and see whether your results are similar. As a final note to this section, the longer the n-gram, or the word sequence, the less frequently it will occur simply because n-grams are embedded in one another. For example, in the four-word sequence on the other hand, the three-word sequences of on the other and the other hand are both present. These would be counted as two separate three-word sequences.

Five-and More-Grams

When you are extracting n-grams from a corpus, you can imagine that your sequences could be endless. However, that is not true. First, all threeword sequences will contain all the two-word sequences in them. Similarly, if you look at four-word sequences, they will have all the three-word sequences in them (and the two-word ones as well). Therefore, the higher the n is in your n-gram, the less frequency you will get for each sequence. This is particularly relevant in lexical bundles where it is not just the sequence that is part of the definition of a bundle but the cut-off point counts as well. To illustrate this, we ran a lexical bundle search in a corpus of webtexts. This was a 1-million-word corpus of cybertexts collected from five internet registers: pop culture news, advertising, forum requests for advice, blogs, and tweets (Connor-Linton, 2012).

Project 3:18: Webtext Bundles

GlobWbe is an additional corpus on the website that contains texts from the internet. Check whether the webtext bundles reported above are present in all and with what frequency. In COCA, you can download the n-grams found up to 5-grams at their site with a specific n-grams website here: www.ngrams.info/samples_words.asp

POS Tags

Marking the words with part of speech (POS) tags [n, v, j, r] can lead us to different types of analyses than those we have seen so far. For example, you can look for specific words and their associated grammatical patterns or you can look for (co-occurring) grammatical patterns independent of the actual words. On the one hand, POS tags can help you be more specific about the words you are searching if you are going through an already existing search engine and if you are searching a corpus that has been tagged for part of speech. On the other hand, POS tags can also give you more options and more flexibility in your search. Below are examples of each. There are many other part of speech categories that could be potentially interesting for any linguistic study, but before going into some analyses we can do when we work with tags, let's clarify some basic grammar.

Each part of speech belongs to one of two basic classes: open or closed. Those part of speech categories that belong to the open class contain an unlimited number of members in them. That is, there is not a set number of members for an open class POS. There can be as many as there are in a language and we do not know how many there are. In contrast, those POSs that belong to the closed class have the characteristic to contain a limited number of members and we know exactly what they are. As another way to understand this distinction, note that we frequently have new open class words coming into the language, but the same is not the case with closed class words -new closed class words are quite rare.

Examples of POS belonging to the open category are the four main parts of speech: nouns, adjectives, verbs, and adverbs. Typically, nouns [n] are modified by adjectives [j], as in, for example, big red car where big is a characteristic of the noun in terms of its size, and red is a characteristic of the noun, telling us the color of the noun, and car is a common noun. Verbs

Whether a POS belongs to the open or closed class, there are endless possibilities to search for association patterns, as shown in Chapter 2. As we have also seen in that chapter, the co-occurring patterns of these categories are the most interesting types of studies from the perspective of register variation because they are able to provide us with more comprehensive and detailed analyses of texts. Now, some corpora (e.g., COCA) have POS tags attached to each word, and some corpora (e.g., Michigan Corpus of Academic Spoken English [MICASE], and Michigan Corpus of Undergraduate Student Papers [MICUSP]) do not have that feature in addition to the actual words in a corpus. Some scholars find it more difficult to do a search on POS tags, and others write their own computer programs to process and count the different grammatical patterns through those tags.

Specifying POS for a Word

First, let's see how specifying part of speech categories with POS tags can help you be more specific about your search words. In the previous section, we compared the use of state and say in spoken and written registers. Go to COCA, hit "Browse" and type in say in the word box. Select all four part of speech categories. As you will see (also in Figure

Using POS tags can broaden your options of looking for patterns -that is, how this will give you more options and more flexibility in your search. These main POS categories identify the word as you type it into the search box. Through the tags, however, we are able to look for variation within POS categories as well. The tags, for example, allow you to look for a given word in different word classes, such as can as a noun and can as a modal verb. The third possibility is to see how often a specific word comes about in texts with different word forms. In the previous examples, we looked at say as a verb. In COCA, you would be typing the following string into the search box: say.[v*]. This indicates that you want to search and see the word say in this form and when it is a verb. What if you want to find out how often the verb say is used in past tense because you would like to make a comparison between the past tense use of say across four registers or see the syntactic position of the past tense use of say. Type the following into the search box:

The fourth option could be that you have an n-gram (e.g., a lexical bundle) and you want to see what kinds of words precede or follow that sequence. Let's try this with if you look at.

Project 3.19: "If You Look at" What?

Go through the following steps in completing this task:

Step 1: Go to COCA. This chapter provides you with an opportunity to use readily available corpora to conduct corpus linguistics projects. In this chapter, you will gain practical experience in using corpus linguistic methodologies and interpreting results. There is a total of ten projects in this chapter with some comments for each project that can guide your analysis. Because there are different ways to conduct analyses with respect to search procedures and analyses, the projects do not have a single correct answer. At the end of each project, we provide commentary for those who may seek more guidance on how to search and interpret findings.

The projects in this chapter use four different corpora that are found on English-Corpora.org (www.english-corpora.org/), an online resource for corpora that is updated regularly and is an excellent resource for getting started working with corpora. Our decision to use this resource is related to cost, accessibility, and coverage. Although you need to register in order to use the corpus, as we mentioned in the previous chapter, access to the corpora is free of charge. Every 10-15 searches you will receive a message asking you to subscribe but subscription is optional. If you choose to subscribe, the current price (2023) is 30.00 USD which is quite cost-effective for the available corpora and data. Institutional licenses are also available so check with your school whether they have an institutional subscription. In addition to being reasonably priced, these corpora have a number of advantages for researchers, teachers, and students of language studies. The corpora include different varieties of English, including American (Corpus of Contemporary American English), British (British National Corpus), and Canadian (Strathy Corpus). There are also corpora focusing specifically on news found on the web (iWeb, GloWbE, Core) as well as television shows, movies, and soap operas. All of the corpora are in English but many of them include different varieties of English. For example, GloWbE contains Chapter 4

Projects Using Publicly Available Corpora

4.1 Word-and Phrase-Based Projects 4.2 Grammar-Based Projects web news from 20 different countries which permits investigations related to variation across different varieties of English. All of the corpora also use the same search interface so that once you learn how to "ask" for information in one corpus, you can conduct searches in all of the available corpora. The home page also includes many helpful resources and videos to guide you in the use of the corpus and use of search terms. There is even a link to training videos for people who are unable to access YouTube.

A list of the corpora we will use for the corpus projects in this chapter is provided in Table

There are a few important points to remember about these corpora that are especially true when conducting projects that compare features across different corpora or when looking at register differences. First, the corpus sizes range from the 1.9-billion-word GloWbE corpus to the 50-millionword Strathy corpus. This means that if one is comparing a different feature across different corpora, frequency comparisons refer to normalized counts. Another important aspect to keep in mind relates to different registers in the corpora. Some corpora (GloWbE and COHA) are comprised of a single register; other corpora (COCA and BNC) contain multiple registers. Even within a single register, there are situational differences that need to be carefully considered. For example, in the BNC, the spoken data include contexts such as oral histories, meetings, lectures, and doctor-patient interactions. In COCA, the spoken examples are all taken from television and radio news and information shows. These different situational variables mean that the term "spoken language" may mean (i.e., represent) different things in different corpora. It is important to understand the situational characteristics of the texts when representing a register (as we discussed in Chapter 2). These potential differences need to be considered when comparing and interpreting language features across corpora. Even though both COCA and the BNC contain a "spoken" component, there are many situational differences between news shows and face-to-face conversation (for one, news shows display grammatical features that are characteristic of more informational types of discourse while face-to-face conversation displays features that have been associated with more involved or interactional types of discourse; cf.

Word-and Phrase-Based Projects Project 4.1: Lexical Change Over Time

The Corpus of Contemporary American English (COCA) corpus is divided into different time periods. Find five words that are recent (have a higher frequency in the most recent time period) and five examples of words that are more common in the earliest time period. For the more recent words, is there a steady growth curve or does the word gain popularity fairly rapidly (i.e., in a single time period)? For the declining words, is there a steady decline or a fairly rapid decline? What are some possible reasons for these tendencies you have found? Comment: For this exercise, you may want to think of words related to technology, political events, or social trends. For example, if you were to look at "text" as a verb, you would see that this verb has increased from 1990 to 2020 while "fax" has moved in the opposite direction. In addition to the frequency of the word over time, you may also consider how the word is used. Do the connotations of the word remain the same over time? Are there any new meanings associated with these words?

Project 4.2: Meanings of "Literally"

The word literally was originally used to mean something similar to exactly, precisely, or actually. In this sense of the word, the meaning is closely related to the concept of something being free from metaphor. This meaning is understood in a sentence such as: I've literally spent more holidays with the animals than I have with my own family.

(example taken from COCA)

In addition to this meaning of the word literally, there is another sense of the word that shows emphasis (with a meaning similar to really). This sense of the word is sometimes the opposite of "being free from metaphor" as in a sentence such as:

There's a story about this in this book and it blows my mind, literally.

(example taken from COCA)

Using COCA, determine the variation in use of both the literal and figurative sense of the word literally by completing the following steps:

Step 1: Develop and state your method for determining whether the word falls into the "literal" category or the "non-literal" category. Do you find any other senses of the word that do not fit into your two categories? If so, describe the extra category or categories you have found and provide examples to support each category. Then, determine whether one sense of the word is more frequent.

Step 2: Register differences: Using the chart function, describe the distribution of literally across the different registers. Are there some registers where this word is used more than other registers? What are some potential reasons for any differences? Using the method you have developed in Step 1, do you find any register differences in the meanings of literally across registers?

Step 3: Looking at the distribution of literally across time periods in COCA, do you find any historical trends in the frequency of the word? Is there an increase in the different senses of the word literally over time?

Comment: Using the word search function will provide many examples (over 39,000) that may be difficult to interpret without some systematic way of analyzing the word. To narrow your search and make an analysis more manageable, you can choose a smaller number of texts to analyze or even create a smaller, virtual corpus. These are both explained in the "KWIC → analyze texts" option under the "guides" tab on the home page.

For Steps 1 and 2, you can get a general overview of the word by choosing "search" and then "word". This provides information such as register frequency, meanings, topics, collocates by word class, frequent clusters containing literally, concordance lines, and references to entire texts that contain the word. For Step 3, make sure that you are still using the COCA corpus and have selected "chart" in the search.

Project 4.3: "Gate" as Post-Fix for Any Accepted Problem After the Term "Watergate"

In addition to its contribution to the demise of the presidency of Richard Nixon, the so-called Watergate Scandal has contributed to the American lexicon through the use of the suffix -gate added to describe controversial political or social incidents. In this project, you will explore the different ways this suffix has been used in American English and look at other language varieties to see if this suffix is also found in other varieties of English.

Complete the following steps:

Step 1: Using COCA, identify at least six cases where this suffix is used with a noun. For each, note its first appearance; then note when each term was most frequently used and whether it is still used today.

Step 2: Use GloWbE to determine whether this suffix is also found in other varieties of English.

Step 3: Interpret your results: What are some possible reasons for your findings? Are there other examples of prefixes or suffixes that are specific to varieties of English?

Comment: A search using the wildcard * + gate will yield a list of words containing -gate. These include the word gate as well as words such as investigate and promulgate. You will also find words such as travelgate and pizzagate that are more relevant. From this list, you can then select specific instances of the word that use this suffix. When using other corpora, you may either search for the exact words you found in COCA or use the same wildcard search to see if there are specific uses of -gate in the other corpora you have used. Using the latter approach will identify any creative uses of the suffix.

Project 4.4: Clichés

Owen Hargraves has written a book titled It's Been Said Before: A Guide to the Use and Abuse of Clichés (2014). In the introduction of this book, Hargraves states:

While it is true that a vast number of expressions have become tired through overuse, there is an opportunity to make even the most worn expressions striking and powerful. How do we decide which among many expressions might be just right for the occasion, or just wrong? (p. xi)

In this project, we will take a close look at some of the clichés Hargraves mentions in his book. Complete the following steps:

Step 1: Come up with a "working definition" of a cliché. How has the concept of a cliché been defined? Is there general agreement on what a cliché is? What differences in definitions do you note?

Step 2: Use the "search" and "word" functions to inform your working definition. Specifically, how does the topic, collocate (what nouns, verbs, adjectives, and adverbs occur most frequently with cliché?) and cluster information help in defining your definition? Step 3: Using both COCA and the BNC, provide the normalized frequency of each of the phrases below:

What do these frequencies tell you about the relationship between these clichés and American and British English?

1. Are there specific registers that use these phrases more frequently than other registers? What might be some reasons for any differences you find? 2. Given your working definition of a cliché, would you define any or all of the six examples as clichés? Are there other examples that are more representative of clichés?

Comment: In addition to deciding if these six phrases are all clichés, you may also want to consider the extent to which these phrases are fixed. For example, you can search for the most frequent collocate that occurs after dizzying or the most frequent collocate that occurs before array (make sure to choose the first slot to the right or left of these words). Doing this for many of these phrases can provide information on how frequently these words collocate. This approach is easier to do with phrases that do not have function words (touch and go; point the finger) since there will be many more words that precede and follow function words such as the function words and/the.

Project 4.5: Collocation of Modifying Elements

In this project, you will look at the most common words that follow the modifiers below.

You will use COCA, the BNC, and GloWbE. Complete the following steps in the project.

Step 1: For both COCA and the BNC, determine the most common word that follows each of the terms above. You can do this by using the dizzying array meteoric rise point the finger perfect storm touch and go totally awesome categorically deeply entirely far-reaching massively "COLLOCATES" function; set the span to "0" on the left and "1" on the right.

Step 2: What similarities and differences in the two language varieties do you find in the types of words that follow the modifiers?

Step 3: Use GloWbE to determine the most common collocate of the five modifiers above. Using the patterns you found for both American and British English, try to find a language variety that patterns like American English and a language variety that patterns like British English for each of the five modifiers. What factors might influence these language varieties to pattern like American or British English? Do you find any patterns that are unlike both American and British English? What are some possible reasons for any new patterns that you find?

Comment: When searching for these modifiers, note the frequency of each word. What are the most common and least common modifiers? Do the frequency counts for these words stay the same across language varieties? Additionally, when looking at collocates of these words, pay attention to the types of words that are found to the right. Are all the words adjectives or are there other word types (e.g., verbs) found as a right collocate?

Project 4.6: Sustainability

According to the Oxford English Dictionary (www.oed.com), the adjective sustainable originally referred to the ability to endure something. In this definition it was synonymous with the adjective bearable. Although this use of the term is now quite rare, there are other meanings of sustainable that are more commonly used in English. These definitions are provided below (definitions are quoted from www.oed.com):

1. Capable of being upheld or defended as valid, correct, or true 2a. Capable of being maintained or continued at a certain rate or level 2b. Designating forms of human activity (esp. of an economic nature) in which environmental degradation is minimized, esp. by avoiding the long-term depletion of natural resources; of or relating to activity of this type. Also: designating a natural resource which is exploited in such a way as to avoid its long-term depletion.

In this project, you will use both COHA (Corpus of Historical American English) and COCA to investigate these different meanings of the word sustainable (and its noun counterpart, sustainability) over time and across registers. Complete the following steps:

Step 1: Using COHA, note the first 50 occurrences of the adjective sustainable. For each use, provide the date of occurrence and note which of the three definitions provided above best fit with the occurrence of the word. Make sure to provide examples from the corpus to support your analysis of their meanings. Is one use of sustainable more prevalent than other uses of it? Is there a tendency for the meaning to change over time?

Step 2: Using COCA, note the register distribution of the adjective sustainable. In which registers is sustainable most common? In which registers is sustainable less common? Are there specific meanings of sustainable that are representative of specific registers? Provide some reasons for any register or meaning differences that you find. Make sure to support your analysis with examples from the corpus.

Step 3: This part of the project asks you to look at the meanings and register distribution of the noun sustainability. According to the online site "Environmental Leader":

Sustainability includes sustainable building, design and operations. Sustainability is the collection of policies and strategies employed by companies to minimize their environmental impact on future generations.

Ecological concerns, such as the environmental impact of pollutants, are balanced with socio-economic concerns such as minimizing the consumption of limited natural resources to maintain their availability for the future. (www.environmentalleader.com/category/sustainability/)

Using COHA, note the first 20 occurrences of the word sustainability. For each use, provide the date of occurrence and note which of the three definitions provided above best fit with the occurrence of the word. Make sure to provide examples from the corpus to support your analysis of their meanings. In which registers is sustainability most common? In which registers is sustainability less common? Provide some reasons for any register distribution differences that you find. Do the meanings of sustainability all relate to environmental or ecological issues or are there other senses of the word that are found in COCA? Comment: It is possible to search for the most common nouns following sustainable using <sustainable NOUN+>. For sustainability, using the first left collocate function will provide a list of the most common adjectives preceding sustainability. It may also be helpful to use the "compare" function to investigate any synonyms of sustainable/sustainability. Do sustainable/sustainability have any viable synonyms? Project 4.7: "Frugal", "Cheap", and "Thrifty"

In this project, we will consider different connotations of the adjectives cheap, frugal, and thrifty. We will also look at how these words may differ in their syntactic positions. There are a group of adjectives in English that can occur in both pre-noun and post-verbal positions. For example, the adjective little can be used in the sentence The little house is painted blue as well as in the sentence The house is little. In the first sentence, the adjective little is called an "attributive" adjective (i.e., it occurs in the attributive [pre-noun] position); in the second sentence, the adjective is called a "predicative" adjective (i.e., it occurs in the predicative [post-verbal] position). Not all adjectives have such a freedom of movement to these different positions. For example, the adjective upset generally is found in the predicative position (The man is upset) and may sound odd in the attributive position (The upset man left the library).

This project will consider the connotations of a group of adjectives that can occur in both attributive and predicative positions. We will start this project by considering the following letter that appeared in "Dear Abby" on

Step 1: Using COCA, report on the frequency and distribution of the adjectives cheap, frugal, and thrifty. Which of these words is the most frequent, and which of these words are less frequent? Are there any register differences in the distribution of these words? If so, what are some possible reasons for any register differences? Step 2: Using the "KWIC" and "COLLOCATES" functions, explain any differences in meaning among these three words. Do some words have a more positive or negative connotation than other words? If so, what evidence can you provide to support your answer? Make sure to use examples to back up your analysis.

Step 3: Using the "POS" function in COCA, determine whether each of these three adjectives is more common in attributive or predicative position. Do all three adjectives have similar syntactic distributions? Are there differences in meaning when the same word is in a different syntactic position? Make sure that you use examples to support your analysis. Also, make sure that you include all of the search terms that you have used.

Step 4: Given what you now know about the different meanings and syntactic positions of the adjectives cheap, frugal, and thrifty, write a response to "Thrifty in Texas" that might provide some helpful advice for how to address his problem.

Comments: In Step 2, you can use the "compare" function and look at the most common words to the left and right of the adjectives. This option allows you to control the number of words to the left and right. Choosing the words directly before or after the adjective will tell you about any potential collocations; choosing a larger span will tell you more about the other types of words that are semantically associated with each adjective. In Step 3, one possible search string for the attributive position is looking for all verbs before the adjective (e.g., <VERB thrifty>) or all nouns following the adjective (e.g., <NOUN cheap>).

Grammar-Based Projects Project 4.8: Variation in the Passive Voice

There are two main "voices" in English. In grammar terms, voice refers to the relationship between the verb of a sentence and the other participants, such as the subject and the object of a sentence. In a sentence such as The boy saw the ghost (an active voice sentence), we know who sees the ghost (the subject of the sentence, the boy) and who is seen (the object of the sentence, the ghost). In this sense, the subject noun phrase the boy serves as the "actor" of the sentence (the one doing the action of the verb) and the noun phrase object the ghost serves as the "patient" or "recipient" of the action (the receiver of the action). The second type of voice in English is called the passive voice. For example, in a sentence such as The boy is seen by the ghost, even though the boy is still in the subject position of the sentence, it is the ghost who is doing the seeing in this type of sentence. Consequently, the voice of the sentence provides information on who or what is doing the action and who or what is affected by the action expressed by the verb. The passive voice is formed by adding an auxiliary verb (be) to the passive voice sentence and substituting the "regular" form of the verb with the past participle. You will be asked to look at this rule a bit closer in the project below.

Another noteworthy aspect of the passive voice includes variation in the extent to which the original actor of the sentence (the subject of the active voice sentence) is present in the passive voice sentence. For example, compare the active voice The girl broke the window with its passive voice counterpart, The window was broken (by the girl). In this passive voice sentence, there are two options, one that states who broke the window and another that doesn't state the subject by the deletion of the entire "by phrase" (e.g., The window was broken). We can call the first of these passive types, the long passive (a passive sentence that includes the "by phrase"), and the second type, the short passive (a passive sentence without the "by phrase").

In addition to the verb be, the passive voice can also be expressed by the auxiliary verb get. See Table

Using COCA, compare the two forms of the passive, initially concentrating on the two auxiliary verbs was and got by using the search strings was _v?n and got _v?n).

1. Which of these two types of passive is more common overall? Are there register differences between the two passive types? What possible reasons might there be for any differences you have found? 2. Is there a difference in the verbs used in was passives versus got passives? How would you describe these differences? 3. Choose five different was passives and five different got passives. For each type, determine whether there is a preference for the "by phrase". Does your data suggest that the "by phrase" is dependent on the verb, on the auxiliary verb, or is it due to some other factor?

Comment: In addition to passive voice differences between was and got, it is also possible to include different forms of the auxiliary verb in your analysis. This can be done by replacing was with is/are/ being or got with gets/getting. Alternatively, the lemma of each verb can be used by using the search terms BE _v?n or

Project 4.9: "Going to" as a Modal Verb

In the two sentences below, the underlined words going to are different types of grammatical constructions. In the first, going is the main verb of the clause and to is a preposition that is followed by a noun phrase (the loo); in the second sentence, going to precedes the verb go. One way to show this difference is to determine when going to can be contracted or simplified to gonna. It is possible to use gonna in the second sentence but not the first sentence.

(a) My father is up all night going to the loo which keeps both him and my mother awake. (b) Yet not everyone is going to go to college, develop software skills, or become an entrepreneur.

(examples from COCA)

The difference between these two examples illustrates the descriptive observation that in cases where a contraction is permissible, the going to is functioning as a modal verb; in cases where gonna is not possible, the construction is comprised of a main verb (going) followed by a prepositional phrase. In this project, you will examine the modal verb going to in detail. Complete the following steps in your analysis of the modal verb going to (gonna).

Step 1: Determine the search term(s) you will use to find examples of going to followed by a verb (e.g., going to VERB) Step 2: Use COHA and determine when this use came into the language.

What patterns of development do you find? Are there certain verbs that tend to occur with going to? Have these verbs changed over time?

Step 3: Are there differences in the way the two forms of going to and gonna are used in COHA? What are some possible reasons for any differences you may find?

Step 4: Using COCA, do you see any register differences in the use of going to and gonna as a modal verb? If so, what are the differences you see? Try to provide some explanation for any register differences you may find.

Comment: Trying different search terms can be helpful in this project. Some possible search terms are: <BE going to> which provides all different forms of the verb be before going to (e.g., are/is/were going to). It is also possible to search for both <VERB going to> to find all verbs preceding going to and <going to NOUN> to find all nouns following going to.

Similar searches can also be done with gon na (<VERB gon na> and <gon na NOUN> which can provide some very interesting results. Doing similar searches in GloWbE will also illustrate potential differences across different language varieties.

Project 4.10: Grammatical Constructions Following "Begin", "Continue", and "Start"

In English, there is a good deal of variation in the forms of non-finite grammatical clauses that follow (i. Note that the infinitive clause is not a possible complement of miss (*… missed to go to regionals) and the gerund clause is not a possible complement of ask (*… asked estimating the surface normal at many points on the drawings). There are other verbs that allow both gerund and infinitive clauses as complements. The verb start, for example, allows both, as seen in (

Step 1: Using COCA, report on the complementation patterns of the three verbs (begin, continue, and start). How do the three verbs compare in their complementation patterns?

Step 2: For each of the three verbs, determine whether there are register differences in the patterns.

Step 3: What reasons account for the variation of complementation patterns in these three verbs?

Comment This chapter will take you through the steps to complete a corpus project. By reference to a specific research question, you will learn how to build your own corpus and then analyze it using both the register functional analysis approach covered in Chapter 2 and the corpus software programs covered in Chapter 3. You will also learn how to use AntConc in this chapter.

Do-It-Yourself Corpora

In the previous chapter, you were exposed to readily available corpora through the projects using the suite of corpora at English-corpora.org. These corpora can be used to explore language variation by reference to different situations of use, such as newspaper writing, fiction, and spoken language from news talk shows. These corpora are not, however, designed to understand language variation in other contexts that may also be of interest. For example, there is no easy way to determine information about the gender or age of those who produced the texts. If you were interested in looking at gender or age differences in language use, these corpora would not be of much use. Certain research questions require "specialized" corpora that are built for specific purposes. Sometimes researchers need to build their own corpora. Corpus building not only allows you to answer a specific research question, but it also gives you experience in corpus construction. There is likely no better way to learn about the issues in corpus design and to appreciate the larger corpora built by other researchers than to build one on your own. Constructing a useful corpus involves a number of steps that are described below. Before covering the steps in corpus building, we should acknowledge potential copyright issues. In some cases, you may use the internet for the texts to include in your corpus. In order to do this, you will need to carefully consider your selection of materials and the potential copyright infringement issues that relate to compiling and storing digital texts. Additionally, it is important to take into account the country in which the corpus materials are used. Different countries have different copyright rules. What might be considered a copyright infringement in one country may not be considered so in another country. If you are using the corpus for educational purposes and do not plan on selling the corpus or any information that would result from an analysis of the corpus (e.g., in publications), the likelihood of being prosecuted as a copyright violator is usually small. Nevertheless, you should take into account the following guidelines when building your own corpora:

• Make sure that your corpus is used for private study and research for a class or in some other educational context. • Research presentations or papers that result from the research should not contain large amounts of text from the corpus. Concordance lines and short language samples (e.g., fewer than 25 words) are preferable over larger stretches of text. • When compiling a corpus using resources from the internet, only use texts that are available to the public at no additional cost. • Make sure that your corpus is not used for any commercial purposes.

• Make sure to acknowledge the sources of the texts that are in the corpus.

For those interested in more information on corpus building and copyright laws, there are some sources to consult at the end of this chapter.

Deciding on a Corpus Project

Corpus research projects take a good deal of time commitment to complete. A worthy research project has a number of different components, including providing a motivation of the significance of the topic, a clear description of the corpus and the methods used in the study, presentation of results, a discussion of the results, and a conclusion that provides a summary and "takeaway message" of the research. In Chapter 8, you will learn more about how to present your research as both a written report as well as an oral presentation. However, before embarking on this project, it is valuable to spend some time thinking seriously about what you want to research and the reasons for conducting the research; i.e., the research goal of the study. Selecting an appropriate research issue is not This list is not intended to be exhaustive. Each of the topics and subtopics described above address issues that are not specific to the field of corpus linguistics but lend themselves to corpus research quite well. For example, a topic examining gender differences could involve creating a corpus of fitness articles with females as the intended audience and comparing this to a corpus of fitness articles with males as the intended audience. A project looking at the language of social media posts could be achieved by creating a corpus of Twitter posts and comparing the language used in the posts with written or spoken language found in existing corpora such as the CORE corpus found English-corpus.org.

In addition to identifying a research goal, you should also write a research question or set of research questions that you seek to answer in your research. Because this book uses register analysis as a framework for interpreting your research, the research questions in your projects all share the similarity of investigating the extent to which situational variables result in different linguistic features for some functional reason. In this sense, all research questions are framed from a particular perspective by reference to a specific methodology (corpus linguistics). The research question of an individual study depends on the specific variables under investigation. Note that all of the research issues described above are in the form of questions. Each research topic has a corresponding question (or set of questions) or a hypothesis that will be answered in the research study.

Whatever issue you select, you should have a convincing explanation of your reason for conducting the research. The first questions you can ask about your project are: "What is my research goal?" and "Why was it important to conduct the research?" If you find, for example, that song lyrics have different linguistic characteristics in different types of music, what is the relevance of this finding? Does it say something about the possible socio-cultural aspects of the consumers of the music or does it say something about the music genre in general? Clear and convincing reasons for choosing a research topic will not only help you in motivating your research, but it will also help you in interpreting the results of your research. Worthy research topics do not need particular outcomes to be interesting or relevant. To use the example of song lyrics and musical varieties again, it would be just as interesting to find little difference in the linguistic characteristics of musical varieties as it would be to find strong differences.

A final consideration relates to the type of corpus that you will build to conduct your project. A vital part of your corpus project is, obviously, the corpus itself! Before deciding on a final topic, you should determine the availability of texts that will enable you to address the issue you propose to research. You will need to make sure that the types of texts you need to carry out your project are available free of charge (so as to decrease the chance of a copyright infringement).

Giving careful thought and consideration to the importance and relevance of your research topic (including a strong justification for your selection of a research topic, i.e., the motivation for your study) is more likely to result in a project that you are proud of and that contributes to an understanding of language variation. Taking the time to consider the significance of your project and its potential application to the field of applied linguistics (or other fields of study such as sociology, business, or art and music) is time well spent.

Building a Corpus

Once you have selected an adequate research goal and corresponding research question (or set of questions), the next step is to build a relevant corpus. The corpus that you will be building for your project will likely not be a large general corpus but will be a smaller, "specialized" corpus that is designed to answer the specific research question(s) you are investigating. One difference between specialized corpora and larger, more general corpora relates to their purpose: Specialized corpora are normally designed to address specific research questions while general corpora are intended for a larger audience and are designed to answer a larger set of research questions posed by multiple researchers. This is not to say that specialized corpora are never used to answer different research questions, but they generally are designed to investigate a restricted set of questions, and therefore, are less likely a representative of language use in general terms. As you will see further in the next chapters, with smaller, specialized corpora, you are only able to draw conclusions in your dataset rather than generalize the results to larger contexts. Even though smaller, specialized corpora are used for more restricted research purposes than general corpora, adopting a sound set of guidelines to build the corpus is still important. A welldesigned corpus includes texts that are relevant to the research goals of the study; are saved into a file format that allows different software programs to analyze the texts; and are labeled with enough relevant contextual material so that the different contexts are easily identifiable in the corpus. We will take a closer look at each of these below.

The selection of the texts to include in your corpus depends on their suitability and their availability. Clearly, the texts need to share relevant characteristics (or variables) that meet your selection criteria for inclusion in the corpus. A project that considers how news writing changes in different time periods would, obviously, require a corpus that includes newspaper articles written at different periods of time. In order to build a corpus to address this issue, you would need to make sure that there is an adequate number of newspaper articles that have been written at different time periods. Additionally, a corpus addressing this issue would need to have sub-corpora of relatively equal size. As illustrated in some of the corpus projects that compared COCA with the BYU-BNC corpus, the unequal sizes of these two corpora did not allow for straight frequency comparisons between the two corpora. Thus, corpus "balance" is a key aspect of reliable corpus building. Note that the balance should consider not only the number of texts in each sub-corpus but should also consider the word count of the sub-corpora.

Frequency comparisons are done on the basis of the number of words, not by the number of texts. If your specialized corpus also contains subcorpora, then you should ensure the sub-corpora is of fairly equal sizes. Another issue related to corpus balance in your corpus relates to text types. A news writing corpus would need to include the various types of news textssports and lifestyle news as well as state, local, national, and international news. If only one of these text types is included then the sample might not account for variation in the different types of news texts. A balanced news writing corpus would either include texts of sports, lifestyle, and general news texts or would select only one of these text types for analysis.

Table

Once you have located relevant texts that can be used to build a balanced specialized corpus, you will need to prepare the text to be read by a software program such as AntConc, a popular and powerful program available free of charge. Different types of texts have different types of character encoding associated with them. If you use texts from the internet, the texts will likely be in Hypertext Mark-Up Language (HTML). A text that is read on a web browser such as Google Chrome or Safari looks like this: You are not very good at parking. You're just not. Unless you happen to be a middle-aged gentleman from China called Han Yue. If you are indeed Mr Yue, then (a) welcome to

This coding scheme would allow you to clearly identify the text with "010001" being the first text in time period A, "010002" being the second text in time period A, and so on. In order to ensure that the text numbers relate to the characteristics of each text, each text will also have the relevant header information described above. Note that this entire corpus -let us call it A Historical Corpus of American News Writing -would consist of three sub-corpora related to each of the three time periods. All of the files in a single time period would be available in a single folder so that each sub-corpus could be loaded separately. Depending on different research questions, the corpus could also be loaded with all three time periods. Note that if the files followed a consistent labeling practice, you would be able to determine the time periods by reference to the file name easily.

An alternative way to name files would be to use transparent file names with "word strings" instead of numbers. This way, the file names are transparent immediately, and information about the extra-textual features of the files can be accessed easily. If you choose to do this, you will need to make sure that the filename length is the same even though you may not have information in a particular category (for easier processing). For example, "news_15_election_00" would mean a news text from 2015 about an election in 2000.

Depending on your research goal and the design of your corpus, you may also want to include specific information on the situational characteristics of each text in individual text files in your corpus. Specific information such as the length of each text (by number of words), the topic, or the type of each text (if you included different types within a specific genre) can also be included in individual text files. This type of information is not a part of the text analysis but it can provide important interpretive information used in the functional analysis of your results. In a corpus of general song lyrics, you would want to include lyrics from different types of music (rock, rap, country, popular music, etc.) in order to achieve balance in your corpus. To be able to identify these different types of song lyrics, you could either come up with a system of naming each file (as described above) or you could include some of this information in each text file. Because you do not want this information counted as part of the linguistic characteristics of your text, you can put this or any other relevant information that you do not want to be a part of the linguistic analysis into angled brackets (< >). This type of information is included in the "headers" of the text but will not be read by the concordance software. Thus, each individual text file can include a relevant header and other extra-textual information as well as the text itself.

Figure

However you go about constructing your corpus, you should use the following questions to guide the process:

1. Do the texts in my corpus allow for an investigation of a specific research issue? 2. Is the corpus constructed in a balanced manner? 3. Are the texts in a file format that will allow for analysis by the corpus software you will use in your analysis?  4. Does each text have a specific code and/or header information so that specific information in each file is identifiable? 5. If relevant to your research goal, is the corpus constructed so that specific sub-corpora are readily retrievable?

Software Programs and Your Corpus

As we mentioned in previous chapters, Laurence Anthony works at Waseda University in Japan (www.laurenceanthony.net/software.html). He develops software programs that are extremely useful for corpus linguistic analyses and he makes them freely available (although you are able to make a donation should you choose to do so). To date, there are 17 software programs available for PCs, Macs, and LINUX. While it is worth finding out about each one of the programs on Anthony's webpage as they are very useful, we will mainly focus on two here, AntWordProfiler for lexical analyses and AntConc for lexical as well as grammatical analyses, as the most pertinent for your use when analyzing your corpus.

AntWordProfiler

The function of Anthony's word profiler is very similar to what we saw with WordandPhrase, except for two main differences: 1) You can use as many texts as you want at once for an analysis; and 2) instead of using COCA as the background or monitor corpus, this one uses two other word lists (General Service List by Michael West, 1953, and Nation's academic word list) on which vocabulary frequency bands are based. (See Figure

1. What kind of information can you get to know about your text(s) through the Vocabulary Profile Tool? 2. What kind of activities can you do through the File Viewer and Editor tool? 3. What do the different menu options do?

Project 5.1: Vocabulary Comparison

Let's say you are interested in finding out about the differences in the way vocabulary is used in a Wikipedia page and your own term paper on the same topic. Take one of the papers that you have written for another class and save it as a text file. Then search for the same topical area on Wikipedia, and copy the text, saving it into a text file. Read both texts into the AntWord Profiler, and run the program twice, once on each individual file.

AntConc

The function of Anthony's concordance program is similar to what we saw at the main interface of COCA. Using your own corpus, you should be able to do KWIC searches through the concordance lines, and other types of lexical as much as grammatical analyses in your own texts. Once again, download the "Help" file to get an overview of what is possible with this particular program

Clearly, this program is capable of facilitating some of the same kinds of analyses COCA did but with your own texts. Among those analyses are: KWIC (keyword in context), n-grams, collocates in a particular text, and word lists. In addition, this program is able to show you how the word (or collocate or lexical bundle or any n-gram) is distributed within each of your texts (a concordance plot) as well as how many texts include examples of your search term.

Read in (i.e., upload) your corpus through the "File" menu ("Open files") and type any search word in the search box that you would like to find out about in your text(s) and hit the "start" button to get a KWIC concordance line. (See Figure

If you press the "Sort" button, the words following the search term will be in alphabetical order. It is important to keep in mind that the colors in AntConc do not denote part of speech categories as they do in COCA; they simply show first and second and third place after the search term. (See Figure

As mentioned above, if you click on the "Concordance plot" tab, you will get a view of the spread of your search term in each of the files you uploaded as part of your corpus (see Figure

It is also possible to identify collocates of your search term. In Figure

If you click on any word on the list, it will bring you to a concordance line listing the instances of that collocate; by clicking on the word, you can take it from here for a larger textual span, as you have seen above. (See Figure

You can also generate an n-gram list based on the texts you have. Click on the "Clusters/N-grams" tab on the top and click on "N-grams" under the search term on the bottom, and also specify how big the window size should be under "N-gram size". If you are interested in lexical bundles, you should also specify what the minimum cut-off is under "minimum frequency" and "minimum range" just below. In Figure

Why Do Statistical Analyses?

When doing register analyses, researchers look for patterns of language use and their associations with the texts' situational characteristics. We need empirical measures to see what these associations are, and we need quantitative measures (e.g., the frequency of a particular language feature) to see how commonly these patterns occur. We can then look at how the frequency of that measure is distributed across the two or more situations that we are interested in.

Descriptive statistics will give us averages through which we can compare typical uses of the features in question. However, this only gives us an impressionistic view of the difference for our dataset. If we rely solely on descriptive statistics, we cannot tell how generalizable those differences may be. To be able to generalize about the "typicality" of patterns of use, we need to use other statistical procedures.

Generalizability means that the results in our sample can be predicted to be true, with a high level of certainty, to samples outside of our own dataset as well. That is, if we were to conduct a new study under the same conditions we are reporting on, we could be 95% or 99% certain to get the same results. In order to have generalizable results, we need to make sure that a set of assumptions about our data is met (see later in this chapter).

Basic Terms, Concepts, and Assumptions

In this section, we outline the basic terms and concepts that are used when doing any kind of statistical analysis. First, we discuss variable types and Chapter 6

Basic Statistics

6.1 Why Do Statistical Analyses? 6.2 Basic Terms, Concepts, and Assumptions 6.3 How to Go About Getting the Statistical Results 6.4 End of Chapter Exercises levels that are critical to know before any test can be done. Second, we introduce measures of central tendency ("typicality" in a dataset) and measures of variability or dispersion ("spread").

Variables and Observations

Variables are typically classified based on a) the type of variable and how they function in the research design, and b) the range of values and levels they can have. It is crucial to think about this ahead of time because the validity and reliability of our research depends on how we define our variables and observations. Also, the variable scale and type determines the types of statistical analyses that can be done.

Variable Types and Functions

"Regular variables" are variables that you either manipulate or want to see change in your design. They can have a range of values (numeric) or levels (non-numeric). Numeric values are relevant if you have frequencies of a linguistic feature (e.g., the number of personal pronouns in a text). Non-numeric values (or levels) are relevant when you refer to a variable in terms of categories, e.g., class size ("small" or "large"). Instead of calling them small or large, you could also give numbers to these categories (e.g., small = 1 and large = 2); however, they are not values, just numeric codes. That is, there is nothing inherently primary or secondary in the number they are assigned to.

Further classification of regular variables is based on the function they have in the design. We distinguish between two types: dependent variables and independent variables. Dependent variables are the variables that you are most interested in for your research because you think that the values of the variable (e.g., frequency) will change (or not) as you are manipulating some external factors around it. The change that will occur (or not) depends on your manipulation of other variables around it. Independent variables are the variables that you manipulate in order to see whether there is a change in the dependent variable. Dependent variables are often called "outcomes" and independent variables are often called "predictors" (of change).

EXAMPLE

You read in an educational journal article that lectures in small classes are more "personable" than in large classes. As there is no linguistic evidence provided in the article for this claim, you want to find out yourself. You decide that you will use first person pronouns (I, we) as a measure of "personable". You are interested in whether the frequency of first person pronouns (I, we -and all their variants) changes at all when you are attending a lecture in a large class with 200 students or in a small, seminarlike class with 20 students. You hope to see that the frequency of first person pronouns will change depending on which class you attend. That is, the use of first person pronouns will depend on the class size (lecture versus seminar-type). The dependent variable in this design is first person pronouns (the frequency of which will change) and the independent variable is class size (the one that you manipulate to see the change). So, your predictor for change in the outcome (pronoun use) is class size.

"Moderator variables" are referred to as other predictors or other independent variables in your design (if you have more than one). Moderator variables are viewed as independent variables potentially interacting with other independent variables. In our example, let's say you want to see whether the instructor's gender also has an effect on the use of first person pronouns in small or large classes. Your independent variable is class size and the moderator (or other independent variable) is gender. In this design, you may be interested in whether it really is class size alone, or gender alone, or the two independent variables together (class size moderated by gender) that cause a change in the use of first person pronouns.

"Control variables" are not real variables in the way we have been describing variables so far. You are not able to measure a control variable in your study; instead, it is just something you control for.

"Intervening variables" are variables that you should have measured in your study but you realize later that you didn't. Typically, these are the variables that are mentioned in the discussion section of an article or report when calling for further research.

Variable Scales

"Nominal scales" (also called categorical, discrete, discontinuous scales) are variables measuring categories. They are used in naming and categorizing data in a variable, usually in the form of identity groups, or memberships. The variable could occur naturally (e.g., sex, nationality) or artificially (experimental, control groups), or any other way, but in all cases, it is a limited number of categories. They represent non-numeric categories (e.g., religion, L1, ethnicity). When they are assigned to numbers, they carry no numeric value. Instead, they are only a category identifier (e.g., there are two sexes: 1 = male, and 2 = female).

"Ordinal scales" are used to order or rank data. There is no fixed interval, or numeric relationship in the data other than one is "greater than" or "lesser than" the other. No fixed interval means that we don't know whether the difference between 1 and 2 is the same as between 4 and 5 (i.e., no fixed interval between values as is the case for interval scales).

Examples of ordinal scales are holistic scoring, Likert scales, and questionnaires. They are numeric in that the numbers represent one being more -or less -than the other, but they do not say how much more.

"Interval scales" reflect the interval or distance between points of ranking. They are numeric, continuous scales, and are the same as ordinal but with fixed intervals. That is, while with ordinal scales we do not know whether the difference between 2 and 3 is the same as between 4 and 5, with interval scales we do. For example, the difference between 18 and 19 milliseconds is the same as between 22 and 23 -that is, one millisecond. The difference between 2 and 3 meters is the same as between 4 and 5 meters -that is, one meter, 100 centimeters, 1,000 millimeters (no matter how we measure it, the difference is exactly the same). This means that we always know how much more or less distance there is between the two measures. Sometimes, frequencies, test grades, or evaluation are considered interval variables; however, it is not really fixed. The best way to deal with frequencies, for instance, is to put them under a scale, at which point they become interval scores. We can do this by norming 1 frequency counts, for example, or by calculating percentages.

"Ratio" only tells us about the relationship between two measures. It is not a very good measure for register studies. Let's say we want to compare two texts to see which one has more nouns.

Text 1: noun/verb ratio =.27 Text 2: noun/verb ratio =.32

We are unable to tell which text has more nouns because it is only in relation to the verbs that we might have more nouns. That is, ratios measure how common one thing is but only in relation to a potentially unrelated other thing.

Variable Values (Levels)

Variables can have multiple values or levels. For example, if participant age is a variable, the (numerical) values can be counted between 0 and 120. If ethnicity is a variable, we can list what ethnicities we would want to include and give each a nominal value. For example, African American = 1, Native American = 2, Asian American = 3, etc.

Observations

Observations are individual objects that you are characterizing. They provide the unit of analysis that will make up your data. For register studies, an observation is typically each text that you enter into your database.

For other linguistic studies, it could be the individual linguistic feature you are considering or the individual test-taker whose language you are characterizing.

EXAMPLE

Let's assume you are interested in how complement clauses are used by younger and older generations and also how they are used by people with different educational backgrounds. You are using a corpus to look for patterns. Take 100 instances of complement clauses and mark each for who uses them in terms of age and educational background (hopefully, this information will be available in the corpus). You can use other contextual variables as well, but the focus should be on the two variables you identified. Instead of listing your findings in a table exemplified by Table

Measures of Central Tendency and Measures of Variability

Central tendency describes typical values for a variable; that is, it is the central point in the distribution of values in the data. Dispersion, on the other hand, is how much variation you get within your data. Both of these are important measures to see patterns.

Measures of Central Tendency

Measures of central tendency tell us about the most typical score for a dataset. There are three types: mode, median, and mean. "Mode" works for any variable scale (nominal, ordinal, or interval). It is the most frequent/common value (whatever value occurs with highest

• If there is not one most frequent score (but more than one -for instance, two, just like two and four above occur with the same frequency, so if those two were the most frequent scores, we could not tell what the mode is), there is no mode. • If each ranked score in the dataset only occurs once, i.e., no score receives a frequency higher than one (i.e., every score in the dataset occurs just once), there is no mode. • The mode is too sensitive to chance scores (when a mistake is made in entering the scores).

"Median" works for any numeric variable (ordinal or interval). It is the 50th percentile (i.e., the middle observation). To calculate the median, rank order all scores and the observation in the middle is the median. If you have an even number of scores, the median will be in between the two middle scores; if you have an odd number of scores, the median is the middle score. The quartiles are located as well in the ranking, and the number of observations that go with each score with the same number of observations from both sides. Let's say we have the average scores for the use of hedges in our corpus of nine texts.  The quartile gives us distributional patterns in the data. In this example, the 25th percentile means that 25% of the texts display a score of 18.5 or less; the 50th percentile means that half of the texts display a score of 20 or more and half of the texts display a score of 20 or less, and finally, the 75th percentile means that 75% of the texts display a score of 21.5 or less.

Median is often used as a measure of central tendency when:

• the number of scores is relatively small • the data have been obtained by rank order measurement (e.g., a Likert scale) • the mean is not appropriate (because the variable is not interval -see below)

Boxplots are typically used as visuals to show the range of scores (minimum and maximum), the 25th, the 50th (median), and the 75th percentile.

A boxplot is also able to show outliers in the dataset. In the example below, we display the use of nouns by teachers and students in the corpus. As you can see, the mode (most frequent scores) and median (the central score after rank ordering all scores) are the same for all three groups. However, the mean becomes vastly different depending on the actual scores in the dataset. In Group A, the scores vary a great deal. Social sciences students use hedges in an idiosyncratic way; that is, it really depends on the individual. Some students use none or very few, and some use a lot! When this is true, the mean is relatively high (especially in comparison with the others). In Group B, the scores are not going into extremes. Instead, they are pretty evenly distributed. That is, the students in this group more or less use hedges the same way, or at least very similarly. In Group C, students overall use hedges similarly but there is one student who hedges a lot. That one student changes the mean score dramatically. The two scores in

The characteristic of a normal distribution is that the mode, the median, and the mean are identical. Group B above has that example. And if you look at the variability of the scores, you can see that it is steadily in order. There are no outliers or extreme scores in the dataset. The scores are simply normally distributed.

Measures of Variability and Dispersion

Measures of variability and dispersion only work with interval scale type data. While range looks at the scores at each end of the distribution, variance and standard deviation measures look at the distance of every score from the mean and average them. More specifically, range only takes the highest and the lowest scores into the computation, and variance and standard deviation take each score into account.

"Range" tells us the spread of scores. We compute the range by subtracting the lowest score from the highest score.

Range x x highest lowest

= -

For example, the range of scores for Group A in the example above is 99, for Group B it is 6, and for Group C it is 82. The problem here is the same as with the mean scores, as it changes drastically when you have more extreme scores (as you see in the examples). Since it is unstable, it is rarely used for statistical reporting, but calculating the range could be informative as a piece of additional information (e.g., to see whether there is an outlier). "Quartile" (interquartile) or percentile measures tell us how the scores are spread in different intervals in the dataset. As outlined above, the median is a measure of central tendency, and by adding the interquartile figures (the percentile figures), we are able to see the spread as well. Once again, the 25th percentile tells us what scores we would get for a quarter of our data, the 50th percentile tells us what score we would get for half of the data and the 75th percentile refers to the score we would get for threequarters of the data.

"Variance" summarizes the distance (i.e., how far) individual scores are from the mean. Let's say our mean is 93.5 (X = 93.5). If we have a score of 89 (x = 89), that means our score is 4.5 points away from the mean, and that is the deviation (value) away from the mean. In this instance, we just discussed one score only. However, what we want is a measure that takes the distribution and deviation of all scores in the dataset into account. This is the variance.

To compute variance, take the deviation of the individual scores from the mean, square each deviation, add them up (oftentimes called the "sum of squares") and average them for the dataset dividing it by the number of observations minus one. As a formula, it looks like this:

"Standard deviation" is a measure of variability in the data from the point of central tendency. Standard deviation tells us the variability of the scores -i.e., the spread of the scores from the central point -and is most often used as a measure of dispersion in studies of a variety of fields, including corpus linguistic studies. But why is this important? Let's take an example that illustrates why it is important to know the spread.

With another example, from

EXAMPLE

Imagine you would like to find out whether one class is more interactive than another. As

Lecture #1: 5 turns, a total of 150 words, average turn length 30 words, each turn is of equal length. Turn 1: 30 words Turn 2: 30 words Turn 3: 30 words Turn 4: 30 words Turn 5: 30 words Total = 5 turns, 150 words Average turn length: 30

Lecture #2: 5 turns, a total of 150 words, average turn length 30 words, turn length varies for each turn.

Turn 1: 2 words Turn 2: 140 words Turn 3: 2 words Turn 4: 3 words Turn 5: 3 words Total = 5 turns, 150 words Average turn length: 30

In both instances, the average (mean) turn length is 30 words, which is the measure of central tendency. But it is clear that one lecture is very different from another in terms of turn length measures. By calculating the standard deviation for each, we are able to tell the spread in the scores; that is, whether the scores are close to each other or they vary, and if the latter, how much they vary (in terms of magnitude measured by a single number).

For Lecture #1, the standard deviation is 0, and for Lecture #2, it is 61.49.

A zero standard deviation says that there is no variation in the scores at all (clearly), and 61.49, being very high, tells us that there is a great variation in the scores. Does this tell us which lecture is more interactive? If we think that relatively shorter turns are making the class more interactive, then Lecture #1 is more interactive. If we think that longer stretches of turns coupled with two-or three-word turns is more interactive, then Lecture #2 it is. Lecture #1 looks to be the best candidate simply because the number of turns and the turn length measure together tell us that people would have more opportunity to express actual ideas rather than just agree to what is happening with one or two words at a time (see Csomay, 2012 for short turn content).

In sum, the larger the standard deviation, the wider the distribution of scores is away from the measure of central tendency (the mean). The smaller the standard deviation, the more similar the scores are, and the more tightly the values are clustered around the mean.

To calculate the standard deviation, all you need to do is to square root the variance (explained above). With this, we are able to see that two groups could be very similar in terms of their means but they can be very different because the distribution of scores away from the mean may be quite different.

Parametric and Non-Parametric Tests, Research

Questions, and Hypotheses

Parametric and Non-Parametric Tests

Non-parametric tests do not require strong assumptions about the distribution of the data. The observations can be frequencies (nominal scores) or ordinal scales and can be rank-ordered. They can be used with interval scales, too, when we are unable to meet the assumptions of parametric tests (e.g., normal distribution in the data). Non-parametric test results can only be interpreted in relation to the dataset in question. That is, no projections or predictions could be made about the population it was drawn from, and the interpretation can only relate to the dataset investigated. A nonparametric test, for example, is Chi-square (see details on this in Chapter 7). Parametric tests, however, do require strong assumptions about the nature and the distribution of the data. These assumptions are:

1. Dependent variables are interval scales (where means and standard deviations are the measures of central tendency and dispersion, respectively) and not frequencies or ordinal data. If you are using corpus data from COCA, for example, you may not want to use the frequency data but the normed score (frequency per million words) to make sure your values are interval. 2. Dependent variables are strongly continuous (rather than discrete as ordinal scores are). That is, we know exactly how much difference there is between two scores and they are always at the same distance. 3. We can estimate the distribution in the population from which the respective samples are taken. That is, the distribution in the "sample" could be projected to the distribution of the "population". A small sample size will make it problematic to do this -a minimum of 30 observations for each variable is needed. If you compare two registers, for example, you will need values for your dependent variable from at least 30 texts (observations) for each register. 4. Data are normally distributed (sometimes we use fewer than 30 observations -remember that is the minimum to assume normality in the distribution -the larger the size, the better, of course). 5. Observations are independent; otherwise, research is confounded, as discussed before -that is, there is no relationship between the observations, or cases.

Why do parametric tests? The reason parametric tests are more powerful than non-parametric tests is because a) they have predictive power (i.e., we can predict that if we followed the same procedures, and did the study the same way, we will gain the same results) and therefore, b) the results are generalizable (i.e., we can generalize that the results are true to the larger population the samples are drawn from -that is, if we repeat the study with the same parameters, we would get the same results). Therefore, they are very powerful!

Research Questions and Hypotheses

According to

(a) They are phrased in the form of statements (rather than questions). (b) Their statements show specific outcomes. (c) They need to be testable.

In other words, a "hypothesis is a statement of possible outcome of research"

Typically, we are looking for either differences between two or more groups or we are looking for relationships between two groups (see Chapter 7 for further explanation).

In looking for differences, our null hypothesis will be stating that there is no difference between two or more groups (independent variables) with respect to some measure (dependent variable). (These are typically parametric tests.)

For example, we may have the following null hypothesis:

H 0 There is no difference in the use of nouns across disciplines.

The alternative hypothesis would be:

H 1 There is a difference in the use of nouns across disciplines.

In looking for relationships between two or more variables our null hypothesis will be stating that there is no relationship between two or more measures.

H 0 There is no relationship between the use of nouns and first person pronouns in university classroom talk.

Alternative hypotheses:

H 1 There is a relationship between the use of nouns and first person pronouns in university classroom talk. H 2 There is a positive relationship between the use of nouns and first person pronouns in university classroom talk. (That is, when nouns occur, first person pronouns will as well.) H 3 There is a negative relationship between the use of nouns and first person pronouns in university classroom talk. (That is, when nouns occur, first person pronouns will not occur.)

We look to reject the null hypothesis of "no difference" or "no relationship". A p <.05 (probability of 5%) means that we have a 95% chance of being right in rejecting the null hypothesis. A p <.01 means that we have a 99% chance of being right in doing so, and a p <.001 means that we have a 99.9% chance of being right in rejecting the null hypothesis. There are two types of errors that we can commit in rejecting the hypothesis: Type 1 and Type 2. See their description below.

When we reject the null hypothesis, we want the probability (p) to be very low that we are wrong. If, on the other hand, we must accept the null hypothesis, we still want the probability to be very low that we are wrong in doing so.

The probability value (alpha, or p) basically tells you how certain you can be that you are not committing a Type 1 error. When the probability level is very low (p <.001), we can feel confident that we are not committing a Type 1 error described above, and that our sample group of students differs from other groups who may have taken the test in the past or who might take it in the future (population). We test whether the data from that sample "fit" with that of the population. A p <.05 tells us that there are fewer than five chances in 100 that we are wrong in rejecting the H 0 . That is, we can have confidence in rejecting the H 0 .

Two-Tailed Test/Hypothesis

In two-tailed tests, we specify no direction for the null hypothesis ahead of time (that is, whether our scores will be higher or lower than more typical scores). We just say that they will not be different (and then reject that if significant). (See first example above.)

One-Tailed Test/Hypothesis

We have a good reason to believe that we will find a difference between the means based on previous findings. The one-tailed tests will specify the direction of the predicted difference. In a positive directional hypothesis, we expect the group to perform better than the population. (See second example above.) In a negative directional hypothesis, the sample group will perform worse than the population. One crucial remark: We cannot repeat tests as often as we may want to. The statistical tests that we introduce in this book are not exploratory statistics, but they follow experimental designs, and test hypotheses. Onetime deal only. Steps for hypothesis testing:

Step 1: State null hypothesis.

Step 2: Decide whether to test it as a one-or two-tailed hypothesis. Question: Is there research evidence on the issue? a. NO: Select two-tailed → will allow rejection of null hypothesis in favor of an alternative hypothesis. b. YES: Select one-tailed → will allow rejection of null hypothesis in favor of directional.

Step 3: Set the probability level (typically p <.05 or lower). Justify your choice based on the literature.

Step 4: Select appropriate statistical test.

Step 5: Collect data -apply statistical test.

Step 6: Report the results and interpret them correctly.

How to Go About Getting the Statistical Results

Several statistical programs are commercially available and are potentially cheaper at a student price, and there are others that are free. While SAS, STATA, and R, for example, are powerful statistical software programs, we will be showing you how to do descriptive statistics and the basic statistical methods we outlined above with SPSS (Statistical Package for the Social Sciences). We consider this program the most user-friendly, as the other three mentioned above require some programming abilities. Also, SPSS is still the most frequently used program at university campuses and is typically available for the students at computer labs free of charge through a university license. In this section, we will show you how to organize your data in SPSS (very different from Excel!) and how to access descriptive statistical results.

Preparing the Data and Descriptive Statistics

In SPSS, the way we organize the data is very different from the way data could be entered in Excel. Therefore, we would like you to completely forget Excel while you are using SPSS. As a start, there are two views you can have in SPSS: the variable view and the data view. Before we explain each view a bit more in detail, let's review one more time the dependent versus independent variables and what the basic unit of analysis is (observations) in the example we use.

When we characterize registers based on one or more linguistic features, the unit of analysis is a text. That means that each observation is a text in which we look for the particular variable that we hope to see variation in (i.e., the dependent variable). In our examples, it has been an individual linguistic feature, such as nouns, or pronouns, etc. Each text then will have other, "extra-textual" features as well. An extra-textual feature is, for example, what register it comes from -that is, whether it is news, or face-to-face conversation, etc. Another extra-textual feature can be the time the text was produced: whether the text comes from the year 1920 or 2015. These are your independent variables, and depending on your research question, you will manipulate these to see if there is variation in the dependent variable.

When we characterize individual speakers' way of using certain language features, the unit of analysis is the text produced by those speakers. The unit of analysis is still the text (because the language was produced and transcribed), but it may not be obviously understood in the same sense as the text above because each text is more associated with individual speakers who would have certain characteristics. Yet, it is the text produced by them, and that will be the basis for comparison.

Finally, when we look at characteristics of individual linguistic features (e.g., article type in subject and object positions), our unit of analysis is each instance of that feature. Then, we characterize each observation for its features, which in this case would be syntactic position and type of article.

Preparing the Data: Entering Data into SPSS

We will now show you the basics in each view, and tell you how to organize your data in these two settings. Let's start with the variable view (see the highlighted tab at the bottom left-hand corner). Here, you will enter the names and characteristics of both your dependent and independent variables. Let's take the example we discussed earlier in this chapter when explaining the mean. Here's the text again: You are interested in finding out whether undergraduate students majoring in natural sciences use fewer "hedges" than students in the humanities or in the social sciences. You look at a corpus of student presentations that includes nine presentations from each area (a total of 27 texts). The data below shows the normed scores for "hedges" for each of the presenters in each of the three areas. Your dependent variable is "hedges" (interval scores, as it is normed to, let's say, 1,000 words) and your independent variable is discipline (nominal) with three levels (the three disciplinary areas). Because SPSS is not good at processing "string" data, i.e., text, for its variables, we need to give a nominal numeric value to each discipline. We name Social Sciences 1, Natural Sciences 2, and Humanities 3. There is no numeric value across these categories.

Your SPSS Variable view will look like Figure

We need to focus on some of these headings, but not all. For example, those that seem less important are "Width", which determines how wide the cell is in your data view, and "Columns", which determines how many columns there are. "Align" is also less important as it sets how you would like to see the text aligned in the data view (to the left, the middle, or to the right), and "Role" is what role you assign this variable in the dataset (it will all be input for us). We really do not need to worry much about these tabs. However, we do need to know more about all the others: "Type", "Decimals", "Label", "Values", "Missing", and "Measure". We will go through each of these one by one:

Type: Numeric (whether it has a numeric value or not -see nominal independent variables above).

Decimals: You can set the number of decimals you want to see. For interval scores, we typically use two decimal points and for nominal scores, we use zero decimal points (since they have no numeric value, they do not and will not have any fractions).

Label: SPSS takes very short names for variable names and only in oneword strings. Labels, then, provide you with the opportunity to give longer names that could be used as the labels for your output results as well.

Values: These are the values that you can assign to the levels. For hedges, we will not have any values assigned. But for the nominal variables, as we mentioned above, we have 1 = Social Sciences, 2 = Natural Sciences, and 3 = Humanities. As you enter each one, make sure you hit the "Add", or else it will not be added to the list. (See Figure

Measure: In this area, you will need to determine what kind of variable you have. In our example, since hedges are interval variables, we will choose "Scale", and since discipline is a nominal variable, we will choose "Nominal". (See Figure

Before we turn to our data view, let's add one more variable, so we can keep track of our observations. The filenames will be portrayed as a string variable called "text_number" (we really are not including this as a variable in any calculations; it is more like a reference for us to know which text file the data is coming from). So it will be string, and it will be a nominal type of data (all strings are nominal). (See Figure

Now that this is all set, let's turn to our "Data View" to see how the data will need to be entered. First, as we see in Figure

Now we can start running some descriptive statistics.

Descriptive Statistics

In order to get information in a stratified manner for your levels, you want to give the following command. On the top bar with "File", "Edit", "View", choose the following set: Analyze → Descriptive Statistics → Explore to get to the window shown in Figure

Following our case study, as you see, your dependent variable (hedges) needs to be under "Dependent List" and your independent variable (discipline) needs to be under "Factor List". This way, your descriptive statistics will be calculated for each level (i.e., for each of your disciplines) versus giving just one mean score of the entire dataset you have. Run the statistics, and see to what extent the results by SPSS match the descriptive statistics we calculated earlier (they really should!). If you only want the numbers, click on "statistics"; if you want a boxplot (described in this chapter) and the numbers, click on "both". Explore what option you may have further by clicking on the "Options" button at the upper right-hand side.

If you only wanted the numbers, it should look like the details in Table

We believe the numbers generated by SPSS match the hand calculations we made in this chapter. In the next chapter, we will look at four different statistical tests that we can apply to our datasets. Before we do that, why don't you test your knowledge based on this chapter? 2. population and sample and their relationship to each other 3. observations 4. standard deviation, variance (include calculation for each) 5. mean, median, mode 6. interval, nominal, ordinal scales 7. normed counts (importance and how to calculate it) 8. parametric versus non-parametric tests and inferential statistics

Variables

For each of the following research designs, determine what the dependent and the independent variables are, and whether they are interval, nominal, or ordinal scores. State the research question(s) and the null and alternative hypotheses.

1. Mary was curious to find out whether the instructor's gender or the course's level of instruction has a greater effect on informational focus in university classroom talk. She built a small but balanced corpus of randomly selected 30 class sessions from multiple corpora (MICASE, BASE, and others) where male and female instructors and levels of instruction (undergraduate, graduate) were both represented. She tagged the texts with a grammatical tagger, counted the appropriate part of speech tags (see Chapter 9 about tagging), and normed the feature counts to 1,000 words each. She defined informational focus by adding up the number of normed counts for nouns, attributive adjectives, nominalizations, and prepositions in each of the courses she included in her corpus. 2. Allen was interested in finding out what kinds of reduced forms correlate (if at all) in highly interactive university classes. He was particularly interested to see whether phrasal types of reduction (pronoun it, demonstrative pronouns, and indefinite pronouns) and clausal types of reduction (that deletion, contraction, do as a pro-verb) have any connection to one another. Examples of the different types of features are as follows:

pronoun it: I read it and graded it. demonstrative pronoun: Look at this here. indefinite pronouns: Does anyone have an answer? that deletion:

I believe [that] you are right. contraction:

I'll do that next week. do as pro-verb:

I like pies and he does too.

Note

multiply it with a number (typically 1,000) for each observation. That is, let's say Text 1 has 45 first person pronouns, and it is 1,553 words long. Then we will calculate the normed count to 1,000 words (as if the text were that long) by (

Difference Tests

When doing difference tests (e.g., One-Way ANOVA or Two-Way ANOVA), we test whether there is a statistically significant difference in the average scores, i.e., mean, between two or more variables. The goal of difference tests is to see the extent to which the independent variable(s) is/ are responsible for the variability in the dependent variable. That is, we are interested in how one or more variables affect another variable. We can make claims about cause and effect, i.e., one variable changes because another variable has a profound (statistically significant) effect on it. We cannot talk about the results in terms of more or less significant, however.

Once the results are statistically significant, we can investigate where the differences are with post-hoc tests and how strong the association between the dependent and independent variable is with Cohen's d measures, or more typically with R 2 .

One-Way ANOVA (Analysis of Variance)

We can do a One-Way ANOVA test when we have one dependent variable and one independent variable, the latter with more than two levels (see example below). The dependent variable has to be an interval score, and the independent variable has to be nominal. With parametric tests, like One-Way ANOVA, we can generalize to the population that the sample was drawn from. Conceptually, with a One-Way ANOVA we are interested in identifying the change in the dependent variable and associate the change with the manipulation of the independent variable. More specifically, we seek to find out whether the variability in the dependent variable is due to the variability of the scores within each level of the independent variable or across the levels (or groups) we are comparing. That is, One-Way ANOVA assesses whether the differences in mean scores are attributed to the variability within the groups or across the groups. If the ratio of these two measures is small -that is, if the "across-group" variation is small relative to the "within-group" variation -there is no statistical difference. If, however, the "across-group" variation is large relative to the "withingroup" variation, there is a statistically significant difference across the groups. That is, the larger this ratio between the "within-group" variability measures and the "across-group" variability measures (F score), the more likely that the difference between the means across the groups is significant. Assumptions and requirements with ANOVA:

1. We have one dependent and one independent variable, the latter with three or more levels. 2. The dependent variable must be reported in interval scores (e.g., normed counts for linguistic features) and must be continuous, and the independent variable must be nominal. 3. Measured variables must be independent (not repeated). 4. Normal distribution of scores is expected in each group. 5. Number of observations is equal in each group (a balanced design), although it is only necessary when we do calculations by hand. The statistical package (e.g., SPSS 1 ) accounts for an imbalance. 6. Values/categories on independent and dependent variables must be mutually exclusive and exhaustive. 7. Cell values cannot be too small. A minimum of five observations per cell is necessary for each level of the independent variable.

EXAMPLE

As an example, let's say you are investigating university classrooms as your context. You analyze your context for all the situational variables outlined in Chapter 2, and you realize that discipline may be a situational variable in the academic context that may have an effect on how language is used in the classrooms. In fact, you have read earlier that the use of pronouns may vary depending on the discipline. Based on your readings, you also know that first person pronouns are more apparent in spoken discourse, and have been associated with situations where the discourse is produced under more involved production circumstances (e.g., where the participants share the same physical space, allowing for the potential of immediate involvement in interaction). Knowing all of this, you isolate this one pronoun type because you are interested in the use of first person pronouns (I, me, we, us). More specifically, you would like to find out whether there is a significant difference in the use of first person pronouns in different disciplines (more than two).

You formulate your research question in one of two ways:

1. How does the use of first person pronouns differ across disciplines? Or 2. Is there a difference in first person pronoun use across disciplines?

The dependent variable is the composite normed score for the first person pronouns as listed above (instead of using frequency scores, which are nominal, use normed counts -an interval score), and the one independent variable is discipline with three levels (nominal score with no numeric value). The three levels are the three disciplinary areas: Business, Humanities, and Natural Sciences.

You formulate your hypothesis:

H 0 : There is no statistically significant difference in the use of first person pronouns across the three disciplines. H 1 : There is a statistically significant difference in the use of first person pronouns across the three disciplines.

The statistical test to use is One-Way ANOVA (one dependent variable [first person pronouns] with interval scores [normed to a thousand words] and one independent variable [discipline] with nominal scores and with multiple levels, in this case, three disciplines). The significance level is set at the .05 level, and to locate where the differences are in case the ANOVA results in a significant difference, we will use a Scheffe post-hoc test.

To illustrate how the statistical program calculates the F score, we will do a step-by-step demonstration. Table

= = = =

(Section 6.3.1), these numbers are entered into SPSS, as illustrated in Table 7.1 below. Each observation (i.e., each text with each normed count) will be in a different row. The two variables are: First person pronoun use in each text normed to 1,000 words (interval variable) and Discipline (nominal with three levels: 1 = Business; 2 = Humanities; 3 = Natural Sciences). But in this chapter, we will go through the steps of calculating the One-Way ANOVA by hand. For this reason, and for hand-counting the ANOVA, we will use a different type of organization, as it is easier to see what is happening within the groups when listed by group (Table

In calculating the F score (the ratio for the mean sum of squares between and across groups), we need to take several steps. Conceptually, we are looking for the mean score for each group and then the variation as to how the scores are dispersed or spread (i.e., how far away each score is from the mean). This way, we can tell whether the variance can be attributed to variation inside each group or across the groups.

We take eight steps to do the calculations and determine where the differences lie. The following are the eight steps:

Step 1: Calculate the mean score for each group and for the entire dataset.

Step 2: Calculate distances across scores (and square them).

Step 3: Calculate degrees of freedom.

Step 4: Calculate mean sum of squares.

Step 5: Calculate F score.

Step 6: Determine whether the F score is significant.

Step 7: Calculate strength of association.

Step 8: Locate the differences with post-hoc tests.

Step 1: Calculate the mean score for each group and for the entire dataset. Step 2: Calculate distances across scores. Before we get into details in this area, it is necessary to make the difference between two notions: a) a score being x mean away from another score, and b) a score being x value away from the mean. The following is the explanation for the difference between these two notions:

a) If a score is x mean away from another score, it means that we are measuring the distance in the value of the mean score. Let's assume, for example, that Mean = 4; Score(1) = 2; and Score(2) = 10. In this case, Score(

In our calculations, we will mostly use the second type of distance measure. In looking at how the scores are dispersed, we need to calculate a) the distance between the individual score and its own group's mean, b) the distance between the group mean and the mean for the grand total, and c) the distance between the individual score and the mean for the grand total.

We will work with the following terminology: within sum of squares (SS W ) (the sum of squares within each group), between sum of squares (SS B ) (the sum of squares across groups), total sum of squares (SS T ) (the sum of squares for the entire dataset), degree of freedom within (Df W ) (degree of freedom within each group) and degree of freedom between (Df B ) (degree of freedom across groups). a) Within each group: How far is each score from its own group's mean?

To calculate the within-group sum of squares (SS W or group variance), take each individual score (x) minus the mean for its group (X group), and square it. Add values gained this way for each group; then add each group together. You will get the within sum of squares (SS W ), or group variance (see Table

Between sum of squares:

Total sum of squares

To calculate the total sum of squares, take each score (x) minus the total mean (X), square it and sum it up (see Table

∑ ( )

An easier way to calculate this score is by adding up the "within" and "between" sum of square values calculated before: SS W + SS B = SS T

Df W (degree of freedom within groups) = all observations minus # of groups Df W = N -N group

For our dataset: 18 -3 = 15, so our Df W = 15. Six observations in three disciplinary areas (6 x 3) minus three groups.

Df B (degree of freedom between groups) = # of groups minus 1 Df B = N group -1

For our dataset: 3 -1 = 2, so our Df B = 2. This will be important in looking up whether our F score is significant.

Step 4: Calculate mean sum of squares.

As an intermediary step between the distance calculations and the degree of freedom, we need an average of the squares. We will use the mean squares within-group (MS W ), and the mean squares between groups (MS B ) as a final step before being able to arrive at the F score. The mean square within the group is the within sum of squares divided by within degree of freedom.

For our dataset: 96/15 = 6.4

The mean square is between sum of squares divided by degree of freedom between groups.  In our example, R 2 = 192/288 =.666 R 2 =.666 means that 66% of the variance in the first person pronoun use can be accounted for by the discipline. That is, if you know the discipline, you can predict the use of pronouns more than half the time.

MS SS Df

Or, by knowing the first person pronoun score, we are able to predict which discipline it comes from with quite good certainty -more than half the time.

Step 8: Locate the differences with post-hoc tests.

With identifying the F score's significance, we can only say that there is a statistically significant difference in the use of, in our case, first person pronouns. What we cannot say is where the statistically significant differences are exactly. In order to be able to say that, we can use a range of post-hoc tests, including Scheffe, Tukey, Bonferroni, Duncan, or LSD. We are using Scheffe for the current question and dataset to illustrate how this works. Table

1. Business -Humanities: The Business mean is 4 values lower than the Humanities mean (hence the negative number). Looking at our mean scores, it is true, since the Business mean was 4 and the Humanities mean was 8. Throughout the analysis, this was considered to be a statistically significant difference. 2. Business -Natural Sciences: The Business mean is 8 values lower than the Natural Sciences mean (hence the negative number). Looking at our mean scores, it is true, since the Business mean was 4 and the Natural Sciences mean was 12. Throughout the analysis, this was considered to be a statistically significant difference. 3. Humanities -Natural Sciences: The Humanities mean is 4 values lower than the Natural Sciences mean (hence the negative number). Looking at our mean scores, it is true, since the Humanities mean was 8 and the Natural Sciences mean was 12. Throughout the analysis, this was considered to be a statistically significant difference.

The rest of the information in the table is a repetition of this but with reversed direction. If we look at our original mean scores, it is true that the Business mean was 4, the Humanities mean was 8, and the Natural Sciences mean was 12. And now we know that these differences are, in fact, statistically significant.

INTERPRETATION

Based on previous readings, we know that first person pronouns are typically associated with a communicative context where language is produced in a shared physical space and under involved production circumstances, allowing for the potential of interaction. We also know through the situational analysis that disciplines may differ in the way the material is presented, and so we want to know to what extent first person pronouns would be an indicator of such difference. The statistical results in our mini-study showed that there is a statistically significant difference across disciplines, and that significantly more first person pronouns are used in Natural Sciences than in either one of the other two disciplines. In addition, it also shows that when compared to Humanities, Business also uses significantly fewer first person pronouns. These results indicate that in Natural Sciences classrooms, language features seem to be similar to those in spoken discourse (rather than written), which then is associated with discourse produced under involved production circumstances suggesting interaction. The fact that Business showed the least number of personal pronouns may be attributed to less interaction in the classroom, and more teacher talk perhaps.

Two-Way ANOVA

We do a Two-Way ANOVA test when we have one dependent variable, and at least two independent variables that could have two or more levels each. As with other parametric tests, the dependent variable has to be an interval score (as the mean has to be the best measure of central tendency and the standard deviation has to be the best measure of dispersion), and the independent variables have to be nominal. With parametric tests, like the Two-Way ANOVA, we can generalize to the population that the sample was drawn from. Conceptually, the Two-Way ANOVA helps us identify which one of the two (or both) independent variables is (are) responsible for the variability in the dependent variable. More specifically, the question is whether the variability in the dependent variable is due to one or both independent variables. Again, we are looking at the variability of the scores within each level group versus across the levels but not only for one independent variable as we did with the One-Way ANOVA but two.

That is, the Two-Way ANOVA assesses whether the difference in mean scores is attributed to the variability within or across the level groups when it comes to two different variables and their combinations. Again, if the cross-group variation is large relative to the within-group variation, there is a statistically significant difference across the groups. That is, the larger this ratio between within-group variability measures and across-group variability measures (F score), the more likely that the difference between the means across the groups is significant. Although we would be using the same calculations if we wanted to calculate a Two-Way ANOVA by hand, as we have just done with the One-Way ANOVA, the computation becomes rather complex with two variables; hence, we will not do that by hand. We will, however, rely on the statistical package to give us the results (as we see it is pretty reliable!). What we need to be careful of here is the interpretation of the results.

EXAMPLE

As you have attended classes at the university, you noticed that teachers talk differently in classes not only from different disciplines (as we have seen the example before), but also in classes with different educational levels. Your primary investigation is discipline but it seems that level of instruction may also be a variable that could intervene in the variability of the data, and you are hoping that it does not affect your previous findings. All in all, you do not know whether the language change is attributed to only one of the variables (discipline/level) or the two together (discipline and level). In your situational analysis then, you take discipline as your main variable, and level of instruction as another, intervening variable. As for the teacher "talking differently", you continue to believe that, based on your previous readings, first person pronoun use is what makes the difference. First, you formulate your research question in one of two ways:

1. How does the use of first person pronouns differ across disciplines and levels of instruction? OR 2. Is there a difference in first person pronoun use across disciplines or across levels of instruction? 3. Is there an interaction between discipline and level of instruction in terms of first person pronoun use?

The dependent variable is first person pronouns (use normed counts as discussed before, as it is an interval score), and the two independent variables are discipline (with three levels) and level of instruction (with three levels).

The three levels for the independent variable "discipline" are Business, Education, and Natural Sciences, and the three levels for the independent variable "instruction" are lower-division undergraduate, upper-division undergraduate, and graduate.

Second, you formulate your hypothesis:

H 0 : There is no effect on first person pronoun use for discipline or level instruction and there is no effect for the interaction. H 1 : There is an effect on first person pronoun use for discipline. H 2 : There is an effect on first person pronoun use for level of instruction. H 3 : There is an interaction effect on first person pronoun use.

The number of observations in each cell is summarized in Table

In this dataset, each of the independent variables is significantly marking the variation in the first person pronoun use. At the same time, the interaction measure (Discipline * Level) is also significant with a p <.05. This means that neither discipline nor level of instruction alone is responsible for the variability in the use of first person pronouns. Instead, the two variables together cause the change in the dataset. In other words, we cannot say that, for example, Natural Sciences consistently use more first -person pronouns than Humanities, because their use of pronouns is connected to the level of instruction. Apparently, they use more in their lower and graduate classes, but not in the upper-division undergraduate classes. This variation is also true for the other two disciplines, and so discipline alone is not a factor for the change in the dependent variable. It also depends on the level of education at least as robustly. All in all, the interaction effect, if significant, overrides the effect of the individual independent variables.

Relationship Tests

When doing relationship tests (e.g., Chi-square and Pearson correlation), we test the relationship between two or more variables. That is, we test how well they go together. We are not interested in how one variable affects another one, as that is the goal of a test of difference seen in previous sections. Therefore, we also cannot make claims of cause and effect with relationship tests. We can only talk about the results in terms of a strong or weak relationship between two or across many variables.

Chi-Square

With Chi-square tests, both the dependent and the independent variables can be nominal data. The results of non-parametric tests, like Chi-square, cannot be generalized to the population the sample was drawn from but we can ask questions related to the given dataset. Namely,

• Is there a relationship between two variables in the dataset? • How strong is the relationship in the data? • What is the direction and shape of the relationship in the data?

• Is the relationship due to some intervening variable(s) in the data?

Conceptually, we typically want to know whether there is a relationship between two variables (and their levels). Chi-square compares the actual observed frequencies of some phenomenon with the frequencies we would expect if there were no relationship at all between the two variables in the sampled dataset. That is, Chi-square tests our actual results against the null hypothesis (i.e., no relationship) and assesses whether the actual results are different enough to overcome a certain probability that they are due to sampling error. The further apart the observed and expected values are, the more likely it is to be a significant Chi-square.

Assumptions and requirements:

1. The sample must be randomly drawn from the population. 2. Data must be reported in raw frequencies (not in scales, e.g., as percentages would be). 3. When frequencies of a phenomenon are counted, the frequency of nonoccurrence will also have to be counted. 4. Measured variables must be independent. 5. Values/categories on independent and dependent variables must be mutually exclusive and exhaustive. 6. Observed frequencies cannot be too small; the expected cell frequency has to be at least 5.

We rarely use One-Way designs in our studies, and therefore, we will focus on a Two-Way design. 2

EXAMPLE

Imagine that you would like to find out about the relationship between article type (a, an, the, zero article) and their position (subject or object).

Here are the steps you need to take:

Step 1: Formulate your research question: Is there a relationship between type of article use and clause position?

Step 2: State your null hypothesis: H 0 -There is no relationship between type of article use and clause position. State your alternative hypothesis: H 1 -There is a relationship between type of article use and clause position.

Step 3: Create a cross tab of frequencies of two nominal variables, article and position.

Each cell reports on how many observations produced that combination of independent and dependent values (see Table

Here are some of the rules:

• For a 1x2 or 2x2 table, expected frequency values in each cell must be at least 5. • For a 2x3 table, expected frequencies should be at least 2.

• For a 2x4 or 3x3 table, if all frequencies but one are at least 5 and if the one small cell is at least 1, Chi-square is still a good approximation.

If you are worried about the frequencies in the cells, you could collapse categories that make sense. In the example above, the two types of indefinite articles (a/an) can be collapsed since their use is dependent on the word following them (whether the following word starts with a vowel or a consonant) and will not affect the syntactic position they are in. Table

If the article distribution were the same in subject and object positions, we would get an equal number of them across article types. So the questions are: "How far is this off?" and "Can we say that they are really off, and whether there is a difference, or not?" That is the real question. In other words, if there were no relationship between the article type and the position, we would get an even distribution of the frequencies. Considering this, the question is: "Is there a relationship between the article type and clause position?" We calculate what we would expect if there were no relationship and compare that with the existing dataset. First, we calculate the row and column totals (Tables 7.14 and 7.15). Second, we calculate the expected value for each cell by taking the row total and the column total, multiplying the two, and dividing it by the grand total. Below is the formula.

For example, to calculate the expected value for the first cell ("the" in subject position) take 108 (Row total) times 146 (Column total) divided by 296 (N) = 53.27. Do the same for each cell (Table

INTERPRETATION

What does this mean? It means that we cannot reject the null hypothesis stating that there is no relationship between the article type and position. In other words, any of the articles could pretty much randomly occur in any position, as there is no relationship between the position and the type of article used. While this example may not have direct relevance to register studies, we could follow up with a register study. Instead of looking at the potential relationship between article type and syntactic position, the focus of the investigation would be to see whether one type of article, when in a certain position, occurs more often in one register over another, and versus in another position.

Correlation

Among the three different types of correlations (Pearson, Spearman Rank Order, and Point-Biserial), Pearson correlation is the most frequently used statistical procedure in corpus studies. With Pearson, we need interval data for both the dependent and independent variables. Conceptually, we are looking for relationships between two or more variables in the dataset. Again, as with Chi-square, we do not look at how one variable affects the other but how they relate to each other. Therefore, the research question also aims at looking for relationships (whether strong or weak), and not differences (whether there is an effect or not).

The null hypothesis for difference studies (e.g., One-Way ANOVA) is like this: "There is no difference between the two variables with respect to some measure".

The null hypothesis for relationship studies is like this: "There is no relationship between two measures".

EXAMPLE

You noticed that I mean and ok often come as a package in spoken discourse. You also noticed that both teachers and students use it, but what you don't know is whether it's the same when they are presenting in front of an audience. Let's assume you would like to find out whether there is a relationship between the use of "I mean" and "ok". You have a small corpus of presentations comprising two sub-corpora: teacher presentations and student presentations. Let's say, the mean score for I mean used for teachers is 39.1, and for students, it is 42.5.

There is too much overlap between the two types of presentations in terms of "I mean" use. That is, if we did a difference type of test (e.g., like ANOVA), there would not be a significant difference between teacher and student presentations in terms of the use of "I mean". In other words, we would not be able to predict whose presentation it is by knowing the "I mean" count.

In contrast, the mean score for ok use for teachers is 150, and for students, it is 328. There is no overlap between the two types of presentations in terms of the use of "ok". That is, in a difference type of test (e.g., One-Way ANOVA), this would show a significant difference between teacher and student presentations in terms of "ok" use. That is, we could predict who gives the presentation by knowing the "ok" count.

"I Mean" and "Ok" Use in Two Settings: Teacher Presentation and Student Presentation

If the uses of the two expressions consistently overlapped, seeing a correlation may be nice. That is, if we knew that the count for one feature is high, we could know that the other feature count will also be high. So, the relationship between the two features would be strong.

With two interval variables, you want to see what the strength is between the two variables so you can predict the occurrence of one by knowing the occurrence of the other. If there is no correlation, there is no relationship. If there is a correlation, then that means that there is a relationship between the two variables. The questions to ask then are: a) what kind of relationship it is, and b) how strong the relationship is. There are two kinds of relationships: positive and negative. In positive relationships, if one score is high, the other score is also high. Translated to our question, if the "I mean" score is high, then the "ok" score will also be high. In a negative relationship, if one score is high, the other score is low. In our case, if the "I mean" score is high, the "ok" score would be consistently low.

The correlation coefficient (r) is between 0 and 1 (whether positive or negative depending on the direction of the correlation explained above), where zero means no correlation (i.e., absolutely no relationship), and +1 means perfect correlation with a 100% overlap. In terms of strength, we need to see at what percentage can we predict one over the other. The direct measure of strength is r 2 , and we are looking at the percent overlap between 0 and 100%.

Let's have a visual about a potential dataset. Look at Figure

We can calculate the Person product r through Excel or SPSS. Table

This means that 78% of the time we can predict that if one feature occurs, the other one occurs too. That is, "the magnitude of r 2 indicates the amount of variance in" one variable "which is accounted for by the other variable" or the other way around

What is a strong overlap and what is a weak one is hard to tell without knowing the question. If you wish to show that one text is very similar to another, the higher the overlap the better. What the cut-off point is (i.e., what counts as an acceptably strong correlation) depends on the field of study. In social science research in general, if the overlap is over 25%, it is considered very high. However, since it is genuine continuous data, there is no need for a cut-off point. The degree will depend on how the disciplines regard this as strong or not, and that is why no significance level is necessary.

Some useful hints when doing correlation studies:

1. Forget the groups. Whether there is a relationship between the two variables is the question. 2. Only use words like strong and weak and not significant when talking about correlations. 3. Be aware that there is a relationship between sample and correlation.

Every correlation has a significance part in terms of correlating or not (as we have seen) and a strength part. The closer we get to a 100% relationship, the stronger the correlation is, but that increasing strength does not affect the significance of the correlation.

I mean ok

% overlap

Figure

In this section, we only looked at how two linguistic variables may relate to one another (or what relationship they may have) but we can look at more than two at once. It is almost like going to a party where you try to figure out who is hanging out with whom and what characteristics they have. In any case, if you look at correlations of more linguistic variables at once, you can start characterizing texts for their comprehensive linguistic make-up. We will briefly discuss this and point you in that direction in the last chapter of this book.

INTERPRETATION

The interpretation here is simple: when one language feature occurs, the other one does as well. That is, there is a positive relationship between the two variables and so we can predict that if there is a high number of one, there will be a high number of the other as well. This kind of study becomes more interesting when we look at more than just two linguistic features co-occurring with one another; i.e., when we are able to detect how a number of features, when thrown in the same pot and having an effect on one another, will behave. This will be discussed in Chapter 9 further as we are looking ahead.

How to Go About Getting the Statistical Results

As in the previous chapter, we will show you how to get the results in SPSS for the four different tests you set out to investigate.

Difference Tests

One-Way ANOVA

To run a One-Way ANOVA test in SPSS, from the tabs select Analyze → Compare Means → One-Way ANOVA. Again, your "Dependent List" will contain the dependent variable. Although you could only have one dependent variable to test a One-Way ANOVA, if you want to run the test on more than one dependent variable at the same time (e.g., you want to see variation in hedges and also in noun use), instead of opening the window for each individually, you can list all of them under the dependent list. The program will take them one by one, and run the test on each separately. Your independent variable with multiple levels will go into the "Factor" window.

Two-Way ANOVA

To run a Two-Way ANOVA test in SPSS, from the tabs select Analyze → General Linear Model → Univariate. Again, you will put your dependent variable in the "Dependent Variable" field, and the independent variable will come under "Fixed Factor(s)" and your intervening variable (your second independent variable) will come under "Random factor(s)". You can also determine what "Post-hoc" test you may want to use in case only one variable significantly accounts for the variability of the data.

Relationship Tests

Chi-Square

Effect Size

With each parametric or non-parametric statistical test, a strength of association (effect size) measure is calculated. This could be Eta-square, R-square, Cohen's d, and others. Conceptually, effect size measures point to how strong an association there is between the dependent and the independent variable. The larger the effect size, the stronger the relationship; that is, the more important the connection is between the two variables. In this section, we will focus on Cohen's d only as an effect size measure as this measure has been used more prominently in recent years.

Cohen's d

After determining statistical significance with parametric or non-parametric tests that compare two groups at a time, for example, with an Independent sample T-test (unlike ANOVA, which compares three or more groups), Cohen's d is used more and more frequently in applied linguistic research

Cohen's d "expresses the mean difference between (or within) groups"

Cohen's d effect sizes can take up positive or negative values depending on whether the larger or the smaller mean score enters the equation first.

While there is no exact cut-off point for Cohen's d, researchers in applied linguistics typically consider ±0.40 a small effect size, ±0.70 a medium effect size, and ±1.00 and above a large effect size

EXAMPLE

Let's say, you want to see how important the difference is between students in one class over another in the use of academic vocabulary (adopted from

The larger the effect size measure, the more robust the differences are.

To calculate Cohen's d in Excel, the following steps need to be taken:

Step 1: Enter your data into Excel by creating two columns, one for each of the levels in your independent variable. (In the example above, one would be the use of academic vocabulary [in percentages] for one group and for the other group.)

Step 2: Calculate the means and standard deviations for each group.

Step 3: Calculate the Cohen's d score based on the measures in Step 2. The formula in Excel is: =(mean_gr_1-mean_gr_2)/SQRT((POWER(sd_ gr_1,2)+POWER(sd_gr_2,2))/2) where mean_gr_1 is mean of Group 1, mean_gr_2 is mean of Group 2, SQRT is square root, POWER and 2 is for the second power, i.e., squared, sd_gr_1 is standard deviation for Group 1, and finally, sd_gr_2 is standard deviation for Group 2.

Step 4: Interpret the result.

INTERPRETATION

Let's say your results for Cohen's d were the following: Mean group1 = 5.64, SD group1 = 1.58, and Mean group2 = 9.22, SD group2 = 1.95. Your Cohen's d = -2.02 (adopted from

End of Chapter Exercises

After reviewing your answers to the scenarios in Section 6.4.2 in the previous chapter, determine what statistical test you would use selecting from the possible tests described in this chapter. With the data provided below, review Section 6.3, enter it into SPSS, and run the appropriate statistical test.

1. Data for Question #1 in Section 6.4.2 2 We must be careful as to how we select them in the corpus, though -and the best way to do so is to get a set of randomly selected relative clause sentences and continue our classification based on that.

DOI: 10.4324/9781003363309-11

The previous chapters of this book have discussed 1) how to do a register analysis (Chapter 2); 2) the types of software that you can use in corpus analysis (Chapter 3); 3) how to do corpus projects using existing corpora (Chapter 4); 4) how to build and structure your corpus for analysis (Chapter 5); and, 5) how to apply and interpret basic statistical techniques (Chapters 6-7). This chapter will guide you through the steps and procedures to actually put the corpus to use and to report on your research findings. In the following section, we will provide some guiding principles for how to go about answering your research question(s) using a register analysis framework and corpus methods. Then, we will describe the different parts of a research study and provide some guidelines for writing up and presenting your research project as well as suggest some approaches to how your corpus project can be assessed.

Doing a Register (Functional) Analysis of Your Project

As illustrated in Chapter 2, a register functional approach includes three components: 1) describing situational characteristics of texts; 2) identifying frequent linguistic characteristics of these texts; and 3) providing a functional interpretation of why these frequent linguistic features are found in the texts. In the following sections, we will discuss the steps in more detail. In order to illustrate some of the concepts in doing a situational and linguistic analysis along with a functional interpretation, we will refer to a small corpus of English as Second-Language writers who were asked to produce problem-solution paragraphs in two different conditions. The research question for this study is: "Do collaborative and individual texts differ in terms of their use of lexico-grammatical features?" The texts from this corpus were collected under two different conditions: First, the students were placed into pairs and asked to write a problem-solution paragraph collaboratively. Later in the semester, each student had to write a problem-solution paragraph as part of an in-class examination. The essays were then typed and saved as text files with headers and codes to show the authors and topics of the essays. The corpus was also divided into two sub-corpora: one consisting of collaboratively written essays (N = 51) and another consisting of individual essays (N = 102). A description of the corpus is shown in Table

Situational Analysis

As you recall in our discussion of register analysis in Chapter 2, the first step in a register functional analysis requires a description of the situational characteristics of your corpus. We follow the work of

In one sense, the distinction between corpus-driven and corpus-based research methods can be misleading. At least with respect to a register functional approach, any feature that is identified through a corpus-driven approach will still merit closer scrutiny and analysis in the corpus. If, for example, a word list shows that one particular word is more frequent in one sub-corpus than in another (a corpus-driven method), then the researcher will still need to look at the distribution and use of this feature more closely in the corpus by investigating its use in some more detail. One might argue that this second step can be seen as corpus-based because it identifies features in the corpus that merit further attention. On the other hand, it might also be argued that the feature was not identified by previous corpus-based research so it is, by definition, not corpus-based. We see the merits of both approaches in trying to understand language use and would encourage the use of both methods, especially in the smaller corpora that serve as the basis for your projects.

In Chapter 5, we mentioned the importance of building sub-corpora of fairly equal sizes (see Section 5.3). Sometimes this is not possible, as in the case of the problem-solution corpus described above. Since the design of this study was focused on writing paragraphs, the researchers had no control over the length of the texts. Furthermore, because the same writers produced texts both individually and collaboratively, it was not possible to simply add more collaborative texts to make the corpora equal. If your project involves comparing the frequency findings of sub-corpora in your corpus (or to another existing corpus), you will need to employ a simple statistical procedure known as "normalization" to ensure that your results are comparable (see a brief mention of this in Chapter 6). Normalization allows frequency counts taken from corpora of different sizes to be compared by providing a count of the frequency of the feature in a similar number of words.

Table

In a linguistic analysis, it is also worthy to note not only the potential frequency differences in shared words across the two types of texts but also the use of words that are different in the texts. In this small sample, we not only see differences in nouns ("English" is the third most frequent word in the individual corpus and "students" is the fourth most frequent word in the collaborative corpus) but also differences in function words ("in" is the fourth most frequent word in the individual texts and "of" is the fifth most frequent words in the collaborative texts). Some differences might be related to topic (as with the nouns) but other differences might be related to the production circumstances or relations among participants. Only a closer examination of these features in the corpus can provide us with evidence to support the analysis. Furthermore, Table

In addition to word lists, the n-gram function can show us potential variation in corpora. As discussed in Chapter 3, n-grams are contiguous sequences of words that can vary in length depending on the interest of the researcher. In Table

The following patterns can be observed in this dataset:

1. No 4-grams are shared between the two groups. 2. All of the 4-grams produced by the first group contain verbs. 3. All of the 4-grams produced by the second group contain at least one (at times two) prepositions (of, in, and among). 4. None of the 4-grams in the first group have prepositions. 5. None of the 4-grams in the second group contain verbs. 6. Overall, the second group seems to have used more 4-grams.

There are many different types of searches that you can do with your corpus. As mentioned in Chapter 5, you should feel encouraged to explore the AntConc program (as well as related literature such as the "read me" files on the AntConc website) to learn about the program. New tools are frequently available so you should visit the site from time to time to learn about the new functions and programs. For example, as mentioned above, an important consideration in understanding the use of any feature (including both word lists and n-grams) relates to the distribution (or dispersion) of a feature in the corpus. A given feature may be frequent, but it is important to make sure that the feature is not used in a few texts at a very high frequency. Although not in a statistical sense as we described dispersion in Chapter 7, you can check the visual of the distribution in AntConc by using the "Concordance Plot" option (see Chapter 5 for details). This function will show you how many different files the given feature occurs in as well as how many times the feature occurs in a single file. This is an easy method to provide a visual representation of the distributional patterns of the feature that you are looking at. Distributional patterns like these can be very helpful in interpreting your results. Seeing the distributional patterns can also help in examining whether your findings for a given feature are, in fact, spread in your corpus or are found in a limited number of texts only. If the latter, you may need to be aware that that language feature is probably used in an idiosyncratic way; that is, it is used only by one or two participants or in only a few of the texts (depending on your unit of analysis).

Functional Interpretation

As

For the specific dataset above, we have described some patterns. The fact that no 4-grams are shared by the two groups (i.e., in the two types of texts) could be attributed to certain differences in the situational characteristics; namely the topic, the production circumstances, and relations among participants. Although both groups wrote problem-solution paragraphs, the topics they chose were different; it is possible that these 4-grams are topic-related. However, the n-grams in the individual texts do not seem to be topic-related; they are more focused on providing solutions to the problem. The collaborative group did have 4-grams that mentioned specific problems, but since the individual group did not do this, it is difficult to see how the topic might have influenced the individual writers to mention solutions and the collaborative writers to mention specific problems. Since the time given to write the essays was the same in both the individual and collaborative assignments, the fact that the collaborative group used more types of 4-grams cannot be attributed to the time they may have had to complete the work. The difference might be attributed to other situational variables such as the difference between an exam and an in-class activity or the relationship between participants and the text construction. While the first group wrote the essays individually in an exam condition, the collaborative group completed the work as an in-class activity. This is a possible explanation, but it seems more likely that the differences have to do with the production circumstances related to constructing texts individually as opposed to constructing texts as writers are interacting with another person to produce a single text collaboratively.

The production circumstances, as one of the situational variables, may also be the reason that one group used verbs, while the other used prepositions. In previous register variation studies

Reporting on Your Project

The previous three sections of this chapter have led you through the steps of analyzing your corpus using a register functional perspective. At this point, you have a well-structured corpus that is guided by a well-motivated research question (see Chapter 5, Section 5.2). You also have a clear methodology for searching for and interpreting your results. It is now time to package your research project so that others can learn about your work. Below, we provide a template for your research paper and include some questions that you can use to guide your research. There are five general parts to a research paper: 1) establishing the research context and significance of the study; 2) introduction and explanation of your data and methodology you used in the study; 3) your results; 4) a discussion of your results; and 5) your conclusion. (Sometimes the results and discussion are found in a single section of the paper but we place them in different sections here.) When writing a paper of this type, you may want to think about writing a small paper for each of these five sections and then putting these sections together for the final product. At the end of this chapter, we also provide you with a sample rubric that can guide your project and give you an idea of how your project can be assessed (see Table

Parts of a Research Paper (and Guiding Questions)

Research context and significance

What is the research issue?

• What is the rationale of the current research?

• Why was it important to conduct the research?

• Is the statement of the problem adequate and convincing?

What other research studies were conducted in the same area?

• What were the main findings?  • What do you conclude from the findings?

• How do the findings relate to your research issue?

• What are the implications of the findings?

Conclusion

• Are the results logically drawn from the analysis?

• Are the conclusion, implications, and recommendations justified by the results? • What are the limitations of the study and why do you think that they are limitations?

Research Presentation

In addition to the research paper, you may also be asked to give an oral presentation of your research project. For example, you may be required to present your work in ten minutes, leaving five minutes for questions at the end. Your presentation should be accompanied by a visual aid such as various slide show programs such as PowerPoint or Google Slides and/or a one-page handout. Should you choose to do a slide show presentation, try not to put too much text on your slides. For example, you could try to have no more than six lines with each line containing no more than six words. When giving the presentation, try not to read every word on the slide. The slides serve as an outline for your presentation. You should expect that your presentation will be evaluated using the following criteria:

Description of your problem/research issues: Explanation of why your issue is important/a real-world problem A description of your corpus: Size, number of texts, how it is structured A description of how you analyzed the corpus: Search terms, commands to the software program you have used Some results and analysis A (tentative) conclusion Format and clarity of your visual aid DOI: 10.4324/9781003363309-12

Beyond simply illustrating how searches can be done with a corpus, the purpose of this book is to show how a complete corpus-based project can be carried out, including some of the technical aspects and some basic statistical analyses. As we have discussed many times throughout the book, one can follow a) a corpus-driven or b) a corpus-based (often called corpus-informed) approach to linguistic analysis. In this book, we made an attempt to illustrate both. We showed how we can do a corpus-based study with already identified language features, whether doing a lexical study or searching for the use of particular grammatical patterns. We have also illustrated the notion of a corpus-driven study, as we extracted lexical items (n-grams) from a small corpus and showed what kinds of questions a keyword analysis can answer. While it is relatively easy to carry out lexical studies with corpus-driven approaches (whether you rely on existing corpora or analyze your own corpus), as available tools allow you to extract lexical patterns from the texts, it is quite difficult to apply corpus-driven approaches to do a full lexico-grammatical analysis of texts. The main reason for this difficulty is related to the fact that texts need to be grammatically tagged so that grammatical categories can be extracted from corpora in the same way that specific lexical items are. Tagged corpora cannot only include specific types of grammatical items (such as nouns, verbs and adjectives) but also sub-categories in these different word types (such as concrete or abstract nouns, private and suasive verbs, attributive and predicative adjectives). Some tagging software is available, but there is an increasing need for corpus researchers to gain computational and statistical skills if they want to carry out more in-depth analyses. If you don't have such skills (yet), perhaps the best solution is to continue doing corpus-based studies. You can continue to look for lexico-grammatical patterns that you find interesting, or you can carry out corpus-based studies that rely on the results of previous, corpus-driven studies. For the latter, you would use the findings and apply them to new datasets. We hope that the reference list after each chapter will help you in that endeavor. Without access to tagged corpora, advanced programming and statistical knowledge, your corpusdriven research will be limited to focusing on word lists, keyword analyses, and n-grams.

Should you choose to expand your corpus linguistic skills, we present below some influential register studies that a corpus-driven approach can offer with the goal of providing comprehensive linguistic characterizations of texts and alternative ways to do keyword analysis in different registers. The purpose of the brief description is to point you to a way forward if you become interested in this type of research.

Unlike the corpus-driven approaches illustrated in this book, more advanced corpus-driven register studies have identified co-occurring linguistic features that have emerged through corpus analyses. While researchers do rely on earlier work to identify functional categories and their associated features before a corpus investigation

When we do this kind of research from the beginning (instead of using patterns already identified), the analytical framework applying this empirically based, statistical method to provide comprehensive linguistic descriptions was developed by

Many studies have applied a multidimensional analytical framework as they describe language variation across registers

On the other hand, researchers may also use an already-existing model where the dimensions (and associated linguistic features and communicative functions) have already been identified prior to the given study. The purpose of these studies is to investigate how their own texts place on the existing continuum of variation and in this sense have a corpus-informed component to them. Examples of these types of studies use an existing dimensional framework, most often referring to