The forms of apposition (raw frequencies) 1.5 Forms of nominal appositions (raw frequencies) 1.6 The form of appositions with proper nouns in one unit (raw frequencies) 1.7 The distribution of APNs across registers (raw frequencies) 1.8 APN pattern 3.1 Sample directory structure 3.2 Parse tree from ICE-GB 4.1 Search results for all forms of talk 4.2 Search results for clusters and N-grams 4.3 Search for all forms of the verb walk 4.4 Search results for all forms of the verb walk 4.5 Examples of forms of walk retrieved 4.6 Search results for words ending in the suffix -ion 4.7 Example parsed sentence in ICE-GB 4.8 An FTF that will find all instances of proper nouns with the feature "appo" Linguistic diversity can be found in other types of corpora too. Parallel corpora contain two languages with one language translated into another language, and the two corpora aligned at the level of the sentence. Many such corpora contain English as one of the languages. For instance, the Europarl Corpus (Release V7) consists of transcriptions of 21 European languages taken from meetings of the European Parliament that were translated into English. Sentences are aligned so that English translations can be directly compared to the original sentences on which they are based. There are many learner corpora, which contain samples of English written by speakers for whom English is a second or foreign language. The ICLE Corpus (The International Corpus of Learner English) contains samples of written English produced by advanced learners of English as a foreign language from 25 different language backgrounds. Learner corpora can be used to study language acquisition, and to develop pedagogical tools and strategies for teaching English as a second or foreign language.

Many new corpora have been created in the area of language change. One of the earlier historical corpora, The Helsinki Corpus, contains 1.5 million words representing Old English to Early Modern English. Parsed versions of part of this corpus are now available and included in the Penn-Helsinki Parsed Corpus of Middle English and the Penn-Helsinki Parsed Corpus of Early Modern English. Historical corpora of English span many periods and include different types of English. The Dictionary of Old English Corpus is a three-millionword corpus containing all surviving Old English texts. The Corpus of Early English Correspondence consists of a collection of corpora containing various types of correspondence written in the fifteenth to seventeenth centuries.

There have been many changes in the ways corpora are created and analyzed. First generation corpora, such as the Brown and LOB Corpora, were relatively short (one million words in length) and contained brief samples (2,000 words) divided into different types of written English (e.g. learned writing, newspaper articles and editorials, fiction). The limited scope and length of these corpora was largely a consequence of the fact that printed texts had to be manually converted into electronic textsa very time-consuming process. But because most texts are now available in electronic form, corpora (as noted earlier) have become considerably longer. Moreover, when initially created, the Brown Corpus, for instance, had to be loaded on to a mainframe computer for analysis, whereas many corpora such as COCA are now available for analysis over the Web or on a home computer. Thus, the creation and dissemination of corpora have become much easier over time, resulting in a more varied selection of corpora that are considerably longer than earlier corpora.

But while printed texts can be easily included in a corpus, spoken texts still have to be manually transcribed: no voice recognition software can accurately produce a transcription because of the complexities of spoken language, particularly spontaneous conversation with its numerous hesitations, incomplete sentences, and reformulations. This is why corpora such as the Santa Barbara Corpus of Spoken American English, which is approximately 249,000 words in length, required a team of transcribers to create the corpus. Some corpora do contain transcripts of television and radio broadcasts (e.g. talk shows), but because the transcripts were created by particular broadcast agencies, their accuracy is open to question. In fact, research has shown that there can be great variability between an "in-house" transcription and what was actually said

Considerable progress has also been made in the annotation of corpora. For instance, word-class tagging has become much more accurate. The TAGGIT Program

Other corpus annotation is used to mark additional features of texts. In a corpus of spoken dialogues, for instance, it is necessary to include tags that identify who is speaking or which sections of speaker turns contain overlapping speech. Some corpora, such as the Santa Barbara Corpus of Spoken American English, are prosodically transcribed and contain detailed features of intonation, such as pitch contours, pauses, and intonation boundaries (cf. www.linguistics.ucsb .edu/research/transcription for further information on the transcription symbols used). Other corpora have been semantically annotated. The FrameNet Project has created corpora containing various types of semantic tags, which mark features of what are called "semantic frames" (

The main advantage of annotation is that it can greatly enhance the kinds of analyses that can be conducted on a corpus. In a tagged corpus, various kinds of lexical categories, such as proper nouns or modal auxiliaries, can be easily retrieved. In a purely lexical corpus (i.e. a corpus containing just the text), only individual words (or sequences of words) can be searched for. But even in a lexical corpus, the numerous concordancing programs that are currently available can greatly facilitate searches. The program AntConc permits searches of, for instance, the verb walk with all of its verb declensions (walks, walking, walked) (www.laurenceanthony.net/software/antconc//). However, the program ICECUP, which comes with ICE-GB, is much more powerful because it can search for grammatical categories (e.g. noun phrases) and is not restricted to lexical items.

This brief overview of the current state of corpus-based research is explored in detail in this second edition of English Corpus Linguistics. The book is divided into four primary chapters.

Chapter 1: The Empirical Study of Language

This chapter focuses on the empirical basis of corpus linguistics. It describes how linguistic corpora have played an important role in providing corpus linguists with linguistic evidence to support the claims they make in the particular analyses of language that they conduct. The chapter opens with a discussion of how to define a corpus, and then traces the history of corpus linguistics, noting, for instance, that corpus-based research began as early as the fifteenth century, when biblical concordances were created based on passages from the Bible. Current conceptions of corpus linguistics started with the creation of the Quirk Corpus (which contained print samples of spoken and written English) in 1955 at the Survey of English Usage at University College London. This was followed (in the 1960s) by the Brown Corpus (which contains 2,000 word samples of various types of edited written American English).

Major research centers continued the development. At Lancaster University, one of the first major part-of-speech tagging programs, the CLAWS Tagger, automated the insertion of part-of-speech tags (e.g. noun, verb, preposition) into computer corpora. At Birmingham University, John Sinclair oversaw not just the creation of corpora but the development of their use to serve as the basis of dictionaries. But as corpus-based research began to expand during this period, its emergence during the dominance of generative grammar in the 1960s created quite a controversy, since to some linguists, corpus linguistics reeked of behaviorism. But as much research in corpus linguistics has demonstrated, empirical data can both enhance and support the claims made in linguistic analyses.

The chapter concludes with a description of the many different areas of linguistics (e.g. lexicography and sociolinguistics) that have benefited from the use of linguistic corpora, followed by a linguistic analysis illustrating that corpus-based methodology as well as the theory of construction grammar can provide evidence that appositives in English are a type of construction.

Chapter 2: Planning the Construction of a Corpus

This chapter describes both the process of creating a corpus as well as the methodological considerations that guide this process. It opens with a discussion of the planning that went into the building of four different types of corpora: the British National Corpus (BNC), the Corpus of Contemporary American English (COCA), the Corpus of Early English Correspondence (CEEC), and the International Corpus of Learner English (ICLE). The structure of each of these corpora is discussed: their length, the genres that they contain (e.g. academic writing, fiction, press reportage, blogs, spontaneous conversations, scripted speech) as well as other pertinent information.

Subsequent sections discuss other topics relevant to planning the building of a corpus, such as defining exactly what a corpus is. Should, for instance, corpora containing samples taken from the Web be considered legitimate corpora, especially since the content of such corpora is sometimes unknown? Although this is an open question, one section of the chapter contains an analysis of web data that precisely specifies the most common registers found in the webpages analyzed.

Other sections of the chapter focus on additional issues relevant to planning the construction of a corpus, such as how to determine the appropriate size of a corpus and the length of particular texts that the corpus will contain (complete texts versus shorter samples from each text, e.g. 2,000 words); how to select the particular genres to be included in a corpus (e.g. press reportage, technical writing, spontaneous conversations, scripted speech); and how to ensure that the writers or speakers analyzed are balanced for such issues as gender, ethnicity, and age.

Chapter 3: Building and Annotating a Corpus

This chapter focuses on the process of creating and annotating a corpus. This involves not only collecting data (speech and writing) but encoding it: transcribing recorded speech, for instance, as well as adding annotation to it, such as markup indicating in a conversation when one person's speech overlaps another speaker's, and in writing where such features as paragraph boundaries occur in written texts.

While written texts are relatively easy to collectmost writing is readily available in digital formatsspeech, especially spontaneous conversations, has to be transcribed, though, as will be discussed in the chapter, voice recognition software has made some progress in automating the process for certain kinds of speech, such as monologues. Other stages of building a corpus are also discussed, ranging from the administrative (how to keep records of texts that have been collected) to the practical, such as the various ways to transcribe recordings of speech.

The chapter concludes with a detailed description of various kinds of textual markup and linguistic annotation that can be inserted into a text. Topics discussed include how to create a "header" for a particular text. Headers contain various kinds of information about the text. For instance, for written texts, the header would include such information as the title of the text; the author(s); if published, where it was published; and so forth. Other textual markup is internal to the text, and in a spoken text would include such information as speaker IDs, and the beginnings and ends of overlapping speech.

Chapter 4: Analyzing a Corpus

This chapter describes the process of analyzing a completed corpus, with an emphasis on quantitative and qualitative research methodologies along with sample corpus analyses that illustrate the direct application of these methodologies in corpus-based research. The chapter also contains discussions of corpus tools, such as concordancing programs, that can facilitate the analysis of corpora by retrieving relevant examples for a given corpus analysis, indicating their overall frequency, and specifying (depending upon the program) the particular genres in which they occur.

The first section of the chapter contains a discussion of what is termed "Trump Speak," the unique brand of language that Donald Trump uses. In addition to illustrating how a primarily qualitative corpus analysis is conducted, the analysis of Trump's speech provides a basis for describing the various steps involved in conducting a corpus analysis, such as how to frame a research question, find suitable corpora from which empirical evidence can be obtained to address the research question, and so forth. The analysis of Trump Speak also contains a discussion of how to use concordancing programs to obtain, for instance, information on the frequency of specific constructions in a corpus as well as relevant examples that can be used in subsequent research conducted on a particular topic.

The remainder of the chapter focuses on more quantitatively based corpus research, such as Douglas Biber's work on multi-dimensional analysis and the specific statistical analyses he used to determine the register distributions of a range of different linguistic constructions, for instance, the higher frequency of pronouns such as I and you in spontaneous conversations than in scientific writing. Descriptive statistics (e.g. the chi-square statistic) are illustrated with a detailed quantitative analysis of the use of a stigmatized linguistic construction, the pseudo-title (e.g. former president Bill Clinton), which is derived from an equivalent or appositive (a former president, Bill Clinton) and found mainly in newspapers. Its usage is analyzed in various regional newspapers included in various components of ICE, such as the United States, Great Britain, and Singapore.

At the end of each of these chapters are exercises that allow for the practical application of the various topics covered in the chapters. In addition, there is an Appendix that contains a listing of all corpora discussed in the text with brief descriptions of their content as well as links to where further information on the corpora can be obtained.

In short, the goal of the second edition of English Corpus Linguistics is to provide a comprehensive description of all aspects of corpus linguistics, ranging from how to build a corpus to how to analyze a finished corpus. In addition, sample corpus analyses are included to illustrate the many theoretical and practical applications that linguistic corpora have.

xvi Preface

As any writer knows, there are many people behind the scenes without whose help a book would never have been written. Such is the case with this book.

I am particularly grateful to Merja Kytö, who read an entire draft and whose insightful and judicious comments greatly improved this book.

I would also like to thank Helen Barton and Isabel Collins of Cambridge University Press for promptly replying to my many questions and for expertly taking me through the production process for the book. Their help was immeasurable.

Rachel La Russo and Minh Nghia Nguyen, doctoral students in the Applied Linguistics Department at the University of Massachusetts, Boston, were assiduous in working on the bibliography. I could not have done without their help. Robert Sigley provided me with very helpful comments on the section of chapter 4 dealing with the statistical analysis of pseudo-titles.

Finally, I wish to thank my wife, Elizbeth Fay. She kept me on track with the book through the years, gently reminding me that I needed to work on my book rather than watch yet another sporting event. And although her area of specialty is not linguistics, she helped me immensely with the many questions I had about particular topics for different chapters. Most important was her constant support.

The Empirical Study of Language

In an interview published in 2000, Noam Chomsky was asked "What is your view of Modern Corpus Linguistics?" His reply was, "It doesn't exist"

pretty soon you'll be able to feed the data into the computer and everything will come out. In fact, there's a field now called corpus linguistics which essentially is the same thing except that they put in the word "Bayesian" every few sentences

To explore the role that corpora play as sources of linguistic evidence, this chapter opens with a discussion of how a linguistic corpus is actually defined, and then continues with a historical overview of the development of corpus-based research, beginning with a discussion of pre-electronic corpora; that is, corpora that were manually collected and compiled and that served as the basis of concordances and reference grammars. While pre-electronic corpora were generally based entirely on printed written texts, this changed in established a corpus containing spoken as well as written English, called the "Quirk" or "Survey of English Usage (SEU) Corpus." This corpus established a methodology for corpus creation and analysis that has continued until the present. Ironically, the corpus was created around the time when generative grammar completely changed the shape of linguistics. In particular, introspection replaced the collection of data as the basis of linguistics analyses, pitting the "armchair linguist," as

In 1964, the Brown Corpus, one of the earliest computerized corpora, ushered in the "electronic" era of corpus linguistics. This corpus contained one million words of edited written American English divided into 500 samples representing different types of writing (such as fiction, technical prose, and newspaper reportage). Each sample was 2,000 words in length, enabling valid comparisons between the different registers in the corpus. The Brown Corpus was extremely important because it provided a catalyst for the many computer corpora that will be discussed throughout this book.

But as corpus linguistics developed alongside the dominant paradigm of generative grammar, obvious differences and disputes resulted.

The corpus linguist says to the armchair linguist, "Why should I think what you tell me is true?", and the armchair linguist says to the corpus linguist, "Why should I think what you tell me is interesting?" However, as the example corpus analysis in the final section of the chapter demonstrates, conducting "interesting" linguistic research based on "real" samples of language are not necessarily mutually exclusive activities: a corpus can serve as a test bed for theoretical claims about language; in turn, theoretical claims can help explain patterns and structures found in a corpus.

Defining a Corpus

Although arriving at a definition of a linguistic corpus may seem like a fairly straightforward process, it is actually a more complicated undertaking than it initially appears. For instance, consider the Microsoft Paraphrase Corpus. This corpus contains 5,800 sentence pairs. One sentence of each pair was obtained from various websites covering the news. The second sentence contains a paraphrase of the first sentence by a human annotator. For each sentence being paraphrased, information is given about its author and the particular news source from which the sentence was obtained. While it is quite common for corpora to contain meta-information about the data that they contain, should a collection of unrelated sentences and associated paraphrases be considered a corpus?

One way to answer this question is to examine the guidelines established by the Text Encoding Initiative (TEI) for the encoding of electronic texts, including linguistic corpora (www.tei-c.org/ release/doc/tei-p5-doc/en/html/CC.html). Because corpora are fairly diverse in size, structure, and composition, the TEI provides a general definition of a corpus.

Several points mentioned in the guidelines are directly relevant to whether or not the Microsoft Paraphrase Corpus (MPC) fits the definition of a corpus. First, a corpus needs to be compiled "according to some conscious set of design criteria." The MPC fits this criterion, as the kinds of language samples to be included in the corpus were carefully planned prior to the collection of data. Second, a corpus can contain either complete texts (e.g. a collection of newspaper articles) or parts of texts (e.g. 500-word samples from various newspaper articles). The MPC satisfies this criterion too: each sentence in the corpus was selected from some larger text of which it is a part. The paraphrase of each sentence makes the MPC similar to a parallel corpusa type of corpus that typically contains sentences from a whole text, with each individual sentence translated into a different language. The major difference is that parallel corpora typically contain samples from larger texts (rather than single sentences), with each sentence in the text translated into a particular language.

A third TEI guideline for defining a corpus is more problematic for the MPC. According to this guideline, a corpus is a type of text that is regarded "as composite, that is, consisting of several components which are in some important sense independent of each other" (www.tei-c.org/release/doc/tei-p5-doc/en/html/DS.html). It is certainly the case that the individual sentences in the MPC are individual entities. The sentences are, as the guidelines continue, "a subdivision of some larger object." But can each sentence in the MPC "be considered as a text in its own right"? (www.tei-c.org/release/doc/ tei-p5-doc/en/html/CC.html).

But whether this question is answered yes or no, it is perhaps more useful, as

(1). machine-readable (2). representative (3). balanced (4). the result of communication occurring in a "natural communicative setting."

A "machine-readable corpus" is a corpus that has been encoded in a digital format. For a corpus to be "representative," it must provide an adequate representation of whatever linguistic data is included in the corpus. For instance, a general corpus of newspaper editorials would have to cover all the different types of editorials that exist, such as oped pieces and editorials produced by an editorial board. For a general corpus of editorials to be "balanced," the editorials would have to be taken from the various sources in which the editorials are found, such as newspapers, magazines, perhaps even blogs. Finally, the speech or writing included in a corpus has to have been produced in a "natural communicative setting." A corpus of spontaneous conversations would need to contain unrehearsed conversations between two or more individuals in the types of settings in which such conversations are held, such as the family dinner table, a party, a chat between two friends, and so forth.

The more a corpus satisfies these four criteria, the more prototypical it would be.

Obviously, the MPC would be a less prototypical corpus as well. While it is available in a machine-readable format, it is too short to be representative of the types of sentences occurring in news stories, and whether the random sampling of sentences produced a balanced corpus is questionable. Because the sentences included in the corpus were taken from news stories, the sentences did occur in a natural communicative setting. Arguably, the translations were created in a natural communicative setting too, as a translator is often called upon to translate bits of language.

Corpora vary in size and composition largely because they have been created for very different uses. Balanced corpora like Brown are of most value to individuals whose interests are primarily linguistic and who want to use a corpus for purposes of linguistic description and analysis. For instance,

In designing a corpus such as the Penn Treebank, however, size was a more important consideration than balance. This corpus was created so that linguists with more computationally based interests could conduct research in natural language processing (NLP), an area of study that involves the computational analysis of corpora often (though not exclusively) for purposes of modeling human behavior and cognition. Researchers in this area have done considerable work in developing taggers and parsers: programs that can take text and automatically determine the word class of each word in the text

Cruden's Concordance is lengthier than the Bible because he included entries not just for individual words but for certain collocations as well. In addition, Cruden does not lemmatize any of the entries, instead including separate entries for each form of a word. For instance, he has separate entries for anoint, anointed, and anointing as well as his anointed, Lord's anointed, and mine anointed. For each entry, he lists where in the Bible the entry can be found along with several words preceding and following the entry. Figure

To create the concordance, Cruden had to manually alphabetize each entry by pen on various slips of paperan enormous amount of work. As

The development of biblical concordances was followed in subsequent years by the creation of concordances of more literary texts. For instance,

Corpora served as the basis of many of the descriptively oriented grammars of English written in the late nineteenth and early to midtwentieth centuries by individuals such as George Curme, Otto His ANNOINTED.

1 Sam. 2:10 exalt horn of his a.

12:3 against the L. and his a. 5 the L. and his a. is witness 2 Sam. 22:51 showeth mercy to his a. Ps. 18:50 Ps. 2:2 and against his a. 20:6 the Lord saveth his a.

28:8 saving strength of his a.

Is. 45:1 saith L. to his a. to C.

With regard to my quotations, which I have collected during many years of both systematic and desultory reading, I think that they will be found in many ways more satisfactory than even the best made-up examples, for instance those in Sweet's chapters on syntax. Whenever it was feasible, I selected sentences that gave a striking, and at the same time natural, expression to some characteristic thought; but it is evident that at times I was obliged to quote sentences that presented no special interest apart from their grammatical peculiarities. (p. vi) Jespersen's corpus is extensive and consists of hundreds of books, essays, and poems written by well-and lesser-known authors (Vol.

A typical entry will be preceded by general commentary by Jespersen, with perhaps a few invented sentences included for purposes of illustration, followed by often lengthy lists of examples from his corpus to provide a fuller illustration of the grammatical point being discussed. For instance, in a discussion of using a plural third person pronoun such as they or their to refer back to a singular indefinite pronoun such as anybody or none,

1.2.1

The Transition from Pre-electronic to Electronic Corpora

The most ambitious pre-electronic corpus, the Quirk Corpus, served as a model for the modern-day electronic corpus. It was created at the Survey of English Usage (SEU) as part of other research being done there on the study of the English language.

Although it now exists in digital form, the Quirk Corpus was originally an entirely "print" corpus. Its creation began in 1955, with final and completed digitization occurring in 1985 (www.ucl.ac.uk/eng lish-usage/about/history.htm). The corpus totals one million words in length and contains 200 samples of spoken and written English, with each sample totaling 5,000 words in length

The idea behind the corpus was to provide as broad a representation as possible of the different types of spoken and written English that exist

Because the corpus was completely orthographic, it had to be prepared in a manner making it accessible to researchers. The spoken texts were transcribed and annotated with "a sophisticated marking of prosodic and paralinguistic features," and the entire corpus was "analysed grammatically"

While the Quirk Corpus was an entirely printed corpus, work on the computerization of corpora began during the same time period. Beginning in the 1960s at the University of Edinburgh and later at Birmingham University, John Sinclair created what would become the first computerized corpus of English: a 135,000-word corpus of conversational British English

135,000 words was almost the maximum that could be comfortably stored and processed, using the programs developed at the beginning of the project, given this particular machine's capacity and the time available.

The spoken part of the Quirk Corpus was later computerized at Lund University under the direction of Jan Svartvik. The London-Lund Corpus (LLC), as it is now known, contains a total of 100 spoken texts: 87 from the original Quirk Corpus, plus an additional 13 texts added after the project was moved in 1975 to the Survey of Spoken English at Lund University in Sweden

But of all the early electronic corpora, the first computerized corpus of written English, the Brown Corpus (described earlier), was really the corpus that ushered in the modern-day era of corpus linguistics. Compared with present-day corpora, this corpus is relatively small (one million words). However, for the period when it was created (early 1960s), it was a huge undertaking because of the challenges of computerizing a corpus in an era when mainframe computers were the only computational devices available. Computers during this period were large machines. As

Analyzing early versions of the Brown Corpus was an equally complicated process because these versions were released on magnetic tape. Using the corpus was a two-step process. The tape had to first be read into a mainframe computer and then punch cards prepared to conduct searches. For instance, in his analysis of punctuation usage in the Brown Corpus,

Other work on the Brown Corpus introduced additional innovations. In 1967 Computational Analysis of Present Day American English

The Brown Corpus was also the first corpus to be lexically tagged; that is, each word was assigned a part of speech designation (e.g. the tag DO for the verb do or DOD for the past tense form did). All 77 tags were assigned to each word in the corpus by a computer program designed at Brown University called "TAGGIT"

The early influences on corpus linguistics discussed in this section do not exhaust the many other factors that affected the current state of corpus linguistics. One of the key developments, as McCarthy and O'Keeffe (2010: 5) note, "was the revolution in hardware and software in the 1980s and 1990s that really allowed corpus linguistics as we know it to emerge." Advances in technology have resulted in, for instance, web-based corpora and textual analysis software, such as concordancing programs, that are fast and can be run on desktop and laptop computers. In their overview of the history of corpus linguistics,

Corpus Linguistics in the Era of Generative Grammar

At the time when the Brown Corpus was created in the early 1960s, generative grammar dominated linguistics, and there was little tolerance for approaches to linguistic study that did not adhere to what generative grammarians deemed acceptable linguistic practice. As a consequence, even though the creators of the Brown Corpus, W. Nelson Francis and Henry Kučera, are now regarded as pioneers and visionaries in the Corpus Linguistics community, in the 1960s their efforts to create a machine-readable corpus of English were not warmly accepted by many members of the linguistics community. W. Nelson

This attitude was largely a consequence of the conflict between what Chomskyan and corpus linguists considered "sufficient" evidence for linguistic analysis. In short, corpus linguists were grouped into the same category as the structuralists -"behaviorists"that Chomsky had criticized in the early 1950s as he developed his theory of generative grammar. For Chomsky, the mere "description" of linguistic data was a meaningless enterprise. It was more important, he argued, that grammatical descriptions and linguistic theories be evaluated in terms of three levels of "adequacy": observational adequacy, descriptive adequacy, and explanatory adequacy.

If a theory or description achieves observational adequacy, it is able to describe which sentences in a language are grammatically well formed. Such a description would note that in English, while a sentence such as He studied for the exam is grammatical, a sentence such as *studied for the exam is not. To achieve descriptive adequacy (a higher level of adequacy), the description or theory must not only describe whether individual sentences are well formed but in addition specify the abstract grammatical properties making the sentences well formed. Applied to the previous sentences, a description at this level would note that sentences in English require an explicit subject. Hence, *studied for the exam is ungrammatical and He studied for the exam is grammatical. The highest level of adequacy is explanatory adequacy, which is achieved when the description or theory not only reaches descriptive adequacy but does so using abstract principles which can be applied beyond the language being considered and become a part of "Universal Grammar." At this level of adequacy, one would describe the inability of English to omit subject pronouns as a consequence of the fact that, unlike Spanish or Japanese, English is not a language which permits "pro-drop," that is, the omission of a subject pronoun recoverable from the context or deducible from inflections on the verb marking the case, gender, or number of the subject. Within Chomsky's theory of principles and parameters, pro-drop is a consequence of the "null-subject parameter"

Because generative grammar has placed so much emphasis on universal grammar, explanatory adequacy has always been a high priority in generative grammar, often at the expense of descriptive adequacy. There has never been much emphasis in generative grammar in ensuring that the data upon which analyses are based is representative of the language being discussed, and with the notion of the ideal speaker/hearer firmly entrenched in generative grammar, there has been little concern for variation in a language, which traditionally has been given no consideration in the construction of generative theories of language.

Chomsky also distinguishes between those elements of a language that are part of the "core" and those that are part of the "periphery." The core is comprised of "pure instantiations of UG" and the periphery "marked exceptions" that are a consequence of "historical accident, dialect mixture, personal idiosyncracies, and the like"

This complexity of structure, however, is precisely what the corpus linguist is interested in studying. Unlike generative grammarians, corpus linguists see complexity and variation as inherent in language, and in their discussions of language they place a very high priority on descriptive adequacy, not explanatory adequacy. Consequently, corpus linguists are very skeptical of the highly abstract and decontextualized discussions of language promoted by generative grammarians, largely because such discussions are too far removed from actual language usage.

Corpus linguists also challenge the clear distinction made in generative grammar between competence and performance, and the notion that corpus analyses are overly descriptive rather than theoretical.

Corpus-based studies also expand the range of data considered for analysis beyond the linguist's intuitions, and ensure the accuracy of any generalizations that are made. For instance, in a discussion of

(a) We consider Kim to be an acceptable candidate. (b) *We consider Kim as an acceptable candidate.

However,

(c) The boys consider her as family and she participates in everything we do.

But while corpora are certainly essential linguistic resources, it is important to realize that no corpus, regardless of its size, will contain every relevant example of a particular linguistic construction or be able to provide a fully complete picture of how the construction is used. At best, a corpus can provide only a "snapshot" of language usage. For this reason, many have argued that corpus data should be supplemented with other means of gathering data.

In contrast, judgment tests, such as the "evaluation" test below, require subjects to assess the acceptability of sentences in various ways. In this case, subjects are asked to rank their preferences for the three sentences, from highest to lowest: He doesn't have a car. He hasn't a car. He hasn't got a car.

The purpose of this test was to examine differences in negation and the use of got in British and American English. Thus, while speakers of American English will rank He hasn't got a car higher than He hasn't a car, speakers of British English will reverse the rankings.

Types of Corpora

There are many kinds of corpora that have been created to fulfill the research needs of those doing corpus-based research. These corpora range from multipurpose corpora, which can be studied to carry out many differing types of corpus-based analyses, to learner corpora, which have been designed to study the types of English used by individuals (from many differing first language backgrounds) learning English as a second or foreign language. This section provides an overview of the many different types of corpora that exist. Less detail is given to corpora discussed in greater detail in subsequent chapters.

Multipurpose Corpora

These are corpora like the London-Lund Corpus, the British National Corpus (BNC), and the Corpus of Contemporary American English (COCA). All of these corpora contain numerous registers of speech or writing (such as fiction, press reportage, casual conversations, and spoken monologues or dialogues) representative of a particular variety of English. For instance, the London-Lund Corpus contains different kinds of spoken British English, ranging from casual conversation to course lectures. The BNC and COCA contain various samples of spoken and written British and American English, respectively.

Multipurpose corpora are useful for many different types of analyses. Because the London-Lund Corpus has been prosodically transcribed, it can be used to study various features of British English intonation patterns, such as the relationship between grammar and intonation in English. For instance, in a noun phrase such as the leader of the country and her cabinet, a tone unit boundary will typically occur before the conjunction and, marking the boundary between two coordinated noun phrases. There is also currently in production an updated version of the London-Lund Corpus: LLC-2 (

The BNC and COCA differ in length: the BNC is 100 million words in length, whereas COCA is a monitor corpus; new texts are constantly added to it so that it currently contains one billion words. But despite the difference in length, each corpus contains many of the same registers, such as fiction, press reportage, and academic writing. While the BNC contains spontaneous conversations that have been manually transcribed, COCA is restricted to spoken registers from which published transcriptions are available. Consequently, it contains no spontaneous conversations. The BNC is a fixed corpus: its structure hasn't been changed since its creation in the 1990s, though a successor corpus, BNC2014, is now in progress (cf.

The International Corpus of English (ICE) contains comparable one million-word corpora of spoken and written English representing the major national varieties of English, including English as it is spoken and written in countries such as Ireland, Great Britain, the Philippines, India, and many other varieties as well.

Learner Corpora

This type of corpus contains texts that represent the speech or writing of individuals who are in the process of learning a language as a second or foreign language. For instance, the International Corpus of Learner English contains samples of written English from individuals who speak English as a foreign language and whose native languages include French, German, Portuguese, Arabic, and Hungarian. Learner corpora enable researchers to study such matters as whether one's native language affects their mastery of a foreign language, in this case English. There are other learner corpora representing numerous languages other than English, including Arabic, Hungarian, and Malay, as well as projects providing various ways that learner corpora can be analyzed (for more comprehensive listing of learner corpora, see

Historical Corpora

The corpora discussed thus far contain examples of various types of modern English. However, there are also diachronic or historical corpora, which contain texts from earlier periods of English.

Much of the interest in studying historical corpora began with the creation of the Helsinki Corpus, a 1.5 million-word corpus of English containing texts from the Old English period (beginning in the eighth century) through the early Modern English period (the first part of the eighteenth century). Texts from these periods are further grouped into sub-periods (ranging from 70 to 100 years) to provide what

To fill gaps in the Helsinki Corpus, ARCHER (A Representative Corpus of Historical English Registers) was created (cf.

The Penn Parsed Corpora of Historical English contains components that cover three different periods of the English language: Middle English, Early Modern English, and Modern British English:

The Penn-Helsinki Parsed Corpus of Middle English, second edition The Penn-Helsinki Parsed Corpus of Early Modern English The Penn Parsed Corpus of Modern British English, second edition Each corpus has been both lexically tagged and syntactically parsed. That is, each word has been assigned a lexical tag identifying its part of speech (such as verb, noun, preposition). In addition, the larger corpora have been syntactically parsed, so that both individual words as well as structures, such as noun phrases, can be retrieved. Users of the corpus will therefore be able to study the development over time of both individual words as well as a variety of different syntactic structures.

Although FLOB (The Freiburg LOB Corpus of British English) and FROWN (The Freiburg Brown Corpus of American English) are not historical corpora in the sense that the Helsinki and ARCHER Corpora are, they do permit the study of changes in British and American English between the periods 1961-1991. Moreover, FLOB and FROWN replicate the LOB and Brown Corpora, respectively, but with texts published in the year 1991. Thus, FLOB and FROWN allow for studies of linguistic change in British and American English over a period of 30 years. Although 30 years is not a long time for language to change, studies of FLOB and FROWN have documented changes in the language during this period (cf., for instance,

The two main historical corpora, the Helsinki and ARCHER Corpora, contain many different texts and text fragments covering various periods of English. There are, however, more focused historical corpora covering specific works, authors, genres, or periods. These corpora include an electronic version of Beowulf, "The Electronic Beowulf " (cf. Prescott 1997 and

The Web as Corpus

Because the Web can be searched, it can be regarded as a corpus with infinite boundaries. Initially, as

Chapter 4 will contain a discussion of "Trump Speak": the type of language that Donald Trump used when he was president. The Web was instrumental in locating additional data for the study, since Trump is quoted extensively in newspapers and other print media, making it possible to find additional examples of his usage of language that proved very useful for the study. For instance, when Marco Rubio was running against Trump in the primaries, Trump repeatedly referred to him as "Little Marco." While this usage shows up regularly in Trump's tweets, an example from the Web containing this usage during a debate helped establish this usage as not simply restricted to tweets: "I will. Don't worry about it, Marco. Don't worry about it," Trump said to an applauding crowd. "Don't worry about it, little Marco, I will."

In addition, there are corpora that have been created based solely on data from the Web. For instance, the iweb Corpus is 14 billion words in length and is searchable online. It was systematically created. Using information from alexa.com, one million of the most popular websites were downloaded. These websites were then screened to ensure that they were from countries in which English is spoken as a native language: the United States, the UK, Canada, Ireland, Australia, and New Zealand. Several processes of elimination were developed to further screen the samples, resulting in a corpus based on 94,391 websites that yielded 22,388,141 webpages.

Parallel Corpora

A parallel corpus is a very specialized corpus. As Gatto (2014: 16) notes, "parallel corpora consist of original texts and their translations into one or more languages." For instance, the English-Norwegian parallel corpus contains texts in both English and Norwegian that are then translated into the other language, for instance English into Norwegian and Norwegian into English (

Uses of Corpora

The previous section provided a description of several different types of corpora. This section provides a sampling of the types of analyses that can be conducted on corpora.

Language Variation

Because corpus linguists are interested in studying the contexts in which language is used, modern-day corpora, from their inception, have been purposely designed to permit the study of what is termed "genre variation"; that is, how language usage varies according to the context in which it occurs. The first computer corpus, the Brown Corpus (a general-purpose corpus), contained various kinds of writing, such as press reportage, fiction, learned, and popular writing. In contrast, the MICASE Corpus is a more specialized corpus that contains various types of spoken English used in academic contexts (e.g. advising sessions, colloquia) at the University of Michigan.

While genre or register variation focuses on the particular linguistic constructions associated with differing text types, sociolinguistic variation is more concerned with how various sociolinguistic variables, such as age, gender, and social class, affect the way that individuals use language. One reason that there are not more corpora for studying this kind of variation is that it is tremendously difficult to collect samples of speech, for instance, that are balanced for gender, age, and ethnicity. Moreover, once such a corpus is created, it is less straightforward to study sociolinguistic variables than it is to study genre variation. To study press reportage, for instance, it is only necessary to take from a given corpus all samples of press reportage, and to study within this sub-corpus whatever one wishes to focus on. To study variation by gender in, say, spontaneous dialogues, on the other hand, it becomes necessary to extract from a series of conversations in a corpus what is spoken by males as opposed to femalesa much more complicated undertaking, since a given conversation may consist of speaker turns by males and females distributed randomly throughout a conversation, and separating out who is speaking when is neither a simple nor straightforward computational task. Additionally, the analyst might want to consider not just which utterances are spoken by males and females but whether an individual is speaking to a male or female, since research has shown that how a male or female speaks is very dependent upon the gender of the individual to whom they are speaking.

But despite the difficulties of creating a truly balanced corpus, designers of the BNC made concerted efforts to produce a corpus that was balanced for such variables as age and gender, and that was created so that information on these variables could be extracted by various kinds of software programs. Prior to the collection of spontaneous dialogues in the BNC, calculations were made to ensure that the speech to be collected was drawn from a sample of speakers balanced by gender, age, social class, and dialect region. Included within the spoken part of the BNC is a sub-corpus known as the Corpus of London Teenage English (COLT). This part of the corpus contains a valid sampling of the English spoken by teenagers from various socioeconomic classes living in differing boroughs of London.

To enable the study of sociolinguistic variables in the spoken part of the BNC, each conversation contains a file header, a statement at the start of the sample providing such information as the age and gender of each speaker in a conversation. A software program, SARA (SGML-Aware Retrieval Application), was designed to read the headers and do various analyses of the corpus based on a prespecified selection of sociolinguistic variables. Using SARA,

Other corpora have been designed to permit the study of sociolinguistic variables as well. In the British component of the International Corpus of English (ICE-GB), ethnographic information on speakers and writers is stored in a database, and a text analysis program designed to analyze the corpus, ICECUP (the International Corpus of English Corpus Utility Program) can draw upon information in this database to search by, for instance, age or gender. Even though ICE-GB is not balanced for genderit contains the speech and writing of more males than femalesa search of lovely reveals the same usage trend for this word that was found in the BNC.

Of course, programs such as SARA and ICECUP have their limitations. In calculating how frequently males and females use lovely, both programs can only count the number of times a male or female speaker uses this expression; neither program can produce figures that, for instance, could help determine whether females use the word more commonly when speaking with other females than males. And both programs depend heavily on how accurately and completely sociolinguistic variables have been annotated, and whether the corpora being analyzed provide a representative sample of the variables. In using SARA to gather dialectal information from the BNC, the analyst would want to spot check the ethnographic information on individuals included in the corpus to ensure that this information accurately reflects the dialect group in which the individuals are classified. Even if this is done, however, it is important to realize that individuals will "style-shift": they may speak in a regional dialect to some individuals but a more standard form of the language with others. In studying variation by gender in ICE-GB, the analyst will want to review the results with caution, since this corpus does not contain a balanced sample of males and females. Software such as SARA or ICECUP may automate linguistic analyses, but they cannot deal with the complexity inherent in the classification of sociolinguistic variables. Therefore, it is important to view the results generated by such programs with a degree of caution.

More recent work has investigated sociolinguistic variables in a number of different corpora.

One topic she discussed to illustrate the role that age plays in language usage focused on the use of hedges in constructions containing verbs (I think and I might) and adverbs (maybe and probably). Table

Lexicography

Studies of grammatical constructions can be reliably conducted on corpora of varying length. However, to obtain valid information on vocabulary items for the purpose of creating a dictionary, it is necessary to analyze corpora that are very large. To understand why this is the case, one need only investigate the frequency patterns of vocabulary in corpora, such as the one million-word BNC. In the BNC, the five most frequent lexical items are the function words the, of, and, a, in (

The five least frequent lexical items are not five single words but rather hundreds of different content words that occur 10-15 times each in the corpus. These words include numerous proper nouns, such as Bond and MacDonald, as well as miscellaneous content words such as bladder, dividends, and woodland. These frequencies illustrate a simple fact about English vocabulary (or, for that matter, vocabulary patterns in any language): a relatively small number of words (particularly function words) will occur with great frequency; a relatively large number of words (content words) will occur far less frequently. Obviously, if the goal of a lexical analysis is to create a dictionary, the examination of a small corpus will not give the lexicographer complete information concerning the range of vocabulary that exists in English and the varying meanings that these vocabulary items will have. Because a traditional linguistic corpus, such as the LOB Corpus, "is a mere snapshot of the language at a certain point in time"

To understand why dictionaries are increasingly being based on corpora, it is instructive to review precisely how corpora, and the software designed to analyze them, can not only automate the process of creating a dictionary but improve the information contained in the dictionary. A typical dictionary, as

Prior to the introduction of computer corpora in lexicography, all of this information had to be collected manually. As a consequence, it took years to create a dictionary. For instance, the most comprehensive dictionary of English, the Oxford English Dictionary (originally entitled New English Dictionary), took 50 years to complete, largely because of the many stages of production that the dictionary went through.

Because so much text is now available in computer-readable form, many stages of dictionary creation can be automated. Using a relatively inexpensive piece of software called a concordancing program, the lexicographer can go through the stages of dictionary production described above, and instead of spending hours and weeks obtaining information on words, can obtain this information automatically from a computerized corpus. In a matter of seconds, a concordancing program can count the frequency of words in a corpus and rank them from most frequent to least frequent. Figure

Note the high relative frequency of function words, the modal verb can, and other vocabulary items, such as words or concordancing, that are topics of the discussion in the paragraph.

In addition, some concordancing programs can detect prefixes and suffixes and irregular forms and sort words by "lemmas"; that is, words such as runs, running, and ran will not be counted as separate entries but rather as variable forms of the lemma run. And as Furthermore, if the lexicographer desires a copy of the sentence in which a word occurs, it can be automatically extracted from the text and stored in a file, making obsolete the handwritten citation slip stored in a filing cabinet. If each word in a corpus has been tagged (i.e. assigned a tag designating its word class), the part of speech of each word can be automatically determined. In short, computer corpora and associated software have completely revolutionized the creation of dictionaries.

As

Corpus-Based Research in Linguistics: A Case Study

Linguists from many different theoretical perspectives have discovered that corpora can be very useful resources for pursuing various research agendas. For instance, lexicographers, as noted above, have found that they can more effectively create dictionaries by studying word usage in very large linguistic corpora. Much current work in historical linguistics is now based on corpora containing texts taken from earlier periods of Englishcorpora that permit a systematic study of the development of English and that enable historical linguists to investigate issues that have currency in modern linguistics, such as whether males and females used language differently in earlier periods of English. Corpora have been introduced into other linguistic disciplines as well, and have succeeded in opening up new areas of research or bringing new insights to traditional research questions.

Chapter 4 ("Analyzing a Corpus") describes in detail how to conduct a corpus analysis, covering such topics as how to frame a research question for a particular corpus analysis and select the appropriate corpora for conducting the analysis. But this section demonstrates how a particular theory of language, Construction Grammar, can provide insights into the variable grammatical category of appositions in English. Such linkages between theory and practice are important because they, on the one hand, avoid the oft-made criticism of corpus linguists that they are mainly "bean counters"linguists who fail to make connections between theory and usagewith too little attention paid to the theoretical underpinnings of the data they are describing.

Appositions as Constructions

Apposition has proven to be a problematic grammatical category, largely because treatments of it disagree about which constructions should be considered appositions. For instance, most studies consider a construction such as Geoffrey Plimpton, police commissioner as consisting of two units in apposition. However, if the two units are reversed, a reversal possible with some but not all appositions, a very different construction, police commissioner Geoffrey Plimpton, resultsone that some studies favoring a more expansive view of apposition consider appositional (e.g.

In more recent work,

To illustrate why appositions are constructions, it is helpful to examine the most frequent apposition that

1.6.2

The Frequency and Distributions of APNs in the Corpora

Figure

(1) The first twenty thousand pounds, the original grant, is committed.

(London-Lund Corpus S.1.2 782-3) Occurring far less frequently were three additional types of appositions: (a) non-nominal appositions, such as the two adjective phrases below:

(2) when the patient closed his eyes, he had absolutely no spatial (that is, third dimensional) awareness whatsoever.  One reason these two adjectives are considered in apposition is that the second unit is preceded by the marker of apposition that is, a marker that can precede more "canonical" appositions consisting of units that are noun phrases. (b) a noun phrase in apposition with some kind of clausal structure, such as the wh-question below:

(3) The Sane Society is an ambitious work. Its scope is as broad as the question: What does it mean to live in modern society?  Although the two units in this example are not co-referential (only noun phrases can co-refer), the first unit, the question, referentially points forward to the What-clause. There is thus a referential relationship between the two units. and (c) an apposition requiring an obligatory marker apposition, such as including:

(4) About 40 representatives of Scottish bodies, including the parents of some of the children flown to Corsica, were addressed by an English surgeon and Dr. and by M. Naessens. (Survey of English Usage Corpus W.12.1-40)

In this example, the marker including is obligatory because it indicates that the reference of the noun phrase it precedes is included in the reference of the noun phrase in the first unit, resulting in the referential relationship of "inclusion" between the two units.

Because of their overall frequency, nominal appositions form the class of prototypical appositions. While the other three types of constructions that have been classified as appositions are more diverse in form and function, they are still within the class of appositions and are related through notions such as the two listed below

Within the category of nominal appositions,

As Figure

Each of the six forms (except for "other") are listed in the examples below, which contain identical content words: (5 In context, although each of the examples would have differences in focus and meaning, they nevertheless are very closely interrelated. Within construction grammar, APNs can be viewed as a type of idiom, specifically as two of the types of idioms described in

Moreover, APNs can also fit into a second class of idioms that

The Structure of APNs

The structure of the APN construction is described in the pattern in Figure

This pattern describes the prototypical apposition containing one unit that is a proper noun. The examples in (

(8) Tory leader Cllr Bob Blackman (ICE-GB:W2C-009 #54:3) However, there was a statistically significant tendency for the first units (highlighted in the following) to be five words or longer in New Zealand and Philippine English: Thus, the actual length of the first unit in a pseudo-title is determined by both intonational limits on the length of the first unit as well as editorial practices on length dictated by the editorial practices of a newspaper that differ regionally.

The Communicative Functions of APNs

The form of APNs is very well suited to the genrepress reportagein which they predominantly occur. Consequently, in other contexts, they sound rather odd. For instance, you might say to someone in a casual conversation:

(14) Jack Smith is a distinguished linguist.

However, you probably would not say:

(15) A distinguished linguist, Jack Smith, is having a drink with me later.

And you definitely would not say:

(16) Distinguished linguist Jack Smith is having a drink with me later.

Examples (

(17) Jessica Seinfeld's broccoli-spiked chicken nuggets recipes are all hers, a federal judge ruled Thursday. Ms. Seinfeld. . .did not copy from another author in her cookbook about sneaking vegetables into children's food, the judge said when she threw out a copyright infringement case brought by a competing author, Missy Chase Lapine.

In example (

Conclusions

This chapter described the role that corpus linguistics has played in the study of language from the past to the present, from pre-electronic corpora to the many differing electronic corpora that currently exist. These corpora can be used to conduct linguistic researchboth theoretical and appliedin many different areas, from genre variation (e.g. how language usage varies in spontaneous conversations versus public speeches) to research that is more applied and pedagogical (e.g. how the study of learner corpora can lead to insights for the teaching of English to individuals learning English as a second or foreign language).

Finally, a case study was presented to demonstrate that corpus analyses and various linguistic theories go hand in hand, and that such studies can do more than simply provide examples of constructions and document their frequency of occurrence. If this is the only information that a corpus analysis could provide, then corpus linguistics would have at best a marginal role in the field of linguistics. Instead, linguistic theories can be used to explain why particular frequencies and examples actually exist in a corpus: in other words, to discuss how theory and data interact. This is why corpus linguistics has grown as an area of linguistics and why many people now are using linguistic corpora for many different purposes.

Planning the Construction of a Corpus

The first step in building a corpus is to decide what the ultimate purpose of the corpus will be. This decision is important because it will determine exactly what the corpus will look like: what types of texts will be included in it (e.g. spoken, written, both), how long the individual texts will be (e.g. full length, text excerpts), how detailed the annotation will need to be (e.g. word-class tagging or a purely lexical corpus with minimal annotation), and so forth. For instance, if the corpus is to be used primarily for grammatical analysis (e.g. the analysis of relative clauses or the structure of noun phrases), the corpus can consist simply of text excerpts rather than complete texts, and will minimally need part-of-speech tags. On the other hand, if the corpus is intended to permit the study of discourse features, then it will have to contain lengthier texts, and some system of tagging to describe the various features of discourse structure.

To explore the process of planning a corpus, this chapter first provides an overview of the methodological assumptions that guided the compilation of two general-purpose corporathe British National Corpus (BNC) and the Corpus of Contemporary American English (COCA)and two more specialized corporathe Corpus of Early English Correspondence (CEEC) and the International Corpus of Learner English (ICLE). The remainder of the chapter then focuses more specifically on the individual methodological considerations (e.g. ensuring that a corpus is "balanced") that anyone planning to create a corpus needs to address.

The British National Corpus

At approximately 100 million words in length, the British National Corpus (BNC) (see Table

In planning the collection of texts for the BNC, a number of decisions were made beforehand: (1) Texts included in the corpus were selected according to domain (the categories listed in Table

Currently, there is a newer version of the BNC, BNC2014, that is under development and that will replicate the structure of the original corpus.

The Corpus of Contemporary American English

Like the BNC, the Corpus of Contemporary American English (COCA) contains various samples of different kinds of spoken and written English, with the exception that the corpus represents American rather than British English (see Table

In addition, COCA is a monitor corpus; that is, a corpus to which new texts are added on an ongoing basis. Moreover, COCA is also much lengthier than the BNC: Currently (as of April 2021), it is 1 billion words in length. While the two corpora share several commonalities, they differ in many key areas:

(1) While both COCA and the BNC each contain samples of speech, COCA contains a more restricted sampling of spoken English; only shows broadcast on television or radio as well as TV and movie subtitles.

Unlike the BNC, it contains no spontaneous conversations of individuals having, for example, a casual conversation during breakfast, or a discussion of a current movie that a group of individuals had seen. (

The Corpus of Early English Correspondence (CEEC)

The CEEC is actually a group of historical corpora that were created over the years at the University of Helsinki.

Two of the corpora in Table

(2) But while gender balance is more easily achieved in modern corpora such as the BNC or COCA, in CEEC it was much more difficult, largely because literacy rates were much higher among men during this period than women. Consequently, fewer female writers were included in the corpus than male writers. (3) Most of the letters were digitized by scanning texts from edited books in which they appeared. However, some of the texts had to be typed in manually because they did not appear in edited editions, a very "laborious method of keying in the text" (www.helsinki.fi/varieng/CoRD/corpora/ CEEC/generalintro.html).

The International Corpus of Learner English (ICLE)

The ICLE (version 2) is a learner corpus: a type of corpus containing samples of speech or writing produced by individuals learning English as a foreign or second language. The ICLE is restricted to writing and contains samples of written English produced by individuals at an advanced level of proficiency learning written English as a foreign language. The samples of written English included in the corpus were obtained from native speakers of 12 European languages and 4 non-European languages:

Bulgarian, Chinese, Czech, Dutch, Finnish, French, German, Italian, Japanese, Norwegian, Polish, Russian, Spanish, Swedish, Tswana, Turkish

The International Corpus of Learner English

Some of the design principles of ICLE are similar to those discussed in previous sections with the differences mainly relating to the unique features of learner corpora:

(1) At 3,753,030 words in length, ICLE is smaller than either the BNC or COCA. But like these two corpora, it is divided into samples, ranging in length from 500 to 1,000 words. However, the type of writing included in the corpus is restricted to primarily argumentative writing

(2) Detailed ethnographic information was recorded for each individual whose writing was included in the corpus, including such features as age, gender, mother tongue, the region in which the writer lives (for languages which are spoken in more than one country), and the other languages the writer may speak

(3) Like the BNC and COCA, ICLE is lexically tagged and comes with a concordancing program that has been customized to search for specific tags or combinations of tags included in the corpus and to also study effects on usage such as the age, gender, mother tongue, and so forth of the writers whose texts have been included in the corpus.

The preceding sections provided a brief overview of four different corpora and some of the methodological considerations that guided the development of these corpora. The remainder of the chapter explores these considerations in greater detail, focusing on such topics as the factors determining, for instance, how lengthy a corpus should be, what kinds of texts should be included in a corpus, and other issues relevant to the design of linguistic corpora.

2.5

What Is a Corpus?

The corpora discussed so far have a clearly identifiable structure: we know how lengthy they are, what texts are in them, and which genres the texts represent. Recently, however, the Web itself has increasingly been used as a source of data for linguistic analysis, giving rise to the notion of "the web as corpus." But as

On the one hand, the traditional notion of a linguistic corpus as a body of texts rests on some correlate issues, such as finite size, balance, part-whole relationship and permanence; on the other hand, the very idea of a web of texts brings about notions of non-finiteness, flexibility, decentering/recentering, and provisionality.

With a corpus such as the BNC, we know precisely what kinds and types of English are being analyzed. With web-based material, however, no such certainty exists.

To determine the kinds of texts that exist on the Web,

They found that three registers predominated: narrative (31.2 percent), informational description/explanation (14.5 percent), and opinion (11.2 percent). In the narrative category, general news reports (52.5 percent) and sports reports (16 percent) were most frequent (p. 24). The informational description/explanation category contained, for instance, research articles and technical reports, though interestingly "academic research articlesthe focus of an extensive body of corpus-based researchcomprise less than 3 percent of the general "'informational

Future chapters will explore in greater detail corpus analyses of web data, and how tools such as the WebCorp concordancing program (www.webcorp.org.uk/live/) can be used to extract data directly from the Web. But the remaining sections in this chapter will focus primarily on issues related to the construction of traditional corpora.

Corpus Size

The Brown Corpus, released in 1962, is one million words in length. A more recent corpus, the Corpus of Web-Based Global English, is 1.9 billion words in length. Although there are many reasons for the disparity in the length of these corpora, the primary reasons can be attributed to advances in technology and the easy availability of texts in electronic form.

First generation corpora, such as Brown and LOB, were relatively short (each of one million words in length), mainly because of the logistical difficulties that computerizing a corpus created. For instance, all of the written texts for the Brown Corpus had to be keyed in by hand, a process requiring a tremendous amount of very tedious and time-consuming typing. In contrast, those creating second generation corpora have had the advantage of numerous technological advances that have automated the process of corpus creation. For instance, the availability of optical scanners made it easier to convert printed texts into digital formats. More recently, so many texts are available in electronic formats that with very little work, they can easily be adapted for inclusion in a corpus. As a consequence, second generation corpora are regularly 100 million words in length or even longer.

But while written texts can easily be digitized, technology has not progressed to the point where it can greatly expedite the collection and transcription of speech, especially spontaneous conversations: There is much work involved in recording this type of speech and manually transcribing it. Speech recognition software, which converts speech into writing, has been greatly improved over the years, but it works best with a single speaker who has trained the software to recognize his/her speech characteristics. These logistical realities explain why 90 percent of the BNC (a second-generation corpus) contains written texts and only 10 percent spoken texts.

Other corpora, such as COCA, which contain a significant amount of speech, generally consist of spoken samples, such as broadcast discussions, for which commercially produced transcripts are available. But as will be discussed in a later section, the wide availability of transcription services has made the transcription of speech more financially feasible. In addition, the creation of the spoken BNC2014 and the London-Lund Corpus demonstrate the feasibility of creating corpora with significant amounts of spoken language.

Ultimately, the length of a corpus is best determined by its intended use.

Not surprisingly, Davies found that individual lexical items were better studied in larger corpora than in shorter corpora. For instance, while adjectives such as fun or tender are among the group of adjectives that are most common in COCA, in the Brown Corpus, they occurred five times or less. In contrast, certain types of syntactic structures, such as modal verbs, have more even distributions across the three corpora, thus being one of the few areas "where Brown provides sufficient data" (Davies 2015: 15). Overall, COCA provides many more examples of the 10 types of constructions that Davies studied. However, in some instances, the number of occurrences is so high that even though the BNC may contain lower frequencies, the numbers are still high enough to permit valid studies. For instance, while COCA contains 2,900,000 be passives, the BNC contains 890,000 examples (Davies 2015: 15), a number of occurrences that is certainly sufficient enough to study this type of passive. Moreover, frequencies alone can only be suggestive, since different types of studies may concentrate on texts with lower frequencies, especially if such studies are more qualitative in focus.

Unfortunately, such calculations presuppose that one knows precisely what linguistic constructions will be studied in a corpus. The ultimate length of a corpus might therefore be better determined not by focusing too intently on the overall length of the corpus but by focusing more on the internal structure of the corpus: the range of genres one wishes to include in the corpus, the length and number of individual text samples required to validly represent the genres that will make up the corpus, and the demographic characteristics of the individuals whose speech and writing will be chosen for inclusion in the corpus.

2.7

The Internal Structure of a Corpus

The BNC, as Table

If the BNC is compared with the International Corpus of English (ICE), a collection of comparable corpora representing the major varieties of English spoken worldwide, it turns out that while the two corpora contain the same range of genres, the genres are much more specifically delineated in ICE Corpora (see Table

While the amount of writing in the BNC greatly exceeded the amount of speech, just the opposite is true in the ICE Corpus: only 40 percent of the texts are written. While creative (or imaginative) writing was the most common type of writing in the BNC, in the ICE it is not as prominent. More prominent were learned and popular examples of informational writing: writing from the humanities, social and natural sciences, and technology (40 percent of the written texts). These categories are also represented in the BNC, although the BNC makes a distinction between the natural, applied, and social sciences and, unlike the ICE, does not include equal numbers of texts in each of these categories. The ICE also contains a fairly significant number (25 percent) of nonprinted written genres (such as letters and student writing), while only 5-10 percent of the BNC contains these types of texts.

To summarize, while there are differences in the composition of the ICE and BNC, overall the two corpora represent similar genres of spoken and written English. The selection of these genres raises an important methodological question: why these genres and not others?

The answer is that both the ICE and BNC are multi-purpose corpora; that is, they are intended to be used for a variety of different purposes, ranging from studies of vocabulary, to studies of the differences between various national varieties of English, to studies whose focus is grammatical analysis, to comparisons of the various genres of English. For this reason, each of these corpora contains a broad range of genres. But in striving for breadth of coverage, some compromises had to be made in each corpus. For instance, while the spoken part of the ICE contains legal cross-examinations and legal presentations, the written part of the corpus contains no written legal English. Legal written English was excluded from the ICE on the grounds that it is a highly specialized type of English firmly grounded in a tradition dating back hundreds of years, and thus does not truly represent English as it is written in the 1990s (the years during which texts for the ICE are being collected). The ICE also contains two types of newspaper English: press reportage and press editorials. However, as

While both the ICE and the BNC have a very clearly defined internal structure, the Corpus of Contemporary American English (COCA) differs somewhat in that it contains collections of texts (of varying length) representing five major registers: spoken (transcripts of dialogical speech taken from various television and radio shows), fiction, newspapers, magazines, and academic writing. Each of these registers contains 103-110 million words of text collected during the years 1990 through 2019 with each year containing approximately 20 million words of text (

As will be discussed in subsequent chapters, general-purpose corpora are useful for the analysis of many kinds of grammatical constructions. However, for those wishing to study a particular register, say press reportage, these corpora do not contain enough samples of the individual genres to permit full-scale studies. Consequently, many special-purpose corpora have been developed. These are corpora with a more specific focus. Two such corpora were discussed earlier in the chapter: the CEEC and the ICLE. But there are many additional such corpora as well. For instance, the Michigan Corpus of Academic Spoken English (MICASE) was created to study the type of speech used by individuals conversing in an academic setting: class lectures, class discussions, student presentations, tutoring sessions, dissertation defenses, and many other kinds of academic speech

At the opposite spectrum of special-purpose corpora like MICASE are those which have specialized uses but not for genre studies. Treebank-3 consists of a heterogeneous collection of texts totaling 100 million words, including one million words of text taken from the 1989 Wall Street Journal, as well as tagged and parsed versions of the Brown Corpus. The reason that a carefully selected range of genres is not important in this corpus is that the corpus is not intended to permit genre studies but to, for instance, "train" taggers and parsers to analyze English: to present them with a sufficient amount of data so that they can "learn" the structure of numerous constructions and thus produce a more accurate analysis of the parts of speech and syntactic structures present in English. And to accomplish this goal, all that is important is that a considerable number of texts be available, and less important are the genres from which the texts were taken.

Because of the wide availability of written and spoken material, it is relatively easy to collect material for modern-day corpora such as the BNC and ICE: The real work is in recording and transcribing spoken material, for instance, or obtaining copyright clearance for written material. With historical corpora, however, collecting texts from the various genres existing in earlier periods is a much more complicated undertaking.

In selecting genres for inclusion in historical corpora, the goals are similar to those for modern-day corpora: to find a range of genres representative of the types of English that existed during various historical periods of English. Consequently, there exist multi-purpose corpora, such as the Helsinki Corpus, which contains a range of different genres (sermons, travelogues, fiction, drama, etc.), as well as specialized corpora, such as the previously mentioned Corpus of Early English Correspondence, a corpus of letters written during the middle English period, with the original CEEC containing letters from late middle English and early modern English and a later version of the corpus (CEECE) letters from the eighteenth century. Other corpora, such as the Corpus of English Dialogues (1560-1760) and the Old Bailey

In gathering material for corpora such as these, the corpus compiler must deal with the fact that many of the genres that existed in earlier periods are either unavailable or difficult to find. For instance, even though spontaneous dialogues were as common and prominent in earlier periods as they are today, there were no tape recorders around to record speech. Therefore, there exists no direct record of speech. However, this does not mean that we cannot get at least a sense of what speech was like in earlier periods. In her study of early American English,

In other situations, a given genre may exist but be underrepresented in a given period. In his analysis of personal pronouns across certain periods in the Helsinki Corpus,

The Length of Individual Text Samples to Be Included in a Corpus

Corpora vary in terms of the length of the individual text samples that they contain. First generation corpora, such as the Brown or London-Oslo-Bergen (LOB) corpora, contain 2,000-word samples. An early corpus of spoken British English, the London-Lund Corpus, contains 5,000-word samples. In the Helsinki Corpus, text samples range from 2,000 to 10,000 words in length. And samples within the BNC vary in length, but are no longer than 40,000 words.

While many corpora contain only text samples, others contain entire texts. For instance, the Lampeter Corpus of Early Modern English Tracts, which is c. 1.1 million words in length, consists of complete texts ranging in length from 3,000 to 20,000 words. The Corpus of Global Web-Based English (1.9 billion words) also contains complete texts of varying length.

Ideally, it would be desirable to include complete texts in corpora, since even if one is studying grammatical constructions, it is most natural to study these constructions within the context of a complete text rather than only part of that text. However, there are numerous logistical obstacles that make the inclusion of complete texts in corpora nearly impossible. For instance, many texts, such as books, are quite lengthy, and to include a complete text in a corpus would not only take up a large part of the corpus but require the corpus compiler to obtain permission to use not just a text excerpt, a common practice, but an entire text, a very uncommon practice. In general, it is quite difficult to obtain permission to use copyrighted material. To avoid copyright infringement, those using the BYU corpora (such as the Corpus of Contemporary American English) are only allowed to view "snippets" in search returns of grammatical items in the corpora they are studying.

Of course, just because only text samples are included in a corpus does not mean that sections of texts ought to be randomly selected for inclusion in a corpus. It is possible to take excerpts that themselves form a coherent unit. For instance, many parts of spoken texts form coherent units themselves, containing sections that have their own beginnings, middles, and ends. Likewise, for written texts, one can include the first 2,000 words of an article, which contains the introduction and part of the body of the article, or one can take the middle of an article, which contains a significant amount of text developing the main point made in the article, or even its end. Many samples in the ICE also consist of composite texts: a series of complete short texts that total 2,000 words in length. For instance, personal letters are often less than 2,000 words, and a text sample can be comprised of complete letters totaling 2,000 words. For both the spoken and written parts of the corpus, not all samples are exactly 2,000 words: a sample is not broken off in mid-sentence but at a point (often over or just under the 2,000-word limit) where a natural break occurs. But even though it is possible to include coherent text samples in a corpus, creators and users of corpora simply have to acknowledge that corpora are not always suitable for many types of discourse studies, and that those wishing to carry out such studies will simply have to assemble their own corpora for their own personal use.

In including short samples from many different texts, corpus compilers are assuming that it is better to include more texts from many different speakers and writers than fewer texts from a smaller number of speakers and writers. And there is some evidence to suggest that this is the appropriate approach to take in creating a corpus.

In addition to studying the distribution of word categories, such as nouns or prepositions, Biber (1993: 250) calculated the frequency with which new words are added to a sample as the number of words in the sample increases. He found, for instance, that humanities texts are more lexically diverse than technical prose texts (p. 252); that is, that as a humanities text progresses, there is a higher likelihood that new words will be added as the length of the text increases than there will be in a technical prose text. This is one reason that lexicographers need such large corpora to study vocabulary trends, since so much vocabulary (in particular, open-class items such as nouns and verbs) occurs so rarely. And as more text is considered, there is a greater chance (particularly in humanities texts) that new words will be encountered.

Determining the Number of Texts and Speakers and Writers to Include in a Corpus

Related to the issue of how long text samples should be in a corpus is precisely how many text samples are necessary to provide a representative sampling of a genre, and what types of individuals ought to be selected to supply the speech and writing used to represent a genre. These two issues can be approached from two perspectives: from a purely linguistic perspective, and from the perspective of sampling methodology, a methodology developed by social scientists to enable researchers to determine how many "elements" from a "population" need to be selected to provide a valid representation of the population being studied. For corpus linguists, this involves determining how many text samples need to be included in a corpus to ensure that valid generalizations can be made about the genre, and what range of individuals need to be selected so that the text samples included in a corpus provide a valid representation of the population supplying the texts.

There are linguistic factors that need to be considered in determining the number of samples of a genre to include in a corpus, considerations that are quite independent of general sampling issues. If the number of samples included in the various genres of the BNC and ICE Corpora are surveyed, it is immediately obvious that both of these corpora place a high value on spontaneous dialogues, and thus contain more samples of this type of speech than, say, scripted broadcast news reports. This bias is a simple reflection of the fact that those creating the BNC and ICE Corpora felt that spontaneous dialogues are a very important type of spoken English and should therefore be amply represented. The reason for this sentiment is obvious: while only a small segment of the speakers of English creates scripted broadcast news reports, all speakers of English engage in spontaneous dialogues.

Although it is quite easy to determine the relative importance of spontaneous dialogues in English, it is far more difficult to go through every potential genre to be included in a corpus and rank its relative importance and frequency to determine how much of the genre should be included in the corpus. And if one did take a purely "proportional" approach to creating a corpus,

In general,

Because the BNC is a relatively lengthy corpus, it provides a sufficient number of samples of genres to enable generalizations to be made about the genres. However, with the much shorter ICE (and with other million-word corpora, such as Brown and LOB, as well), it is an open question whether the forty 2,000-word samples of academic prose contained in the ICE, for instance, are enough to adequately represent this genre. And given the range of variation that

The answer is no: While these corpora are too short for some studies, for frequently occurring grammatical constructions they are quite adequate for making generalizations about a genre. For instance,

Pseudo-titles are constructions such as rock vocalist Iggy Pop, which are similar to equivalent appositives (a rock vocalist, Iggy Pop), except that there is no comma pause between the first unit in the appositive and the second unit. While pseudo-titles are very common in American newspapers, they are stigmatized in British newspapers, occurring more frequently in tabloids than in broadsheets.

All told,

Social scientists have developed a sophisticated methodology based on mathematical principles that enables a researcher to determine how many "elements" from a "sampling frame" need to be selected to produce a "representative" and therefore "valid" sample. A sampling frame is determined by identifying a specific population that one wishes to make generalizations about. For instance, in planning the creation of the Santa Barbara Corpus of Spoken American English, it was decided that recordings of spontaneous conversations would include a wide range of speakers from around the United States representing, for instance, different regions of the country, ethnic groups, and genders. In addition, speakers would be recorded using language in a variety of different contexts, such as conversing over the phone, lecturing in a classroom, giving a sermon, and telling a story (www.linguistics.ucsb.edu/research/santa-barbara-corpus). The idea behind this sampling frame was to ensure that the corpus contained samples of speech representing the ways that people speak in different regions of the country and in different contexts.

Social scientists have developed mathematical formulas that enable a researcher to calculate the number of samples they will need to take from a sampling frame to produce a representative sample of the frame.

Sampling methodology can also be used to select the particular individuals whose speech and writing will be included in a corpus. For instance, in planning the collection of demographically sampled speech for the BNC, "random location sampling procedures" were used to select individuals whose speech would be recorded

In using sampling methodology to select texts and speakers and writers for inclusion in a corpus, a researcher can employ two general types of sampling: probability sampling and nonprobability sampling

Although probability sampling is the most reliable type of sampling, leading to the least amount of bias, for those who created first generation in corpora, this kind of sampling presented considerable logistical challenges. The mathematical formulas used in probability sampling often produce very large sample sizes, as the example above with books illustrated. And there were simply not enough resources available to create corpora beyond the million words in length. However, for many second-generation corpora, such as COCA, GloWbE, or many of the corpora accessible through Sketch Engine (www.sketchengine.co.uk/), size is not an issue, since it currently requires fewer resources to create corpora that contain millions, even billions of words. But for smaller corpora, particularly those containing copyrighted material fully accessible to the user, size becomes a more significant issue, since obtaining copyright permission is a timeconsuming and difficult task.

Judgment, or purposive, or expert choice sampling, a second type of sampling, was used to create the Brown Corpus. That is, prior to the creation of the Brown Corpus, it was decided that the writing to be included in the corpus would be randomly selected from collections of edited writing at four locations:

(1) for newspapers, the microfilm files of the New York Public Library;

(2) for detective and romantic fiction, the holdings of the Providence Athenaeum, a private library; (3) for various ephemeral and popular periodical material, the stock of a large secondhand magazine store in New York City; (4) for everything else, the holdings of the Brown University Library.

(quoted

The Time Frame for Selecting Texts

Most corpora contain samples of speech or writing that have been written or recorded within a specific time frame. Synchronic corpora (i.e. corpora containing samples of English as it is presently spoken and written) contain texts created within a relatively narrow time frame.

For instance, the Brown and LOB Corpora contain written texts published in 1961. The written and spoken texts included within the BNC were published/recorded in the late twentieth century (www .natcorp.ox.ac.uk/). The COCA contains texts created between the years 1990 and 2019, with new texts being added on a yearly basis. The Collins Corpus, used as the basis for creating the COBUILD dictionaries, is a monitor corpus that is currently 4.5 billion words in length (

In creating a synchronic corpus, the corpus compiler wants to be sure that the time frame is narrow enough to provide an accurate view of contemporary English undisturbed by language change. However, linguists disagree about whether purely synchronic studies are even possible: New words, for instance, come into the language every day, indicating that language change is a constant process. Moreover, even grammatical constructions can change subtly in a rather short period of time.

With diachronic corpora (i.e. corpora used to study historical periods of English), the time frame for texts is somewhat easier to determine, since the various historical periods of English are fairly well-defined. However, complications can still arise. For instance,

The Linguistic Background of Speakers and Writers Whose English Is Included in a Corpus

The previous sections provided descriptions of various corpora of English, and the many methodological issues that one must address both in the creation and analysis of a corpus. But one issue that has not been discussed so far concerns the linguistic backgrounds of those whose speech or writing has been included in a corpus. If one is creating a corpus of, say, spoken American English, should the speech of only native speakers of American English be included?

And if the answer is yes, how does one determine exactly what a native speaker is?

As

It is therefore quite important that those creating corpora explicitly define the population whose speech will be sampled. For instance, all ICE corpora contain texts representing "the English of adults (age 18 or over) who have been educated through the medium of English to at least the end of secondary schooling" (www.ucl.ac.uk/englishusage/projects/ice.htm). Note that this description does not necessarily exclude bilingual speakers from the corpus. It simply assures that individuals whose speech will be included in the corpus have had exposure to English over a significant period. The BNC is defined as "a monolingual British English corpus: it comprises text samples which are substantially the product of speakers of British English" (www.natcorp.ox.ac.uk/docs/URG/BNCdes.html#spodes). This restriction allows for a certain degree of flexibility as well, as it would permit a variety of different speakers and writers of British English to be represented in the corpus.

In non-native varieties of English, the level of fluency among speakers will vary considerably: as

To determine whether an individual's speech or writing is appropriate for inclusion in a corpus (and also to elicit the sociolinguistic variables), one can have individuals contributing texts to a corpus fill out a biographical form in which they supply the information necessary for determining whether their native or non-native speaker status meets the criteria for inclusion in the corpus. For instance, individuals can be asked what languages they speak, how long they have spoken them, and in what contexts they have used them, such as in the home, workplace, school, and so forth. If the residence history on the biographical form is unclear, it is also possible to interview the individual afterwards, provided that he or she can be located; if the individual does not fit the criteria for inclusion, his or her text can be discarded.

Determining native or non-native speaker status from authors of published writing can be considerably more difficult, since it is often not possible to locate authors and have them fill out biographical forms. In addition, it can be misleading to use an individual's name alone to determine native speaker status, since someone with a non-English sounding name may have immigrant parents and nevertheless be a native speaker of English, and many individuals with English sounding names may not be native speakers: One of the written samples of the American component of ICE had to be discarded when the author of one of the articles called on the telephone and explained that he was not a native speaker of American English but Australian English.

In spoken dialogues, one may find out that one or more of the speakers in a conversation do not meet the criteria for inclusion because they are not a native speaker of the variety being collected. However, this does not necessarily mean that the text must be excluded from the corpus, since there is annotation that can be included in a corpus indicating that certain sections of a sample are "extra-corpus" material; that is, material not considered part of the corpus for purposes of word counts, generating KWIC (key word in context), and so forth.

Controlling for Sociolinguistic Variables

There are a variety of sociolinguistic variables that will need to be considered before selecting the speakers and writers whose texts are being considered for inclusion in a corpus. Some of these variables apply to the collection of both spoken and written texts; others are more particular to spoken texts. In general, when selecting individuals whose texts will be included in a corpus, it is important to consider the implications that their gender, age, and level of education will have on the ultimate composition of the corpus. For the spoken parts of a corpus, several additional variables need to be considered: the dialects the individuals speak, the contexts in which they speak, and the relationships they have with those they are speaking with. The potential influences that these variables have on a corpus are summarized in the following categories.

Gender Balance

It is relatively easy when collecting speech and writing to keep track of the number of males and females from whom texts are being collected. Information on gender (which is defined biologically in corpus linguistics) can be requested on a biographical form, and in written texts, one can often tell the gender of an individual by his or her first name.

Achieving gender balance in a corpus involves more than simply ensuring that half the speakers and writers in a corpus are female and half male. In certain written genres, such as scientific writing, it is often difficult to achieve gender balance because writers in these genres are predominantly malean unfortunate reality of modern society. To attempt to collect an equal proportion of writing from males and females might actually misrepresent the kind of writing found in these genres. Likewise, in earlier periods, men were more likely to be literate than women and thus to produce more writing than women. To introduce more writing by females into a corpus of an earlier period distorts the linguistic reality of the period. A further complication is that much writing, particularly scientific writing, is co-written, and if males and females collaborate, it will be difficult to determine precisely whose writing is actually represented in a sample. One could collect only articles written by a single author, but this again might lead to a misrepresentation of the type of writing typically found in a genre. Finally, even though an article may be written by a female or a male, there is no way of determining how much an editor has intervened in the writing of an article and thus distorted the effect that the gender of the author has had on the language used in the article.

In speech, other complications concerning gender arise. Research has shown that gender plays a crucial role in language usage. For instance, women will speak differently with other women than they will with men. Consequently, to adequately reflect gender differences in language usage, it is best to include in a corpus a variety of different types of conversations involving males and females: women speaking only with other women, men speaking only with other men, two women speaking with a single man, two women and two men speaking, and so forth.

To summarize, there is no one way to deal with all the variables affecting the gender balance of a corpus. The best that the corpus compiler can do is to be aware of the variables, confront them head on, and deal with them as much as is possible during the construction of a corpus.

Age

There are ranges of age groups that have been included in the many corpora that have been created. For instance, there are special-purpose corpora containing the speech of individuals up to the age of 16. The Child Language Data Exchange System, or CHILDES Corpus, includes transcriptions of children engaging in spontaneous conversations in English and other languages. The Bergen Corpus of London Teenager English (COLT) contains the conversations of adolescents aged 13-17 years. The Polytechnic of Wales Corpus contains transcriptions of conversations between children (aged 6-12 years) and a "friendly" adult concerning their "favourite games or TV programmes."

Overall, though, corpora have tended to contain the speech of adults, largely because to collect the speech of children and adolescents, one often must obtain the permission not just of the individual being recorded but of his or her parents as well, a complicating factor in an already complicated endeavor.

Dialect Variation

It is also important to consider the extent to which a corpus should contain a range of dialects, both social and regional, that exist in any language.

In many respects, those creating historical corpora have been more successful in representing regional variation than those creating modern-day corpora: The regional dialect boundaries in Old and Middle English are fairly well-established, and in the written documents of these periods, variant spellings reflecting differences in pronunciation can be used to posit regional dialect boundaries. For instance,

Because writing is now quite standardized, it no longer contains traces of regional pronunciations. However, even though the modernday corpus linguist has access to individuals speaking many different regional and social varieties of English, it is a significant undertaking to create a spoken corpus that is balanced by region and social class. If one considers only American English, a number of different regional dialects can be identified, and within these major dialect regions, one can isolate numerous sub-dialects (e.g. Boston English within the coastal New England dialect). If social dialects are added to the mix of regional dialects, even more variation can be found, as a social dialect such as African-American Vernacular English can be found in all major urban areas of the United States. In short, there are numerous dialects in the United States, and to attempt to include representative samplings of each of these dialects in the spoken part of a corpus is nothing short of a methodological nightmare.

What does one do, then, to ensure that the spoken part of a corpus contains a balance of different dialects? In selecting speakers for inclusion in the BNC, twelve dialect regions were identified in Great Britain, and from these dialect regions, 100 adults of varying social classes were randomly selected as those whose speech would be included in the corpus

Because it is not logistically feasible in large countries such as the United States or Great Britain to create corpora that are balanced by region and social class, some corpus linguists have devoted their energies to the creation of corpora that focus on smaller dialect regions.

Social Contexts and Social Relationships

Speech takes place in many different social contexts and among speakers between whom many different social relationships exist. When we work, for instance, our conversations take place in a specific and very common social contextthe workplaceand among speakers of varying types: equals (e.g. co-workers), between whom a balance of power exists, and disparates (e.g. an employer and an employee), between whom an imbalance of power exists. Because the employer has more power, he or she is considered a "superordinate" in contrast to the employee, who would be considered a "subordinate." At home (another social context), other social relationships exist: a mother and her child are not simply disparates but intimates as well.

There is a vast amount of research that has documented how the structure of speech is influenced by both the social context in which speech occurs and the social relationships existing between speakers. As

The various components of the ICE (www.ice-corpora.uzh.ch/en/ design.html) contain spontaneous conversations taking place in many different social contexts, ranging from face-to-face conversations to classroom discussions. The Michigan Corpus of Academic Spoken English (MICASE) contains samples of academic speech occurring in many different academic contexts, such as lectures given by professors to students as well as conversations between students in study groups. This ensured that the ultimate corpus created would represent the broad range of speech contexts in which academic speech occurs

Mega Corpora

The previous sections have discussed several methodological issues that need to be considered as one plans and creates a corpus. But as corpora have grown larger, it has become a much more complicated undertaking to ensure that a corpus is both balanced and representative. This is especially the case with web-based corpora, which are quite large in size and whose content is sometimes difficult to determine. For instance, at 1.9 billion words in length, the Corpus of Global Web-Based English is so lengthy that it would be impossible to determine not just the content of the corpus but the distribution of such variables as the gender of contributors, their ages, and so forth.

At the other end of the spectrum are those who would question the necessity of highly planned corpora such as the BNC.

Conclusions

To create a valid and representative corpus, it is important, as this chapter has shown, to carefully plan the construction of a corpus before the collection of data even begins. This process is guided by the ultimate use of the corpus. If one is planning to create a multi-purpose corpus, for instance, it will be important to consider the types of genres to be included in the corpus; the length not just of the corpus but of the samples to be included in it; the proportion of speech versus writing that will be included; the educational level, gender, and dialect backgrounds of speakers and writers included in the corpus; and the types of contexts from which samples will be taken. However, because it is virtually impossible for the creators of corpora to anticipate what their corpora will ultimately be used for, it is also the responsibility of the corpus user to make sure that the corpus he or she plans to conduct a linguistic analysis of is a valid corpus for the particular analysis being conducted. This shared responsibility will ensure that corpora become the most effective tools possible for linguistic research.

in italics or boldface, and to mark other features that are particular to written texts.

Linguistic annotation: The process of annotating a corpus involves running software that can (1) tag a corpus (add part-of-speech tags to all of the words in the corpus, such as nouns, prepositions, and verbs), or (2) parse a corpus (add markup that identifies larger structures, such as verb phrases, prepositional phrases, and adverbials). Grammatical markup is inserted when a corpus is tagged or parsed.

Metadata is a key component of any corpus: users need to know precisely what is in a corpus. Textual markup is important too, though corpora will vary in terms of how much of such markup they contain. For instance, marking segments of overlapping speech can be a timeconsuming process. Consequently, some spoken corpora may not mark where speakers overlap. Linguistic annotation varies from corpus to corpus as well. While it is quite common for corpora to be lexically tagged, parsing a corpus is a much more complicated process. Therefore, relatively few corpora have been parsed.

Even though the processes of collecting and encoding data for inclusion in a corpus are described above as separate processes, in many senses they are closely connected: After a conversation is recorded, for instance, it may prove more efficient to transcribe it immediately, since whoever made the recording will be available either to transcribe the conversation or to answer questions about the recording to aid in its transcription. If a text is collected, and saved for later computerization and annotation, the individual who collected the text may not be around to answer questions, and information about the text may consequently be lost. Of course, logistical constraints may necessitate collecting texts at one stage and computerizing and annotating them at a later stage, in which case it is crucial that as much information about the text be obtained initially so that, at a later stage, those working with the text will be able to easily recover this information. The kinds of information to collect will be discussed in greater detail in Section 3.2.

General Considerations

As discussed in Chapter 2, before the actual data for a corpus is collected, it is important to carefully plan exactly what will be included in the corpus: the kinds and amounts of speech and/or writing, for instance, as well the range of individuals whose speech and writing will become part of the corpus. Once these determinations are made, the corpus compiler can begin to collect the actual speech and writing to be included in the corpus. However, it is important not to become too rigidly invested in the initial corpus design, since obstacles and complications may be encountered while collecting data that may require changes in the initial corpus design: It might not be possible, for instance, to obtain recordings for all the genres originally planned for inclusion in the corpus, or copyright restrictions might make it difficult to obtain certain kinds of writing. In these cases, changes are natural and inevitable and, if they are carefully made, the integrity of the corpus will not be compromised.

The International Corpus of English (ICE) Project provides a number of examples of logistical realities that forced changes in the initial corpus design for some of the components. After the project began, it was discovered that not all the regional groups involved in the project would be able to collect all of the text categories originally planned for inclusion in the corpus. For instance, in ICE-East Africa (which includes texts collected in Kenya and Tanzania), it was not possible to collect examples of scripted monologues, such as legal presentations and scripted commentaries. However, to make up for this deficiency, additional texts were collected in the other categories: 120 scripted monologues (e.g. news broadcasts and speeches) instead of the 50 samples required in the other components of the ICE (

Collecting Samples of Speech

Speech is the primary mode of human communication. As a result, there are various types of speech: not just spontaneous multiparty dialogues but scripted and unscripted monologues, radio and television interviews, telephone conversations, class lectures, and so forth. Given the wealth of speech that exists, as well as the logistical difficulties involved in recording and transcribing it, collecting data for the spoken part of a corpus is much more labor-intensive than collecting written samples. As

In collecting any kind of speech, the central concern is obtaining speech that is "natural." This is a particularly important issue when gathering spontaneous multi-party dialogues, such as informal talks between two or more individuals. If multi-party dialogues are not carefully collected, the result can be a series of recordings containing very stilted and unnatural speech. Collecting "natural" multi-party dialogues involves more than simply recording people as they converse. As anyone who has collected language data knows, if speakers know that their speech is being recorded, they will change the way that they speak, a phenomenon called the "observer's paradox"

To avoid this problem, those creating earlier corpora, such as the London-Lund Corpus, recorded people surreptitiously, and only after recordings were secretly made were individuals informed that they had been recorded. While it may have been acceptable and legal back in the1950s and 1960s to record individuals without their knowledge, now such recordings are not only considered unethical within the scientific community but may in fact be illegal in many locales. It is therefore imperative both to inform individuals that their speech is being recorded and to obtain written permission from them to use their speech in a corpus. This can be accomplished by having individuals being recorded sign a release form prior to being recorded. In addition, many universities and other organizations have Institutional Review Boards that review any research conducted on "human subjects" by faculty members and will grant approval for use of such subjects only if accepted practices are followed.

Since it is not possible to include surreptitious speech in a corpus, does this mean that non-surreptitiously gathered speech is not natural? This is an open question, since it is not possible to answer it with any definite certainty. However, there are ways to increase the probability that that the speech included in a corpus will be natural and realistic.

First, before individuals are recorded, they should be given in written form a brief description of the project in which they are participating. In this description, the purpose of the project should be described, and it should be stressed that speech is being collected for descriptive linguistic research, not to determine whether those being recorded are speaking "correct" or "incorrect" English. In a sense, these individuals need to be given a brief introduction to a central tenet of modern linguistics: that no instance of speech is linguistically better or worse than any other instance of speech, and that all types of speech are legitimate, whether they are perceived as standard or nonstandard.

A second way to enhance naturalness is to record as lengthy a conversation as possible so that when the conversation is transcribed, the transcriber can select the most natural segment of speech from a much lengthier speech sample, for instance, 30 minutes or longer. This length of conversation increases the probability that a natural and coherent sample of speech can be extracted from this longer sample. The initial part of the conversation can then be discarded, since people are sometimes nervous and hesitant upon first being recorded but after a while become less self-conscious and start speaking more naturally. Moreover, a lengthier segment allows the corpus compiler to select a more coherent and unified segment of speech to ultimately include in the corpus.

How much speech needs to be recorded is also determined by the type of speech being recorded. Spontaneous dialogues, for instance, may require lengthier segments of recorded speech because of features such as hesitations, pauses, and interruptionsall of which slow down the pace of speech. On the other hand, monologues (especially scripted monologues) contain far fewer pauses and hesitations. Thus, less speech is needed to reach these necessary numbers of words for the particular corpus being created.

When it comes time to actually make recordings, whoever is making the recordings needs to follow a few basic principles of recording to ensure the most natural recordings as possible. Probably the least desirable way to make a recording is to have the research assistant sitting silently nearby with microphone in hand while the people being recorded converse. This all but ensures that the individuals being recorded will constantly be reminded that they are part of a "linguistic experiment." As much as possible, those making recordings should try to record individuals in the actual environments in which they typically speak, such as the family dinner table, the workplace, restaurants, the car, informal get-togethers and so forth.

Because the goal is to record people in natural speaking environments, it is best for the research assistant not to be present during recordings. He or she can simply set up the recording equipment, turn on the recorder, and leave. Alternatively, the people being recorded can be loaned the recording equipment and taught to set it up and turn on the recorder themselves. This latter option was used to gather speech in the demographic component of the British National Corpus (BNC). Individuals participating in the project were given portable tape recorders and an adequate supply of cassettes, and were instructed to record all of the conversations they had for periods ranging from two to seven days (Crowdy 1993: 260; www.natcorp.ox.ac.uk/docs/URG/BNCdes .html#body.1_div.1_div.5_div.1). To keep track of the conversations they had, participants filled out a logbook, indicating when and where the recordings occurred as well as who was recorded. This method of collection habituates participants to the process of being recorded and, additionally, ensures that a substantial amount of speech is collected.

In certain natural speaking environments, such as restaurants or automobiles, there will often be a considerable amount of ambient noise, resulting in recordings containing variable amounts of inaudible speech that cannot be accurately transcribed. This problem can be handled with annotation during the actual transcription of the recording that marks certain sections as inaudible. It can also sometimes be minimized using commonly available audio editing software. If high-quality recordings are desired, individuals can be recorded in an actual recording studio, as was done in collecting speech samples for the Map Task Corpus, a corpus of conversations between individuals giving directions to various locations (see

While collecting natural speech is a key issue when recording multi-party dialogues, it is less of an issue with other types of speech. For instance, those participating in radio and television broadcasts will undoubtedly be conscious of the way they are speaking, and therefore may heavily monitor what they say. However, heavily monitored speech is "natural" speech in this context. Therefore, this is precisely the kind of speech one wants to gather. Other types of spoken language, such as public speeches (especially if they are scripted), are also heavily edited.

When recording any kind of spoken English, it is important to consider the quality of the audio recorder and microphone to be used. In earlier corpora, most corpus compilers used analog recorders and cassette tapes, since such recorders were small and unobtrusive and cassette tapes were inexpensive. However, with the rise of digital technology, there are now a variety of recorders that are widely available, such as digital audiotape (DAT) recorders as well as Minidisc recorders. These recorders make high-quality digital recordings, which can then be exported and used in special software programs that aid in the transcription of speech.

It is equally important to consider the quality and type of microphone to be used to make recordings. A low-quality microphone will produce recordings that are "tinny" even if a good tape recorder is used. It is therefore advisable to invest resources in good microphones, and to obtain microphones that are appropriate for the kinds of recordings being made. To record a single individual, it is quite acceptable to use a traditional uni-directional microphone, a microphone that records an individual speaking directly into the microphone. For larger groups, however, it is better to use omni-directional microphones: microphones that can record individuals sitting at various angles from the microphone. Lavaliere microphones, which are worn around the neck, are useful for recording individuals who might be moving around when they speak, as people lecturing to classes or giving speeches often do. Wireless microphones are appropriate in recording situations of this type too, and avoid the problem of the speaker being constrained by the length of the cord attaching the microphone and recorder. There are also extra-sensitive microphones for recording individuals who are not close to the microphone, as is the case in a class discussion, where those being recorded are spread out all over a room and might not be close enough to a traditional microphone for an audible recording to be made. For recording telephone conversations, special adapters can be purchased that record directly off landline telephones. For cell phones, there are apps that can be used to record conversations. High-quality microphones can be fairly expensive, but they are worth the investment.

To find information on specific recorders and microphones, it is useful to consult what researchers who specialize in the study of spoken language use to make voice recordings. For instance, The Division of Psychology and Language Sciences at University College London provides a listing of recorders and microphones used by individuals doing research in areas such as phonetics and speech pathology (www .phon.ucl.ac.uk/resource/audio/recording.html). Anthropologists and ethnographers who do fieldwork also make use of recording equipment for recording spoken language (

Even with the best recording equipment, however, assembling a large number of recordings suitable for inclusion in a corpus is a timeconsuming and sometimes frustrating process: for every 10 recordings made, it may turn out that some of them are unusable. For instance, conversants might be speaking naturally for certain periods of time, only to stop and say something irrelevant, such as "Is the recorder still working?" Remarks such as this illustrate that no matter how hard one tries, it is impossible to make many individuals forget that their speech is being monitored and recorded. Other problems include excessive background noise that makes all or part of a conversation inaudible, or people who are given a recorder to record their speech and then operate it improperly, in some cases not recording any of the conversation they set out to record. Those compiling spoken corpora should therefore expect to gather much more speech than they will actually use to compensate for all the recordings they make that contain imperfections preventing their use in the ultimate corpus being created.

While most of the recordings for a corpus will be obtained by recording individuals with microphones, many corpora will contain sections of broadcast speech. This type of speech is best recorded not with a microphone but directly from a radio or television by running a cord from the audio output plug on the radio or television to the audio input plug on the tape recorder. It is also possible to get a wide variety of written transcriptions of broadcast speech from such sources as talk shows, interviews, and news conferences. But using such secondhand sources raises questions about the accuracy of the transcriptions: the extent to which what is transcribed accurately matches what was actually said.

Inaccuracies in such transcriptions can potentially be a problem. Molin (2007), for instance, found that in the Hansard Corpus of Canadian parliamentary proceedings, transcriptions of the proceedings did not always capture the precise language that was used by conversants. In many cases, changes were made so that usages that parliamentarians actually uttered conformed more to prescriptive norms: contracted forms were changed to full forms (e.g. don't to do not) or be going to was replaced with will (p. 207). In contrast, in a corpus containing transcripts from the broadcast network

A perusal of several transcripts and recordings made by the broadcast network NPR (www.npr.org/templates/transcript/transcript.php? storyId=15166387) revealed occasional mistranscriptions, but overall, a close fidelity with the actual language transcribed. One of the few errors observed involved transcribing react in the example below instead of the form actually used in the recording: interact.

It's also changed the way they react with doctors, their families, and even with strangers.

But it is important to note to that there will always be errors in any transcription, whether it is based directly on a recording or taken from a transcription made available by a second party. The ultimate goal is to get as much accuracy as possible and be practical. Therefore, the time saved in using second-party transcripts is worth the tolerance of a certain level of error, provided that some checking is done to ensure overall accuracy in the transcripts included in a corpus. The process of transcribing speech will be discussed in greater detail in 3.x.

Collecting Samples of Writing

Although collecting samples of writing is considerably less complicated than collecting samples of speech, one significant obstacle is encountered when collecting writing: copyright restrictions. Under the "fair use" provisions of current U.S. copyright laws, it is possible in certain circumstances to use copyrighted material without receiving the explicit permission from the copyright holder. However, the "circumstances" are stated rather generally and are subject to interpretation (www.copyright.gov/fair-use/more-info.html). Thus, including something in a corpus without getting explicit permission from the copyright holder could involve copyright infringement.

In many first-generation corpora, such as the Brown and BNC, clearance was obtained for all copyrighted material. But because of the huge size of recent mega-corpora, obtaining such clearance is simply not possible. With the Corpus of Contemporary American English (COCA), though, workarounds were implemented that allowed users full access to the corpus without violating copyright law. For instance, searches of the corpus retrieve only "very short 'Keyword in Context' displays, where users see just a handful of words to the left and the right of the word(s) searched for." Additionally, the corpus can be used only for academic research (www.english-corpora.org/copyright.asp).

If attempts are made to secure permission to use copyrighted material in a corpus, it is best to over collect the number of texts to include in each part of the corpus: A written text may be gathered for possible inclusion in a corpus, and its author (or publisher) may not give permission for its use, or (as was a common experience gathering written texts for inclusion in the American component of ICE) contact is made requesting permission but no reply is ever received.

Because of the difficulties in obtaining permission to use copyrighted materials, most corpus compilers have found themselves collecting far more written material than they are able to obtain permission to use: for ICE-USA, permission had been obtained to use only about 25 percent of the written texts initially considered for inclusion in the corpus. Moreover, some authors and publishers will ask for money to use their material: one publisher requested half an American dollar per word for a 2,000-word sample of fiction considered for inclusion in ICE-USA! Therefore, if a corpus is to be used only for non-profit academic research, this should be clearly stated in any letter of inquiry or email requesting permission to use copyrighted material and many publishers will sometimes waive fees. However, if there will be any commercial use of the corpus, special arrangements will have to be made both with publishers supplying copyrighted texts and those making use of the corpus.

In gathering written texts for inclusion in a corpus, the corpus compiler will undoubtedly have a predetermined number of texts to collect within a range of given genres: twenty 2,000-word samples of fiction, for instance, or ten 2,000-word samples of learned humanistic writing. However, because there is so much writing available, it is sometimes difficult to determine precisely where to begin to locate texts. Since most corpora are restricted to a certain time frame, this frame will of course narrow the range of texts, but even within this time frame, there is an entire universe of writing.

In earlier corpora, written texts needed to be scanned or re-typed to be converted into digital formats. Currently, however, most written texts are also available in digital formats. For instance, newspapers and magazines are accessible in digital formats that can be easily used in corpora. Many commercial publications, such as novels, are also available digitally, but often in formats that cannot be copied.

Keeping Records of Texts Gathered

As written texts are collected and spoken texts are recorded, it is imperative that accurate records are kept about the texts and the writers and speakers that created them. For ICE-USA, research assistants filled out a written checklist supplying specific information for each spoken and written text that was collected.

First of all, each text was assigned a number that designated a specific genre in the corpus in which the sample would be potentially included. For instance, a text numbered S1A-001 would be the first sample considered for inclusion in the genre of "direct conversations"; a text numbered S1B-001, on the other hand, would be the first sample considered for inclusion in the genre of "classroom lectures." A numbering system of this type (described in detail in Greenbaum 1996b: 601-14) allows the corpus compiler to keep easy record of where a text belongs in a corpus and how many samples have been collected for that part of the corpus. After a text was numbered, it was given a short name providing descriptive information about the sample. In ICE-Great Britain, sample S1A-001 (a spontaneous dialogue) was named "Instructor and dance student, Middlesex Polytechnic" and sample S1B-001 (a class lecture) was entitled "Jewish and Hebrew Studies, 3rd year, UCL." The names supplied to a text sample are short and mnemonic and give the corpus compiler (and future users of the corpus) an idea of the type of text that the sample contains.

The remaining information recorded about texts depended very much on the type of text that was being collected. For each spoken text, a record was kept of when the text was recorded, where the recording took place, who was recorded, who did the recording, and how long the recording was. For each person recorded, a short excerpt of something they said near the start of the recording was written down so that whoever transcribed the conversation would be able to match the speech being transcribed with the speaker. It can be very difficult for a transcriber to do this if he or she has only the recording to work with and must figure out who is speaking. For speech samples recorded from television or radio, additional information was written down, such as what station the recording was made from, where the station is located, and who should be contacted to obtain written permission to use the sample. For written texts, a complete bibliographical citation for the text was recorded along with the address of the publisher or editorial office from which permission to use the written text could be obtained.

In addition to keeping records of the texts that are recorded, it is equally important to obtain ethnographic information from individuals contributing either a sample of speech or a written text. The particular information collected will very much depend on the kind of corpus being created and the variables that future users of the corpus will want to investigate. Because ICE-USA is a general-purpose corpus, only fairly general ethnographic information was obtained from contributors: their age, gender, occupation, and a listing of the places where they had lived over the course of their lives. Other corpora have kept different information on individuals, relevant to the particular corpus being created. The Michigan Corpus of Academic Spoken English (MICASE) collected samples of spoken language in an academic context. Therefore, not just the age and gender of speakers in the corpus were recorded but their academic discipline (e.g. humanities and arts, biological and health sciences), academic level (e.g. junior undergraduates, senior faculty), nativespeaker status, and first language

Ethnographic information is important because those using the ultimate corpus that is created might wish to investigate whether gender, for instance, affects conversational style, or whether younger individuals speak differently than older individuals. It is important for these researchers to be able to associate variables such as these with specific instances of speech. While it is relatively easy to obtain ethnographic information from individuals being recorded (they can simply fill out the form when they sign the permission form), tracking down writers and speakers on radio or television shows can be very difficult. Therefore, it is unrealistic to expect that ethnographic information will be available for every writer and speaker in a corpus. And indeed, many current corpora, such as the BNC and ICE, contain missing information on many speakers and writers. After all the above information is collected, it can be very useful to enter it into a database, which will allow the progress of the corpus to be tracked. This database can contain not just information taken from the forms described above but other information as well, such as whether the text has been computerized yet, whether it has been proofread, and so forth. Creating a corpus is a huge undertaking, and after texts are collected, it is very easy to file them away and forget about them. It is therefore crucial to the success of any corpus undertaking that accurate information be kept about each text to be considered for inclusion in the corpus.

Encoding Spoken and Written Data

Encoding spoken data is a much more complicated process than encoding written data.

To encode spoken data, recordings of speech first need to be transcribed, an extremely lengthy process requiring the transcriber to listen to the same segments of speech repeatedly until an accurate transcription is achieved. Although the process of transcription has been automated, current voice recognition technology has not reached the level of sophistication to be able to accurately transcribe the most common type of speech: spontaneous conversations.

In earlier corpora, written texts, which existed in printed form, had to be optically scanned into a computera process that often produced digitized texts with numerous scanning errors that had to be manually corrected. However, so many written texts are now available in digitized formats that scanning is mainly restricted to texts dating back to the pre-electronic era.

There are a number of general considerations to bear in mind when beginning the process of computerizing both spoken and written texts. First, because texts to be included in a corpus will be edited with some kind of text editing program, it may be tempting to save computerized texts in a file format used by a word processing program (such as files with the extension .doc in Microsoft Word). However, these files will be incompatible with any of the programs customarily used with corpora, such as concordancing programs.

In earlier corpora, the standard was the ASCII (or text) file format, a format that had both advantages and disadvantages. The main advantage at the time was that ASCII was a universally recognized text format that could be used with any word processing program and the numerous software programs designed to work on corpora, such as taggers and parsers and concordances. The disadvantage was that because ASCII has a fairly limited set of characters, many characters and symbols cannot be represented in it. The creators of the Helsinki Corpus had to develop a series of special symbols to represent characters in earlier periods of English that are not part of the ASCII character set: the Old English word "ðaet" ("that"), for instance, is encoded in the corpus as "+t+at" with the symbol "+t" corresponding to the Old English thorn "ð" and the symbol "+a" to the Old English ash "ae"

When creating a corpus, it is easiest to save individual texts in separate files stored in directories that reflect the hierarchical structure of the corpus. This does not commit one to distributing a corpus in this format: the ICAME CD-ROM (2nd ed.) allows users to work with an entire corpus saved in a single file. But organizing a corpus into a series of directories and sub-directories makes working with the corpus much easier, and allows the corpus compiler to keep track of the progress being made on corpus as it is being created. Figure

Each ICE component consists of texts in two main directoriesone containing all the spoken texts included in the corpus, the other all the written texts. These two directories, in turn, are divided into a series of sub-directories containing the main types of speech and writing that were collected: the spoken part into monologues and dialogues, the written part into printed and non-printed material.  Each text included in a given ICE component is assigned an identification letter and number indicating the type of speech or writing that it represented. For instance, for the 90 texts labeled S1A-001 to S1A-090, the letter S indicates that each 2,000-word sample represents spoken English; the numerals 001-090 that it was a private conversation (either a spontaneous conversation or a telephone call); and the uppercase A that it was a dialogue. While this numbering system is unique to ICE components, a similar system can be developed for any corpus project.

Other directories can be created to fit the needs of the research team building a particular corpus. For instance, a "draft" directory is useful for early stages of corpus development and can contain spoken texts that are in the process of being transcribed or written texts that have been computerized but not proofread. Once a draft version of a text contains "metadata" and "textual markup", it can then be placed into a "lexical (pending proofreading)" directory to indicate that the text will be ready for use as a lexical version of the corpus once it has been proofread. There are then two stages of proofreading. The first stage involves proofreading each individual sample in a corpus with samples being placed into a "proofread 1" directory. A second round of proofreading is done after completion of the entire corpus so that the corpus can be viewed as a whole.

While a text is being worked on at a particular stage of analysis, it receives an additional file extension to indicate that work on the text is in progress. For instance, while a draft version of a text in the category of business transactions is being created, the text is saved as "S1B-071di", the "i" indicating that work on the text is incomplete. As a particular text is being worked on, a log is maintained that notes what work was done on the text and what work needs to be done. At each stage of analysis, to avoid duplication of work, it is most efficient to have a single person work on a text; at the proofreading stage, it is best to have the text proofread by someone not involved with any prior version of the text, since he or she will bring a "fresh" perspective to the text.

Finally, although inserting "structural" markup into a text is separate from the process of actually computerizing the text, there are many instances where markup can be inserted while texts are being computerized. For instance, in transcribing spontaneous conversations, the transcriber will encounter numerous instances of overlapping speechindividuals speaking at the same time. The segments of speech that overlap need to be marked so that the eventual user of the corpus knows which parts overlap in the event that he or she wishes to study overlapping speech. If annotating overlapping segments of speech is done separately from the actual transcription of the text, the individual doing the annotation will have to go through the tape repeatedly to reconstruct the overlapsa process that could be done more efficiently by the person doing the transcription. Likewise, speaker identification tagstags indicating who is speakingare more efficiently inserted during the transcription of texts. With written texts, if two line breaks are inserted between paragraphs while a text is being computerized, then paragraph tags can be inserted automatically at a later stage. Of course, some markup is probably better inserted after a text sample is computerized. But because computerizing and annotating a text is such an integrated process, it is best to combine the processes whenever this is possible.

But there are caveats to the process of creating a corpus outlined in this section. As corpora have become larger and larger, oftentimes containing millions of words of text, the feasibility of, for instance, proofreading a corpus or placing individual samples into neatly delineated sections becomes less viable: Such corpora are simply too large for any kind of proofreading to be done, or for a single text type (such as a press editorial from a particular newspaper) to be placed into a single directory.

Transcribing Speech

Traditionally, speech had been transcribed using a special transcription machine that has a foot pedal that stops and starts a cassette tape and also automatically rewinds the tape to replay a previous segment. As anyone who has ever transcribed speech knows, the flow of speech is much faster than the ability of the transcriber to type. Therefore, it is extremely important to have the capability of automatically replaying segments.

Because of recent advances in computer technology, it is now possible to use software programs designed specifically to transcribe samples of speech that have been digitized. "VoiceWalker 3.0b" was developed to aid in the transcription of texts included within the Santa Barbara Corpus of Spoken American English. "SoundScriber" is a similar program used to transcribe texts that are part of the MICASE (www-personal.umich.edu/~ebreck/code/sscriber/). Both of these programs are available at the above URLs as freeware, and work very much like cassette transcription machines: the transcriber opens a word processing program in one window and the transcription program in another. After a sample of digitized speech is loaded into the program, short segments of the sample can be automatically replayed until an accurate transcription is achieved. These programs accept as input a variety of different audio formats, including WAV files as well as MP3 files. And for files in different audio formats, the open source program Audacity can be used to convert many different audio files into WAV or MP3 files (www .audacityteam.org/). This program can also be used to convert cassette tapes to digital formats.

While transcription machines and software can ease the process of transcribing speech, there is no getting around the fact that speech must be manually transcribed. And while there are no immediate prospects that this process will be automated, speech recognition programs have improved considerably in recent years, and, in the near future, offer the hope that they can at least partially automate the process of transcribing speech.

Early transcription programs, such as Dragon Dictate (

More recently, there have been advances in the development of programs that can automatically transcribe samples of digitized speech. However, the accuracy of such transcriptions depends upon the type of speech that is being transcribed. Monologues (particularly those that are scripted rather than spontaneous) are the easiest types of speech to automatically transcribe because the voice of a single person is all that needs to be recognized. In contrast, spontaneous unscripted dialogues are much more difficult to transcribe because such speech contains multiple speakers whose conversations contain, for instance, hesitations, overlaps, re-formulations, and incomplete sentences.

But despite these difficulties, there has been progress in developing programs that may in the future help automate the transcriptions of spontaneous dialogues.

To test their system,

Transcribing speech is in essence a highly artificial process, since an exclusively oral form of language is represented in written form. Consequently, before any transcription is undertaken, it is important to decide just how much that exists in a spoken text one wishes to include in a transcription of it. Compilers of corpora have varied considerably in how much detail they have included in their transcriptions of speech. The spoken sections of the COCA do not contain speech that was recorded and transcribed in-house. Instead, spoken samples in this corpus consist entirely of transcriptions of numerous radio and television programs that were created by a thirdparty. Given the cost and effort of creating a corpus of speech, it is understandable why corpora of this type exist, and while they do not contain spontaneous face-to-face conversations, they do provide a substantial amount of spoken language occurring in other speechbased registers.

At the other extreme are corpora of speech that attempt to replicate in a transcription as much information as possible about the particular text being transcribed. The Santa Barbara Corpus of Spoken American English, for instance, contains not only an exact transcription of the text of a spoken conversation (including hesitations, repetitions, partially uttered words, and so forth) but also annotation marking various features of intonation in the text, such as tone unit boundaries, pauses, and pitch contours. This kind of detail is included because creators of this corpus attached a high value to the importance of intonation in speech. The main drawback of this kind of detailed transcription is the lengthy amount of time it takes to annotate a text with information about intonation. The advantage is that very detailed information about a spoken text is provided to the user, thus ensuring that a broad range of studies can be conducted on the corpus without any doubt about the authenticity of the data.

Whether one does a minimal or detailed transcription of speech, it is important to realize that it is not possible to record all the subtleties of speech in a written transcription. As Cook (1995: 37) notes, a spoken text is made meaningful by more than the words one finds in a transcription: How a conversation is interpreted depends crucially upon such contextual features as paralanguage (e.g. gestures and facial expressions), the knowledge the participants have about the cultural context in which the conversation takes place, their attitudes towards one another, and so forth. All of this extra-linguistic information is very difficult to encode in a written transcription without the corpus compiler developing an elaborate system of markup identifying this information and the transcriber spending hours both interpreting what is going on in a conversation and inserting the relevant markup. It is therefore advisable when transcribing speech to find a middle ground: to provide an accurate transcription of what people actually said in a conversation, and then, if resources permit, to add extra information (e.g. marking for various features of intonation).

In reaching this middle ground, it is useful to follow Chafe's (1995) principles governing the transcription of speech. A transcription system,

A General Overview of Metadata, Textual Markup, and Linguistic Annotation

As noted earlier, in order to make a corpus "usable" for linguistic analysis, it is necessary for the corpus compiler to insert Metadata, Textual Markup, and Linguistic Annotation into the corpus. Metadata and Textual Markup are more descriptive, noting, respectively, what is in a corpus and where, for instance, paragraph boundaries in a written corpus occur or speakers pause in a spoken corpus. Linguistic Annotation is more grammatical in nature, providing linguistic information about the various structures occurring within a particular corpus. For instance, if a corpus is lexically tagged, each word in the corpus is assigned a part-of-speech designation, such as noun, verb, preposition, and so forth. In contrast, if a corpus is syntactically parsed, various types of grammatical information is provided, such as which structures are noun phrases, verb phrases, subordinate clauses, and imperative sentences.

The following sections describe the three different ways of describing the content of a corpus, beginning with a brief overview of Metadata and more detailed descriptions of Textual Markup and Linguistic Annotation.

Metadata and Textual Markup

In earlier corpora, there was no standardized system of indicating in a spoken corpus, for instance, sequences of overlapping speech: places in a transcription where the speech of two or more individuals overlaps. Likewise, because written corpora were encoded in text files, there was no way to indicate certain features of orthography, such as italicization or boldface fonts. Consequently, different corpora contained different kinds of markup to describe the same linguistic phenomena. More recently, however, the Text Encoding Initiative (TEI) has developed a standardized system of annotation not just for linguistic corpora (www.tei-c.org/release/doc/ tei-p5-doc/en/html/CC.html) but for electronic texts in general (www .tei-c.org/). Metadata and textual markup are important because they can narrow the range of possible choices when a corpus is searched.

One important feature of a TEI-conformant document is what is called the TEI header (www.tei-c.org/release/doc/tei-p5-doc/en/html/ HD.html), which supplies Metadata about a particular electronic document. Although such headers can get quite complicated, one important part of the header is the file description, which provides information about a document, as illustrated below in a header for The Written Component of ICE-USA: Each part of the header is nested. For instance, the first part of the header, <teiHeader>, is enclosed in angle brackets; the last part of the header, </teiHeader>, is also enclosed in angle brackets but with a / (slash) after the first bracket to indicate that all the information between the first header and second header are part of the header. Other such relationships can be found throughout the header: the title of the document, The Written Component of ICE-USA, is enclosed with an open marker, <title>, and a close marker, </title>. Various levels of indentation are also used to illustrate the hierarchy.

There is also TEI-conformant Textual Markup to describe features occurring within a particular corpus. For instance, an unscripted conversation will contain features of speech such as speaker turns, pauses, or partially articulated words. In contrast, a written text will contain features of orthography, such as paragraph boundaries or font changes. The next section will describe this annotation as it applies to spoken language; a later section will focus on the annotation used in written texts.

Textual Markup for Features of Spoken Texts

While the particular words of a conversation are easy to transcribe using standard orthography, there are other features of speech (e.g. overlapping speech) for which particular Textual Markup is necessary. Because of the complexity of the TEI system of annotation for speech, the discussion will be more illustrative than comprehensive. This is in line with

Utterances

Unlike written texts, spoken texts do not always contain grammatically well-formed sentences. For instance, in the conversational excerpt below (taken from the East African component of ICE), the first speaker (B) leaves out the subject, I, before think, and does not finish the sentence, instead pausing with uh before the next speaker begins talking. In the first part of the second turn, the speaker (C) utters only a partial sentence, the negative particle Not followed by a prepositional phrase: out of uh the question:

<B> <u>think Mr Juma wants to say something maybe uh</u> <C> <u> Not out of uh the question </u> <u>What you're trying to discuss now is about the current situation and the near future of about the position of the</u> <B> <u> the political situation</u> <C> <u>Yeah and the position of Zanzibar President then is that what you're discussing</u>

All the units are marked with the tag <u>, which indicates that each construction is an utterance: a sequence of words that has meaning, even though it is not a grammatically well-formed sentence. In written texts, which overwhelmingly contain grammatical sentences, a different set of tags would be used: <s> and </s>, which mark the beginning and end of a sentence. Note too that each of the speaker turns receives a speaker identification tag: a capital letter (B or C) within angle brackets < >. In spoken corpora, the only punctuation marks typically used are apostrophes for contractions and possessives, capitalization of proper nouns, and hyphens for hyphenated words.

Vocalized Pauses and Other Lexicalized Expressions

Speech contains a group of one-or two-syllable utterances that are communicative but not fully lexical in nature. In the TEI system, these utterances are characterized as non-lexical (e.g. snorts, giggles, laughs, coughs, sneezes, or burps) or semi lexical

The next example contains the semi-lexical expression uh, which is very common in speech and serves the function of allowing the speaker to think of something that he or she wishes to say next in a conversation:

It is also common in speech to find examples of expressions that are spelled as two separate words, but pronounced as one word. For instance, got to, have to, and going to are commonly pronounced as, respectively, gotta, hafta, and gonna. Additional examples include kinda, sorta, and lotsa, which are shortened forms of kind of, sort of, and lots of, respectively. Practice will vary, but typically in corpora where the distinction is made, the form that matches the pronunciation is the one that will be transcribed.

Partially Uttered Words and Repetitions

Speech (especially unscripted speech) contains several false starts and hesitations resulting in words that are sometimes not completely uttered. In the example below, the speaker begins uttering the preposition in but only pronounces the vowel beginning the word. <$D> There are more this year than <.> i </.> in in your year weren't there (ICE-GB)

In the ICE Project, such incomplete utterances are given an orthographic spelling that best reflects the pronunciation of the incompletely uttered word, and then the incomplete utterance is enclosed in markup, <.> i </.>, that labels the expression as an instance of an incomplete word.

Repetitions can be handled in a similar manner. When speaking, an individual will often repeat a word more than once as he or she is planning what to say next. In the example below, the speaker repeats the noun phrase the police boat twice before she completes the utterance: <$B> <}_><-_>the police boat<-/> <=_>the police boat<=/> <}/ _>we know but the warden's boat we don't you know he could just be a in a little rowboat fishing and (ICE-USA)

To accurately transcribe the above utterance, the transcriber will want to include both instances of the noun phrase. However, this will have the unfortunate consequence of skewing a lexical analysis of the corpus in which this utterance occurs, since all instances of the, police, and boat will be counted twice. To prevent this, the ICE Project has special markup that encloses the entire sequence of repetitions (<}_><}/_>) and then places special markup (<=_>the police boat<=/>) around the last instance of the repetition, the only instance counted in analyses done by ICECUP, the text analysis program used in the ICE Project.

Unintelligible Speech

Very often when people speak, their speech is unintelligible. If two people speak simultaneously, for instance, they may drown out each other's words and the speech of both speakers will become unintelligible. Anyone doing a transcription of spoken dialogues will therefore encounter instances where speech cannot be transcribed because it is not understandable. In the example below (taken from ICE-USA), the TEI tags <unclear> and </unclear > surround the word them because the transcriber was uncertain whether this was actually the word the speaker uttered.

<u> What was Moses doing going off in <unclear> them </unclear> jeans </u> 3.9.5

Changing the Names of Individuals Referred to in Spoken Texts

In any conversation, speakers will address themselves by name, and they will talk about third-party individuals, sometimes in unflattering waysone spoken sample from the American component of ICE contains two brothers talking quite disparagingly about their parents.

In transcribing a recording taken from a public broadcast, such as a radio talk show, it is of little concern whether the actual names of individuals are included in a transcription, since such a conversation was intended for public distribution. In transcribing private conversations between individuals, however, it is crucial that names be changed in transcriptions to protect the privacy of the individuals conversing and the people they are conversing about. Moreover, many universities and other organizations have strict rules about the use of human subjects in research and the extent to which their anonymity must be preserved. And if the recordings accompanying the transcriptions are to be made available as well, any references to people's names (except in publicly available recordings) will have to be edited out of the recordingssomething that can be done quite easily with software that can be used to edit digitized samples of speech. In changing names in transcriptions, one can simply arbitrarily substitute new names appropriate to the gender of the individual being referred to or, as was done in the London-Lund Corpus, substitute "fictitious" names that are "prosodically equivalent to the originals"

Iconicity and Speech Transcription

Because writing is linear, it is not difficult to preserve the "look" of a printed text that is converted into an electronic document and transferred from computer to computer in text format: although font changes are lost, markup can be inserted to mark these changes; double-spaces can be inserted to separate paragraphs; and standard punctuation (e.g. periods and commas) can be preserved. However, as one listens to the flow of a conversation, it becomes quite obvious that speech is not linear: Speakers very often talk simultaneously, and while someone is taking their turn in a conversation, another party may fill in brief gaps in the turn with backchannels, expressions such as yea and right that tell the speaker that his or her speech is being actively listened to and supported. Attempting to transcribe speech of this nature in a purely linear manner is not only difficult but potentially misleading to future users of the corpus, especially if they have access only to the transcription of the conversation, and not the recording.

To explore the many options that have evolved for making transcriptions more iconic, it is instructive, first of all, to view how conversations were transcribed by early conversational analysts, whose transcriptions occurred mainly in printed articles, not in computerized corpora, and how using this early convention of transcription in computerized corpora has certain disadvantages. The conversational excerpt below contains a system of transcription typical of early systems. oh yeah I remember we did it before

In the excerpt above, the brackets placed above and below segments of speech in successive speaker turns indicate overlapping segments of speech. For instance, speaker A's uttering of "to edit now" overlaps with speaker B's uttering of "no it's figure." However, instead of representing these overlaps with markup (as a TEI-conformant system would do (cf. www.tei-c.org/release/doc/tei-p5-doc/en/html/TS.html#TSSAPA), this system attempts to indicate them iconically by vertically aligning the parts of the conversation that overlap to give the reader of the conversation a sense of how the flow of the conversation took place. While such visual representations of speech are appealing, the implementation of such a system, as

To overcome problems like this

In the excerpt above, segments of speech in adjoining cells of the table overlap: edit now in speaker A's turn, for instance, overlaps with no it's fig in speaker B's turn.

An alternative way to represent iconically not just overlapping speech but the flow of conversation in general is to lay it out as though it were a musical score. In the HIAT system (Ehlich 1993), a speaker's contribution to a conversation is represented on a single horizontal line. When the speaker is not conversing, his or her line contains blank space. If speakers overlap, their overlaps occur when they occupy the same horizontal space. In the example below, T begins speaking and, midway through his utterance of the word then, H overlaps her speech with T's. For Speakers S1, S2, and Sy, lines are blank because they are not contributing to the conversation at this stage.

Other attempts at iconicity in the above excerpt include the use of the slash in T's turn to indicate that H's overlap is an interruption. we already did figure three we did it before

(instantly) H:

Shall I (wipe it out)? S1: S2: Sy:

While iconicity is a worthy goal to strive for in transcriptions of speech, it is not essential. As long as transcriptions contain clearly identified speakers and speaker turns, and appropriate annotation to mark the various features of speech, a transcription will be perfectly usable. Moreover, many users of corpora containing speech will be interested not in how the speech is laid out but in automatically extracting information from it.

Computerizing Written Texts

Because written texts are primarily linear in structure, they can easily be encoded in text format: Most features of standard orthography, such as punctuation, can be maintained, and those features requiring some kind of description can be annotated with a TEI-conformant tag. For instance, in the example below, the tag "hi" indicates that the word very is highlighted in this context because it is italicized (www.tei-c.org/release/doc/tei-p5-doc/en/html/examplesemph.html).

The child was <hi rend="italics">very</hi> tired.

But while many features of writing can be annotated in a written corpus with TEI-conformant markup, annotating all of these features may prove to be unnecessary. While many users of a spoken corpus will potentially be interested in analyzing overlapping speech, there will likely be very little interest in studying italicized words, for instance, in a written corpus. Consequently, much of what could potentially be annotated in a written text is likely to be of marginal interest to future users of the corpus.

Earlier computer corpora, such as the Brown Corpus, contained texts taken from printed sources, such as newspapers, magazines, and books. To computerize texts from these sources, the texts had to be either keyed in by hand or optically scanned. Nowadays, however, so many written texts exist in digital formats that they can be easily adapted for inclusion in a corpus. For instance, the 14 billion-word iWeb Corpus consists entirely of texts taken from websites.

However, if one is creating a historical corpus and thus working with texts from earlier periods of English, converting a text into an electronic format can be a formidable task and, in addition, raise methodological concerns that the corpus linguist working with modern texts does not need to consider.

Because written texts from earlier periods may exist only in manuscript form, they cannot be optically scanned but must be typed in manually. Moreover, manuscripts can be illegible in sections, requiring the corpus creator to reconstruct what the writer might have written. Describing a manuscript extract of the Middle English religious work Hali Meidenhad,

Although only two versions of Hali Meidenhad have survived, thus reducing the level of difference between various versions of this text that the corpus compiler would have to consider, other texts, such as Ancrene Riwle, can be found in numerous manuscript editions:11 versions in English, 4 in Latin, and 2 in French

In theory, the corpus compiler could create a corpus containing every manuscript version of a text that exists, and then either let the user decide which version(s) to analyze, or provide some kind of interface allowing the user to compare the various versions of a given manuscript. The Canterbury Project gives users access to all 80 versions of Chaucer's Canterbury Tales and allows various kinds of comparisons between the differing versions

One of the earlier and more well-established historical corpora of English is the Helsinki Corpus. This corpus contains texts representing three periods in the development of English: Old English (850-1150), Middle English (1150-1500), and Early Modern English (1500-1710). The texts included from these periods represent various dialect regions of England, such as West-Saxon in the Old English period and East Midlands in the Middle English period. For texts taken from later periods, some sociolinguistic information is provided about some of the writers, such as their age and social status. Various registers are also included in the corpus, such as law, philosophy, history, and fiction.

In 2011, a TEI-XML version of the corpus was released. This represented the first attempt to create a historical corpus conforming to the standards of TEI. One reason for creating a TEI-XML version of the corpus was to avoid the fate of many corpora using their own annotation systems because "as new systems emerge, older ones in limited use are gradually forgotten and the data is rendered effectively inaccessible" (

Linguistic Annotation

Of the two types of Linguistic Annotation, word-class annotation is much more common in the field of corpus linguistics than annotation used to mark larger grammatical structures, such as noun phrases or verb phrases. The primary reason for this difference is that it is much easier to automatically assign word class "tags" to individual words in a corpus

While the focus in this section will be primarily on the most common types of annotationword class and grammatical annotationother types of annotation are possible too.

Word Class Annotation

Over the years, a number of different tagging programs have been developed to insert a variety of different tagsets. The first tagging program was designed in the early 1970s by

All of these programs are designed to assign various lexical tags to every word in a corpus. For instance, listed below is a lexically tagged sentence from the BNC2014, the second version of the BNC: Although the tags may seem rather idiosyncratic, there were several guiding principles that influenced the various abbreviations that were used.

While the CLAWS tagsets were developed to facilitate the study of the linguistic structure of various kinds of spoken and written texts, other tagsets were created to enable research in the area of natural language processing (NLP), an area of language inquiry that is more focused on the computational aspects of designing taggers (and also parsers) to annotate and study corpora. One of the more well-known tagsets is the Penn Treebank Tagset, which contains 36 tags (

Programs designed to insert word class tags into corpora are of three types: they can be rule-based, stochastic/probabilistic, or a hybrid of the two previous types. In a rule-based tagger, tags are inserted on the basis of rules of grammar written into the tagger. One of the earlier rule-based taggers was the "TAGGIT" program, designed by

Of the words reaching this stage of analysis, 61 percent will have one tag, and 51 percent of the remaining words will have suffixes associated with one tag. The remaining words will have more than one tag and are thus candidates for disambiguation. Initially, this is done automatically by a series of "context frame rules" that look to the context in which the word occurs. For instance, in the sentence The ships are sailing, the word ships will have two tags: plural noun and third person singular verb. The context frame rules will note that ships occurs following an article, and will therefore remove the verb tag and assign the tag plural noun tag to this word. Although the context frame rules can disambiguate a number of tags, the process is quite complex, as

While the TAGGIT program had a relatively low accuracy rate, subsequent rule-based taggers have increased overall accuracy rates considerably. For instance, the rule-based tagger EngCG-2 (cf. Samuelsson and Voutilainen 1997) was designed to overcome some of the problems in early rule-based taggers like TAGGIT. In particular, rules in EngCG-2 have wider application than in TAGGITand are able to "refer up to sentence boundaries (rather than the local context alone)"

While rule-based taggers rely on rules written into the tagger, other taggers are probabilistic/stochastic; that is, they assign tags based on the statistical likelihood that a given tag will occur in a given context.

The hybrid tagger CLAWS4 was used to tag the BNC2014, the most current version of the written component of the BNC. This tagger employs "a mixture of probabilistic and non-probabilistic techniques" (

The earlier stages of processing with the CLAWS4 tagger yielded an accuracy rate of 97 percent. While this seems like a very high accuracy rate, in a large corpus, the remaining 3 percent of words with multiple tags can constitute a very sizable number of words. To accurately assign a single tag to these words, two rule-based processes were developed, in the form of what are termed "template rules." For instance, words such as after, before, and since, which can be either prepositions or subordinating conjunctions, were consistently mis-tagged:

1. I will arrive after dinner. [preposition] 2. After the movie, we will go out for drinks. [preposition] 3. I will call you after I finish the report. [subordinating conjunction] 4. I will contact the editor after reviewing the book contract [subordinating conjunction]

To correctly identify each of these instances of after as either a preposition or a subordinating conjunction, the following rule was developed (

Basically, what this rule states is that after is more likely to be a preposition than a subordinating conjunction if after 16 words none of the following constructions appear:

a. a finite verb b. a verb having the form of a past participle c. a comma

In example (1) below, after is clearly a preposition because no verb follows it in the remainder of the sentence. In example (2), after is also a preposition because a comma follows movie. In contrast, in example (3), after is clearly a subordinating conjunction because a verb, finish, follows two words after it. Likewise, after is a subordinating conjunction in example (4) because a past participle, reviewing, occurs directly after it.

(1). I will arrive after dinner.

As the discussion in this section has shown, tagsets vary considerably in the number of part-of-speech tags that they contain. These variations reflect not just differing conceptions of English grammar but the varying uses that the tags are intended to serve.

The Penn Treebank tagset (www.ling.upenn.edu/courses/Fall_ 2003/ling001/penn_treebank_pos.html) contains 36 tags that provide information about the basic form classes in English: nouns, adjectives, adverbs, verbs, and so forth. For instance, there are four basic tags for the class of adverbs in English: RB Adverb (however, usually, naturally, here, good) RBR adverb, comparative (better) RBS adverb, superlative (best) WRB wh-adverb (when, where)

Although the Penn Treebank can be used purely to conduct linguistic analyses, its main purpose is to advance research in the area of Natural Language Processing (NLP), an area of research that does not always require a more finely graded system of lexical tags.

For those using corpora to conduct more detailed grammatical analyses, larger tagsets are more desirable because they allow the retrieval of a wide range of grammatical constructions. For instance, the ICE tagset is based on the view of grammar articulated in

The second type of adverb is in the class of Wh-Adverbs:

when ADV(rel)

The remaining six semantic classes contain tags for classifying various other types of adverbs expressing such notions as additive (add) both/ neither; exclusive (excl) only/merely; intensifier (inten) and very/too; particularizer (partic) mainly, in particular.

In contrast, the Penn Treebank tagset is much smaller (36 tags), mainly because this tagset was developed not necessarily to enable detailed linguistic analyses but to advance research in the area of natural language processing (a point described in detail earlier in this chapter). Thus, this tagset contains, as noted earlier, only four basic tags for adverbs.

While word-class annotation is very well established in corpus linguistics, there are other types of annotation as well. For instance, semantic tagging involves annotating a corpus with markup that specifies various features of meaning.

Parsing a Corpus

Tagging has become a very common practice in corpus linguistics, largely because taggers have evolved to the point where they are highly accurate: many taggers can automatically tag a corpus (with no human intervention) at accuracy rates exceeding 95 percent. Parsing programs, on the other hand, have variable accuracy rates, largely because it is computationally much more difficult to analyze larger structures, such as phrases and clauses, than individual lexical items. Copestake (2016: 500, note 1) also mentions the difficulty of evaluating the accuracy of parsers. She notes that accuracy rates can reach 90 percent "when trained and tested on newspaper text." But other text types can prove more difficult to parse, resulting in lower accuracy rates.

In corpus linguistics, parsed corpora serve two purposes: to enable the analysis of larger syntactic structures, such as phrases and clauses, by individuals conducting linguistic analyses, and to provide testbeds for those in the area of natural language processing interested in the development of parsers. This is not necessarily to suggest that these are mutually exclusive categories. For instance, while the Penn Treebank was created by linguists interested in the design of parsers, it can also be used to study the particular parsed linguistic constructions in the 2,499 articles that it contains from the Wall Street Journal (

One of the most extensively parsed corpora used for linguistic research is ICE-GB, the British component of ICE. Each component of ICE contains a variety of different genres of speech and writing, ranging from spontaneous conversations to scientific writing. Because these genres are so linguistically heterogeneous, a rather complicated methodology was developed to parse ICE-GB.

As was noted in an earlier section, ICE-GB contains a comprehensive set of lexical tags. As Figure

Figure

Following the release of ICE-GB, a second parsed corpus using the same architecture was created: the Diachronic Corpus of Present-Day Spoken English (DCPSE) (www.ucl.ac.uk/english-usage/projects/ dcpse/index.htm). This corpus contains 400,000 words of spoken English from ICE-GB and an additional 400,000 spoken words from the London-Lund Corpus, a corpus that was based on texts recorded between 1960 and 1980. Because of the complexity of parsing a corpus, there are relatively few corpora parsed in as much detail as ICE-GB and the DCPSE.

Much more common than fully parsed balanced corpora are Treebanks: corpora that are syntactically or semantically parsed but that lack the genre variation typically found in corpora used to conduct linguistic analyses. For instance, one of the more widely known Treebanks is the Penn Treebank. The latest version of the Penn Treebank, Treebank-3, contains a heterogeneous collection of texts, including fully parsed articles from the Wall Street Journal as well as parsed versions of the Brown Corpus and the Switchboard Corpus (

Treebanks contain sentences that have been either wholly or partially parsed, and a parser can make use of the already parsed structures in a Treebank to parse newly encountered structures and improve the accuracy of the parser. The example below contains a parsed sentence from the Lancaster Parsed Corpus:

The first line of the example indicates that this is the second sentence from sample "A01" (the press reportage genre) of the LOB Corpus, sections of which (mainly shorter sentences) are included in the Treebank. Open and closed brackets mark the boundaries of constituents: "[S" marks the opening of the sentence, "S]" the closing; the "[N" preceding a move marks the beginning of a noun phrase, "N]" following to stop its ending. Other constituent boundaries marked in the sentence include "Ti" (to-infinitive clause to stop Gaitskell from. . .), "Vi" (non-finite infinitive clause to stop), and "Vg" (nonfinite -ing participle clause nominating). Within each of these constituents, every word is assigned a part of speech tag: a, for instance, is tagged "AT", indicating it is an article; move is tagged "NN", indicating it is a singular common noun; and so forth. Although many Treebanks have been released and are available for linguistic analysis, their primary purpose is to train parsers to increase their accuracy. The Survey of English usage at University College London has as a page on its website (www.ucl.ac.uk/english-usage/projects/ice-gb/ compare.htm) that provides a detailed description and comparison of the various Treebanks and parsed corpora that are available.

Conclusions

The process of collecting and computerizing texts is, as this chapter has demonstrated, a labor-intensive effort. For instance, recording and transcribing spontaneous conversations requires considerable time because individuals need to be recorded and their recordings transcribed. While voice recognition software has made tremendous progress recently, it still works best on monologic speech: an individual speaking slowly into a microphone that is then transferred into written text in a document. Such programs do not work well with dialogic speech.

Written texts, in contrast, are now widely available in digital formats and can easily be incorporated in a corpus after permission has been received to use a given text. While lexically tagging corpora with part-of-speech information can now be done quickly and quite accurately, parsing a corpus is a much more difficult undertaking, since the level of accuracy decreases when identifying structures such as noun phrases or imperative sentences.

The Web has also increased the availability of texts, even certain types of spoken texts. The BYU corpora, for instance, contain different kinds of public speech made available in transcripts that are easily obtainable. While their accuracy cannot be guaranteed, the level of error does not appear to be high. It is also the case that voice recognition software will continue to improve so that in the future it could very well be possible to automate the process of converting speech to text, and thus expedite the inclusion of, for instance, spontaneous dialogues in corpora.

Analyzing a Corpus

The process of analyzing a completed corpus is in many respects similar to the process of creating a corpus. Like the corpus compiler, the corpus analyst needs to consider such factors as whether the corpus to be analyzed is lengthy enough for the particular linguistic study being undertaken and whether the samples in the corpus are balanced and representative. The major difference between creating and analyzing a corpus, however, is that while the creator of a corpus has the option of adjusting what is included in the corpus to compensate for any complications that arise during the creation of the corpus, the corpus analyst is confronted with a fixed corpus, and has to decide whether to continue with an analysis if the corpus is not entirely suitable for analysis, or find a new corpus altogether.

This chapter describes the process of analyzing a completed corpus. To illustrate how such analyses are conducted, the chapter opens with a discussion of Former President Donald Trump's usage of language, termed "Trump speak," and how corpora of his Twitter posts, transcribed speeches, and other spontaneous commentary can be used to study his unique uses of language. The discussion in this section illustrates how to (1) frame a research question, (2) select relevant corpora to carry out the analysis, (3) use a concordancing program to locate appropriate examples for analysis, and then (4) explain the results drawing upon relevant research on language usage, particularly theories of politeness.

The remainder of the chapter provides an overview of qualitative and quantitative research methodologies, discussing the differences between the two methodologies and providing examples of each. For instance, many of the corpus-based reference grammars of English, such as Quirk et al.'s A Comprehensive Grammar of the English Language, are more qualitative, as they draw upon linguistic corpora for authentic examples to illustrate the many points of English grammar discussed throughout the grammar.

In contrast, other corpus studies are more quantitative, subjecting the results of a corpus analysis to, for instance, statistical analyses to determine whether the particular linguistic differences in corpora under study are significant or not. For instance,

Trump Speak: Framing a Research Question

To determine exactly what research question one wishes to pursue, it is first of all necessary to review relevant articles or books written on the topic so that the ultimate question selected does more than merely repeat what others have written on the topic.

Although Donald Trump (hereafter DT) was new to the political scene -President of the United States was his first elected officehe was quite well-known prior to becoming President as a businessperson and television celebrity. However, in his relatively short time as president, he has established a very distinct persona. In particular, he flouts on a regular basis the norms of speech that one would expect from someone in his position. Consequently, his style of speaking has drawn considerable interest from corpus linguists.

Clarke and Grieve (2019) conducted a stylistic analysis of Trump's tweets in the Trump Twitter Archive between the years 2009 and 2021. This archive is very comprehensive and contains every Tweet that Trump posted from 2009 to January 8, 2021, when Trump was banned from Twitter and his account was closed. In their analysis, they note, for instance, changes in the length and frequency of tweets over time and instances when Trump was particularly active in criticizing a particular individual. For instance, the tweets increased in frequency when Trump engaged in a lengthy campaign questioning Obama's citizenship (p. 3).

To study Trump's style of communication, Clark and Grieve adapted Biber's notion of multi-dimensional analysis (see Section 4.6) to isolate certain features of Trump's tweeting style. In his work on register variation, Biber develops a series of what he calls dimensions: general parameters that describe a particular style of communication. For instance, Clark and Grieve's (2019: 18) Dimension 5: Advisory Style characterizes tweets in which Trump is giving advice: Sorry losers and haters, but my I.Q. is one of the highest -and you all know it! Please don't feel so stupid or insecure, it's not your fault

Other corpus linguists have also written about Trump's style of speaking. For instance, Xueliang Chen, Yuanle Yan, and Jie Hu (2019) conducted a corpus analysis of the use of language by Hillary Clinton and Donald Trump when they were running against each other for president. They considered two research questions in their analysis: To answer these questions, they analyzed two comparable corpora that they created, both containing, respectively, speeches presented by Trump and Clinton as they were running against each other. After analyzing the keywords (words with unexpected high frequencies) in the two corpora they created, they concluded that while Clinton's speeches were more positive in nature with top keywords such as women's rights, social justice, and kind, Trump's keywords were much more negative: bad, illegal, disaster. In fact, two of the top keywords in Trump's speeches were Hillary and Clinton, reflecting his frequent reference to her in his speeches and the emphasis he put on critiquing her and her policies.

Selecting Suitable Corpora to Address a Particular Research Question

Because a study of Trump's very often negative use of language is very focused and is a rather narrow topic of research, it will be necessary to draw upon data from very specific corpora to carry out this study. As a consequence, three corpora were analyzed: the Trump Twitter Archive (as described above), the Web, and a 500,000-word archive of, for instance, Trump's speeches and interviews (www.rev.com/blog/transcript-category/donald-trump-tran scripts). Because these are not traditional sources of data, it is worth discussing why they can be considered corpora.

First, corpora typically contain excerpts of texts taken from larger sources. For instance, the Brown Corpus contains 2,000-word samples taken from complete texts (e.g. newspaper articles). But because tweets are relatively short and are not part of any larger text, can they be considered texts themselves? For

Biden was asked questions at his so-called Press Conference yesterday where he read the answers from a teleprompter. That means he was given the questions, just like Crooked Hillary. Never have seen this before! (Trump Twitter Archive, July 1st, 2020)

The tweet above has a topic: Biden's reading his answers to questions from the press on a Teleprompter, and enough sentences to develop this topic so that the entire tweet can stand alone as a coherent unit. Consequently, it has unity of structure. In addition, the tweet has unity of texture: links that tie together the various parts of the tweet. For instance, the tweet opens with an initial reference to Biden. As the tweet develops there are two instances of the pronoun he that co-refer to Biden, tying sections of the tweet together and thus creating cohesion and ultimately coherence.

While the Web has increasingly become a corpus used for linguistic analysis, the other two sources of dataan archive of tweets and a collection of Trump's speeches and interviewsare specific to this particular analysis. But as was discussed in Chapter 1 (Section 1.5), there is a range of other types of corpora that can be used for analysis, including multi-purpose corpora, learner corpora, historical corpora, and parallel corpora.

Extracting Information from a Corpus

There are various ways that grammatical and lexical information can be retrieved from corpora. In the pre-electronic era, such information had to be manually retrieved. For instance, when Jespersen was looking for authentic examples to illustrate the various grammatical constructions he included in his seven-volume A Modern English Grammar on Historical Principles, he, along with a number of student helpers, had to read numerous books and periodicals to obtain authentic examples to illustrate the grammatical points that he was making. Obviously, this involved considerable time and effort.

As sources of data became computerized, concordancing programs were created that allow for various constructions (e.g. words or phrases) to be automatically retrieved from a corpus. For instance, in the analysis of Trump Speak, various examples of usages by Trump were retrieved using either AntConc (described in greater detail later in this section) or a concordancing program specifically designed to retrieve constructions in the Trump Twitter Archive.

Concordancing programs retrieve structures and present them in KWIC (key word in context) concordances. For instance, below is a sample list of examples of Trump's use of the word loser(s). This archive contains a built-in concordancing program that highlights the construction being searched, and also contains a short context in which, in this case, loser (158 examples) or losers (156 examples) occur. Below are some sample tweets retrieved by this program:

(3) Steve Scully of @cspan had a very bad week. When his name was announced, I said he would not be appropriate because of conflicts. I was right! Then he said he was hacked, he wasn't. I was right again! But his biggest mistake was "confiding" in a lowlife loser like the Mooch. Sad! (Oct 16th, 2020 -8:15:38 AM EST) (4) How dare failed Presidential Candidate (1% and falling!) @CoryBooker make false charges and statements about me in addressing Judge Barrett. Illegally, never even lived in Newark when he was Mayor. Guy is a total loser! I want better Healthcare for far less money, always. . . (Oct 13th, 2020 -5:56:41 PM EST)

(5) Joe Biden is a PUPPET of CASTRO-CHAVISTAS like Crazy Bernie, AOC and Castro-lover Karen Bass. Biden is supported by socialist Gustavo Petro, a major LOSER and former M-19 guerrilla leader.

Biden is weak on socialism and will betray Colombia. I stand with you! (Oct 10th, 2020 -2:38:59 PM EST) (6) Because I've beaten him and his very few remaining clients so much, and so badly, that he has become a blathering idiot. He failed with John McCain and will fail again with all others. He is a total loser. @MarshaBlackburn is a Tennessee Star, a highly respected (WINNER! Oct 7th, 2020 -8:18:13 AM EST)

This search yielded 314 examples of Trump's use of the expression loser(s). KWIK concordances are useful because they can easily and quickly be created and also provide the context in which search terms occur. Consequently, when it comes time to include examples in an article or presentation, one can easily integrate the examples into the discussion, and obtain frequency information that indicates whether the usage is common or uncommon.

The results of searches can also help in establishing trends in a corpus. For instance, when Trump was running for the Republican nomination for president, he had nicknames for each of the individuals against whom he was running. For instance, Marco Rubio was commonly referred to by Trump as Little Marco (109 mentions in the Trump Twitter Archive); Ted Cruz was Lyin' Ted Cruz (23 mentions); and Jeb Bush was Low Energy Jeb (or variations, e.g. Jeb low energy) (4 mentions). During his run for the presidency, Trump frequently referred to Hillary Clinton as Crooked Hillary (366 mentions).

A search of these names on the Web yields huge returns. For instance, a search for Little Marco yielded 345 million hits (May 16th, 2021). Similarly, large figures could be found for the other candidates as well. However, frequency information from the Web has to be cautiously interpreted because the returns may, for instance, come from webpages with identical content. Still, web examples and frequencies can provide at least a preliminary sense of how frequent the constructions occurfrequencies that can then be double-checked in other sources.

The concordancing program included with the Trump Twitter Archive is only able to retrieve individual strings of words from a corpus of Trump's tweets. However, other concordancing programs, such as AntConc, allow searches of corpora that are directly linked with AntConc. For instance, the concordancing window in Figure

Notice in the table that all of the search terms that were retrieved are highlighted and vertically aligned for easy reading. Also included is a short context containing a span of text that precedes and comes after the search terms. The size of the search window can be widened or narrowed depending upon how much text is desired both before and after the search term.

AntConc, as the top row reveals, can do other kinds of searches. Figure

N-grams or clusters are groups of words that co-occur. For instance, the expression talk about occurred 6 times in the corpus. Retrieving N-grams in corpora can, for instance, be useful for studying collocations: words that commonly occur together.

The BYU corpora are a set of many different corpora taken from various online resources. The corpora available range from a corpus of American English, the Corpus of Contemporary American English (COCA), to the Coronavirus Corpus, a corpus containing texts retrieved from the Web containing discussions of the Covid-19 virus. All told, 19 corpora are available for online analysis, and all are quite large. For instance, COCA is currently one billion words in length. The size of each corpus constantly changes because all corpora featured on the site are monitor corpora: corpora to which new texts are added on a regular basis.

All BYU corpora can be searched online with a custom-made concordancing program. The search terms specify that walk as a verb be retrieved rather than, for instance, walk as a noun. In addition, the symbol * ensures that all forms of the verb walk will be retrieved, ignoring forms of the  Because of the size of COCA, each of these forms are displayed in separate KWIK concordances. Therefore, selecting walk (which occurred 92,178 times) generates the following KWIC concordance (Figure

Another concordancing program, BNCweb (

The concordancing programs described thus far provide representative examples of the types of searches that such programs allow. However, there are additional programs available on the "Tools for Corpus Linguistics" website (

While concordancing programs are one way to extract constructions from corpora, there are other ways to do so as well.

Concordancing programs are able to locate and identify various sequences of words. However, if a corpus is parsed (i.e. contains larger structures such as phrases, clauses, and sentences that are annotated), it is possible to retrieve through searches much larger structures, such as phrases and clauses.

One lexically tagged and grammatically parsed corpus is ICE-GB, the British component of the International Corpus of English. Each individual word in the corpus is assigned a lexical tag (e.g. noun, verb, preposition, etc.). In addition, phrases and clauses are annotated as well as sentence functions, such as subject, direct object, and adverbial.

Because ICE-GB is both tagged and parsed, it is possible to retrieve many different types of structures, not just individual words but phrases and clauses as well as other types of constructions, such as appositives. Figure

All parse trees in ICE-GB contain features, functions, and categories.

In the parse tree in Figure

For instance, A son, Alistair is functioning as "subject" (SU) in the main clause in which it occurs. Categories are represented at both the phrase and word level: A son is a "noun phrase" (NP), as is Alistair.

To find all instances of proper nouns annotated with the feature "appo," ICECUP requires that a "fuzzy tree fragment" (FTF) be constructed (see Figure

As searches are conducted, it is necessary to collect other types of information that may be relevant for a future paper or article. For instance, any statistical information, such as frequencies, can be included in a spreadsheet. Relevant examples can be saved in a word processing program and include detailed information where the examples originated. Including this information as data is being

Integrating Relevant Linguistic Theories into Corpus Results

A common complaint about corpus analyses is that they often rely heavily on frequency information and statistical analyses, ignoring how this information is connected with relevant linguistic theories. While corpus linguistics has very little to say to those with interests in, for instance, generative grammar, it is of great value to those who are interested in studying pragmatics: how language is used.

One area of pragmatics that has been extensively studied is politeness: how people adjust their manner of speaking to conform to the norms of politeness for the language that they speak. These norms are contextually based: what would be considered impolite in one context may be perfectly acceptable in another context. An important characteristic of Trump Speak is that it often violates the norms of polite speech for both a candidate running for the presidency as well as an elected president. Although theories of politeness in human language are generally conceptualized to explain face-to-face conversations, these theories can be extended to a newer medium, such as Twitter, which is conversational in structure but, unlike interactive human communication, is monologic. For instance, one way that Trump amplifies the impoliteness of his tweets is by assigning particularly negative adjectives to rival candidates' first names. Consider below a series of tweets all containing the expression Crooked Hillary:  Stupid! RNC Further similarly negative expressions include "weak" (156); "dope" or "dopey" (117); "moron(s)" (54); and "dishonest" (115). Most of the examples documented above are insults, a type of speech act that violates many norms of politeness, particularly because Trump's insults of individuals occur in very public forums, such as Twitter, debates, and campaign rallies.

In their discussion of politeness in human language,

Even a cursory overview of Trump's tweets reveals numerous instances of FTAs -Michael Bloomberg is a loser, Hillary Clinton is crooked, Mitt Romney is the dumbest and worst candidate. Other individuals are stupid, weak, dopey, and dishonest. And while these insults, as noted earlier, would have had greater impact if they were uttered in face-to-face interactions with Trump, the fact that Twitter and other media forums are very public, these insults resonate perhaps more than if they were done privately in a face-toface conversation.

The frequent co-occurrence of negative descriptors with particular individuals can also result in what is referred to as a negative semantic prosody. For instance, if the expressions "Crooked Hillary" or "Little Marco" occur so frequently, over time, Hillary Clinton will be viewed as crooked and Marco Rubio as diminutive in stature.

One of

Although this maxim is not universal, it is a given in Western culture that it is better to praise rather than dispraise someone, and if dispraise is given, such speech acts are mitigated. Thus, a teacher might say to a student that his/her paper "has good ideas, but needs some work" rather than "this paper is a disaster, and needs to be completely rewritten." However, in Trump Speak, individuals are dumb, stupid, dopey, and dishonest. Nothing is mitigated. While much of Trump Speak is highly negative, there are two adjectives that Trump regularly uses that are much more positive: tremendous (297) and incredible (356). As the frequency counts demonstrate, these adjectives are two of the most frequently occurring usages in Trump Speak. In addition, they are generally highly positive in contrast to the highly negative connotations of so many of his other usages. While the first example below contains a usage of tremendous that is rather neutral in tone, the other usages are quite positive. For instance, he receives "tremendous support" from unions, the win by the US golf team was a "tremendous win." Likewise, in the last example, the Minneapolis police are characterized as "incredible." These usages are noteworthy because all the other previously discussed usages have been predominantly negative in tone.

• As bad as the I.G. Report is for the FBI and others, and it is really bad, remember that I.G. Horowitz was appointed by Obama. The term Fake News, which occurs quite frequently in his tweets, is typically used to disparage negative news reports against him in the media. For instance, in the second tweet below, he disparages a former government employee who made negative comments about him. In the third tweet, which is a response to media claims that he has not dealt very well with the coronavirus pandemic, he shifts the topic to the Obama administration's handling of the H1N1 Swine Flu: response by the ObamaBiden team to the H1N1 Swine Flu was considered a weak and pathetic one. Check out the polling, it's really bad. The big difference is that they got a free pass from the Corrupt Fake News Media! • Aug 16, 2020 12:31:20 PM. @FoxNews is not watchable during weekend afternoons. It is worse than Fake News @CNN. I strongly suggest turning your dial to @OANN. They do a really "Fair & Balanced" job! • Aug 15, 2020 03:06:50 PM More Fake News ! One final characteristic of Trump Speak has less to do with his repetition of particular vocabulary items and more with his general use of language, particularly his propensity to frequently lie. For instance, as of January 20, 2021 (the last day of his presidency), the Fact Checker Database had documented 30,573 "fake or misleading claims" that Trump has made since becoming President (www .washingtonpost.com/graphics/politics/trump-claims-database/?utm_ term=.27babcd5e58c&itid=lk_inline_manual_2&itid=lk_inline_ manual_2).

Although Trump's tendency to lie may seem outside the realm of linguistic theory, one of the maxims of

(7) Do not say what you believe to be false (8) Do not say that for which you lack adequate evidence

"We built the greatest economy in history, not only for our country, but for the world. We were number one, by far."

This claim, made 360 times by Trump, is proven to be false because there is ample economic documentation that many other countries over a span of many years have far exceeded the current state of the economy during Trump's tenure as president. The conversational implicature of Trump's frequent violation of the quality maxim is that, in general, many people do not trust the veracity of just about any claim that he makes.

Although the analysis of Trump speak in this section has included some statistical informationmainly frequency counts of the various usages that were discussedthe analysis has been primarily qualitative rather than quantitative: it has relied on an extensive analysis of the data rather than a statistical analysis of the frequency counts that are described. In fact, the frequency counts are included mainly to illustrate that the usages discussed occurred frequently enough to demonstrate the rather negative and, in some cases, derogatory nature of Trump Speak. The next section explores in greater detail the differences between qualitative and quantitative corpus analyses.

4.5

Quantitative and Qualitative Analyses

Although quantitative and qualitative approaches to corpus analysis are often viewed as differing ways of analyzing corpora, the difference between the two approaches is not necessarily discrete. For instance,

4.5.1

Qualitative Corpus Analysis

She notes, for instance, that such analyses are "exploratory" and "inductive" and permit "in-depth investigations of authentic language use" (p. 951). To explain particular patterns of usage, qualitative analyses, she continues, can incorporate non-linguistic elements, including various speaker variables, such as age, gender, and economic background. These variables are important for qualitative analyses because they help reveal how non-linguistic elements (e.g. an individual's age or gender) can influence language usage.

In contrast, quantitative analyses rely more heavily on statistical informationinformation that is very often based on corpora that have been annotated with lexical tags (marking, for instance, nouns or verbs) and/or syntactically parsed constructions (such as subjects, objects, and adverbials). These constructions can be automatically retrieved and subjected to statistical analyses that determine whether, for instance, the distributions of the constructions in one genre or another (e.g. fiction or press reportage) is statistically significant. This is not to suggest that there is always an absolute difference between quantitative and qualitative analyses. The two types of analyses can work together to produce an analysis involving what is sometimes termed mixed methods. Thus, one can view the difference between the two types of analyses being on a gradient: a spectrum on which there are varying degrees of quantitative and qualitative analyses.

Qualitative corpus analyses, as Hasko (2020: 952) comments, have a long tradition, dating back to the pre-electronic era. And there is ample evidence to support this claim. For instance, the lexicographer James Murray developed a methodology for collecting authentic usages of language to both determine and illustrate the meanings of words in the first edition of the Oxford English Dictionary. Early grammarians such as Otto Jespersen relied extensively on authentic examples from written texts to write their grammars.

The linguist Randolph Quirk created what is now known as the Quirk Corpus, a corpus of spoken and written British English collected between 1955 and 1985. Before they were digitized, citations of examples illustrating various grammatical categories were only available on slips in file drawers housed at the Survey of English Usage (University College London).

But the first computer corpus for the computational analysis of language, as Hasko (2020: 952) comments, was the Brown Corpus, a corpus that paved the way for not only quantitative corpus research but qualitative studies of language as well, as demonstrated by the particular registers included in the Brown Corpus.

The Brown Corpus marked the beginning of the era of computerized corpora that could be used as the basis for conducting empirically based linguistic investigations of English. Although by modern standards it was fairly short (one million words), it set a standard for the design of many subsequent linguistic corpora, not just of English but of other languages as well (cf. Chapter 1 for a more detailed description of the Brown Corpus).

The Brown Corpus contains 2,000-word samples of edited written American English grouped into two general categories: informative prose (374 samples) and imaginative prose (126 samples) (see Table

There were several methodological considerations that guided the structure of the corpus. First, to adequately represent the various kinds of written American English, a wide range of different types of written English were selected for inclusion in the corpus. For instance, the bulk of the corpus contains various kinds of informative prose, including press reportage, editorials, and reviews; government documents; differing types of learned writing; learned writing from, for instance, the humanities and social sciences. In addition to informative writing, the corpus contains differing types of imaginative prose, such as general fiction and science fiction. More examples of informative than imaginative writing were included because informative writing is much more common than imaginative writing.

Since neither informative nor imaginative writing is homogeneous, the corpus contained 2,000-word samples from a range of different articles, periodicals, books, and so forth to provide as broad a range as possible of different authors, books, periodicals, and so forth. This sampling procedure was also used to ensure that an extensive range of different authors, books, and periodicals were represented. Had lengthier samples been used, the style of a particular author or periodical, for instance, would have been over-represented in the corpus and thus have potentially skewed the results of studies done on the corpus. The Brown Corpus set the standard for how corpora were organized, and as a consequence, was the catalyst for the creation of several additional corpora that replicated its composition. For instance, the London/Oslo/Bergen (LOB) Corpus contains one million words of edited written British English as well as the same genres and length of samples (2,000 words) as the Brown Corpus with only very minor differences: In several cases, the number of samples within a given genre vary: popular lore contains 44 samples, while in the Brown corpus the same genre contains 48 samples (cf.

The inclusion in the Brown Corpus of many different types of written English made it quite suitable for the qualitative analysis of language usage. A select overview of research conducted on this corpus makes this point very clear. For instance,

(9) I know that he is here.

(10) I know he is here.

Because the inclusion of that is optional in such constructions, Elsness was interested in determining the various factors that influenced the retention or deletion of that. For instance, he considered such factors as whether the formality of the particular type of writing influenced the choice of either retaining or deleting that; whether that deletion was less common in formal writing than in informal writing; whether the particular verb influenced use or non-use of that; and whether including or not including that resulted in a difference in meaning.

To address these research questions, Elsness restricted his analysis to four different registers of the Brown Corpus: Press Reportage; Belles Lettres, Biography, etc.; Learned and scientific Writings; and Fiction: Adventure and Westerns. Because these are very different registers, Elsness would be able to determine, for instance, whether that-deletion has a restricted usage: occurring, for instance, less frequently in more formal registers, such as learned and scientific writing, than in less formal registers, such as fiction. Essentially, what Elsness is doing in this article is using frequency counts, which are quantitative in nature, to support qualitative claims about the usage of that-deletion in various genres of English in the Brown Corpus.

The Brown Corpus was also the first corpus to be lexically tagged. Consequently, it can be viewed as ushering in a new era of quantitative analyses of corpora. For instance, one of its earliest applications involved supplying word frequency lists for the American Heritage dictionary

As corpus linguistics developed as a discipline, and the technology used both to create and analyze corpora increased in sophistication, it became easier to create computerized corpora, conduct more automated analyses of them, and also to subject the results of analyses to statistical analyses. As a result, more quantitative analyses of corpora were able to be conducted.

Quantitative Corpus Analysis

This section explores various ways that quantitative analyses of corpora can be conducted. Such analyses involve the application of various statistical tests to determine whether the hypotheses being tested are statistically significant. Because of the complexity of statistical analyses, the discussion in this section will be restricted to two areas:

(11) The first section describes how descriptive statistics, such as Chi square or log likelihood, can be used to determine whether pseudo-titles in various stigmatized constructions such as Microsoft president Bill Gates occur with differing frequencies in numerous samples of press reportage taken from newspapers appearing in the various components of ICE.

Pseudo-titles are related to appositives. Thus, in the example above, a full appositive equivalent would be the president of Microsoft, Bill Gates. Appositives contain two co-referential noun phrases separated by a comma pause. In pseudo-titles, the first unit acts more like a modifier: no co-referential relationship exists between the two noun phrases, and there is no comma pause separating them. Pseudo-titles also have a very restricted usage, occurring predominantly in press reportage. However, their usage varies, with some newspapers banning them entirely, always requiring full appositives, while other newspapers, often tabloids but broadsheets as well, use them quite frequently.

(12) The second section describes how multi-dimensional analyses in the work of Douglas Biber can be used to study register variation: how various linguistic constructions occur more or less commonly in differing registers, ranging from press reportage to fiction.

The Statistical Analysis of Pseudo-Titles

Previous research on pseudo-titles has documented their existence in American, British, and New Zealand press reportage, and demonstrated that because their usage is stigmatized, certain newspapers (particularly in the British press) prohibit their usage. To examine their usage empirically, the press reportage in seven regional varieties of ICE were examined to determine their usage globally. Table

As Table

Although the results displayed in Table

(13) Lawyer Paul Muite and his co-defendants in the LSK contempt suit wound up their case yesterday and accused the Government of manipulating courts through proxies to silence its critics. . .Later in the afternoon, there was a brief drama in court when a lawyer, Ms Martha Njoka, was ordered out after she defied the judge's directive to stop talking while another lawyer was addressing the court. (ICE-East Africa)

Exploring a corpus qualitatively allows the analyst to provide descriptive information about the results that cannot be presented strictly quantitatively. But because this kind of discussion is subjective and impressionistic, it is better to devote the bulk of a corpus study to supporting qualitative judgements about a corpus with quantitative information.

Using Quantitative Information to Support Qualitative Statements

In conducting a microscopic analysis of data, it is important not to become overwhelmed by the vast amount of statistical information that such a study will be able to generate, but to focus instead on using statistical analysis to confirm or disconfirm the particular hypotheses one has set out to test. In the process of doing this, it is very likely that new and unanticipated findings will be discovered: A preliminary study of pseudo-titles, for instance, led to the discovery that the length of pseudo-titles varied by national variety, a discovery that will be described in detail below.

One of the most common ways to begin testing hypotheses is to use the "cross tabulation" capability found in any statistical package. This capability allows the analyst to arrange the data in particular ways to discover associations between two or more of the variables being focused on in a particular study. In the study of pseudo-titles, each construction was assigned a series of tags associated with six variables, such as the regional variety the construction was found in, and whether the construction was a pseudo-title or a corresponding apposition. To begin investigating how pseudo-titles and corresponding appositives were used in the regional varieties of ICE being studied, a cross tabulation of the variables "country" and "type" was generated. This cross tabulation yields the results displayed in Table

The results of the cross tabulation in Table

When comparing results from different corpora, in this case differing components of ICE, it is very important to compare corpora of similar length. If different corpora of varying length are compared and the results are not "normalized," then the comparisons will be distorted and misleading. For instance, if one were to count and then compare the number of pseudo-titles in one corpus of 40,000 words and another of 50,000 words, the results would be invalid, since a 50,000-word corpus is likely to contain more pseudo-titles than a 40,000-word corpus, simply because it is longer. This may seem like a fairly obvious point, but in conducting comparisons of the many different corpora that now exist, the analyst is likely to encounter corpora of varying length: corpora such as Brown or LOB are one million words in length and contain 2,000-word samples; the London-Lund Corpus is approximately 500,000 words in length and contains 5,000-word samples; and the British National Corpus is 100 million words long and contains samples of varying length. Moreover, often the analyst will wish to compare his or her results with the results of someone else's study, a comparison that is likely to be based on corpora of differing lengths.

To enable comparisons of corpora that differ in length,

The choice of norming to 1,000 words is arbitrary, but as larger numbers and corpora are analyzed, it becomes more advisable to norm to a higher figure (e.g. occurrences per 10,000 words).

Although the percentages in Table

Data that are normally distributed will yield a "bell curve": most cases will be close to the "mean", and the remaining cases will fall off quickly in frequency on either side of the curve. To understand why linguistic data are not normally distributed, it is instructive to examine the occurrence of pseudo-titles in the 40 texts that were examined (cf. Table

As the figures in Table

Because most linguistic data behave the way that the data in Table

In essence, the Chi square test calculates the extent to which the distribution in a given dataset either confirms or disconfirms the "null hypothesis": in this case, whether or not there are differences in the distribution of pseudo-titles and equivalent appositives in the four regional varieties of ICE being compared. To perform this comparison, the Chi square test compares "observed" frequencies in a given dataset with "expected" frequencies (i.e. the frequencies one would expect to find if there were no differences in the distribution of the data). The higher the Chi square value, the more significant the differences are. The application of the Chi square test to the frequencies in Table

With three degrees of freedom, the Chi square value of 65.686 is significant at less than the .000 level.

While it is generally accepted that any level below .05 indicates statistical significance, it is quite common for more stringent significance levels to be employed (e.g. p≦:001). Because the significance level for the data in Table

The Chi square test applied to the data in Table

Significance Level # of Tests Performed Corrected Value Table

The results in Table

While the Chi square statistic is a very useful statistic for evaluating corpus data, it does have its limitations. If the analyst is dealing with fairly small numbers resulting in either empty cells or cells with low frequencies, then the reliability of Chi square is reduced. Table

Three of the cells in the category of "Total Equivalence" contain fewer than five occurrences, making the Chi square statistic invalid for the data in Table 4.7. One way around this problem is to combine variables in a principled manner to increase the frequency for a given cell and thus make the results of the Chi square statistic more valid. One reason for recording the particular correspondence relationship for an appositive was to study the stylistic relationship between pseudo-titles and various types of equivalent appositives: to determine, for instance, whether a newspaper prohibiting pseudo-titles relied more heavily than those newspapers allowing pseudo-titles on appositives related to pseudo-titles by either determiner deletion (e.g. the acting director, Georgette Smith ! acting director Georgette Smith) or total equivalence (Georgette Smith, acting director ! acting director Georgette Smith). Because these two correspondence relationships indicate similar stylistic choices, it is justifiable to combine the results for both choices to increase the frequencies and make the Chi square test for the data more valid. Table

It was expected that ICE-GB would contain more instances of appositives exhibiting either total equivalence or determiner deletion, since in general British newspapers do not favor pseudo-titles and would therefore favor alternative appositive constructions. And indeed, the newspapers in ICE-GB did contain more instances. But the increased frequencies are merely a consequence of the fact that, in general, the newspapers in ICE-GB contained more appositives than the other varieties. Each variety followed a similar trend and contained fewer appositives related by total equivalence or determiner deletion and more related by partial equivalence. These findings call into question

While combining values for variables can increase cell values, often such a strategy does not succeed simply because so few constructions occur in a particular category. In such cases, it is necessary to select a different statistical test to evaluate the results. To record the length of a pseudo-title or appositive, the original coding system had six values: one word in length, two words, three words, four words, five words, and six or more words. It turned out that this coding scheme was far too delicate and made distinctions that simply did not exist in the data: many cells simply had too low a frequency to apply the Chi square test. And combining categories, as is done in Table

In cases like this, it is necessary to apply a different statistical test: the log-likelihood (or G 2 ) test.

The results of the log-likelihood test point to a clear trend in Table

Table

One reason for the general difference in length of appositives and pseudo-titles is that there is a complex interaction between the form of a given pseudo-title or appositive and its length. In other words, three variables are interacting: "type" (pseudo-title or appositive), "form" (simple noun phrase, genitive noun phrase, noun phrase with post-modification), and "length" (1-4 words or 5 words or more). Table

A Chi square analysis of the trends in Table

Because only three variables were being compared, it was decided to use a saturated model to investigate associations. This model generated the following potential associations:  Likelihood ratio and Chi square tests were conducted to determine whether there was a significant association between all three variables (a), and between all possible combinations of two-way interactions (b-d). In addition, the variables were analyzed individually to determine the extent to which they affected the three-and two-way associations in 16a-d. The results are presented in Table

The first line in Table

To determine which of these associations were strongest, a procedure called "backward elimination" was applied to the results. This procedure works in a step-by-step manner, at each stage removing from the analysis an association that is least strong and then testing the remaining associations to see which is strongest. This procedure produced the two associations in Table

Interpreted in conjunction with the frequency distributions in Table

The loglinear analysis applied to the data in Table

Although the GoldVarb program was specifically designed for linguists working with variable rules, most corpus linguists can usefully apply the many differing statistical tests provided by any of the commonly available statistical programs. While the Chi square test is one of the more common tests used with linguistic data, as Oakes' (1998: 1-51) survey of statistical tests for corpus data demonstrates, there are a range of other tests as well. There is also a successor to GoldVarb, Rbrul (www.danielezrajohnson.com/rbrul .html), that has similar capabilities but a more user-friendly interface and that runs more quickly on a computer.

4.6

Multi-dimensional Analysis

In his 1988 book, Variation across Speech and Writing, Douglas Biber introduced the notion of multi-dimensional analysis and how it could be used to identify significant linguistic differences between spoken and written language. He critiques traditional linguistic beliefs concerning the differences between speech and writing, such as the claim "that speech is more elaborated and complex than writing" (p. 5). Instead, he argues that it is more informative to view the differences in terms of a series of dimensions that he proposes. The purpose of these dimensions is to demonstrate empirically that "neither speech nor writing is primary; that they are rather different systems, both deserving careful analysis" (p. 7).

Below is a list of the six dimensions that Biber proposed, dimensions that are based on a "methodology to empirically analyze the ways in which linguistic features co-occur in texts and the ways in which registers vary with respect to those co-occurrence patterns"

To understand how each of the Dimensions work, consider how the registers that are associated with Dimension 1 illustrate the spectrum between those registers that are more highly involved (i.e. interactional) versus those that are more informational (i.e. focused primarily on content):

Registers that are more involved: telephone conversations, face-to-face conversations, personal letters, spontaneous speeches and interviews

Registers that are more informational: Biographies, press reviews, academic prose, press reportage, official documents summation of risks at the individual level as it does with chronic diseases. <#>To relate risk assessments at the individual and population levels, knowledge of contact patterns is essential. <#>The purposes of this paper are 1) to demonstrate the lack of stability of chronic disease risk measures with contagious diseases, 2) to demonstrate how risk assessment for contagious diseases depends upon assessment of contact patterns even when contact patterns do not cause appreciable differences in the overall epidemic pattern, and 3) to present a new formulation for the action of one important determinant of contact patterns in sexually transmitted diseases, namely biased selection of partners from the potential partners encountered. <#>This new formulation supersedes our previous selective mixing formulation (1).</p> (www.ice-corpora.uzh.ch/en/joinice/Teams/iceusa.html)

Before examining the two samples of speech and writing, it is worth noting that while spontaneous conversations share many similarities, there are considerable differences between different types of academic writing. For instance, there are, as

Table

Thus, the use of chronic here is more of a report back to a scientific study on radiation rather than part of, for instance, a casual conversation.

The words in the table from the spoken texts also predominate in speech, but their usages, though less frequent, do occur in the written texts as well. Because the pronoun you is clearly interactional, involving more than one speaker, it overwhelmingly predominates in speech.

Well, first of all, I am a member of the Democratic leadership. I've been in the Democratic caucus. But this is what I will also say, if you look at polling in this country, what you find is that a whole lot of people are dissatisfied with both the Democratic and Republican parties.

However, you can also occur in written texts. In the example below, you is used because the two sentences occur in an instructional context. Thus, you is used in two imperative sentences.

Write your sentences as quickly and as clearly as possible. Make sure you complete four sentences.

Before a multi-dimensional analysis can be performed, a corpus needs to be both lexically tagged and then analyzed by a particular statistical method termed factor analysis. As was noted in Chapter 3, a corpus that is lexically tagged contains part-of-speech information for each word in the corpus. For instance, in the very simple sentence I like pizza, the pronoun I would be tagged as a first person pronoun; the verb like would be as a present tense lexical verb; and the noun pizza as a singular common noun. Of course, different tagging programs will have different terminology and provide varying levels of detail about the items being tagged. Lexical tagging is crucial because it will enable the factor analysis program to determine where the various parts of speech occur: e.g. first person pronouns in more interactive texts; passive verbs in more informational texts.

A tagging program used to conduct multi-dimensional analyses is the Biber tagger

Lexically tagged texts serve as input for a multi-dimensional analysis, an analysis that Biber describes as: a methodology to empirically analyze the ways in which linguistic features co-occur in texts in the ways in which registers vary with respect to those cooccurrence patterns

To discover these patterns, it is necessary to subject the data to a factor analysis, a statistical program that is able to isolate both positive and negative correlations between variables

Note, for instance, how first person pronouns correlate positively with questions, but negatively with passives and have a very weak positive correlation with nominalizations. Questions, in turn, correlate negatively with passives and nominalizations. These correlations match the distributions of these constructions in the registers of conversation and academic writing discussed earlier in this section.

Since the publication of

Conclusions

As

In the pre-electronic era, textual analysis was largely a matter of analyzing 'static' texts: written texts existing only in printed form that had to be analyzed by hand.

He comments that the major earlier grammarians from this era, such as Otto Jespersen and Hendrik Poutsma, based their grammars on "primarily canonical written texts (e.g. novels)," from which they manually extracted relevant examples to illustrate the points of grammar that they discussed. Since this era, advances in corpus linguistics have resulted in the creation of many different types of corpora containing authentic texts ranging from spontaneous conversations to technical writing in the social and natural sciences. Because these texts are in an electronic format, they can be searched with software such as concordancing programs, and relevant linguistic constructions can be instantly retrieved. While it is certainly not the case that any particular linguistic item can be automatically retrieved instantlymany linguistic constructions are simply too complex for this type of "instant" retrievalnevertheless, the process of corpus analysis has been greatly automated.

Concluding Remarks

The final chapter of the 1st edition of this book (hereafter ECL1), "Future Prospects in Corpus Linguistics," began with the following paragraph:

In describing the complexity of creating a corpus,

In the context of current work done in corpus linguistics, the points made in the previous paragraph raise some interesting issues:

(1) Planning a Corpus: In ECL1, it is noted that "as more and more corpora have been created, we have gained considerable knowledge of how to construct a corpus that is balanced and representative and that will yield reliable grammatical information" (138). It is certainly the case that many recently created corpora are balanced and representative. Even some mega corpora, such as the one billion-word Corpus of Contemporary American English (COCA), contain various registers, such as fiction, speech, press reportage, and academic writing. But there are many challenges involved in creating "small and beautiful corpora," such as the British National Corpus (BNC) and the International Corpus of English (ICE). The situation is certainly understandable. It requires considerable resources to create balanced corpora, such as the BNC, whereas COCA contains texts downloaded from websites, requiring much less effort than building a corpus such as the BNC.

(2) Collecting and Computerizing Data: In ECL1, it is noted that "because so many written texts are now available in computerized formats in easily accessible media. . . The collection and computerization of written texts has become much easier than in the past" (139). A similar situation exists in the present, and because the Web has grown even larger in recent years, plenty of texts can be downloaded. The situation with spoken texts was quite different: they had to be transcribed manually, a very timeconsuming endeavor. In the present, there are transcriptions of different types of public speech, such as press conferences or talk shows, that can be downloaded and that are reasonably accurate. In fact, COCA contains quite a bit of public speech.

(3) Annotating a Corpus: Corpus annotation has changed considerably since ECL1. For instance, the opening of the section in ECL1 on "Structural Markup" makes reference to ASCII (American Standard Code for Information Interchange) text formats and standard generalized markup language (SGML). ASCII has since been replaced by Unicode, which has far more characters.

(4) Tagging and Parsing a Corpus: Lexically tagging a corpus has become quite routine, particularly because the accuracy of current tagging programs is quite high. Parsing a corpus is a more complicated undertaking, since larger structures, such as phrases and clauses, must be identified. Consequently, more post-editing has to be done after a corpus has been parsed.

(5) Analyzing a Corpus: Great strides have been made in developing software that can enhance the analysis of corpora. For instance, there are concordancing programs that can be used to identify and retrieve various grammatical constructions in a corpus, such as particular words or phrases. And if a corpus has been lexically tagged, concordancing programs can retrieve various grammatical structures: lexical items, such as nouns or verbs; phrases, such as noun phrases or verb phrases; and clauses, such as relative clauses or adverbial clauses.

In the future, we can expect further advances to enhance the creation and analysis of linguistic corpora. In particular, we can expect an increase in accuracy for certain corpus tools, such as improvement in the accuracy of software used to parse and lexically tag corpora, and the increased accuracy of tools, such as concordancing programs, used to retrieve constructions from corpora.

Discussion Topics

Below are a series of questions that explore the topics discussed in the first four chapters of the book. The questions go beyond fill in the blank-or short essay-types of responses and require respondents to think more deeply and critically in their answers. Consequently, there are no correct or incorrect answers.

Chapter 1

1. In the opening section of the chapter, a distinction is made between the "armchair linguist," whose sources of data are essentially sentences that he/she makes up, and the "corpus linguist," who believes that it is better to draw upon authentic data as the basis of linguistic analyses. In the book, it is noted that these two approaches are not necessarily mutually exclusive: the generative linguist, for instance, could very well work with examples that are corpus-based rather than examples that are made up. But is this true? Couldn't the generative linguist very well work with only made-up sentences, especially since his/her goal is to theorize about language, and spending additional time finding relevant data in a corpus is simply unnecessary? 2. What is the difference between "corpus" and "experimental" data?

In the chapter, it is stated that these two different kinds of data "complement" each other. However, couldn't it also be argued that the two types of data are so different that they are incompatible? Drawing upon evidence in Section 1.3 of the chapter, argue that the two types of data are either compatible or incompatible. 3. Section 1.4 contains a discussion of how the corpus methodology has many different applications. For instance, it can be used to study language variation or to help in the creation of dictionaries. Select one of the areas described in this section and briefly discuss how corpus methodology has changed the way that research is done in the area. You may want to focus your response on areas that are discussed in greater detail in Section 1.4 than some of the other areas. 4. Section 1.6 contains a discussion of how the theory of construction grammar can shed light on the "problematic" nature of apposition in English. Do you agree with this claim? What evidence exists to support the claim?

Chapter 2

1. The chapter opens with a discussion of three different types of corpora: the British National Corpus (BNC), the Corpus of Contemporary American English (COCA), and the Corpus of Early English Correspondence (CEEC). Below are some questions to consider: a. The BNC was released in 1994. Is it too out of date to be of use anymore? If not, what value might it have? For instance, how could it be compared with the BNC2014, a more modern corpus that is directly modeled after the BNC? Think of additional corpora it could be compared to. b. The COCA is a web-based corpus. It can be analyzed online.

What advantages does it have being online rather than locally on a personal computer? c. The CEEC is actually a group of corpora containing letters written by individuals during various time periods, ranging from 1410 to 1663. The corpora were designed so that a range of sociolinguistic variables, such as age, social class, and gender, could be studied. Because these are historical corpora, what kinds of sociolinguistic variables would be especially worth studying? One point that is made in the chapters is that fewer letters written by women are in the corpus than letters written by men. 2. The idea of the Web as a corpus was initially a controversial notion, largely because at the time, more traditional corpora were the norm, and analyzing the Web, with its seemingly endless size and unknown content, contradicted the whole idea of what a corpus was thought to be. In the chapter,

How would you construct the corpus? For instance, would you want the corpus to contain a specific section of a newspaper (e.g. editorials). Or would you want more broad representation? Would you want complete texts, or only text excerpts? how would you ensure that the corpus is balanced? Consider other variables discussed in the chapter in your response.

Chapter 3

1. As is noted in the chapter, collecting and transcribing spoken language is one of the more labor-intensive parts of creating a corpus. But including spoken language in a corpus is very important because spoken language is the essence of human language, particularly spontaneous conversation. There are ways to get transcriptions of spontaneous speech, but mainly from conversations broadcast on radio or television, a highly restricted type of language. If you were considering creating a corpus of spontaneous conversations, how would you go about recording and transcribing them? Draw upon information in the chapter to write your response. 2. Let's say you want to create a corpus of newspaper editorials. How would you create a balanced corpus of them? For instance, how would you achieve gender balance? Would it be necessary to control for the types of newspapers from which you collect texts (e.g. broadsheets vs. tabloids)? What about other variables such as age and ethnicity? What are some variables whereby it would be too difficult to obtain information? 3. Go to the following website, which contains a much fuller description of the International Corpus of English (ICE) than the chapter does: www.ice-corpora.uzh.ch/en/design.html. Would you consider the ICE corpora balanced corpora? Why or why not? 4. What is the difference between metadata and textual markup?

Why is it important that a standard system, such as the Text Encoding Initiative, is developed to ensure that everyone follows a standardized system of annotation for representing metadata and textual annotation?

Chapter 4

1. Section 4.5 discusses how corpus analyses can be quantitative, qualitative, or a combination of the two (sometimes termed mixed methods). What's the difference between the three types of analysis? Is any one type of analysis better than another? 2. Conducting a corpus analysis is a multi-stage process, involving framing a research question, finding a suitable corpus or corpora for the analysis, extracting relevant information from the corpus or corpora chosen for the analysis, and integrating information from relevant articles or books into the analysis. Why are all these stages necessary? What would happen if, for instance, you did your analyses but did not integrate relevant research into the writeup (book, article, presentation) of your results? 3. Go to www.english-corpora.org/ and select a corpus from the list of corpora on the page. After you have selected a corpus, you will need to create an account to use the corpus. Once you have created an account, you should replicate some of the searches in Section 4.3 of the chapter but try your own searches too. You could also try BNCweb (

BYU Corpora: a set of many different corpora taken from various online resources (www.english-corpora.org/): see also entry under C for the Corpus of Contemporary American English.

C

CallHome Corpus: 120 spontaneous 30-minute phone conversations between intimates, either friends or members of the same family. (

Child Language Data Exchange System, or CHILDES Corpus: includes transcriptions of children engaging in spontaneous conversations in English and other languages. (

Collins Corpus: a 4.5 billion-word monitor corpus used as the basis for creating the COBUILD dictionaries. (

Corpus of Age and Gender (see Murphy 2010): an internal corpus not publicly available. (

Corpus of Contemporary American English (COCA): a one billion-word corpus containing various registers of American English that can be searched online. (

Corpus of Early English Correspondence: consists of a collection of corpora containing various types of correspondence written in the fifteenth to seventeenth centuries. (www2.helsinki.fi/en/researchgroups/ varieng/corpus-of-early-english-correspondence)

Corpus of Global Web-Based English (GloWbE): a 1.9 billionword corpus containing samples of English from 20 different countries in which English is used. (

Corpus of Middle English Prose and Verse: contains the works of Chaucer and other Middle English writers. (www.hti.umich.edu/eng lish/mideng/) D Diachronic Corpus of Present-Day Spoken English (DCPSE): 400,000 words of spoken English from ICE-GB and an additional 400,000 spoken words from the London-Lund Corpus. (www.ucl.ac.uk/english-usage/projects/dcpse/index.htm) Dictionary of Old English Corpus: a three million-word corpus containing all surviving Old English texts. (www.sheffield.ac.uk/library/ cdfiles/doec#:~:text=The%20Dictionary%20of%20Old%20English,the %20collected%20works%20of%20Shakespeare) ICLE Corpus (The International Corpus of Learner English): contains samples of written English produced by advanced learners of English as a foreign language from 25 different language backgrounds. (

E

Y

York English Corpus: a 1.5 million-word corpus that has been subjected to extensive analysis and that has yielded valuable information on dialect patterns (both social and regional) particular to this region of England. (www.researchgate.net/figure/Sample-designof-the-York-English-Corpus_tbl1_227609144)