This often involves careful reading through the corpus manual to familiarize oneself with the corpus composition, inspecting the concordance lines to see the actual examples of language use behind the numbers we have obtained and producing overviews and simple graphs that reveal the main tendencies in the dataset.
One might argue that this second step can be seen as corpus-based because it identifies features in the corpus that merit further attention.
Because many of the built-in editors, such as Windows Notepad or TextEdit on the Mac, are either not powerful enough (the former), or first need to be configured in special ways to handle plain text by default (the latter), I will make some recommendations for editors I consider suitable for corpus processing for Windows, Mac OS X, and Linux below, and also try to explain some of their advantages for basic corpus processing.
We also showed how to measure the lexical richness of a corpus using the type/token ratio, and presented the limitations of such a measurement.
On a very general level, the frequency information a corpus offers is exploited in four different ways, which will be the subject of this chapter: frequency lists (Section 2.2), dispersion (Section 2.3), lexical co-occurrence lists/collocations (Section 2.4), and concordances (Section 2.5).
We add a numeric second-level fixed effect which specifies the token frequency for each level of LEMMA in the following R code.
For example, if we want to decide whether learners are fluent in French idioms such as "mettre le feu aux poudres" (to stir up a hornet's nest) or "avoir un poil dans la main" (to be extremely lazy) through a corpus study, we will have to look for them in a corpus of learners' productions.
After the loop, we will again use exact.matches.2 to retrieve the tags from the matches, we will use gsub to retrieve the forms only (i.e., discard all annotation), and we will use substr to define the lemma of each match (by retrieving the first letter, which gives us enough information to distinguish run and walk) and using ifelse to define a lemma column).
No matter the way of acquiring data, an important point is to save the corpus files into a format which can then be used by a concordancer.
Following the online tutorial on YouTube referenced above, identify the press reports sub-corpus in both the American and the British collections (AmE06_PressRep and BE-06_PressRep) and feed them into the tool.
This corpus type might be helpful in foreign language learning studies and language pedagogy.
This type of "literature review" is common in the introductory sections of research articles, and the effects of corpus use have been the object of several extensive narrative syntheses (e.g.
In general, a corpus should not be used for establishing a contrast between elements which have not been balanced during the corpus compilation phase.
Finally, a case study was presented to demonstrate that corpus analyses and various linguistic theories go hand in hand, and that such studies can do more than simply provide examples of constructions and document their frequency of occurrence.
However, it is important not to become too rigidly invested in the initial corpus design, since obstacles and complications may be encountered while collecting data that may require changes in the initial corpus design: It might not be possible, for instance, to obtain recordings for all the genres originally planned for inclusion in the corpus, or copyright restrictions might make it difficult to obtain certain kinds of writing.
It should be obvious that we can calculate the mean length of words or other constituents in a corpus, a particular sample, a particular position in a grammatical construction, etc.
A corpus of general English, for instance, must sample both spoken and written data (of various kinds) in order to be representative; however, what should the relative proportions of speech versus writing be?
She tagged the texts with a grammatical tagger, counted the appropriate part of speech tags (see Chapter 9 about tagging), and normed the feature counts to 1,000 words each.
Concordance lines and short language samples (e.g., fewer than 25 words) are preferable over larger stretches of text.
In the next major section (Chapters 5-10), we then investigated various techniques for analysing language data using established methods of corpus linguistics.
We have already discussed some of the challenges inherent in the creation of large corpora, in terms of accurate metadata and word-level annotation.
The chapter opens with a discussion of how to define a corpus, and then traces the history of corpus linguistics, noting, for instance, that corpus-based research began as early as the fifteenth century, when biblical concordances were created based on passages from the Bible.
It should be a synchronic corpus, corresponding to current uses of the language.
This implic-itly excludes a range of language technologies such as information retrieval, document classification , data mining, and speech processing, as well as areas of artificial intelligence like natural language generation / understanding and machine learning.
Each text that is part of the corpus will also have metadata such as the date and place it was recorded, who spoke or signed, who else was present, and what media files are associated with the text.
Keywords are calculated by assessing all the word frequencies in each of the two (sub)corpora and doing either chi-squared tests or log-likelihood measurements to assess what words are statistically more frequent in one (sub)corpus than in another.
Because they are based on analysis of data abstracted from language use, all three are naturally suited to quantitative and more specifically statistical methods, and, as corpus linguistics has developed in recent years, the corresponding research communities have increasingly adopted it, albeit unevenly.
Each corpus and software program has its own idiosyncrasies and we have found that these different corpora and software programs are sometimes confusing to students who do not have access to the corpora and/ or struggle to learn one program or search interface in a corpus and then have to learn another.
Another criticism of academic corpus studies is that, until fairly recently, these have been largely text-focused so that features seem rather abstract and disembodied from real users.
For example, in CQPweb, it is possible to categorise examples according to an annotation scheme and then to use the scheme to explore the data.
The second one is not followed by a space because its characters were the last ones in the vector tag text and, more often, they might not be followed by a space because they are immediately followed by a punctuation mark.
The next script does something seemingly elementary -we will create a frequency list of word-tag combinations (so as to be able to distinguish run as a noun from run as a verb) -but we will do it on a relatively large data set, the complete BNC World Edition with XML annotation, and we will use the annotation well by utilizing information provided by the BNC's multi-word tags, i.e., tags that mark multi-word units such as because of, in spite of, on behalf of, etc.
This type of information is called corpus metadata.
Although corpus-based studies of language structure can look back on a tradition of at least a hundred years, there is no general agreement as to what exactly constitutes corpus linguistics.
Corpus linguistics plays a role among academic disciplines.
Capturing L2 accuracy developmental patterns: Insights from an error-tagged EFL learner corpus.
In particular, it needs to be specified what distinguishes an organization from other groups of human beings (that are to be categorized as human according to the annotation scheme).
That is, to what degree are corpus-based frequency lists generalizable to target discourse domains?
Markup is information added to a corpus to represent features of the original texts other than the running words themselves -for instance, in the case of a written text, features such as the beginning and end points of sentences or paragraphs, or the position of page breaks, or the position and content of elements such as illustrations which would be omitted in a plain text corpus.
This knowledge is more abstract in nature, and it is partly a reflection of the more abstract kinds of research questions explored in corpus-based research today, as well as of a shifting focus towards an increasingly statistical orientation in research design.
Other corpora such as the new version of the Frantext literary text corpus (see section 5.2) are accessible via an annual renewable subscription.
Corpus is the main data that a corpus linguist (or a researcher aiming to explore the use of language) needs to investigate a specific area of a particular language(s).
We will then provide more details on exactly which type of information is presented in the corpus-informed books, but also how is it presented.
The impossibility of this task is widely acknowledged in corpus linguistics.
The transcription process itself is very time-consuming and its complexity depends on the exact type of annotation that is added to the data (prosodic contours, etc.).
Borrowing terms from statistics, we can talk about the whole of the language variety we are approaching as the population, and the corpus we are using to look at that language variety as the sample.
We will also recommend a software tool, AntConc, to carry out a keyword analysis (further details on the software is in Chapter 5).
As mentioned above, concordance lines highlight the word you pick and provide additional text around it.
A related question is the variable realisation of objects as either pronoun or zero, and in 8.4 we have presented such a corpus-based study for Vera' a in some detail.
Corpus linguists' concern with variation is key to this programme, labelled language variation and change within variationist sociolinguistics after William Labov, since variation is a necessary condition of change.
Like boilerplate, such documents can skew word frequency counts so it is desirable to remove them from the corpus.
These matters include attempting to achieve comparability and representativeness, two important but sometimes mutually exclusive goals.
Future advances in historical corpus linguistics are likely to have theoretical as well as practical effects on how scholars use registers.
Given that historical data are not collected in the carefully controlled ways in which psycholinguists (try to) collect language acquisition corpus data, such historical data are often quite heterogeneous so that here, too, it is useful to be able to group temporal data and at the same time clean the data of outliers in a principled fashion.
Therefore, we can furnish a general corpus with standard text samples of contemporary writings and a calculated and proportional representation of texts can suffice our requirement.
It is not suitable as a final characterization of corpus linguistics yet, as the phrase "distribution of linguistic phenomena" is still somewhat vague.
However, this balance needs to be redressed and projects such as the Role Play Learner Corpus and the Louvain International Database of Spoken English Interlanguage are particularly welcome.
Callies concludes his chapter by addressing potential bias in annotation methods within Learner Corpus Research (LCR) and related disciplines.
The term corpus (plural corpora) is simply the normal word in Latin for 'body', as in a body of text.
Our aim in carrying out the case study was not to come up with the conclusion that all grammar textbooks should include corpus-based information on all grammar points.
This is the case of the EMA écrits scolaires corpus (Boré and Elalouf 2017), a longitudinal corpus bringing together the text productions of primary and middle school children.
In order to distinguish corpus linguistics proper from other observational methods in linguistics, we must first refine this definition of a linguistic corpus; this will be our concern in Section 2.1.
Third, in the process of corpus construction we have to think about the relationship between the population and the sample in a somewhat different way to how this is done in fields where sampling can be done on a random basis.
It is also argued that the observed tendencies -such as short-before-long and animate referents firstare in line with synchronic corpus-based and experimental findings about general cognitive principles underlying the framework of probabilistic grammar.
Specifically, I shall discuss potentially problematic choices or omissions in the area of general corpus statistics, in particular the choice of association measures for co-occurrence data, that is, measures with which corpus linguists quantify the degree of association between two linguistic expressions (e.g.
He specializes in corpus linguistics, statistics and applied linguistics, and has designed a number of different tools for corpus analysis.
However, given the advent of technology and corpus linguistics, it is now possible to study and analyse these patterns of usage.
The question of 'what is language like' is one that Corpus Linguistics seeks to answer.
In order to encourage and enable the application of meta-analysis in corpus linguistics, we have also introduced and demonstrated the main procedures involved including how to (1) search for primary studies, (2) develop and implement a coding scheme, (3) aggregate effect sizes using different R packages to obtain estimates of the relationship in question as well as estimates of variance and of different moderator effects.
The CLAWS tagger, for example, assigns a hyphenated POS tag, referred to as an "ambiguity tag", when its tagging algorithm is unable to unambiguously assign a single POS to a word.
Section 7.3 is devoted to comparative corpus annotations in the context of corpus-based typology.
The development of new techniques of corpus analysis is often spearheaded by research on contemporary performance data.
Moreover, if corpus pragmatics is concerned with the interpretation of meaning in context, another disadvantage associated with the relationship between corpus linguistics and pragmatics is that many larger corpora are impoverished both textually and contextually (Ru ¨hlemann 2010).
By resorting to many different speakers and including them in reference corpora of speakers coming from different geographical areas, corpus linguistics makes it possible to respond to this problem in a much more satisfactory way.
In contrast to linguistic knowledge, language use is directly observable and recordable, and the texts in a corpus essentially comprise such records of language use.
Second, when we analyse a corpus we must always be aware of dispersionlooking out for the difference, for instance, between a word that occurs 100 times in our corpus where all 100 instances are bunched close together in one text, and a word with the same 100 occurrences that are spread out across the whole corpus.
For example, to be able to compare the language of women and that of men, this variable must be clearly identifiable in the metadata of a corpus.
Natural language processing pipelines provide one means of applying separate annotation tasks to texts in user-defined sequences (e.g., applying tokenization, then part-of-speech tagging, then syntactic parsing to unannotated materials), which can greatly assist in constructing annotated corpora of all sizes.
For example, raw data can be presented in brackets in a table next to the relative frequencies, as shown in In the text of an article, the two frequency elements can be reported as follows: "There were 94 occurrences of parce que in the corpus, corresponding to a relative frequency of 3.61 occurrences every 10,000 words".
English for Specific Purposes and TESOL Quarterly, founded in 1980 and 1967, respectively, appeared after the turn of the century and have been consistently cited by the corpus linguistics papers.
Lemmatization can be an extremely useful tool in the corpus builder's toolkit, especially when searches of the annotated corpus may need to be run over many inflected forms.
As we described in the previous chapter, after logging into the latest version of COCA (currently 2023), you have different choices, namely, choose "list", which will provide you with concordance lines, "chart" which will provide a chart of the frequency of your selected keyword in different registers, or "word" which will give you a plethora of information about the word you are searching (e.g., collocations, synonyms, clusters, related words, and much more).
In corpus linguistics (and the social sciences more generally), hypotheses of this type are the exception rather than the norm -we are more likely to deal with statements about tendencies (think Most swans are white or Most examples of windscreen are British English), where the search for counterexamples is not a viable research strategy.
Administrative metadata provides documentary information about the corpus itself, such as its title, availability, revision status, etc.
Corpus research has broadly supported the view that the schemata of L2 and L1 writers differ and influence how they write in English (e.g.
The task of sampling of texts has to be done with collected text materials based on the character of a corpus.
As such, the following review sets a tractable limit on the literature it surveys, and that limit is the literature specific to corpus linguistics.
For example, the corpus compiler may include information based on interviews with participants or participant observation.
However, one cannot use a simple frequency list of an English engineering corpus, because its most frequent words would still be the, of, in, .
Finally, it should be noted that in addition to the effect size measure we should also compute the confidence intervals (95% CIs) for effect size to be able to estimate the range within which the effect is likely to occur in the population (language use in general).
Corpus-based contrastive linguistics was first pioneered by Stig Johansson in the 1990s and has been thriving ever since.
Consequently, much of what could potentially be annotated in a written text is likely to be of marginal interest to future users of the corpus.
Apart from re-using and enriching existing spoken corpora, future corpus-based research might also increasingly make use of combining existing corpora for U.
The tagged version of the BROWN corpus does not contain quotation marks because they have intentionally been stripped from the text.
But they are probably one reason why so much grammatical research in corpus linguistics takes a word-centered approach.
This means that the difference observed in the corpus (sample) is likely to be a true difference in the population (all language use).
Specifically, I will make a case for what I call Open Corpus Linguistics.
Thus, while in principle learner corpus data can be collected from learners at all proficiency levels, most corpora to date represent the more advanced stages.
If anything, we must thus replace the reference to corpus analysis software by a reference to what that software typically does.
Given especially the predominance of work on English in corpus linguistics, until rather recently many corpora came in the so-called ASCII (American Standard Code for Information Interchange) character encoding, an encoding scheme that encodes 2 7 = 128 characters as numbers and that is largely based on the Western alphabet.
However, some learner corpus researchers have adopted a more explanatory research design, which uses learner corpora to revisit important SLA findings.
To summarize, the pitfall related to POS annotation in the case of category change is that even though word classes are treated as static entities in corpus annotation, they are in fact dynamic; word classes exhibit both category-internal and category-external gradience, and this gradience may be a result of ongoing language change.
However, corpus analysis has revealed that the predominant collocations are rather neutral, as in jeune mec (a young guy) or positive, as in mec bien (a good guy), beau mec (a handsome guy).
The goal of language documentation is to create a lasting record of languages' use, which crucially includes a corpus of transcribed and translated records gathered during fieldwork (cf.
Again, metonymy is a vastly under-researched area in corpus linguistics, so much work remains to be done.
For the time being, following the ideals of Open Corpus Linguistics might in some cases require entering legal grey zones.
Accordingly, corpus linguistics is bound to be, and has been, used as one methodological approach amongst many in studies which use mixed methods and orient to triangulation.
These are sometimes good reasons for eliciting a special-purpose corpus rather than collecting naturally occurring text.
That meticulous counting method resulted in what was probably a more accurate representation of the nature of the lexis in the corpus from which the 1953 GSL was derived, with counts that reflected separate lexemes, including multi-word expressions.
There are three types of additional information that can be added to a corpus, namely metadata, markup and annotation.
After defining a variable (or set of variables) and deciding on the type and number of values, the second step in creating an annotation scheme consists in defining what belongs into each category.
In corpus linguistics, it is also often used to measure the strength of association between phonemes or between a word and a construction it can occur in.
In §3.1, I shall discuss the issue of studying temporally-ordered corpus data in a way that is both bottom-up/exploratory and principled/objective; in §3.2, I shall turn to the field of learner corpus research and the question of how to make the best use of what native and non-native learner corpora have to offer.
Data extraction can also be based on a prior automatic analysis of the chosen corpus, such as lemmatization or part-of-speech tagging (see Chapter 7, section 7.4).
To support detailed quantitative analysis of phenomena in traditional corpus research, data is often marked-up and annotated first, to make specific features searchable using concordancing tools.
In many early corpus linguistics works, you will find frequency tables as a primary account of data.
Corpus research of variant alternation is done at all linguistic levels: semantics, phonetics, phonology, morphology, syntax, and discourse.
The issues of the metadata accompanying learner corpora and the annotation of learner corpora are also discussed, and the challenges they involve are highlighted.
Next, get the token count from the 'Make/edit subcorpora' page and paste it into the spreadsheet, ideally at the top and to the right of the second frequency list, as we may need to shift some items in the list down later to align the data.
The problem of manual tracking and counting of occurrences is all the more acute since corpus linguistics is often based on large amounts of data which have not been drawn from a single book, in view of observing the multiple occurrences of a certain linguistic phenomenon and thus apprehending its specificities.
These measures play a role in situations where a negative outcome of a test is relevant (for example, with medical diagnoses); in corpus linguistics, this is generally not the case.
Aatu Liimatta's chapter (Chapter 7) focusses on the challenges posed by variation in text length and the specific issue of short texts, an issue which so far has not received much attention from quantitative corpus linguists.
So, to begin with, it is necessary to convert the files included in the corpus to text format.
There are, however, also many other applications of corpus linguistics I've been unable to cover, but which may be of interest to you now that you've mastered the basics, and which I'd like to mention here briefly.
However, corpus-based register studies could have far greater impact than they currently do.
A text corpus is not the linguist's data -measurements of such things as lexical frequency are.
If they are provided with some kind of language-neutral annotation (for parts of speech, syntax, etc.
At first, this will seem like a somewhat abrupt shift from the topics and phenomena we have discussed so far -it may not even be immediately obvious how they fit into the definition of corpus linguistics as "the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus", which was presented at the end of Chapter 2.
Corpus linguistics as a discipline has matured in parallel with the development of more powerful computers and software tools.
The main reason is that I have found, in my many years of teaching corpus linguistics, that most available textbooks are either too general or too specific.
We then passed from the concordance to close reading of the co-text around occurrences of Mubarak, a very common process in corpus-assisted discourse studies.
However, the method of collecting citations cannot be regarded as a scientific method except for the purpose of proving the existence of a phenomenon, and hence does not constitute corpus linguistics proper.
Corpus-based research has also made it possible to show that certain derivations deemed impossible from the point of view of theoretical morphology did nonetheless exist.
This development is in tandem with the move toward XML corpus annotation and, more generally, UTF-8 becoming the most widely used character encoding on the internet.
The implication is again that responsibility for interpreting the structure of a corpus moves from the corpus compiler to the researcher.
In the sample sentence, in winter is "thematised," and Hoey shows that this phrase when occurring in Theme position collocates in his corpus with the verb be, and with the names of places, again proportionally more frequently than the alternative phrases.
Journal of English for Academic Purposes, along with English for Specific Purposes and Cognitive Linguistics, is another example of a specialized journal actively being cited by corpus linguistics papers.
The review in this section demonstrates that corpus linguistics has enabled large-scale collocation analysis and foregrounded collocation in linguistic research while corpus-based collocation studies over the past decades have uncovered a range of interesting collocational behavior and semantic prosody which have been hidden from intuition and can only be revealed by examining a large amount of attested data simultaneously.
In order to develop this understanding thoroughly, as so often in corpus linguistics we need to look at this task from at least two different angles, a theoretical and a practical one.
Whatever the case, we need an annotation scheme -an explicit statement of the operational definitions applied.
As opposed to earlier LCR studies that did not include any statistics, most current studies now follow the general trend in corpus linguistics by providing some sort of statistical analysis.
The Santa Barbara Corpus of Spoken American English, for instance, contains not only an exact transcription of the text of a spoken conversation (including hesitations, repetitions, partially uttered words, and so forth) but also annotation marking various features of intonation in the text, such as tone unit boundaries, pauses, and pitch contours.
For some text types, however, matters are quite obvious: for instance, the relatively small amount of spoken texts even in large or super-large corpora of well-studied languages like English is quite clearly at odds with the reality of language use, and more recent corpus building projects have been aiming at including greater proportions of spoken texts, for example, the International Corpus of English (ICE) where 60% of texts are spoken.
Before starting the annotation process itself, we first have to define the categories which will be annotated in the corpus.
This correlation is not perfect, as common nouns can also encode old information, so using Part of Speech of Nominal Expression as an operational definition for Givenness is somewhat crude in terms of validity, but the advantage is that it yields a highly reliable, easy-to-annotate definition: we can use the part-of-speech tagging to annotate our sample automatically.
Nonetheless, on a qualitative level, the corpus research and concomitant issues taken on by Thorndike and colleagues over a century ago remain largely the same.
With its 24.4 million words it is a sizeable corpus, but what really sets it apart is the kind of text material it contains, with published transcripts of the spoken interactions in court.
Type/token ratio (TTR) expresses the proportion of types (different word forms) relative to the proportion of tokens (running words).
For better-studied languages, we will often have at least some common-knowledge idea of attested text varieties, but corpus compilers will also need to draw on relevant findings from studies of text varieties (e.g.
While it is true that corpus linguistics has imposed itself in an incontestable manner in the English-speaking world and that a significant proportion of French-speaking researchers currently use these methods, the teaching of corpus linguistics still remains marginalized in France.
His focus is on the second hypothesis, which he tests on the LOB corpus by, first, identifying all occurrences of both verbs with both complementation patterns and, second, categorizing them according to whether the verb in the complement clause refers to an activity, a process or a state.
A special computer program is often designed to process the text and to look at sequences of words in a window of text as they emerge in a corpus.
The development of corpus linguistics has led to the creation of new methods for collecting and analyzing linguistic data, which were made possible thanks to the development of computers and the arrival of the Internet.
As demonstrated by the studies reviewed in this chapter, the corpus-based approach to dialectology is a growing field of inquiry that allows for new types of research questions to be pursued and new types of dialect variation to be identified.
This chapter highlights genre categorizations as a pitfall at the intersection of corpus linguistics and literature and problematizes the use of the genre category tag from the perspectives afforded by both fields.
This chapter is mainly concerned with the design and analysis of parallel corpora in the two fields of corpus-based contrastive linguistics and corpus-based translation studies.
Corpora of 100 applied linguistics articles produced lists with 80.8% overlap, whereas the same corpus design applied to environmental science produced lists with only 75.8% overlap.
Stepping from the articles presented at the 4th International Conference that was conducted to convey many topics in the domain of Corpus Linguistics (CILC2012, Jaén, Spain).
In this chapter, to avoid confusion, we refer to a concordance as the list of citations, distinguishing it from a concordance line, which is a single citation.
In order to be able to draw quantitative conclusions, it is necessary to carry out inferential statistical tests, which will indicate whether the results obtained on the corpus sample can be generalized to the entire population.
Each of the topics and subtopics described above address issues that are not specific to the field of corpus linguistics but lend themselves to corpus research quite well.
It is this regression approach that precisely answers the core question of learner corpus research-in this linguistically and maybe contextually complex situation where the NNS had to make a choice, did he make a nativelike choice, 'Yes or no?'.
Once you're happy with the results of your frequency list, no matter which output format you've chosen, you can save the list to a text file again, just like you were able to do with the concordance output.
While the first corpora were compiled in the 1960s, it took some 30 years before the first learner corpora started to be collected, both in the academic world (International Corpus of Learner English (ICLE)) and in the publishing world (Longman Learners' Corpus).
Each of the following sections is intended to illustrate one general problem associated with historical corpus linguistics by discussing individual studies that we have carried out previously.
Unfortunately, though, as useful as AntConc is for most purposes, since it's a stream-based concordancer it'll ignore all line breaks and match more than we want to see in our concordance, so using it here is not an option.
A balanced news writing corpus would either include texts of sports, lifestyle, and general news texts or would select only one of these text types for analysis.
It is a well-known fact in corpus linguistics that words occur in combinations that we call collocations.
Somewhat polemically speaking, being able to enter a URL and type in a search word shouldn't make you a corpus linguist.
Most of the methodological thinking (not only in corpus linguistics but other disciplines as well) has been biased towards looking for differences and ignoring similarities.
A final issue has received very little attention in corpus-driven studies of phraseology: the extent to which specific lists of lexical phrases are reliable (i.e.
It, therefore, taken by corpus linguistics, position and that of the anti-positivists.
Even though the authors allow researchers to use the text in their corpus, some of the owners of the writing texts may not allow researchers to share the corpus with other researchers worldwide.
It includes key terms with examples from corpus research and is ordered from basic concepts to more complex ones which rely on the understanding of the previous terminology.
Likewise, revisions that have been made at different stages of the annotation process must be documented, as well as successive versions of the corpus that have been produced, if applicable.
This is useful, for example, when a classification has been made on a scale of very negative, negative, neutral, positive, and very positive type values, but one may want to combine the results to return the corpus grouped into positive, neutral, and negative.
In the long run, linguistic theorising without a firm grip on statistical methods will lead corpus linguistics into a dead-end street.
A software program, SARA (SGML-Aware Retrieval Application), was designed to read the headers and do various analyses of the corpus based on a prespecified selection of sociolinguistic variables.
The first column has numerical identifiers of each corpus word form.
A final challenge for current multimodal corpus research relates to more recent discussions and developments in this field.
Let us look at one more example of the type/token distinction before we move on.
In some sense at least, this book is an introduction to corpus linguistics.
We hope this textbook provides a good first foundation for corpus linguistics and generates your excitement about this large and diverse field.
On the one hand, the linguist must accept that while sociocultural contextualization remains as important as ever, it is not always possible to check every corpus text or concordance line manually.
For instance, semantic tagging involves annotating a corpus with markup that specifies various features of meaning.
In other words, the corpus-linguistic identification of keywords is analogous to the identification of differential collocates, except that it analyses the association of a word W to a particular text (or collection of texts) T in comparison to the language as a whole (as represented by the reference corpus, which is typically a large, balanced corpus).
We address several applications of bootstrapping, including the measurement of sample estimate accuracy, the validation of statistical models, the estimation of corpus homogeneity, and random forests.
The annotation scheme defines an organization as a referent involving "more than one human" with "some degree of group identity".
Along the same lines, the next example does something similar but does not retrieve data values, but values of an attribute, namely lemma annotation.
Work in Corpus Linguistics has grown exponentially over the last three decades, and the quantitative tools it routinely uses have become more sophisticated.
In the case of windscreen and windshield, we actually find counterexamples once we increase the sample size sufficiently, but there is still an overwhelming number of cases that follow our predictions.
Alternatively, we may apply selective sampling methods that are used in the Brown Corpus and the LOB corpus or consider more suitable methods of text representation keeping in mind the goal and purpose of a corpus.
However, methods of identifying word co-occurrence provide a way of organising a corpus to lead to new insights.
The primary goal of research in corpus linguistics is to be able to draw conclusions in relation to a linguistic phenomenon, not only at the level of the observed data sample, but also to generalize it to all data of the same type.
As an example, consider the so-called Silverstein Hierarchy used to categorize nouns for (inherent) Topicality (after Deane 1987: 67): Note, first, that there is a lot of overlap in this annotation scheme.
One is concerned with the fact that words can theoretically have identical type and token frequencies, but may still be very differently distributed.
Even if works related to corpus linguistics have existed for a long time (such as the indexing of the Bible by theologians or the file-based construction of dictionaries by scholars like Antoine Furetière in French or Samuel Johnson in English), this discipline was only able to properly take off after the arrival of computing.
A common measure of (token) accuracy is to report the number of correct tags in a corpus as a percentage of the total number of tags in the corpus, or more likely in some sample of the whole corpus extracted for the explicit purpose of checking accuracy (cf.
A common way of collecting metadata is by asking corpus participants to fill out a questionnaire which has been carefully designed by the corpus compiler so as to include information likely to be relevant with respect to the specific context of the discourse included and the people represented.
You generate a frequency list when you want to know how often something -usually words -occur in a corpus.
Let us define a corpus somewhat crudely as a large collection of authentic text (i.e., samples of language produced in genuine communicative situations), and corpus linguistics as any form of linguistic inquiry based on data derived from such a corpus.
We have highlighted tools and techniques that are already used in corpus linguistics that can be considered as visualisation: concordances, concgrams, collocate clouds, and described new methods of collocational networks and exploratory language analysis in social networks.
Next, place the cursor in the first empty cell in the rel_g column, 3 across from the word about, type =, then click in the cell below freq_g that contains the frequency for this word, type /, and click on the number that contains the total for the tokens in the general corpus (Excel), or type n_general (Calc), then press the enter key.
In the most straightforward case, it may be possible to use existing annotation software to automatically apply a given set of POS tagging conventions to corpus source materials.
A historical or diachronic corpus is a collection of texts from different periods.
Even though many people assert that corpus-based studies are essential in explaining the actual use of language, it should be kept in mind that corpus studies are challenging and require a lot of time and energy.
Therefore, all newly created files for a corpus should be directly saved into text format.
In reality, this is difficult to achieve and may in fact be undesirable, since we might, for example, want our corpus (or sample) to reflect the real-world correlation of speaker variables).
Methods and tools for corpus linguistics have developed in tandem with the increasing power of computers and so it is to the computational side that I look in order to take a peek into the future of corpus software.
The main one is probably that, in addition text-processing capabilities, R offers a large number of ready-made functions for the statistical evaluation and graphical representation of data, which allows you to perform just about all corpus-linguistic tasks within only one programming environment.
Notably, NVivo is not a good source of frequency data, as the types of frequency lists that are common in corpus analysis packages are absent.
Linguistic annotation: The process of annotating a corpus involves running software that can (1) tag a corpus (add part-of-speech tags to all of the words in the corpus, such as nouns, prepositions, and verbs), or (2) parse a corpus (add markup that identifies larger structures, such as verb phrases, prepositional phrases, and adverbials).
Thus, an important aspect in constructing a query is to annotate a random sample of our corpus manual for the phenomenon we are interested in, and then to check our query against this manual annotation.
All of the above are issues that are often largely neglected in the analysis of corpus data, especially in more automated and quantitatively oriented types of corpus analysis, where the norm still seems to be to assume that the traditional 'definition' of a word is relatively unproblematic, and that synonymous expressions generally consist of single words only.
For this type of case, the use of a syntactically annotated corpus becomes compulsory.
But what does this mean for our data from the BROWN corpus -is there really nothing to be learned from this sample concerning our hypothesis?
What does one do, then, to ensure that the spoken part of a corpus contains a balance of different dialects?
For example, the part-of-speech tool (using spaCy) only offers visualisations of POS-frequencies but does not give direct access to the tagged texts or include them in the downloadable content sets, which effectively means that the morpho-syntactic annotation provided by Digital Scholar Lab is of no use for corpus research.
The corpus was also divided into two sub-corpora: one consisting of collaboratively written essays (N = 51) and another consisting of individual essays (N = 102).
This focus is closely related to key concerns in corpus linguistics showing that frequent patterns are not necessarily those that language users are aware of.
Metadata can be integrated into a corpus in various ways.
If your specialized corpus also contains subcorpora, then you should ensure the sub-corpora is of fairly equal sizes.
Another type of corpus with special annotation is a speech or phonetic corpus.
One means to represent different situationally defined text types evenly in a given corpus is to aim for balance.
The most rigorous form of concordance analysis will systematise the aspects of each example that are considered, building a matrix of different features -grammatical, semantic or pragmatic -and the values these features have for each concordance line.
When comparing the information in the manuals, you'll hopefully spot that the composition of the different corpora is roughly modelled on that of the Brown Corpus, with only small variations in categories and numbers of sample texts.
The second function is that metadata allows us to isolate and compare different sections of a corpus.
The minimal requirement of an incremental and collaborative research cycle is what we might call retraceability: our description of the research design and the associated procedures must be explicit and detailed enough for another researcher to retrace and check for correctness each step of our analysis and when provided with all our research materials (i.e., the corpora, the raw extracted data and the annotated data) and all other resources used (such as our annotation scheme and the software used in the extraction and statistical analysis of the data) and all our research notes, intermediate calculations, etc.
The paper illustrates some of the latest developments in learner corpus research, such as a solid grounding in theories and a combined aggregate and individual approach.
However, it would be quite difficult to create the type of student specific corpus mentioned above.
Indeed, this type of annotation is more difficult to perform automatically than part-of-speech tagging, since it requires conceptual knowledge in context and this is still a major challenge for artificial intelligence.
For instance, consider the task of part-of-speech (POS) annotation (i.e., determining the correct POS of each word in a given sentence of a text).
However, as we saw in the preceding section and in Chapter 3, it is not always possible to define a corpus query in a way that will retrieve all and only the occurrences of a particular phenomenon.
The dependent variables were more complex: the cardinal variable Length obviously has a potentially infinite number of values and the ordinal variable Animacy was treated as having ten values in our annotation scheme.
A similar thing can be done by expanding abbreviations to their full form, but with both lemmatisation and expansion, one always has to bear in mind that the individual forms may potentially also cause a change in meaning of the whole textual unit in which they occur, or may have been used deliberately as a (stylistic) feature of a particular type of genre.
Of particular prominence in corpus linguistics is the contexts of words, and related to this the corpus linguistic concepts of collocations.
In corpus linguistics, there could be clusters of observations defined by individual speakers, registers, genres, modes, lemmas, etc.
The purpose of the study determines the corpus type the researcher will come up with at the end of the compilation of the texts.
This is why it is wise to test the categories to be annotated on a small portion of occurrences, ranging, for example, from 50 to 100 depending on the difficulty of the annotation scheme and the total number of occurrences to annotate, then refine the criteria or even redesign the categories on the basis of this first annotation.
While the term markup is sometimes used to indicate the physical act of marking specific parts of a text using specific symbols, and, in contrast, annotation may often refer to the interpretative information added, the two may also be used synonymously.
For any of these types, corpus building involves the selection and/or collection of texts or text excerpts and their inclusion in some form of data infrastructure.
Don't worry, you'll always be able to re-run the concordance to get the original results back Save the results of the search that you've just pruned under a suitable file name.
Here's the example I use to teach that difference: made-up frequencies of the words horrible, horrifying, and horrid in the Brown and the LOB corpus (lines 222-230 in the code file): The first chi-squared test tests the first scenario, the second chi-squared test tests the second, because it determines whether the frequencies in the Brown corpus differ from those expected from LOB, but not also vice versa.
In 2011, a TEI-XML version of the corpus was released.
The first main section (Section 2) explores the state of the art in collocation analysis, covering definitional and methodological issues, meaning arising from collocation, collocational phenomena beyond lexical level, as well as the importance of collocation in language use.
We do not share views here (occasionally uttered in corpus linguistics literature) that findings from any given corpus merely apply to this corpus and nothing beyond.
A similar situation would arise in a study of learner corpus data (even of the same alternation phenomenon) with a learner grouping factor if we also knew that the number of years learners have learned a language influences their performance with regard to a specific phenomenon.
Secondly, we will present a set of concordancers, which are corpus analysis tools, and discuss their main functionalities.
Even the diachronic ones are, because they've not been designed to be added to later, even if, for example, at some point in time a further Old English epic may somehow be unearthed and could thus theoretically be included in a new edition of the Helsinki Corpus.
For example, they note that there are obvious differences between the types of sports whose vocabulary differentiates between the two corpora (baseball is associated with the BROWN corpus, cricket and rugby with the LOB corpus), reflecting the importance of these sports in the two cultures, but also that general sports vocabulary (athletic, ball, playing, victory) is more often associated with the BROWN corpus, suggesting a greater overall importance of sports in 1961 US-American culture.
Furthermore, these differences can only emerge on the basis of a quantitative corpus study, which highlights the differences in frequency and context of use.
Given that the purpose of a corpus is to serve as an empirical basis for the study of human languages, it is inevitable that a corpus has to be treated as a representation of language use beyond the specific texts included in it.
This is an important issue since corpus files these days will typically be in Unicode encoding, specifically UTF-8.
Another particularly positive development is the integrated contrastive model which establishes a close link between learner corpus studies and contrastive studies, thereby paving the way for more rigorous investigations of transfer.
If metaphor were indeed located at the word level, it should be straightforwardly amenable to corpus-linguistic analysis.
One common method used in corpus research is to look at the environment of a particular word or phrase to see what other words are found (i.e., "collocate") with the reference word.
Examples abound in -learner corpus research, to document potential over-/underuse by learners compared to native speakers; -language acquisition corpora, to document how children acquire patterns as they increase the number of different verbs (i.e.
Here, I've deliberately advocated a form of annotation I call 'Simple XML', which still makes it possible to read and edit the corpus data, or annotate it further, but without the need for complex interfaces many less computer-literate corpus linguists may have a hard time to even install, let alone use without accepting a steep learning curve.
IDs pointing from annotations to corresponding speaker metadata), it requires great manual effort to make use of it for automatic corpus searches (see also Chap.
One might have expected corpus linguistics to spearhead this development because, following the above logic, corpus linguistics is inherently quantitative.
Most corpus tools require plain text versions with optional XML encoding, so where material is sourced in another form, some format conversions will be in order.
However, this is no time to rest on our laurels-now that corpus linguistics has become mainstream, and that's a good thing, we too must continue to refine our methods just as other fields have to.
In principle this can be done by using the symbols of the IPA to render the corpus text.
While much of that work has, of course, been successful because, for example, high token frequencies in b and c are positively correlated with high type frequencies, and high token frequencies in a are negatively correlated with clumpy distributions, it is unclear how potentially skewed the results are for cases where those correlations do not hold.
The creation of numerous learner corpora, as well as the development of new methods and annotation tools, has largely contributed to this evolution.
While this approach is very versatile, it is also labor intensive: for each new type of information, a new script must be written which traverses the corpus in search of some information.
Indeed, in the literature we can see objections to corpus linguistics which follow the anti-positivist critique, most notably with reference to context and subjectivity.
However, if the available types of discourse are not already classified in some reliable way, as in the case of spoken language, it means that the corpus builder will have to dedicate a great deal of time to researching the characteristics of the target discourse in order to develop valid and acceptable selection criteria.
Then we use if and any (see the very simple definition at ?any ¶) to let R check whether the file has XML or SGML annotation; depending on that, search.expression.alph and search.expression.ord will be defined in the required way.
More fundamental are matters of linguistic analysis proper, which present all the problems associated with lemmatization, tagging and annotation of synchronic data to a much higher degree (see Chap.
CHILDES (Child Language Data Exchange System) can be a good corpus for researchers focusing on acquisition.
Potentially ordinal data are actually frequently treated like nominal data in corpus linguistics (cf.
Whatever Corpus Linguistics is, it is not static.
These are generally captured in corpus linguistics as part of the external contextual features.
Balanced or representative corpus consists of texts selected in predefined proportions to mirror a particular language or language variety.
While we will see below that corpus linguistics has a lot to offer to the analyst, it is worth pointing out that, strictly speaking at least, the only thing corpora can provide is information on frequencies.
One notable milestone in the field of corpus linguistics was the publication of "Computational Analysis of Present-Day American English" in 1967 by Kučera and Francis.
More recently, however, the Text Encoding Initiative (TEI) has developed a standardized system of annotation not just for linguistic corpora (www.tei-c.org/release/doc/ tei-p5-doc/en/html/CC.html) but for electronic texts in general (www .tei-c.org/).
Conversely, many other annotations made on corpus studies are devoted to a single type of element to be studied.
In order to avoid this type of bias, in addition to word frequency, it can be useful to calculate lexical dispersion in a corpus.
This tool makes it possible to perform a time-aligned annotation with audio or video files, to insert metadata and to have access to the results of the annotation in different formats.
It is therefore essential for corpus compilers to make available to future corpus users ample metadata and a corpus manual detailing the corpus compilation and annotation process.
The results of corpus studies carried out on native and learner corpora can help textbook designers prioritize features, be it on the basis of frequency of use (native corpora) or of difficulty in acquisition (learner corpora).
However, some languages are more suited to the tasks commonly carried out by corpus linguists, i.e., reading large text files into memory, processing text strings, counting tokens, calculating statistical relationships, formatting data outputs, and storing results for use with other tools or for later use.
Her categories differ in number (between four and ten) and semantic granularity across the four words, let us design a stricter annotation scheme with a minimal number of categories.
It also demonstrated that such corpus-based studies may result in very specific hypotheses about the function of lexicogrammatical structures that may become the basis for claims about mental representation.
A fundamental statistic in assessing the saliency of any linguistic feature is frequency of occurrence, or, simply, the number of times a feature of interest occurs in a data set.
Brainstorm nominal, continuous, discrete variables you might really use for a corpus study.
The Helsinki Corpus, which was to be published in 1991, would revolutionize the historical research of English for decades to come, and its importance for historical corpus linguistics cannot be overstated.
While an understanding of the principles, approaches, and advantages of using corpora provides the necessary foundational knowledge of this approach to language analysis, there is a technical side to corpus linguistics that is best acquired through practice and experience with corpora.
There are several ways to store metadata for a corpus, but one popular way is to insert a chunk of descriptive information at the beginning of each corpus text in a fixed, specified format: this metadata chunk is called the header of the text.
We To answer this question, correspondence analysis was used with all the speech samples in the corpus plus transcript X (34 + 1).
Not ideal as the sole basis of a text corpus, but could be used for phonetic measurements.
The practical implications of Zipf's law for corpus linguistics are as follows: we have to learn to critically evaluate the amount of evidence we have for our claims.
This requires that texts be similar in two regards: text data format and pre-processing and annotation.
Lesser-studied languages typically lack these things, and here corpus linguistics often means to build corpora in the first instance, and these corpora will then often be relatively small, which then comes with various restrictions as to their usability in research.
While software like CLAWS also provide probabilistic information about word class, in practice this information is typically not available to the end users of the corpus.
The smaller the corpora compared are, the easier it'll of course become to narrow down such selections, but essentially, the technique itself is similar to creating basic stopword lists, only that, in this case, a word list from a whole corpus is used as a stopword list.
There is a range of possible considerations to take in deciding about sampling procedures for a corpus, one of which concerns to what extent to organize the overall design around text production or text reception.
It should also be noted that they can be easily extended or adapted to carry out tasks that are quite difficult or impossible to achieve with the most popular tools used in corpus linguistics today.
However, the comparative keyword analysis reported here did not involve a general reference corpus (which may conflate or obscure gender differences in the specific data sets.
For example, the distinctions between different degrees of Animacy need to be defined in a way that allows us to identify them in corpus data (this is the annotation scheme, cf.
The way that corpus creators typically aim to achieve this is by including in the corpus different manifestations of the language it is meant to represent in proportions that reflect their incidence in the speech community in question.
However, it only takes eight token before we reach the first repetition (the word a), so while the token frequency rises to 8, the type count remains 9 Morphology constant at seven and the hapax count falls to six.
For instance, many texts, such as books, are quite lengthy, and to include a complete text in a corpus would not only take up a large part of the corpus but require the corpus compiler to obtain permission to use not just a text excerpt, a common practice, but an entire text, a very uncommon practice.
Video recordings also allow us to document sign languages; see Chapter 4.3 for some details on their corpus-based investigation.
We then present an overview of several applications of bootstrapping that have been used in prior corpus linguistics research.
If we, however, take a corpus with 100 male and 100 female speakers, where the difference between the groups is exactly the same (16 and 14), we get more evidence for the effect (the dataset is available from the companion website): d = 0.3 95% CI [0.03, 0.59].
The z-score test is a measure which adjusts for the general frequencies of the words involved in a potential collocation and shows how much more frequent the collocation of a word with the node word is than one would expect from their general frequencies.
Moreover, a lengthier segment allows the corpus compiler to select a more coherent and unified segment of speech to ultimately include in the corpus.
There are several different ways in which information about the corpus design can be disseminated.
All of this complicates the formation of bridges between corpus linguistics and the social sciences.
First, tools which provide Computer Assisted Qualitative Data Analysis (CAQDAS), such as ATLAS.ti, NVivo, QDA Miner, and Wordstat, incorporate some very similar methods to those described here but are not widely used in corpus linguistics.
This is a question that is shared by linguistics and other areas of the social sciences, so it is perhaps understandable that annotation schemes are another area where there has been a flow of ideas between corpus linguistics and the social sciences.
This case study demonstrates that central metaphors for a given target domain can be identified by applying a keyword analysis to a specialized corpus of texts from that domain.
One approach would be to have the entire data set annotated by two annotators independently on the basis of the same annotation scheme.
For instance, Xueliang Chen, Yuanle Yan, and Jie Hu (2019) conducted a corpus analysis of the use of language by Hillary Clinton and Donald Trump when they were running against each other for president.
Describing corpus linguistics as a 'field' or subdiscipline of linguistics is a statement which requires qualification, however.
It is known to us that corpora can be of many types with regard to texts, languages, modes of data sampling, methods of corpus creation, manners of text annotation, and goals of text utilization, etc.
In recent years most compilers of web corpora have worked on the assumption that maximising corpus size is likely to result in a more representative corpus, even if we have no reliable way of measuring this (see Representative Study 2 for a discussion of the related topic of balance in web corpora).
Pasting the token counts from the 'Make/edit subcorpora' page also presents no problem because, this time, we only have one single piece of 'text' without HTML codes, so there's nothing to misinterpret for the spreadsheet program.
Although far less common, a corpus-based approach to data collection also has several advantages, including allowing for dialectologists to collect large amounts of data from a large number of informants, observe dialect variation across a range of communicative situations, and analyze quantitative linguistic variation in large samples of natural language.
After this brief excursion, let's return to investigating how we can make use of PoS tagging in BNCweb.
Unlike the corpus-driven approaches illustrated in this book, more advanced corpus-driven register studies have identified co-occurring linguistic features that have emerged through corpus analyses.
This chapter will explain some of the most frequently used quantitative techniques in corpus linguistics today, such as logistic mixed-effects regression, as well as focus on some newer techniques becoming more popular, such as classification trees (and random forests) which are types of recursive partitioning.
The main limitation to the use of corpus data for studying linguistic changes from a diachronic perspective lies in the fact that linguistic changes generally first take place in the spoken language.
Moreover, while the term lemmatisation is derived from the term lemma which in corpus linguistics (but not necessarily in lexicography) is used fairly often as an equivalent of lexeme.
Deignan does not explicitly present an annotation scheme, but she presents dictionary-like definitions of her categories and extensive examples of her categorization decisions that, taken together, serve the same function.
In a diachronic corpus of narrative fiction this inevitably affects the frequencies of specific grammatical patterns associated with either descriptive or more action-driven narrative passages.
For instance, consider a short text of only five words (such as a tweet, a postcard, or a sticky note) which contains one instance of a feature, for example, a single first-person pronoun.
On the other hand, the development of a text corpus addresses issues like the size of a corpus, representation of text types, question of nativity of language users, determination of target users, selection of time-span of production of texts, coverage of disciplines, selection of text documents, collection of source text materials, methods of data sampling, manners of data collection, manners of text normalization, management of corpus files, types of text annotation, and issues of copyright, etc.
In the future, we recommend investigating the use of statistical power calculations in corpus linguistics.
The case study also shows that word frequency may have effects on grammatical variation, which is interesting from a methodological perspective because not only is corpus linguistics the only way to test for such effects, but corpora are also the only source from which the relevant values for the independent variable can be extracted.
The replication crisis in linguistics is highly relevant to corpus-based research: Many corpus studies are not directly replicable as the data on which they are based are not readily available.
Collocates are words that co-occur with the node in a specifically defined span around the node, which we call the collocation window.
If a written text is planned to be used in a corpus, researchers should take the required permissions from the author of the text, which will prevent copyright issues that might be faced in the future.
We will first outline general corpus design principles and then turn to the more practical issues of data collection and/or selection in Section 6.2.
The term collocation refers to the co-occurrence of multiple lexemes, often two lexemes, called a bigram and a cooccurrence of three lexemes is called a trigram.
Two broad alternatives have been proposed in corpus linguistics.
Without knowledge and practice using a specific framework, we have found that students can be quite enamored with the "button-pushing" aspects of corpus linguistics at the expense of interpreting the results of their searches.
Predictably, I think the answer is still "yes" and "yes, even a second edition," and the reason is that this introduction is radically different from every other introduction to corpus linguistics out there.
There are many different descriptive and theoretical frameworks that are used in corpus linguistics.
We have seen above that the creators of the Brown corpus (and other subsequent corpora following the same design principles) sampled written English texts according to their intuitions about the proportions of different text types.
Their study convincingly illustrates the need to adopt multifactorial research designs in corpus-based translation studies, as these make it possible to go beyond the monofactorial designs where, typically, only the "translation status" variable is considered (translated vs. non-translated).
Again, whether type or token frequency is the more relevant or useful measure depends on the research design, but the issue is more complicated than in the case of words and grammatical structures.
In discussing the corpus frequency of words, it is useful to introduce the distinction between word types and word tokens.
In corpus-based linguistics the research domain is some collection of natural language utterances.
As a result, most research articles in corpus linguistics include discussion of corpus compilation, annotation, and/or the computational methods used to retrieve linguistic data.
The development of the field of learner corpus research is sketched, and possible future directions are examined, in terms of the size of learner corpora, their diversity, or the techniques of compilation and analysis.
Each chapter also includes critical discussion of the corpus-based methods that are typically employed for research in this area, as well as an explicit summary of the state of the art: what do we know as a result of corpus research in this area that we did not know previously?
Robustness is typically operationalized in terms of text coverage -the extent to which words on the list account for the total running words in a corpus.
Each corpus developer is, however, free to adopt unique methods in language representation, text collection, and text application.
Finally, some layers, such as 'constituent syntax', contain a complex subgraph: an NP node is attached to the token I, and a VP node is attached to know, and together they attach to the S node denoting the clause.
Corpus-driven studies begin by analyzing a corpus to identify the set of important lexical phrases, and then study further the use of those phrases in discourse contexts to interpret the initial results.
It is for this reason that the study of language production in patients is often carried out via constrained production tasks, such as the ability to name images or to repeat non-words or sentences (see, for example, Seiger-Gardner and Almodovar 2017), rather than based on corpus data alone.
The growing demand in corpus linguistics for statistical sophistication and the lack of appropriate resources for beginner and intermediate users of statistics can thus easily lead to frustration.
But organizing a corpus into a series of directories and sub-directories makes working with the corpus much easier, and allows the corpus compiler to keep track of the progress being made on corpus as it is being created.
Representativeness, albeit also difficult to measure, may be more easily achievable, especially for domain-specific corpora or limited fields of investigation, because often there are relatively clearly definable criteria for what represents a certain genre of text or domain.
Check out the small example in the code file to see how ranger gives you all possible collocate positions for a corpus vector of a certain length (and pads the remaining positions with NAs so all output is reliably equally long).
Another issue related to corpus balance in your corpus relates to text types.
As a lot of work has been done on named entity recognition in the field of computational linguistics, a key to address the problems raised in commonly used, large corpora in corpus linguistics would be to increase the collaboration between the areas, and to integrate the systems and practices developed in NLP and computational linguistics into corpora such as the BNC.
Unlike many other areas of linguistics, there is a fairly clear difference between corpus-based and corpus-driven research on phraseology, and both approaches have been applied productively.
However, there are 41 data points in our sample, so the rank positions will range from 1 to 41, and there are only 10 rank values in our annotation scheme for Animacy.
Although this lemma occurs in a majority of corpus parts (Range 10) it has a very low Juilland's D (0.2).
Even the same metaphors are used: Such similarities are of course not surprising, given that both Digital Humanities and corpus linguistics deal with textual data.
Not entirely unrelated to this is the second front of current progress in the field -namely, the continuing development of more refined techniques and tools for corpus analysis.
The present paper is an attempt to provide a snapshot of current problems, both in corpus linguistics in general and in selected hot topic areas, as well as to provide ideas and (first) suggestions about how to cope with these problems; I hope it will succeed as a call to (methodological) arms, and thus trigger developments that will help our field advance once more.
To get a concordance for the collocate, this time you need to follow the hyperlinked frequency in the column for 'Observed collocate frequency', while the link in the 'Word' column provides a breakdown of the statistics for the word form, including scores for the other statistical measures available and distribution of collocates within the given span.
Through years, corpus mechanisms have moved out the domain of verifiable research in relevance to linguistic studies and language education to the use of corpora to serve certain functions in conducting research in specific areas of study.
The latter is even true if we work with commercial corpora but make our concordances and analysis scripts available, as is increasingly common in corpus linguistics.
Another group of researchers, possibly novices in the discipline, cited general references to corpus linguistics, such as Introduction to Corpus Linguistics, Corpora in Applied Linguistics, and Foundations of Statistical Natural Language.
On the other hand, in case of a monitor corpus, which tries to represent all possible text varieties, we may include all non-standard and ordinary text samples along with standard and established text samples.
The key difference between multimodal corpus linguistics and the multimodal analysis of corpora using MDA is that, as with traditional text-based corpus enquiry, multimodal corpus analysis includes not only detailed qualitative analyses, but also quantitative analyses of emerging patterns of language-in-use.
Using fresh manuscript material for corpus compilation is a timeconsuming enterprise: in-depth philological and computational work is often required to transfer the manuscript readings into searchable computer files.
That means, based on the type of text, one has to address various issues of corpus generation.
The responsibility of the corpus compiler is involved when it comes to texts with potentially defamatory content.
A key items analysis is performed as follows: first, frequency lists are created for two texts or corpora -either two corpora which we wish to contrast, or a single text of interest and a generic reference corpus.
For example, in order to determine whether two words are synonymous or not, the analysis of corpus data makes it possible to determine whether these two words can appear in the same linguistic context or not.
For each of these 27062 diphones, we considered the following variables: DictDiphoneAbsent, with values TRUE or FALSE, depending on whether the dictionary diphone was realized by the speaker; this is the response variable for our analyses; DictDiphonePosition, an integer indicating the position of the dictionary diphone in the word; DictDiphoneCount, an integer with the number of dictionary diphones in the word; PhraseInitial, with values TRUE or FALSE, indicating whether the word carrying the diphone is phraseinitial; PhraseFinal, with values TRUE or FALSE, indicating whether the word carrying the diphone is phrase-final; PhraseLength, an integer indicating the length in words of the phrase; PhraseRate, the speech rate (number of syllables per second); LogDuration, the logarithm of the duration of the word (in seconds); DictDiphoneActDiversity, a measure, based on discriminative learning, gauging the lexical uncertainty caused by the diphone; WordActDiversity, a measure gauging the lexical uncertainty of the carrier word in a semantic vector space model derived with discriminative learning; SemanticTypicality, the extent to which the semantic vector of the carrier word is similar to the average semantic vector; and CorpusTime, the position of the diphone in the corpus (ranging from 1 to 27062) but scaled and centered to 575 make this variable commensurable with the other numeric predictors.
Instead, we need to look at the type-token ratio and the hapax-token ratio.
Part IV of the handbook surveys these major areas of application, including classroom applications, the development of corpus-based pedagogical materials, vocabulary studies, and corpus applications in lexicography and translation.
While this seems like a very high accuracy rate, in a large corpus, the remaining 3 percent of words with multiple tags can constitute a very sizable number of words.
There is a substantial body of corpus-linguistic research based on designs that combine the two inherently represented variables Word (Form) and Text; such designs may be concerned with the occurrence of words in individual texts, or, more typically, with the occurrence of words in clusters of texts belonging to the same language variety (defined by topic, genre, function, etc.).
In the following sections, we will discuss in depth the different central points of the definition, indicated in bold, in order to better understand the theoretical and methodological anchoring of corpus linguistics.
The most common form to display a keyword in context (KWIC) is through concordance lines.
While obtaining sorted frequency lists that reveal which collocates or constructions occur most often or are most likely around an element E is straightforward, much corpus-linguistic research has gone a different route and used more complex measures to separate the wheat (linguistically revealing co-occurrence data) from the chaff (the fact that certain function words such as the, of, or in occur with everything a lot, qua their overall high frequency.
This is done by first generating a "word list" of the lexical items in the dataset and the reference corpus, using the appropriate program function.
For this study, the occurrences of the eight abovementioned connectives were drawn from three parallel corpora (Europarl for the parliamentary debate genre, a corpus of newspaper articles and the TED corpus of online conferences; see Chapter 5 for a description of these corpora).
For example, a corpus created for studying the pronunciation of vowels in Paris and Marseille and which has not been compiled representing a balanced sample of different age groups cannot be used carelessly for studying the evolution in the pronunciation of vowels between generations.
Therefore, based on the journals most cited by corpus linguistics papers, corpus linguistics studies not only refer to general linguistic studies, but also specialized journal papers for new areas of language education, cognitive linguistics, and corpus linguistics itself.
For both the spoken and written parts of the corpus, not all samples are exactly 2,000 words: a sample is not broken off in mid-sentence but at a point (often over or just under the 2,000-word limit) where a natural break occurs.
The names supplied to a text sample are short and mnemonic and give the corpus compiler (and future users of the corpus) an idea of the type of text that the sample contains.
In order to study differences in the representation of men and women, we can query the pronouns he and she separately to obtain representative samples of male and female speech act events without any annotation.
A numbering system of this type (described in detail in Greenbaum 1996b: 601-14) allows the corpus compiler to keep easy record of where a text belongs in a corpus and how many samples have been collected for that part of the corpus.
In reviews of historical developments in corpus linguistics, reference is often made to the fact that concordances are not an invention of corpus linguistics, but have been used in the study of literature even before computers existed, for instance, to compile concordances of the Bible or of works of Shakespeare.
Metadata and textual markup are important because they can narrow the range of possible choices when a corpus is searched.
Imagine further that a number of corpus-based studies had examined this issue using different corpora, with each study yielding a result expressed in occurrences per million words.
In the case of relatively well-resourced languages like English, for which many corpus annotation standards and tools exist, many common annotation tasks can be accomplished with 'off-the-shelf' software tools and minimal customization of annotation standards or procedures.
In qualitative statistical analysis, on the other hand, we look at multidimensional analysis of observed phenomena in a corpus and try to find factors behind the occurrence of a feature or an element in a text.
For example, what if you wanted to generate a frequency list of a reasonably large corpus such as the 100 millionword BNC, where you do not know the exact number of word tokens and types in advance?
In particular they need to be annotated, at least transcribed, and it is the transcription that will eventually resemble our corpus text.
The with an uppercase T does not occur in the tagged LOB corpus, because case is normalized such that only proper names are capitalized.
However, a word may have a high frequency count in the BNC not because it is widely used in all sections of the corpus, but because it has a very high frequency in only certain parts of the corpus and not in others, e.g.
Although the use of bootstrapping methods in corpus linguistics is still in its infancy, writing this book chapter has given us reason to be optimistic about the future of bootstrapping in corpus linguistics.
It has to "learn" them from a corpus that has been annotated by hand by skilled, experienced annotators based on a reliable, valid annotation scheme.
More specifically, the concordancer determines which of the words are used significantly more (or even less used in the case of negative keywords) in the search corpus compared to the reference corpus.
Other sections of the chapter focus on additional issues relevant to planning the construction of a corpus, such as how to determine the appropriate size of a corpus and the length of particular texts that the corpus will contain (complete texts versus shorter samples from each text, e.g.
Corpus linguistics takes real samples that are used to generalise about language, and corpus-based typology helps us to confirm (or confront the idea) that these assumptions are applicable for a more diverse sample.
Similar tools have been developed with invaluable contributions to the field of Corpus Linguistics.
It is also common to describe these two groups of words as overused (for positive) and underused (for negative), particularly in the learner corpus literature (cf.
Some programs output only a user-specified number of occurrences, usually either the first n occurrences in the corpus or n randomly chosen occurrences from the corpus.
Among the assumptions that lie behind Corpus Linguistics, though, is the view that there are aspects of language use that are important but that are invisible to the human reader of texts.
It turns out the only data structure required is the input vector, which above was called qwe, and this is because there is only one other data structure we use, which is asd, but asd can be derived from qwe.
It also demonstrated that semantic categories can be included in a corpus linguistic design in the form of categorization decisions on the basis of an annotation scheme (in which case, of course, the annotation scheme must be documented in sufficient detail for the study to be replicable), or in the form of lexical items signaling a particular meaning explicitly, such as adverbs of gradualness (in which case we need a corpus large enough to contain a sufficient number of hits including these items).
Where ELF is special is in certain principles of corpus compilation.
Let' s apply a binary classification tree to our Vera' a data, excluding here Speaker and Text because their many levels may cause too many splits in the tree, making it difficult to read.
Gardner and Davies' study exemplifies a contemporary approach to constructing and validating a frequency list of academic vocabulary-one based on lemmas rather than word families-by taking advantage of technological advances (e.g., part of speech tagging; the ability to compile substantially larger corpora) and the application of statistics to building frequency lists.
Constructing a word list usually involves four key steps: 1) compiling a corpus representing a target discourse domain; 2) deciding on word selection criteria; 3) applying selection criteria & extracting the list from the corpus; 4) evaluating the list.
If the aim is to represent a whole language variety, ensuring that appropriate registers are included in the corpus is one of the ways in which historical corpus linguists can enhance the representativeness of corpora.
No matter how explicit our annotation scheme, we will come across cases that are not covered and will require individual decisions; and even the clear cases are always based on an interpretative judgment.
The choice of lemmatization software often depends on the kinds of language found in the corpus materials.
Thus, corpus "balance" is a key aspect of reliable corpus building.
In this case, since you are more interested in how your keyword is used in different registers, you would want 100 sample concordance lines randomly selected from spoken discourse, and 100 concordance lines randomly selected from academic prose.
While not every five-word text contains a first-person pronoun, it is also not that unusual if one does.
Questions 1) In addition to morphosyntax discussed in this chapter, what are the other aspects of language acquisition that are well suited for corpus-based research?
Some of the possibilities of corpus annotation are presented in the next chapter.
Those chapters are then followed by chapters on the use of corpus analysis to document the linguistic characteristics of other types of varieties: literary styles, regional dialects, world Englishes, English as a lingua franca, and learner English.
Furthermore, this double annotation process will make it possible to indicate whether the categories have been poorly defined.
The idea that collocation is of high or indeed primary importance to understanding the meaning of a word or other linguistic form originates with J.R. Firth and is very important in the neo-Firthian school of corpus linguistics; but collocation analysis itself is a commonplace of most or all approaches to corpus linguistics.
We will also need to make sure that the corpus collected in this way includes approximately the same speaking time or the same number of words pronounced by men and women.
Questions 1) The tables below show the number of occurrences of the (lemmatized) words bateau and je, as well as their English equivalent boat and I, in the bilingual corpus of children's literature ParCoGLiJe (see Chapter 5).
In corpus linguistics, the explicandum is typically some aspect of language structure and/or use, while the explicans may be some other aspect of language structure or use (such as the presence or absence of a particular linguistic element, a particular position in a discourse, etc.
Of course, corpus data are usuallyhopefullya bit larger than the above data (and see Section 2.2 below for a larger sample size) and they do not usually exhibit the kind of complete separation shown above.
Corpus linguistics needs to "catch up" with regard to both of these groups.
One example from Cameron's writing is the significantly above-average use of is; which was the fifth most frequent keyword in her corpus.
Below, we will use it to analyze a corpus based measure, namely the type-token ratio (TTR) of different texts that have the same overall length.
Conversely, tagging can disambiguate the words: break is both a noun and a verb, and broken is both a participle and an adjective -POS tags allow a computer to distinguish the different grammatical functions.
In the corpus linguistics field, it is also known as lexical bundles, recurrent combinations or clusters.
For example, given a rich historical record and a long tradition of high-quality text editions, it is perfectly possible to create a sizeable corpus of Old French, as shown by the FRANTEXT database.
If annotating overlapping segments of speech is done separately from the actual transcription of the text, the individual doing the annotation will have to go through the tape repeatedly to reconstruct the overlapsa process that could be done more efficiently by the person doing the transcription.
In other words, following the principles of Open Corpus Linguistics requires us to invest considerable time in Research Data Management (RDM).
As noted above, methodological issues relating to corpus design and analysis have been dealt with at length in previous textbooks.
But because this kind of discussion is subjective and impressionistic, it is better to devote the bulk of a corpus study to supporting qualitative judgements about a corpus with quantitative information.
Conducting a corpus analysis is a multi-stage process, involving framing a research question, finding a suitable corpus or corpora for the analysis, extracting relevant information from the corpus or corpora chosen for the analysis, and integrating information from relevant articles or books into the analysis.
One sub-type is a reference corpus: a general corpus taken to be a standard reference for corpus-based research on a given language.
Indeed, we speculate that in years to come, much, if not all, pragmatics research will involve corpus linguistics.
Some concern methodology within corpus linguistics.
Every journal title has its own aims and scope of publication; therefore, the analysis of the cited journal titles identified trends in how corpus linguistics research has been communicated within the research community.
Standard deviation is a classic measure of dispersion, which is used very often also outside corpus linguistics.
When selecting the sampling frame we are often predisposed to see as salient the text types that have traditionally been included in language corpora such as those in the Brown family sampling frame (see • Legal considerations bias: corpus designers often face the problem of copy- right; this is especially the case when corpus creators want to share their corpora with other researchers.
Second, as with key items, a rigorous collocation analysis will always go beyond the raw list of statistical collocates to look at actual instances of those collocates in usage alongside the node.
As in the heading of this chapter, we frequently encounter two different terms when talking about the process of storing and enriching linguistic data, markup and annotation.
Moreover, potentially conflicting desiderata such as comparability and representativeness make it necessary for scholars to consider carefully the make-up of their corpora and the extent to which results based on a selection of registers can be generalized to the language as a whole.
Basically, these lists can be created in two different ways by a program (or a person), either by producing a token list first, then sorting it and counting how many times a given word form occurs in a row, or, more efficiently, by keeping track of each word form encountered and increasing its count as it recurs in the text.
Information on gender (which is defined biologically in corpus linguistics) can be requested on a biographical form, and in written texts, one can often tell the gender of an individual by his or her first name.
Various types of corpus annotation can make these aspects explicit and searchable.
By providing a manually checked, dual POS tag which encodes both form and function (VOICE Project 2013), the XML corpus can be searched for all cases of an "innovative" form, in Cogo and Dewey's terms.
Against that background, it would be easy for a student to imagine that corpus linguistics developed only in the 1980s and 1990s, responding to the need to base linguistic descriptions on empirical analyses of actual language use.
The BNC is defined as "a monolingual British English corpus: it comprises text samples which are substantially the product of speakers of British English" (www.natcorp.ox.ac.uk/docs/URG/BNCdes.html#spodes).
The bad news for Windows users is that, with R for Windows at least, it is often not as straightforward to handle Unicode as it is with R for MacOS X or R for Linux, which is mainly due to the fact that Mac OS X and Linux are UTF-8 operating systems so they play well with input files that have the same encoding, which is one of many reasons (see Section 3.8.2 for another one) why I recommend doing corpus processing with R on a Linux machine (and it takes very little time to install a long-term support version of, for instance, Kubuntu or Linux Mint on a desktop computer).
In this paper, we revisit Rissanen's ideas in order to identify some of the present-day pitfalls in historical corpus linguistics.
And even if this number represents only 0.01% of all the words in the written parts of the BNC, the number of potential errors, which appear mainly due to tokenisation errors, is staggering, particularly when considering that this affects only one of the parts of speech represented in the corpus.
Section 3 focuses on some pitfalls related to big corpora, and our examples in this section concern both the reliability of the semi-automated sampling of such resources and the comparability of the research results when new genres are introduced to the corpus.
On the other hand, though, if a corpus is too small, it may not be very useful for general purpose research because the amount of data needed to conduct research into, for example, collocations (the habitual co-occurrence of words; see Section 10.5) apparently increases exponentially (c.f.
The median token frequency is 1 morpheme, the median type frequency is 2 morphemes.
It is still an emerging field and will help shape the more general perspective on corpus linguistics as a field in the 21st century.
A researcher should then balance the efforts and rewards of making changes throughout an entire corpus; keeping some glosses underspecified, as described above, is a relevant technique here.
Now that you know about some of the issues that may affect our counting of words in a text/corpus, we can start looking at frequency lists properly.
Collocational strength is particularly relevant in corpus-based studies of lexical relations, for example, where collocations point to semantic differences between lexemes that are often thought of as synonyms (cf.
Since a corpus contains text samples either from a written or spoken discourse, annotation is done by adopting a value-encoding process, which attaches interpretative information at different levels of a text.
This is why corpus linguistics has grown as an area of linguistics and why many people now are using linguistic corpora for many different purposes.
Alternatively, the y-coordinates of all ten points are the number of unique types you have seen at each position if you've read all tokens from slot 1 to that position, i.e., what is here called type.freqs: For instance, note how the rightmost value is 7, i.e., the type-token ratio of the whole vector (0.7) times the length of the vector (10).
Build: corpus design and compilation 3.
These corpora are certainly impressive in terms of their size, even though they typically contain mere billions rather than trillions of 2 What is corpus linguistics?
On the one hand, there are textbooks that provide excellent discussions of the history of corpus linguistics or the history of corpus design, or that discuss the epistemological status of corpus data in a field that has been dominated far too long by generative linguistic ideas about what does and does not constitute linguistic evidence.
This implies that numerous factors control the content, type, and use of a corpus as well as numerous ways that one can employ to classify corpora.
We carry out a more indepth analysis of two of the four research questions listed above, namely how corpus-based research applications are presented, and how (and how much) corpus-based research has been incorporated into materials.
An early introduction to corpus linguistics written for students in language education.
Specific attention is called for in tagging elements like expert terminology, metalinguistic language use, and quoted passages to ensure the accuracy of word counts and concordance analyses.
An important element to take into account when downloading a corpus from the Internet is its encoding format (this not to be confused with the compression format, when applicable, that needs to be handled by means of specific archiving tools).
This phenomenon of trust in corpora is triggered through several factors such as easy and customized accessibility of corpus data, objective analysis and description of a language based on actual empirical evidence, utilization of language data and information in various domains of language application and utilization of language-based technology in development of new societies empowered with linguistic support systems.
Diachronic/historical corpus linguistics is an important area that aims at answering many of the as yet unanswered questions in language history, including differences in style, vocabulary, grammar, and even pragmatics.
A plain text format (such as .txt) is often used for corpus files.
Chapter 3 included a discussion of counterexamples and their place in a scientific framework for corpus linguistics.
This dependency is due to the fact that our ability to analyse and examine complex and extensive multimodal corpora in a systematic and accurate way (particularly when integrating traditional corpus-based methods into the approach), is reliant on the availability of appropriate technology.
Gu focuses on different modalities within the analysis, beyond what the majority of multimodal corpus studies typically afford, making this work of particular relevance to the final section of this chapter: projections for the future directions of this field.
Another distinction that can be made regarding the types of existing corpora relates to the type of processing carried out on the linguistic data of the corpus.
The flipside is that this may potentially compromise the representativeness and balance of the filtered sample, and this needs to be evaluated on a case-by-case basis due to the uneven and erratic distribution of OCR errors in the database.
It is the longest-established form of corpus annotation, with the first efforts in the direction of automatic taggers going back as far as the early 1960s, and the first practical, high-accuracy tagging software emerging in the 1980s -although even the best POS taggers have a residual error rate of around 3-5 per cent which can only be removed by painstaking manual post-editing.
For a long time after the publication of the corpus in 1964, many people thought that 1 million words of text represented a general-enough sample to provide sufficient information about the makeup of the English language.
The representativeness of a corpus also depends on its balance and the choice of samples it contains.
One major concern in the corpus-based semantic analysis is word sense disambiguation which is not inferable from the surface structure of the corpus text itself and needs to be determined by a human interpreter.
Furthermore, journals that first ranked on the list after the early or late 2000s and remained dominant in corpus linguistics until now were English for Specific Purposes, TESOL Quarterly, International Journal of Corpus Linguistics, and Cognitive Linguistics.
The first part of the book begins with an almost obligatory chapter on the need for corpus data (a left-over from a time when corpus linguistics was still somewhat of a fringe discipline).
Another is how dispersion measures relate to issues outside of corpus linguistics such as, again, psycholinguisticallyor cognitively-informed approaches.
We then load each corpus file with scan, merge it with paste(...,•collapse=...), gsub line-initial spaces away and replace them by spaces, strsplit the file apart again, unlist, and do further cleaning with multiple gsubs, before we strsplit up all utterances into words and retain (with grep) only those with at least one word character.
An essential requirement for corpus building is that all aspects of text production be available in some way to the users of the corpus.
In 21st-century corpora this means that the corpus text and the linked media file be accessible from a single entry point, for example, a website or a corpus query software.
In contrast, a line-based concordancer, such as the one built into my Simple Corpus Tool (downloadable from martinweisser.org), will only extract and display the immediate context found on the same line, plus a number of surrounding lines specified.
In practice, some corpus tools let us set the minimum cut-off limits for the frequencies of words in C and R before considering them in the keyword procedure.
For this information to be made available for future users of the corpus without separating it from the rest of each sample portion, it should be possible to include it in the files, but in such a way that it is not taken into account by a concordancer when counting the words of the corpus.
Throughout the nineteenth and much of the twentieth centuries corpus linguistics was exclusively paper-based.
Of course, the issue of genre balance also affects smaller diachronic corpora like the Helsinki Corpus, because some genres may disappear and new ones appear over time, and compromises must be made in the compilation process in terms of representativeness vs. diachronic comparability.
The chapter concludes with a detailed description of various kinds of textual markup and linguistic annotation that can be inserted into a text.
The methods described in this chapter can be considered a starting point for providing us with some quick hints as to which particular type of language, or perhaps even genre, we're dealing with in our corpus analysis by investigating how frequently certain words occur in a corpus.
The historical timeline of corpus retrieval software can be divided into four generations.
One question that was asked, however, that was often not asked as much in the wake of computerized corpus linguistics: Is it enough to simply provide a list of words?
Thus, a monitor corpus becomes a worthy representation of texts that are ever produced in a language.
We'll see some examples and different ways for applying markup to text later on, but for now, we're more interested in the reasons that exist for applying markup to our data in the first place, and roughly which different types of linguistic annotation currently exist.
In addition, sample corpus analyses are included to illustrate the many theoretical and practical applications that linguistic corpora have.
By bridging quantitative and qualitative perspectives on language data, concordance analysis is and will remain a centerpiece of corpus-linguistic methodology.
So, again, it remains to be hoped that analytical strategies like this one will gain more ground in learner corpus research, the research on varieties, and any other domain where one part of the corpus data can be considered a standard or target with which the others can be meaningfully compared.
As a consequence, even though the creators of the Brown Corpus, W. Nelson Francis and Henry Kučera, are now regarded as pioneers and visionaries in the Corpus Linguistics community, in the 1960s their efforts to create a machine-readable corpus of English were not warmly accepted by many members of the linguistics community.
The next one has the word form followed by lemma annotation.
In this section, we will focus on Cohen's d only as an effect size measure as this measure has been used more prominently in recent years.
Because this introductory book contains some of the basics of how to conduct a corpus research project, we do not cover many of the relevant issues that corpus linguistics is presently addressing in its research.
This has an extremely important consequence: both in psycholinguistics and in corpus linguistics, the vast majority of datasets violate the assumptions of the simplest test that an analyst might normally think of first -namely, that the data points are independent of one another.
The first comprises primarily corpus-external evidence, focused on corpus design (i.e., Do samples represent the diversity of text types, topics, etc.
It sometimes happens that parallel corpora contain only texts translated into different languages from another language that has not been included in the corpus, or it may occur that the original text cannot be identified among all the texts.
The best that the corpus compiler can do is to be aware of the variables, confront them head on, and deal with them as much as is possible during the construction of a corpus.
Now, some corpora (e.g., COCA) have POS tags attached to each word, and some corpora (e.g., Michigan Corpus of Academic Spoken English [MICASE], and Michigan Corpus of Undergraduate Student Papers [MICUSP]) do not have that feature in addition to the actual words in a corpus.
With particular relevance to this chapter, a diachronic interest in PDE has developed through the compilation of corpora enabling a precise study of changes in English over the recent past: for example, the Brown family of corpora, Bas Aarts's Diachronic Corpus of Present-day Spoken English (DCPSE) and Mark Davies's Corpus of Contemporary American English (COCA) and, with a longer time span, his Corpus of Historical American English (COHA).
As we can see, the TTR and HTR of both affixes behave roughly like that of Jane Austen's vocabulary as a whole as we increase sample size: both of them 9.1 Quantifying morphological phenomena However, note that -ify has a token frequency that is less than half of that of -ise/-ize, so the sample is much smaller: as in the example of lexical richness in Pride and Prejudice, this means that the TTR and the HTR of this smaller sample are exaggerated and our comparisons in Tables 9.4 and 9.5 as well as the accompanying statistics are, in fact, completely meaningless.
With less data-a smaller sample-the claims one is able to make based on one's corpus findings will be more modest.
To achieve this goal, they compare various keyness measures that are primarily based on the frequency of words in a corpus together with their own "text dispersion keyness" measure, which is based on the number of texts in which a word appears.
And are animate and abstract 3 Corpus linguistics as a scientific method incompatible, or would it not make sense to treat the referents of words like god, demon, unicorn, etc.
Some concordancers can calculate the probabilities of collocations between certain words, rather than simply establishing the list of words which co-occur in the corpus.
How can metalinguistic knowledge be recorded during language documentation and how are they relevant to corpus building?
The concordance arrangement with the search item aligned centrally in the middle of each line provides the main window on to the underlying text for a corpus linguist.
The most useful output format in many areas of corpus linguistics for such a list is generally to have a list that is first sorted according to the frequency in descending order, that is, with the most common words occurring first, and then 'alphabetically' if the same number of tokens occurs for more than one type.
In this section, I discuss how Open Corpus Linguistics can work in practice, and how it contributes to overcoming the pertinent problems addressed in Section 2.
This meticulously annotated VOICE dataset should put to rest the exaggerated claims of word-level variation in ELF, as the corpus findings are unequivocal.
In the case of a special corpus, we like to include text samples produced by non-native users as the aim of this corpus is to highlight linguistic peculiarities that are typical to non-native users.
This is obviously not immediately comparable to corpus sizes given in word tokens since time length does not relate directly to text size: texts may differ in the amount and length of pausing and the speech rate of speakers, so that texts of similar duration in time may exhibit very different numbers of wordform tokens.
Finally, thanks to the existence of a corpus of oral data, corpus linguistics also makes it possible to answer questions related to phonology and sociolinguistics.
A text corpus literally embodies a set of texts, a collection of a certain number of texts for study.
One way to view "linguistic/distributional " representativeness, especially with regard to corpora designed to represent lexical distributions in a given discourse domain, might be through the lens of replicability.
For example, we can calculate the mean rate of occurrence for nouns in a single corpus sample of size n. However, if this sample is biased in some way, as most samples are, then our estimated rate of occurrence (i.e.
There are grey areas in copyright law and copyright infringement is looked at in different ways in different parts of the world, so it is difficult to find universally valid advice on the topic, but generally speaking copyright may prove quite a hindrance for corpus compilation.
For example, the corpus study could help identify one type of common error, and one type of rare mistake.
This fully corpus-driven approach is rather resource-intensive, yet is theoretically important because it makes it possible to account for frequent discontinuous sequences of words that are not associated with a moderately frequent lexical bundle.
What about a file per text included in the corpus?
While in corpus linguistics concordancing has become a mainstream method, in literary criticism it does not seem to play a major role.
As we will see in Chapter 7, TEI tagging can also be used for making more detailed annotations than dividing the text into sentences.
For this reason, sampling is an essential issue that a corpus compiler needs to consider.
Speech corpora are a special type of spoken corpus (see 4.2.2) that are intended for corpus-based investigations into the phonetic and phonological structure of spoken language use.
We will turn to the role of corpus linguistics in sociolinguistics in Chapter 9.
In being clear about what makes corpus linguistics distinct and what it has to offer, the role of corpus linguistics as offering one further methodological tool in the researcher's toolbox in the social sciences Corpus linguistics and social sciences will be easier to make, and the engagement of corpus linguistics with the social sciences will be more easily facilitated.
We should beware that there are a very large number of them and that their more or less suitable character depends on the type of annotation the researcher has in mind.
For the moment, the important point to bear in mind is that corpus linguistics often resorts to a quantitative methodology (see section 1.5) so as to be able to generalize the conclusions observed on the basis of a linguistic sample to the whole of the language, or belonging to a particular language register.
What is relevant for our purposes here is that corpus and tagset developers entertain a considerable degree of freedom in designing tagsets to serve their specific needs, so that tags do not necessarily reflect the absolute exactness of linguistic analysis and classification.
In the British component of the International Corpus of English (ICE-GB), ethnographic information on speakers and writers is stored in a database, and a text analysis program designed to analyze the corpus, ICECUP (the International Corpus of English Corpus Utility Program) can draw upon information in this database to search by, for instance, age or gender.
Several special corpora contain only specific types of text, which could also be included in a general corpus.
It is for this reason that the approach that we will adopt in this book corresponds to a corpus-based approach, considering these as available tools for linguists to be able to test their hypotheses.
Obviously, a corpus tagged for parts of speech could improve the precision of our search results somewhat, by excluding cases like (9c-d), but others, like (9a), 4.1 Retrieval could never be excluded, since they are identical to the ditransitive as far as the sequence of parts-of-speech is concerned.
The book surveys the corpus linguistics discipline, providing a brief overview of 25 years of corpus linguistics studies, including descriptive corpus studies of syntax and semantics, as well as second language acquisition with specialized corpora.
However, as pointed out above, many (if not most) hypotheses in corpus linguistics do not take the form of universal statements ("All X's are Y", "Z's always do Y", etc.
All these universals on language use are statistical universals of language use 1 : They reflect tendencies in language use, and hence, require approaches like corpus linguistics to be discoverable.
After briefly introducing learner corpora, this paper clearly presents the different stages that can be involved in a learner corpus study: choice of a methodological approach, selection and/or compilation of a learner corpus, data annotation, data extraction, data analysis, data interpretation and pedagogical implementation.
Corpus linguistics, as a form of data analysis methodology, can of course be carried out on a number of different operating systems, so I'll also try to make recommendations as to which programs may be useful for the most commonly used ones, Windows, Mac OS X, and Linux.
COCA is not a published article; rather, it is a web-based corpus complied of a 520 million-word database from newspapers, magazines, fiction, and other academic text documents.
This part begins with chapters on the corpus-based description of spoken English, written academic English, and patterns of variation (synchronic and diachronic) among a wider range of spoken and written registers.
In contrast, other corpus studies are more quantitative, subjecting the results of a corpus analysis to, for instance, statistical analyses to determine whether the particular linguistic differences in corpora under study are significant or not.
If there is a presence or absence of grammatical marking due to certain context features, this can be appropriate for a corpus study, particularly as part of the language description processes.
The first option is routinely chosen in the case of automatically annotated variables like Part of Speech, as in the passage from the BROWN corpus cited in Here, the annotation (i.e., the part-of-speech tags) are attached to the data they refer to (i.e., words) by an underscore (recall that alternatively, a vertical format with words in the first and tags in the second column is frequently used, as are various kinds of XML notation).
In the case of corpus linguistics, data are collected in a natural context and then observed, whereas in the case of experimental linguistics, one or more causes are manipulated within a controlled context in order to observe their effects.
However, the practical limitations on corpus building outlined in Chapter 6 are particularly relevant in documentation projects.
Because of this, corpus compilers will often use special characters in a special way so that metadata is regarded as such.
You might want to think about these and other cases in the concordance, to get a sense of the kind of annotation scheme you would need to make such decisions on a principled, replicable basis.
Plain-text editors usually save their files as pure plain text, often using the extension .txt by default, and the more useful ones also allow you to specify a default encoding (which should generally be UTF-8 these days to ensure exchangeability of data), run sophisticated search-and-replace operations based on regular expressions (see Chapter 6), do syntax highlighting for special annotation formats (see Chapter 11), display line numbers, allow the user to run word counts, or even set up macros, i.e.
For instance, a factor drastically lowering the representativeness of written corpora is the fact that we often do not know anything about specific circumstances of text production and editing before the written texts were published.
If you cannot make the full texts freely available for copyright reasons, try to make the corpus as accessible as possible, for example, by allowing queries via flexible search engines such as NoSketchEngine or CQPweb and by publishing word and lemma lists (and ideally, n-gram lists), or by publishing it in password-protected form via a repository that allows for closed-access corpora (e.g., CLARIN).
The ways corpora are used and integrated are also in need of further study: how do controlled, teacher-led corpus tasks compare with the type of more serendipitous, independent hands-on corpus work traditionally associated with Johns' data-driven learning?
Type frequencies are then lower than token frequencies, and the type/token ratio can reveal interesting insights into properties of texts, essentially telling us how lexically variable a text or corpus is.
A final and very important point is to ensure that copyright is respected, either because the text is copyright-free or because the author has provided written consent granting permission to include their text in the corpus.
The alignment is not complete; howeverwhile the critique in favour of introspection denies the utility of corpus linguistics on similar grounds to those of the anti-positivists, the scientia rationalis approach of the Chomskyans is clearly strongly aligned to a logical form of positivism.
The complexity of the corpus architecture results from its annotations: as the data is collected, student annotators iteratively apply a large number of annotation schemes to their data using different formats and tools, including document structure in TEI XML, POS tagging, syntactic parsing, entity and coreference annotations and discourse parses in Rhetorical Structure Theory.
Finally, this corpus should contain a synchronic type of data, corresponding to the current pronunciation, rather than to its diachronic evolution.
These technologies work with natural language text and speech and thereby have much in common methodologically with corpus linguistics, including application to cluster analysis to text corpora; indeed, many of the concepts and techniques presented in the foregoing chapters come from their literatures.
In corpus linguistics a fundamental unit of reference is the wordform.
This brings us to the problem of representativeness: shouldn't we consider all the language use in a given language?
Buysse (2020) compared the oral productions of French-speaking and Dutch-speaking learners of English in the LINDSEI corpus with the productions of native English speakers in the LOCNEC corpus, a corpus which was compiled to be comparable with the LINSDEI (see Chapter 4 for a definition of the concept of comparability).
LDOCE5 is the only dictionary that provides collocation boxes and phrase banks for almost each word, while MEDAL2 deserves special mention for offering collocation boxes at the level of a word sense rather than for the word in general.
Open the copy in your editor and place the cursor at the end of line 66, which should give us enough text to evaluate the tagging.
The reality of contemporary corpus linguistics is that the corpora we rely on, in most cases, are simply too large for manually adding annotation, and the automated annotation of electronic texts has become the primary focus in the development of annotation methods.
When we ask about word types we are asking about how many different word forms there are in the text/corpus.
Furthermore, even if we were not to include an atypical speaker, a corpus could never represent more than a tiny language sample when compared to all the oral and written productions in any language.
Because of these advantages, corpus-based dialect studies have greatly expanded our knowledge of dialect variation, showing that social and regional linguistic variation are far more complex and pervasive than has previously been assumed.
The impact of insufficient metadata is also the topic of Chapter 3 by Mark Kaunisto, who examines the problems seen in the annotation of named entities in corpora, delving into the problems that arise in interpreting corpus data.
Much of this work is contrastive in the sense that NNS language is compared to the target of the learner as well as his L1(s), and an increasing amount of work approaches learner corpus data from a cognitively-informed perspective.
Interval variables are uncommon in corpus linguistics.
Nation provides useful recommendations for researchers through all steps in frequency list development, from designing (or choosing) a corpus to choosing an appropriate unit of analysis (including dealing with homoforms and multi-word units), to determining criteria for word selection and ordering.
The last few years have witnessed a general refinement of the methods of analysis in learner corpus research.
At the end of the data collection process, a corpus such as CHILDES, which aims to explore the language acquisition process of the children, will be created.
This chapter reviews the use of cluster analysis in corpus linguistics to date.
Basing the estimation of the proportion of language varieties on a different source would, again, have yielded a very different corpus design.
No matter the purpose of a corpus collection, all corpora need to provide metadata to ensure that other scholars can also benefit from the corpus.
The problem with the type/token ratio is that it is very sensitive to the size of a corpus (for this reason, the type/token value 9/12 = 0.25 obtained for sentence (1) is in no way representative).
But corpus linguistics is also used for conducting research in fundamental areas of linguistics such as the study of syntax, since it makes it possible to identify the types of syntactic structures used in different languages.
It is applied to a corpus text which has already been annotated and translated, as required.
However, in corpus linguistics, this is even worse: as in psycholinguistics, speakers/writers often provide more than one data point, and we have more than one instance of, say, a constructional choice per verb, but we also have the hierarchically nested/multi-level structure that the corpus comes in: perhaps the speaker (or as a convenient heuristic, the file) is not even the right level of resolution for the current phenomenon -perhaps most of the variability must be explained by looking at registers?
A study of the word (in either or both of its senses) would obviously require that we look at the lemma PAVEMENT, comprising at least the word forms pavement (singular), pavements (plural) and, depending on how the corpus is prepared, pavement's (possessive).
The combination of learner corpus data and experimental data is another area in need of further exploration.
At the time, Rissanen was leading a group of scholars responsible for the compilation of the first diachronic corpus of English, the Helsinki Corpus of English Texts.
Following this procedure, the sample corresponding to the first 18-year-old male participant from Marseille registered in the corpus would be saved in a file called "221001.txt".
Standard deviation tells us the variability of the scores -i.e., the spread of the scores from the central point -and is most often used as a measure of dispersion in studies of a variety of fields, including corpus linguistic studies.
From a practical point of view, the annotation of instances of multilingual practices in learner corpora facilitates their automatic search and identification through corpus software such as concordancers.
Both approaches have their place in different kinds of corpus-based study.
It may be tempting, in corpus linguistics in general and in LCR in particular, to limit the analysis to a quantitative approach.
Part of the manually annotated dataset could also be used as a gold-standard corpus against which to compare the output of the automatic tagging system in order to evaluate its accuracy.
Using a variety of different kinds of corpora, corpus-derived data, and other data, you will learn in detail how to write your own programs in R for corpus-linguistic analyses, text processing, and some statistical analysis and visualization in detailed step-by-step instructions.
If we are examining a series of examples extracted by searching a corpus, for instance, it is often very useful to be able to see, via the metadata, precisely what text each example comes from: knowing, for example, whether a particular instance comes from a novel or from a newspaper report can be a valuable aid to interpretation.
Two aspects of discourse coherence are particularly relevant for which corpus annotation systems have been developed.
Above, we mostly looked at retrieving the data values of our XML data, but of course we want to also use the often detailed annotation that is within the tags.
In short, this study confirmed that corpus analysis can help us to study not only the style of an author or a text genre but also the way in which the different characters in a written production are represented.
While it is relatively easy to carry out lexical studies with corpus-driven approaches (whether you rely on existing corpora or analyze your own corpus), as available tools allow you to extract lexical patterns from the texts, it is quite difficult to apply corpus-driven approaches to do a full lexico-grammatical analysis of texts.
This is particularly true in terms of the accuracy of annotation -both at the word level (e.g.
Nevertheless, there are disadvantages to the corpus-based approach as well.
This is because the purpose of a diachronic corpus is to enable comparison of language use across time spans, and that would hardly be possible if the texts included in each temporal section were vastly different, for example, personal diaries in some time spans and newspaper articles in others.
Yet, in order to ascertain the 'spokenness' of a text/corpus until such time, we may often want to handle contractions as single units.
The transcription of spoken raw data and idiomatic translations of corpus texts are dealt with in Chapter 6 as part of corpus building.
In corpus linguistics, we worry about both type frequency and token frequency (cf.
In this book titled Corpus Linguistics, Context, and Culture by Wiegand & Mahlberg (2019) Corpus Linguistics, Context and Culture explain the possibility of corpus linguistic methods for discussing language manners across a domain of contexts.
Thus, studies based on such corpora must be regarded as falling somewhere between corpus linguistics and psycholinguistics and they must therefore meet the design criteria of both corpus linguistic and psycholinguistic research designs.
In contrast, if we go by number of cases, then cases with very little difference in frequency count just as much as cases with a vast difference in frequency.
For corpus building efforts, in particular, one needs to consider character encoding.
The main distinguishing characteristic of corpus-driven studies of phraseological patterns is that they do not begin with a pre-selected list of theoretically interesting words or phrases.
As is often the case in corpus-based translation studies, parallel data are used with a view to identifying the source-text items/structures that have triggered the use of a given item/structure in the translations (in this case, the presence of a trigger term in the English source texts, such as unit, job, or team).
The authors examine a set of case studies, highlighting issues like part-of-speech annotation, miscategorization challenges, and problems related to metadata and digitized texts in historical databases, such as the 10.5-billion-word Eighteenth Century Collections Online, a database which was originally not compiled for linguistic purposes.
As almost no programs for corpus analysis can deal with documents in 'graphical' formats, such as PDF, or proprietary formats, such as MS Word, the only logical choice for working with corpora is to use either plain text or other types of documents that contain minimal or easily recognisable annotations, such as HTML or XML documents.
One of the most prominent of these is the EAGLES standard for PoS tagging in different languages which yields basically comparable annotations (cf.
Annotation also provides training and testing data for automatic word sense disambiguation.
I've therefore had to restrict my discussion of relevant topics in corpus linguistics to what I consider the most essential ones for beginners.
The assumption underlying basically all corpus-based analyses, however, is that formal differences reflect functional differences: Different frequencies of (co-)occurrences of formal elements are supposed to reflect functional regularities, where functional is understood here in a very broad sense as anything -be it semantic, discourse-pragmatic, etc.
As constructing the BNC was a major exercise involving the digitisation of very large amounts of text, sorting out meta-information as much as possible, and PoS tagging and annotating the data in a number of ways, the care taken in checking and correcting the final result of the tagging has, at least to some extent, been sub-optimal.
The kinds of texts and interactions that are recorded and transcribed as part of a language documentation project may not be the kinds of texts one would record if planning a corpus.
Diversity is a useful safeguard for a monitor corpus against skewed representation.
The arrival of these tools has greatly accelerated research in corpus linguistics.
Furthermore, some journals that appeared in the middle of the time span became actively cited by the corpus linguistics papers since then.
Corpus linguistics, by contrast, is not concerned uniquely with any single facet of language, but rather is an approach which can be applied to many or all aspects of language.
Unlike the sorting options we had for concordance lines, where we were able to sort according to n number of words to the left or the right quite freely, in this case, we have a more limited set of options, based on the options for combinations of output for types and frequencies, as already mentioned above.
As in the case of contrastive studies, translation studies has benefited from the availability of multilingual corpora, as well as theoretical and methodological advances in corpus linguistics.
An example drawn from recent research on language acquisition will better illustrate the impact of annotation choices.
This corpus-based research approach is opposed to an approach which considers corpus data as the only point of reference, both in a theoretical and a methodological sense.
The "Counter" object is a very fast and memory efficient "key-value" data structure that is used to store the word types as keys and their growing frequencies as values as each corpus file is processed.
Even our definition of corpus linguistics makes reference to this fact when it states that research questions should be framed such that it enables us to answer them by looking at the distribution of linguistic phenomena across different conditions.
What we might recommend today (a certain corpus tool, a certain programming package, or module) might not be available three years from now, or might have advanced so much that our information is out-of-date.
Frequency refers to the number of instances in which a node and collocate occur together in a corpus.
Finally, sentences may contain a pragmatic annotation of the speech act involved (e.g.
In addition, a concept known as 'semantic sequences' was developed -this suggested that concordance lines can be read to identify 'what is often said', thus moving from lexis to grammar to discourse.
One issue that needs consideration is whether in the context of a specific research design it is more appropriate to compare two texts potentially representing different ideologies directly to each other, as Rayson does, or whether it is more appropriate to compare each of the two texts to a large reference corpus, as the usual procedure in keyword analysis would be.
Ihrmark examines various methods employed in corpus stylistics, such as keyword analysis and n-gram searches, and their reliance on genre categorization for comparative studies.
However, even despite efforts to preserve genre balance, any diachronic comparison across this timespan has to take into account that the language used in such a corpus changes from a relatively standardized written language with more or less direct roots in a contemporary vernacular to a functionally impoverished and much more writing-dependent language used mainly in religious contexts and as a lingua franca among European scholars and scientists.
Next, the predictions must be tested -in the case of corpus linguistics, corpora must be selected and data must be retrieved and annotated, something we will discuss in detail in the next chapter.
Since these words don't actually tell us much about the lexical richness or the content of a text/corpus, anyway, they're often regarded as redundant and thus lists that exclude them, at least in theory, ought to help us develop better insights into the nature of any text or corpus under investigation.
Some collocation measures such as MI highlight rare exclusivity of the collocational relationship, favouring collocates which occur almost exclusively in the company of the node, even though this may be only once or twice in the entire corpus.
Free software such as VocabProfile online (www.lexutor.ca/vp) or AntWordProfiler offline (www.antlab.sci.waseda.ac.jp/) allows a teacher to input a text which is then returned with the lexis color-coded according to the frequency of each word in the BNC or COCA corpus.
Corpora often come with "read me" files where the corpus design is accounted for.
How should we ideally approach the copyright problem now if we want to follow the principles of Open Corpus Linguistics?
Once annotated, the elements of the corpus can be searched by their POS tags by means of a concordancer.
Typical morphosyntactic annotations are part-of-speech tagging (PoS tagging) which captures the word class and other morphological and syntactic properties of token wordforms in a corpus; such corpora can be loosely classified as tagged corpora.
In the following section, we will provide some guiding principles for how to go about answering your research question(s) using a register analysis framework and corpus methods.
Can you also think of some ordinal or interval IVs or DVs that might be used in corpus research?
Both from a theoretical and application point of view, corpus linguistics is a powerful method, which is empirical, scientific, realistic, authentic, open-ended, verifiable, and reliable.
As we saw at the beginning of the chapter, these decisions influence the annotation process and must be clearly documented.
By listing properties that a group must have to count as an organization in the sense of the annotation scheme, the decision is simplified considerably, and by providing a decision procedure, the number of unclear cases is reduced.
Moreover, it is advisable for corpus linguists to be cognisant of such concepts and the debates surrounding them, as these are likely to colour the perception that social scientists form of corpus linguistics.
Imagine further that corpus linguist is looking at that list to identify verbs and adjectives within a certain frequency range -maybe because he needs to (i) create stimuli for a psycholinguistic experiment that control for word frequency, (ii) identify words from a certain 100 S. Th.
As we saw when discussing the case of pavement in Chapter 3, a corpus query for a string of characters like ⟨ pavement ⟩ may give us more than we want -it will return not only hits corresponding to the word sense 'pedestrian footpath', which we could contrast with the synonym sidewalk, but also those corresponding to the word sense 'hard surface' (which we could contrast with the synonym paving).
In order to allow the students to gain experience in "doing" corpus linguistics, we have intentionally limited the corpora and software used in the corpus projects.
Rather, we hope to summarize and evaluate what we have learned about language use and variation from previous corpus-based research, to identify and discuss the most important of those previous studies and research findings, and to discuss the methodologies that work best for such research.
Given this assumption, the procedure described here clearly falls under our definition of corpus linguistics.
Furthermore, as real corpus linguistics is not just about getting some 'impressive' numbers but should in fact allow you to gain real insights into different aspects of language, you should always try to relate your results to what you know from established theories and other methods used in linguistics, or even other related disciplines, such as for example sociology, psychology, etc., as far as they may be relevant to answering your research questions.
The second important question concerns the size of the sample that will be included in the corpus.
In concrete terms, the changes are noticeable on the level of educating students about the basics of corpus linguistics: with the evolving nature of the field, there is a constant need to update course materials to provide novices with an overview of both the possibilities and challenges relevant to corpus study.
Such a representative instance/word form in a frequency list is referred to as a type, and each individual occurrence of a particular type as a token, which is why splitting a text into individual word-level units is also referred to as tokenisation.
In general, it is preferable to choose an annotation scheme as neutral as possible from a theoretical point of view and, in any case, to stick to categories clearly identified and widely accepted in the literature.
This involves manually annotating samples of corpus text as examples of how the new annotation categories should be assigned.
This research specifies that all forms of the lemma acteur will be retrieved from the corpus when a word tagged as an adjective appears to its right.
Many of the most pertinent issues and methodological challenges faced in multimodal corpus research are tied to the construction and availability of resources for further research.
XML is a hierarchical format (that lends itself well to representation as a tree that does not allow cross-nesting/overlapping) in which you add to data markup and annotation in the form of either start and end tags (which may contain attribute-value pairs), or just start tags (with attribute-value pairs), and in fact you have seen examples above that are similar to that annotation already.
It is therefore crucial to the success of any corpus undertaking that accurate information be kept about each text to be considered for inclusion in the corpus.
To come up with a reliable annotation scheme for this categorization task would be quite a feat.
This is, in short, why corpus linguistics matters.
While we acknowledge the use and importance of combining these two approaches, in the rest of the book we will focus on the quantitative approach to corpus linguistics, which poses its own theoretical and methodological challenges.
Notwithstanding these issues and challenges, parallel corpus research to date has yielded invaluable empirical insights into cross-linguistic contrasts and translation.
Co-reference relations are probably the prime example of implicit information that can only be determined by human interpreters and that for this reason require corpus annotation to make them explicit.
Corpus-based research on grammatical variation is a wide research area, so the review we are offering is somewhat selective.
The second part of the book focuses on some of the tasks you might do in conducting your own corpus study including querying a corpus and evaluating the results (Chapter 5), and composing and building your own corpus (Chapter 6).
The relative frequency can therefore be considered as the mean frequency ('mean' is a statistical term for a type of average; see Section 1.3)that is, the mean of the frequencies of the word in hypothetical samples of x tokens from the corpus, where x is the basis for normalization (in this case one million).
In that area, corpus data are both longitudinal and cross-sectional and in order: (i) to discern longitudinal trends in the data for one or more children, (ii) to identify children at comparable levels of development for cross-sectional analysis, or (iii) to increase sample sizes and/or filter out outliers, it is often useful to be able to group the temporal data for children into different stages.
In lemmatization, each orthographic word encountered in a corpus is assigned a lemma, or 'base form', which provides a level of abstraction from any inflection that might appear in the original orthographic word.
Different XML tags are used for markup, metadata and annotation.
In this study, semantic domains were annotated and analysed using UCREL's (University Centre for Computer Corpus Research on Language, based at Lancaster University) Semantic Analysis System (USAS) in Wmatrix.
This chapter provides you with an opportunity to use readily available corpora to conduct corpus linguistics projects.
The third thing which can be added to the raw text of a corpus is metadata.
The other form of corpus annotation that is often applied automatically is parsing, the annotation of syntactic structures and relations.
In corpus linguistics, parsed corpora serve two purposes: to enable the analysis of larger syntactic structures, such as phrases and clauses, by individuals conducting linguistic analyses, and to provide testbeds for those in the area of natural language processing interested in the development of parsers.
While the type-token ratio differs as a measure from the typical calculated normalized frequencies, its relationship to text length still bears discussing in this context.
The keywords approach is a method to compare two word frequency lists using statistical metrics in order to highlight interesting items whose frequency differs significantly between one corpus that is being analyzed and a much larger reference corpus.
Against this background, it is interesting to note that Rissanen felt compelled to discuss some potential pitfalls related to historical corpus linguistics two years prior to the publication of the Helsinki Corpus.
The results received from a study based on learner corpus can be used for educational purposes.
As such, they can refer to either a particular format for enriching a text, so as to r categorise parts thereof, or r make these parts more salient, or to the actual process of applying this form of annotation.
Results of this analysis demonstrate that the list performs considerably better coverage-wise in academic writing than in other written genres, in both in the academic portion of the corpus from which the list was constructed (13.8%), and a different academic sub-corpus from the BNC (13.7%) (compared to 8.0% for newspaper articles and 3.4% for fiction).
In the example of a sociolinguistic research looking at the use of swearwords by men and women we would get a p-value that would give us the probability of seeing the observed or even more extreme difference between the two groups if the null hypothesis were true, that is, if there really was no difference in the population and the difference observed in the corpus (sample) was merely due to chance as the result of a sampling error.
While it is not found in a large number of studies, its use seems to be on the rise to answer questions in corpus-based variationist linguistics.
Why is it important that a standard system, such as the Text Encoding Initiative, is developed to ensure that everyone follows a standardized system of annotation for representing metadata and textual annotation?
But even though it is possible to include coherent text samples in a corpus, creators and users of corpora simply have to acknowledge that corpora are not always suitable for many types of discourse studies, and that those wishing to carry out such studies will simply have to assemble their own corpora for their own personal use.
Corpus linguistics is an extremely versatile methodology of language analysis applicable in a wide range of contexts, in linguistics, social science, digital humanities and elsewherethe book thus aims to facilitate meaningful use of corpora for as wide a range of users as possible.
Corpus retrieval software, our focus here, is intended to facilitate exploration of the annotated corpus data using a variety of quantitative techniques.
We'll discuss issues like this further in Chapter 9, but, for the moment, suffice it to say that the PoS tagging of such languages normally requires programs to artificially introduce spaces during the so-called tokenisation process.
This would mean that the hapaxes in any sample would overwhelmingly cease to be hapaxes -the larger our corpus, the fewer hapaxes there will be.
Each sample was 2,000 words in length, enabling valid comparisons between the different registers in the corpus.
Run the tagging operation, and copy and paste the results into another text file in your editor.
In a certain study titled "Corpus Linguistics and its Applications in Higher Education" by Fuster Márquez & Clavel Arroitia (2010), they are set out to depict implied essentials of corpus linguistics and its progress in relevance to theoretical linguistics and its implementations in modern teaching pursuits.
Finally, we will illustrate the use of inferential statistics by presenting a commonly used test in corpus linguistics, namely the chi-square test, which determines whether frequency differences between categories are significant.
There is a variety of very good reasons for this, some of them related to corpus linguistics, some more general.
A synchronic or diachronic corpus is representative to the extent that the study of that corpus can replace the study of the textual universe it represents.
The result of this analysis is a collection of language patterns that are recurrent in the corpus and either provide an explanation of language use or serve as the basis for further language analysis.
First, intelligent editors to support manual annotation and second, automatic taggers which apply a particular type of analysis to language data.
A much simpler way to search the data, though, is to use a line-based concordancer, such as my own Simple Corpus Tool, where you can both look at the whole file easily once it's been loaded and also devise a suitable regex that will match two occurrences of the tag used for prepositions occurring in a row at the end of the line.
This is probably quite common -not only in corpus linguistics but also in other domains, for example, when it comes to publishing programming scripts.
Because learner corpus and SLA researchers use their data to study L2 production and development, it is of utmost importance that the data are valid, that is, they represent "authentic" L2 production, which means that the data must stem from the studied learners' own language production.
In the final chapter (Chapter 9), we ask students to consider more advanced types of corpus research with the hope that this book will serve as an introduction to the field and encourage students to pursue these ideas at a more advanced level and perhaps even impact the field in significant ways.
These texts are then reviewed and their POS tags corrected, creating more data for the next round of training and annotation (cf.
Very similar discourse is often found in corpus linguistic literature in support of a corpus-based approach.
In the case of corpus studies, this is not a problem, however, since the files will not be read by humans, but processed by a concordancer.
Although size does matter on the scale of the 'reliability' of a corpus, in reality, a corpus, however big it may be, is a small representation of the vast amount of speech and text varieties that are produced in a language.
A heterogeneous monitor corpus includes text samples from all subject areas and the individuality of the source of a text and its writer are well attested.
In the early 1960s, when computer technology was not conducive to collecting a large amount of language data, a corpus of one million words was considered good enough (e.g., Brown Corpus, Lancaster-Oslo-Bergen (LOB) Corpus, KCIE (Kolhapur Corpus of Indian English)).
As can be seen from the above discussion, the 'web for corpus' approach is rather more complex than the previously described 'web as corpus surrogate' approach.
There are various ways of processing (e.g., data analytics, concordance, collocation, keyword marking, local-word-grouping, lexical clustering, lemmatization, morphological processing, sentence processing, and named entity marking), which are applied to a corpus with appropriate technological supports.
The utility of POS tagging, especially for languages like English, is that many words are ambiguous in terms of their part-of-speech.
We'll explore plain-text-based file formats, such as HTML and XML, that may contain such markup further in later sections, as well as discussing their use(fulness) for linguistic annotation/analysis.
The result was a spreadsheet with one row for each concordance line and one column for each variable considered for analysis.
Most relevant for corpus linguistic studies of variation in language use is the register dimension: this is characterised by the situational properties of texts.
The work produced by such researchers often aligns much more strongly with naturalism than corpus linguistics does.
An important difference between sampling in corpus linguistics and in other sciences is that a corpus is never a random sample of the population.
While both the five-word text and the 1,000-word text have the same rate of occurrence of first-person pronouns, surely the 1,000-word text with 200 first-person pronouns is much more unusual than the five-word text with one first-person pronoun.
We must agree that quantitative data retrieved from a corpus is necessary not only in language technology but also in many areas of linguistics (e.g., speech analysis, lexicography, discourse analysis, language teaching, stylometrics, translation, and language planning).
Imagine a corpus that is overall fairly representative in terms of size and composition, but lacking metadata.
It should be noted that this is fundamentally rather similar to the procedure of a key items analysis, except that we are comparing the vicinity of the node to the rest of the corpus, rather than comparing two different corpora, and some of the same considerations apply.
Therefore, we will create a subdirectory into which we will save interim output after each corpus file, and after having gone through all 4,049 corpus files we will then merge the 4049 output files into the one data structure we want, results.df.
As a corpus community, we need to agree on better guidelines and expectations for filtering results in terms of minimum frequencies and significance and effect size values rather than relying on ad hoc solutions without proper justifications.
Logistic regression models are easier to fit and easier to interpret than multinomial regression, and are what you will see most commonly in multivariate quantitative corpus linguistics.
The exploitation of a native corpus, used in combination with the learner corpora, makes it possible to see how the learner data are situated in relation to a certain reference norm (without being limited by it), whereas the inclusion of the L1 variable gives a glimpse of the possible influence of the mother tongue.
It is for this reason that it is difficult to maintain a perfect balance between the different parts of these corpora, whose representativeness cannot be fully guaranteed.
This stage notably involves the transformation of raw figures corresponding to the number of occurrences observed in the corpus into figures reporting relative frequencies, following a base of normalization.
Corpus enrichement is an important way forward, and this applies not only to metadata but also to linguistic annotation.
After assessing what is present in the collection, then you can plan for what you would like to add and focus on that in your corpus building enterprise.
It will be interesting to see whether some of the major insights generated within monomodal, or text-based, corpus linguistics will be replicated once we are able to truly analyse multimodality at scale.
However, there still is no one-size-fits-all solution to the problems caused by text length and short texts in quantitative text-analytic corpus-linguistic studies.
Through this study, we were interested in finding out how learners of English from different mother-tongue backgrounds use DMs, and how their use compares to that of native speakers (quantitatively and qualitatively), but we also wanted to demonstrate that when doing learner corpus research one should consider individual data in addition to pooled data.
The final section then takes the notion of adding linguistic information to your data further, and illustrates how to enrich corpus data using basic forms of XML in order to cyclically improve your analyses or publish/visualise analysis results effectively.
In contrast to a learner corpus (see Chapter 23 this volume), an ELF corpus seeks to include speech in a natural, often complex mix, rather than selecting for given L1 backgrounds and comparable proficiency levels.
Although FLOB (The Freiburg LOB Corpus of British English) and FROWN (The Freiburg Brown Corpus of American English) are not historical corpora in the sense that the Helsinki and ARCHER Corpora are, they do permit the study of changes in British and American English between the periods 1961-1991.
The data provided by these contrastive corpus-based studies are not only useful in theoretical linguistics for understanding how languages work, but may be helpful for other applications, notably for the development of tools such as bilingual dictionaries (see section 4.6).
We should also be careful to choose the French corpus and to determine a sufficiently long time period, for example from 1800 to 2000.
Parts IV-VI aim to offer a user-friendly introduction to the variety of statistical techniques that have been used, or have started to be used, more extensively in corpus linguistics.
By regular addition of synchronic data, a corpus attains a diachronic dimension.
Type frequency -a count of all the unique types there are of something in a corpus So, in a corpus of a million words, there will be a million word tokens.
For instance, in a spoken corpus, who said what is often critical to an analysis: only the markup of utterance boundaries and speaker identities allows us to know this.
More drastically, LOB and FLOB treat some sequences of orthographic words as multi-word tokens belonging to a single word class: in front of is treated as a preposition in LOB and FLOB, indicated by labeling all three words IN (LOB) and II (FLOB), with an additional indication that they are part of a sequence: LOB attaches straight double quotes to the second and third word, FLOB adds a 3 to indicate that they are part of a three word sequence and then a number indicating their position in the sequence.
Other tools, such as WordSmith and BNCweb, permit the user to manually categorize concordance lines and this can be viewed as a form of corpus annotation.
For example, imagine a very small corpus that contains 11,000 tokens, including the type homeostasis which appears only once in the corpus.
Analysis of corpus texts shows that apart from pure intralinguistic information, a corpus also carries several kinds of extralinguistic information.
The integration of learner corpus data (and use of error correction exercises) is another case in point.
The chapter opens with a history of programming in the field of corpus linguistics and presents various reasons why corpus linguists have tended to avoid programming.
The case study was also meant to provide an example of a corpus-based design where it is more useful to operationalize one of the constructs (Frozenness) as an ordinal, rather than a nominal variable.
All words without a subscript are new types and hapax legomena at the point at which they appear in the text; if a word has a subscript, it means that it is a repetition of a previously mentioned word, the subscript is its token frequency at this point in the text.
Recall that corpus linguistics includes both quantitative and qualitative analysis.
Both of these mistakes basically may cause you to lose valuable time that could be spent on actually analysing your data, thus 'throwing away' one of the most important advantages of using corpus linguistics as a methodology, which is that it allows you to save a significant amount of time finding a large amount of potentially relevant and interesting data quickly.
If a text is collected, and saved for later computerization and annotation, the individual who collected the text may not be around to answer questions, and information about the text may consequently be lost.
However, under the definition of corpus linguistics adopted in this book, Sinclair's observations would be just the first step towards a full analysis.
Finally, whether you work on well-researched or lesser-studied languages, considerations of corpus design will also apply to compilations of small, focused research corpora which may or may not draw on larger, pre-existing corpora.
It is therefore important that we always ensure we understand how the text of a corpus is internally structured, and to critically relate these to our linguistic analyses and categories.
Another large-scale annotated corpus study was carried out to explore the question of the placement of the attributive adjective, either before or after the noun it modifies.
The three chapters included in Part I provide this discussion, dealing with current issues relating to corpus design and composition, tools and methods for the linguistic analysis of corpora, and quantitative research designs and statistical methods used to describe the patterns of use across corpora.
Perspectives on the matter vary between scholarly approaches: the criteria for determining what is "good data" -for example, considerations on the accessibility and processing of the data -can be quite different in Digital Humanities and corpus linguistics.
Bilingual or multilingual parallel-corpus analysis is inherently more complex than monolingual corpus analysis, requiring a descriptive framework includingcruciallya tertium comparationis, or common platform of comparison, and for this reason mostly relying on the direct observation of parallel concordances.
The plain-text words of the corpus are sometimes called the raw data of the corpus.
Corpus linguistics depends on computer science for various reasons.
In some cases, the variables and their values will be provided externally; they may, for example, follow from the structure of the corpus itself, as in the case of british english vs. american english defined as "occurring in the LOB corpus" and "occurring in the BROWN corpus" respectively.
In particular, a corpus-based approach is especially conducive to the analysis of quantitative grammatical variation, because it allows for large samples of natural language to be collected for analysis, and for dialect variation to be observed and compared across a variety of communicative situations.
For POS tagging, for example, the many spelling errors found in written learner corpora have been shown to lower the accuracy of POS taggers (de Haan 2000; Van Rooy and Schäfer 2002).
For all the words studied, the results indicated significant differences between the results of the corpus study and the entries of different French dictionaries.
Although formal meta-analysis is now fairly common in a number of disciplines such as psychology, second language acquisition, medical science etc., its application in corpus linguistics has been problematic due to the general lack of reporting of effect size measures.
Then, we will present two types of descriptive statistics that, respectively, make it possible to measure lexical diversity (the type/token ratio), and to calculate lexical dispersion in a corpus.
In this chapter we introduced four basic analytic methods, but a number of more complex approaches, often based on advanced corpus statistics, have been introduced over the years, such as collocation network analysis, advanced multivariate statistics applied to concordance analysis and corpus frequencies, and so on.
The case study provided in Section 7 illustrated a common focus of corpus-based studies of writing and increasingly speech, namely stance devices; it also provided an example of a new direction in spoken discourse analysis in its exploration of variation within spoken interactions.
While the raw text of an unannotated corpus has its own unique value, (wisely) annotated corpora offer great advantages over the raw text when it comes to the investigation of linguistic phenomena.
An area in which this is very obvious, and which has therefore become one of the most heavily researched areas in corpus linguistics, is the way in which words combine to form so-called collocations.
First, we removed the content posted before 2016 in order to limit the likelihood of picking up diachronic effects; the tweets in the original corpus date back to 2006.
What I would have wanted and needed when I took my first steps into corpus linguistic research as a student is an introductory textbook that focuses on methodological issues -on how to approach the study of language based on usage data and what problems to expect and circumvent.
However, as we have seen, the keyword list is a result of multiple decisions in the process starting with the selection of the reference corpus and finishing with the choice of the particular statistic.
If you don't have such skills (yet), perhaps the best solution is to continue doing corpus-based studies.
Representations of foreigners were largely concerned with political institutions like the foreign office, although a sample of 21 out of 100 concordance lines taken at random (using an online random number generator) showed negative constructions of foreigners involving stereotyping, implying they were taking up British resources or jobs, or controlling British interests.
A corpus usually represents a sample of language, i.e.
The resulting sorted list contains two kinds of keyword: positive (those which are unusually frequent in the target corpus relative to the reference corpus) and negative (words which are unusually infrequent in the target corpus).
Thus, I will attempt in this chapter to sketch out a broad, and, I believe, ultimately uncontroversial characterization of corpus linguistics as an instance of the scientific method.
Looking at the treatment of morphological productivity in different acquisition models, including generative and usage-based models, the authors put forward a number of hypotheses, which are then tested against a learner corpus.
The reason that a carefully selected range of genres is not important in this corpus is that the corpus is not intended to permit genre studies but to, for instance, "train" taggers and parsers to analyze English: to present them with a sufficient amount of data so that they can "learn" the structure of numerous constructions and thus produce a more accurate analysis of the parts of speech and syntactic structures present in English.
In other words, the specific texts included in a corpus are meant to stand for language use more generally, that is, represent it.
Very few publications discuss specific requirements for extending corpus retrieval software (c.f.
There is yet another distinction that needs to be applied to the concept of word, since it is fundamental for corpus linguistics.
For example, good practice for building a corpus is to accurately document the type of language it contains.
Together, this research explicitly contradicts the view that corpus linguistics takes an impoverished, decontextualized view of texts and replaces it with a detailed picture of how students and academics write in different genres and disciplines.
A crucial characteristic of a diachronic corpus is that the texts it contains are comparable across periods.
In this parlance, a frequency list lists the types in one column and their token frequencies in the other; often you will find the expression type frequency referring to the number of different types attested in a corpus (or in a 'slot' such as a syntactically defined slot in a grammatical construction).
While corpus linguistics has very little to say to those with interests in, for instance, generative grammar, it is of great value to those who are interested in studying pragmatics: how language is used.
Suppose we want to know whether the connective car was used consistently by the 10 students who compiled a dossier in the Littéracie avancée corpus (L2_DOS_SORB sub-corpus).
These reflect rough distinctions of texts in terms of their external The rationale behind this kind of corpus composition is that each cross-category (section/year) has the same share of the overall corpus data and will thus contribute approximately equally to results from any corpus query.
This issue, too, can be illustrated with the help of the same sample corpus from Hiltunen (2021), Corpus A, which comprises complete issues of the four newspapers sampled at 10-year intervals.
It also makes the interesting methodological point that, through corpus annotation, categorization of the data can be made explicit and available to other researchers.
In contrast, what a corpus study will not help you to do is establish with certainty the factors influencing the number of errors.
Representative of another type of language register, TED conferences have recently made it possible to create a large parallel corpus.
When a researcher taking part in corpus-based research is recognized, his/her names are employed as key terms to deal for research offspring in basic scholastic data.
Under 'Save as type:', select 'Web Page, HTML only' or 'Webpage, HTML only" for the type.
Further typical options include whether to consider punctuation as a boundary to collocation window spans or impose minimum frequencies on collocates or node words.
It is a line plot that shows positions of all tokens of a vector on the x-axis (as in Section 5.1.2) and all type frequencies on the y-axis; in other words, each point's y-coordinate divided by each point's x-coordinate provides the point's type token ratio.
Corpus studies do not make it possible to draw this type of conclusion.
As we move through the text word by word, initially all words are new types and hapaxes, so the type-and hapax-counts rise at the same rate as the token counts.
The general argument is that texts produced by native language users should get priority over texts produced by nonnative users in a general and monitor corpus.
Mastering these terms will make reading of the rest of the book, and many papers in corpus linguistics, much easier.
This is most straightforwardly accomplished within the context of a concordance analysis.
Since this era, advances in corpus linguistics have resulted in the creation of many different types of corpora containing authentic texts ranging from spontaneous conversations to technical writing in the social and natural sciences.
In these cases, the most common operationalization strategy found in corpus linguistics is reference to a dictionary or lexical database.
The more specific concern of corpus linguistics is to account for these decisions by systematically investigating related variants and conditions on their choice.
In the following we focus on longitudinal designs, which are more prevalent in research on language development, but many issues such as annotation layers or metadata are relevant for both corpus types.
Part II is organized as a progression of the linguistic levels, beginning with corpus-based analyses of prosodic characteristics, moving on to chapters dealing with lexical characteristics (keywords, collocations, and phraseology), followed by chapters on grammatical features (descriptive grammar, grammatical variation, grammatical change, and the intersection of grammar and lexis), and finally concluding with chapters on the corpus-based study of discourse functions and pragmatics.
The route proposed should, in our view, maximise corpus linguistics' engagement with disciplines across the social sciences.
Many corpus interfaces and tools allow for the analysis of the strongest collocates of words based on different methods of assessment: Mutual information, log-likelihood, T-scores, raw frequency ranking, and so on.
There are many linguistic and technical issues relating to corpus sanitation (e.g., text normalization, orthographic error correction, spelling error correction, real word-error correction, grammatical error correction, punctuation error removal, and tokenization).
For example, although spoken language is considered 'primary' in many senses, corpus architectures usually treat aligned audio/video information (A/V for short) as a type of annotation, and this can have consequences for corpus architecture.
Like the corpus compiler, the corpus analyst needs to consider such factors as whether the corpus to be analyzed is lengthy enough for the particular linguistic study being undertaken and whether the samples in the corpus are balanced and representative.
We will return to annotation in the next section; let us deal briefly here with metadata and markup by discussing some examples of how they may be used to enhance a linguistic analysis -and, thus, the reason corpus builders invest effort into adding them into the corpus in the first place!
However, they are limited in terms of scope (e.g., only two core functions of corpus analysis toolkits are presented) and functionality (e.g., the KWIC tool does not include a sorting function).
Although we can use collocation as a general covering term for different kinds of cooccurrence phenomena observed in corpora, we can also use a set of more specific terminology, developed largely by John Sinclair and colleagues, to distinguish co-occurrence patterns at different linguistic levels.
When "doing" corpus linguistics, students need to gain experience searching a corpus and interpreting the results of their corpus searches so that they can use this information to explain why their analysis and findings are important or relevant.
Very few of the existing techniques are tailored for the specific methods in corpus linguistics, and in addition, the existing corpus visualisations do not scale to large bodies of texts, a key requirement to tackle the growing size of corpora.
Imagine, finally, the frequency range he is currently interested in is between 35 and 40 words per million words and, as he browses the frequency list for good words to use, he comes across an adjective and a verbenormous and staining -that he thinks he can use because they both occur 37 times in the Brown corpus (and are even equally long) so he notes them down for later use and goes on.
An alternative explanation for the emergence of the introductory references of corpus linguistics would be because the period was the optimal time for establishing corpus linguistics as a part of linguistics after a "hodgepodge" multi-directional development of corpus linguistics.
In the context of corpus linguistics, their use is somewhat questionable, and the reader should be aware that it is regrettably not unusual to find these tests employed in ways which do not fit well with their assumptions.
Some of the entries contain (usually at the end of the entry) information about the typical errors made by learners and those errors come from the Cambridge Learner Corpus.
The command also helps you to calculate the type/token ratio (see Chapter 8), which represents a measure of lexical diversity.
Corpus descriptions are sometimes published in peer-reviewed journals, especially if the corpus is breaking new ground (as is the case in Representative Study 2 below), so that the research community can benefit from discussions on corpus design.
The review is in two main parts: the first part outlines work on quantitative methods in corpus linguistics to serve as a context for the second, which deals with the cluster analytic work specifically.
We showed how we can do a corpus-based study with already identified language features, whether doing a lexical study or searching for the use of particular grammatical patterns.
However, in this chapter, the scope needs to be limited carefully to computational methods and tools employed for corpus linguistics research.
The corpus analysis involves recording all multiword combinations with specified characteristics that appear in a corpus, usually tracking the frequency with which each combination occurs.
Corpus linguistic research offers strong support for the view that language variation is systematic and can be described using empirical, quantitative methods.
The 'n' here theoretically means 'any number of', but usually n-grams are restricted to relatively short sequences, such as bi-or tri-grams, because finding n-grams can be very expensive computationally as long lists have to be kept in memory, and their size grows with increasing length of a text/corpus, and each increment of n. Thus, if you're working on a relatively sizable corpus, be prepared to wait for a few minutes for n-gram calculations to finish.
Grammatical markup is inserted when a corpus is tagged or parsed.
What does this mean for corpus analysis?
The fifth shows how these methods can serve as the basis for generation of linguistic hypotheses, and the sixth reviews existing applications of cluster analysis in corpus linguistics.
This book has been designed as study material for teaching corpus linguistics at university initiatory phases, as well as a tool for students wishing to be trained in the use of corpora.
Unlike World English corpora (see Chapter 21 this volume), an ELF corpus does not seek to capture a local or regional variety of English, which would then lend itself to comparisons across others of the same kind.
Although they are not necessarily viewed as such, some existing techniques in corpus linguistics can be considered as visualisations.
This might begin with the number of hits from a corpus query using regular expressions to find matches of the phenomenon in question, how these were winnowed down by disregarding false positives, the result of sampling procedures or data transformation procedures, etc.
However, with all p-values nearing zero and no thresholds of log-likelihood included, it is unclear whether the most salient semantic categories were, in fact, included and populated.
The main point really is that interested users have access to the data and can understand how it can be used, which in turn requires metadata on the corpus as a whole.
For the comparison of the first and last recording, we just generate regular scatterplots of type-token ratios against mlus, but we jitter the points to avoid overplotting and add the usual grid.
In cross-linguistic research, it is of paramount importance to know, for instance, whether a given text was translated from Spanish into German or vice versa, because corpus studies have shown that translation direction influences translation choices, and hence the linguistic make-up of translated text (e.g.
For example, a frame occurring 200 times with 25 distinct fillers would have a type-token ratio of .
It should also provide information on the corpus processing that preceded the annotation process.
This largely depends on the research question and the type of study (research design) we are dealing with.
For language acquisition research, corpus data and, to a certain extent, psycholinguistic experiments are the only sources of data available, and historical linguists must rely completely on textual evidence.
We argue in this chapter that bootstrapping is underused relative to its potential in corpus linguistics, and that quantitative corpus linguists would do well to add this tool to their repertoire.
When comparing two groups such as two subcorpora, the effect size measure Cohen's d is often used (see Section 6.3).
On the one hand, corpus linguistics has the advantage of favoring the observation of natural data, that is, those which are not influenced by an experimental context.
The great advantage of part-of-speech tagging is that it can be done automatically with almost the accuracy of a manual annotation, regardless of the amount of text to be annotated.
Another recent trend in the corpus linguistics research, according to the current study, was the emergence of large web-based corpus (e.g., COCA).
For example, in the spoken corpus of French from French-speaking Switzerland OFROM (see Chapter 5), the type/token ratio is 0.02, whereas in the French section of the children's literature corpus ByCoGliJe, it is equal to 0.04.
As already mentioned, corpus linguistics has been criticized in relation to its suitability for the study of speech acts.
When talking about a query in a particular corpus, I will use the annotation (e.g., the part-of-speech tags) used in that corpus, when talking about queries in general, I will use generic values like noun or prep., shown in lower case to indicate that they do not correspond to a particular corpus annotation.
But as corpus-based research began to expand during this period, its emergence during the dominance of generative grammar in the 1960s created quite a controversy, since to some linguists, corpus linguistics reeked of behaviorism.
But while many features of writing can be annotated in a written corpus with TEI-conformant markup, annotating all of these features may prove to be unnecessary.
Corpus-based typology can be represented variably in corpora, and this means that case marking needs to be investigated as a variable of language use as well.
For example, by making a corpus study, it is possible to determine in which textual genres the passive voice is most commonly used.
One of the key questions for Corpus Linguistics is how a corpus might satisfactorily be 'analysed'.
If, for example, the aim of a spoken corpus is to study the lexical specificities of a language variety, the prosodic information contained in the interactions will be of little use.
Much work in corpus linguistics has oriented itself towards real world problems, including in areas such as climate change research (e.g.
In practical terms, collocation as a method refers to the counting of the co-occurrence of two words in a corpus depending on their relative proximity to one another, and usually includes the calculation of a statistic or metric to assign significance values to the amount or type of co-occurrence relationships.
The first type of annotation targets individual sound segments and is essentially a type of transcription of spoken language raw data, phonetically a very close and exact one.
With more words, the frequency of each word increases and since the keyword analysis is based on frequency in one corpus over another (see tutorial), this may be problematic if you have different-sized corpora.
In addition, the ability to highlight negative keywords in AntConc may also allow you to investigate under-use of specific vocabulary relatively easily, for instance when comparing learner data with that produced by native speakers, etc., an option that, obviously, a pure frequency list of only the source corpus is unable to provide.
Depending on the type of corpus and the age of the sources, it may be possible to find corpus material in electronic form already and then the keyboarding or scanning stages can be avoided.
In this sense, all research questions are framed from a particular perspective by reference to a specific methodology (corpus linguistics).
A general corpus usually includes standard text samples as it aims to identify what are central and common features of a language.
It is clear, then, that tokenization and part-of-speech tagging are not inherent in the text itself, but are the result of decisions by the corpus makers.
Achieving gender balance in a corpus involves more than simply ensuring that half the speakers and writers in a corpus are female and half male.
A collocation graph is a visual representation of the collocational relationship between a node and its collocates.
As tagging longer texts may take a fairly long time, let's first prepare a relatively short sample.
We conclude with a summary of the chapter and some comments on potential applications of bootstrapping in corpus linguistics that have yet to be attempted.
In the following sections, I shall discuss digital font generation, corpus generation, corpus processing, corpus annotation, and application of corpus in different areas of human knowledge.
This corpus was created so that linguists with more computationally based interests could conduct research in natural language processing (NLP), an area of study that involves the computational analysis of corpora often (though not exclusively) for purposes of modeling human behavior and cognition.
In corpus linguistics, the object of research will usually involve one or more aspects of language structure or language use, but it may also involve aspects of our psychological, social or cultural reality that are merely reflected in language (a point we will return to in some of the case studies presented in Part II of this book).
However, in practice, all techniques for corpus analysis -these two most basic methods, and all the more complex methods built upon them -are nowadays supported by the use of various pieces of corpus analysis software.
It has been widely accepted with Corpus Linguistics that collocation needs to be measured within a fairly short span: + / -5 words from the node is common.
Finally, corpus-based approaches are without an alternative in diachronic studies and yield particularly interesting results when used to study changes in the quality or degree of productivity, (cf.
However, beyond those treatments, there is need for a more general discussion of the current state of the art concerning corpus design and analysis.
Depending on your research goal and the design of your corpus, you may also want to include specific information on the situational characteristics of each text in individual text files in your corpus.
Needless to say, corpus linguistic studies of this sort cannot explain why languages opt for one or the other type of subject and object encoding, and the explanation for this lies in deep and intricate histories of language evolution with many stages of development, each a story worth telling in itself.
This may appear to be a relatively high value, but it is in fact just above the 10th percentile for all users in the corpus (0.73); at least within this dataset, this is suggestive of a comparatively and the bilingualism score (y-axis).
Exercise 6 has hopefully already alerted you to the fact that it isn't easily possible to just use any document that we can read in some way on the computer equally well as a source for our corpus-linguistic analysis, simply because it contains text.
The annotation of the results for proper name or common noun status can be done in various ways -in some corpora (but not in the BROWN corpus), the POS tags may help, in others, we might use capitalization as a hint, etc.
However, here the focus has been on the tools and methods used in the field of (English) corpus linguistics.
In addition, the quantitative analysis that is typical of corpus research facilitates comparisons of linguistic features' distributions across registers and judgments about what is common or rare in a particular register.
When this level of technology is reached, corpus-based dialect studies will become the norm.
Furthermore, it contains descriptions of existing phonological corpora and presents a wide range of popular tools for spoken corpus compilation, annotation, searches and archiving.
The two variables are: First person pronoun use in each text normed to 1,000 words (interval variable) and Discipline (nominal with three levels: 1 = Business; 2 = Humanities; 3 = Natural Sciences).
There is likely no better way to learn about the issues in corpus design and to appreciate the larger corpora built by other researchers than to build one on your own.
It is in this way that the corpus linguistic approach bears great potential not only for the study of language use but also for the demarcation of possible structures.
In a nutshell, the keyword list of a corpus is very useful to identify its main topics, provided that the comparison with the reference corpus is appropriate.
The interpretation of a keyword or key tag list, done properly, is an onerous task, as it will typically involve undertaking at least a partial concordance or collocation analysis of a large part of the highest-scoring key items.
She does not provide an annotation scheme for categorizing the metaphorical expressions, but she provides a set of examples that are intuitively quite plausible.
Examples of type b) make it possible to identify phonotactic features, such as the presence or absence of reduplication and its effect on pronunciation, while type c) may be useful for selecting or extracting words of different length in order to establish potential correlations between word length and complexity, or, if we assume that shorter words are indeed less complex, to extract simpler vocabulary from texts in order to teach it at less advanced learner levels.
The correlation between two numeric variables is referred to as Pearson's product-moment correlation r (for regression), but its significance test, too, comes with an assumption that corpus data usually do not meet, namely that the population from which your samples were taken is bivariately normally distributed, which in practice is often approximated by testing whether each variable is distributed normally.
Although always important, this issue is especially important for corpus-based studies, which generally seek to produce more generalizable results than many other approaches and to facilitate comparisons between different corpora.
The reference dataset in corpus linguistics studies is usually a general corpus, representative of some language or variety of language.
For instance, issues relating to speech corpus generation differ from issues relating to text corpus development.
In corpus linguistics, a text is a record of any stretch of language as produced on a specific occasion at some point in time, at some place, and as received or receivable at some place and time.
However, analyses at higher or lower levels of linguistic structure can also be represented as corpus annotation.
Collocation boxes where salient collocates as identified by a statistical analysis of corpus data are organized by part of speech.
Critically, markup, metadata and annotation can be processed by computer, just like the actual words of the corpus texts, and therefore they can be exploited in automated corpus analysis.
While there has been considerable discussion of the best statistical methods to identify important phrases within a corpus, there has been almost no discussion of the replicability of phraseological findings, or the ways in which corpus design and composition influence the results of this kind of research.
However, as also discussed in Chapter 2, software-aided queries are the default in modern corpus linguistics, and so we take these as a starting point of our discussion.
This obviously isn't as useful because there's no way to directly investigate the context by clicking a link whenever you find an interesting example, but of course better than nothing -or looking through hundreds of concordance lines generated for single prepositions in the hope of finding suitable combinations, which you could still do by using the KWIC display option and sorting according to first word on the right of the node.
We call this process establishing the random co-occurrence baseline and the resulting value is called the expected frequency of collocation.
Investigating saturation presupposes a dynamic perspective on corpus content and its coverage of linguistic forms available in a language variety.
However, given that the intended audience of this handbook is expected to have limited resources for corpus compilation, it seems useful to provide an example of a study where it was possible to use part of an already existing corpus.
In this case, devices are much more frequently referred to as electric and less frequently as electrical than expected, and, as in the LOB corpus, the nouns in the category industry are more frequently referred to as electrical and less frequently as electric than expected (although not significantly so).
You have a small corpus of presentations comprising two sub-corpora: teacher presentations and student presentations.
A more common approach is to begin with a single word (or lemma) of interest, called the node, for which a concordance search has been performed; the surrounding text is then scrutinised to establish what elements occur frequently in the vicinity of the node.
The University of West Indies Learner Corpus or UWI L2 Corpus created by Hughes Peters includes material spoken by adult French learners (16 in total) who were also speakers of English and Jamaican Creole, and who had studied French at university.
The Introduction defined corpus linguistics for the purposes of the present discussion as a methodology in the service of the science of language.
When building general reference corpora from the web we need to choose seed words that are likely to appear across a wide range of topics, but there has been some debate about exactly what constitute good seeds for a corpus of this type.
We'll label the root element for this document 'dialogue' in order to identify our text as a dialogue, and we'll also give it two attributes with the names 'corpus' and 'id' and corresponding values of 'test' and '01' respectively.
Should you choose to expand your corpus linguistic skills, we present below some influential register studies that a corpus-driven approach can offer with the goal of providing comprehensive linguistic characterizations of texts and alternative ways to do keyword analysis in different registers.
If a potential spoken corpus user interested in the grammatical variation between older and younger speakers cannot find information on the age of the speakers represented in the corpus, and if a researcher interested in the interplay between prosody and syntax in a language cannot interpret the transcription symbols used for prosody, re-use of corpora is impossible.
Take the example of british english and american english used in Chapters 3 and 2: If we accept the idea that the LOB corpus contains "British English" we are accepting an interpretation of language varieties that is based on geographical criteria: British English means "the English spoken by people who live (perhaps also: who were born and grew up) in the British Isles".
In a similar way to Knight, Gu concentrates on providing some guidelines for an approach to multimodal corpus analysis.
Which of these is worse in corpus research: linguistics without statistics or statistics without linguistics?
Corpus-based studies begin with lexical expressions (e.g.
Our goals in the Cambridge Handbook of English Corpus Linguistics (CHECL) are to survey the breadth of these research questions and applications in relation to the linguistic study of English.
In Part I, the first three chapters focus on corpus design and address issues related to corpus compilation, corpus annotation, and corpus architecture.
Due to these problems, and the fact that being able to measure lexical diversity in a meaningful way would be very desirable for many linguistic questions, the question of whether a method which is less sensitive to text length could be devised has received a decent amount of attention from corpus linguists and others.
A learner corpus is a collection of texts from learners of a particular language.
The contemporary standard for corpus markup and annotation is XML (eXtensible Markup Language), where added information is indicated by angle brackets <>, as illustrated in Fig.
There, we concentrate on our strength: variationist corpus linguistics.
Unless a spoken text corpus has been phonetically annotated (cf.
In Firthian terms, collocation refers to the relationship between a word and its surrounding context where frequent co-occurrence with other words or structures helps to define the meaning of the word.
If this so-called interrater reliability is sufficiently high, the annotation scheme can safely be applied to the actual data set by a single annotator.
Before making claims, it is important to consider a sample of expanded concordance lines and to maintain vigilance in terms of spotting lines that potentially may be functioning differently to a first glance.
As when using other corpus annotations that have been produced by automatic or semi-automatic procedures, understanding the limitations of automatic lemmatization and treating its outputs accordingly with a degree of circumspection is often a necessary part of the corpus annotation and analysis process.
Corpus linguists' most-cited publications which served as the foundations of corpus linguistics were constantly referenced.
It does not pose particular problems for corpus-based morphology, which studies structures smaller than the word.
Additionally, a corpus addressing this issue would need to have sub-corpora of relatively equal size.
Corpus annotation involves enormous amounts of work.
However, associations in general and associative learning are certainly not (always) symmetric, which is why, ideally, corpus linguistics would explore the use of directional AMs.
For all others, a discussion of both issues -the alleged uselessness of corpus data and the alleged superiority of intuited data -seems indispensable, if only to put them to rest in order to concentrate, throughout the rest of this book, on the vast potential of corpus linguistics and the exciting avenues of research that it opens up.
Thus, corpus comparability needs to be addressed in a critical evaluation of ICE for the study of World Englishes.
Qualitative analysis of corpus examples enables a matrix table to be built up, whereby criteria showing resemblance to the be + adjective category or to the passive category can be plotted against numbers of tokens, and degrees of similarity to and difference from the passive can be established.
Of particular interest is the sub-corpus from WordPress and Blogger, which includes 95 million words of blog posts and a further 86 million words of reader comments on those posts.
If our corpus represents a sample rather than the population (which is typically the case), we should consider the amount of evidence we have in the sample.
The study is a good example of a fully corpus-driven analysis which allows generalizations to emerge bottom-up from the learner data.
When analysing corpora, we also need to think about the type and format of the data that needs to be obtained from the corpus in order to answer the research question.
A type is a unique word form in the corpus.
Thus, our third case study shows that the existing visualisation technique previously used for exploring the network of relationships in an online social network can also be used to explore the linguistic similarity of specific subcorpora at the word level.
The most striking difference in type frequency for types occurring in both subcorpora is that for upon, which occurs more than 10 times as frequently in the general corpus (ratio 10.604).
It needs to be explained that pragmatic research which truly focuses on language in use has to be corpus-based because there is no convenient way of making speakers say what you want them to say where language is unpredictable, messy, and extended over many turns.
In fact, the size of corpora has increased exponentially over the past 20 years, and corpus analysis tools have also made considerable progress.
In addition, a Brown-based frequency list (for all words in the corpus) would be quite sparse.
The aim of the present discussion is to show how cluster analysis can be used for corpusbased hypothesis generation in linguistics by applying it to a case study of a dialect corpus, the Diachronic Electronic Corpus of Tyneside English , and thereby to contribute to the development of an empirically-based quantitative methodology for hypothesis generation in the linguistics community.
Besides, it is also necessary to determine which metadata will be associated with each corpus sample.
Of course, you could fix this manually, but, since we've now separated the data from our ability to concordance on it, we'd have to go back into BNCweb to look at the original frequency list.
Digital electronic technology has been a boon the corpus linguistics community, but in linguistics, as in life, it's possible to have too much of a good thing.
To achieve this, we need annotations and metadata to turn collections of data into corpora that will form the basis for corpus-linguistic investigations of language use, and what this tells us about the structure of a specific language's grammar, lexicon, etc., as well as the conventions of the language use and that of the community members' thereof.
In general, it is preferable to store every language sample in a separate file.
We know that unless a corpus represents the population (all language use), absence of evidence is not evidence of absence (see Section 1.
Such situations are very common in corpus linguistics.
As an initial step, BootCaT fetches 10 hits from Bing for each tuple then downloads and processes the corresponding web pages to build a corpus in the form of a text file.
However, corpus analysis shows that the most frequent case in many language registers is, on the contrary, the simple aspect.
All of this extra-linguistic information is very difficult to encode in a written transcription without the corpus compiler developing an elaborate system of markup identifying this information and the transcriber spending hours both interpreting what is going on in a conversation and inserting the relevant markup.
Third, a range of other interesting statistics can help corpus linguistics tackle other statistical challenges.
However, POS tagging is not usually done by skilled, experienced annotators, bringing us to the second, completely different way in which POS tags are based on operational definitions.
This written form is what we treat as the corpus text since this is what can be searched and further annotated.
In quantitative statistical analysis, we classify linguistic properties or features in a corpus, count their frequency of occurrence, and construct statistical models to explain what we observe.
Chapter 4 ("Analyzing a Corpus") describes in detail how to conduct a corpus analysis, covering such topics as how to frame a research question for a particular corpus analysis and select the appropriate corpora for conducting the analysis.
In our case study, we illustrated how this distinction is not absolute: some previous studies of discontinuous lexical frames (like Biber 2009 and Ro ¨mer 2010) are intermediate along this continuum, beginning with a corpus-driven approach to identify a set of continuous lexical sequences, but carrying out the corpus-based investigation of only those sequences, to analyze the extent to which they occur as discontinuous frames.
The first word (w 1 ) occurs only in one text, at corpus positions 1 -5 -10 -15 and 20, while the second word (w 2 ) is evenly distributed at positions 1 -200,000 -400,000 -600,000 and 800,000.
Note that Firth, although writing well before the advent of corpus linguistics, refers explicitly to frequency as a characteristic of collocations.
Such scores are useful in information retrieval or machine learning, but less so in corpus-linguistic research projects, where precision and recall must typically be assessed independently of, and weighed against, each other.
The variety of topics, the copra volume employed and languages studied to provide us a powerful tone that corpus and corpus-based research is a lifelike domain of research.
In public projects, a proprietary search engine tailored explicitly for a specific corpus is often programmed, which cannot easily be used for other corpora.
Metadata is also relevant to what kind of research we can do with a given corpus.
Adding this perspective to our definition, we get the following: Definition (Fourth attempt, linguistic interpretation) Corpus linguistics is the investigation of linguistic research questions based on the complete and systematic analysis of the distribution of linguistic phenomena in a linguistic corpus.
The corpus-based approach to dialectology contrasts with the standard approach, which is based on analyzing language elicited through interviews and questionnaires.
To summarize, there is no one way to deal with all the variables affecting the gender balance of a corpus.
They should keep the speech transcriptions and the written data in ASCII (text) format as it is the standard format used in corpus linguistics.
In this way, we can see how corpus linguistics can provide translators with tools that may help them adapt their translation choices on the basis of a better identification of the recurrent linguistic properties at work in a text.
The OPUS database includes many free access parallel corpora, including the Europarl corpus, described below, as well as corpora with subtitles and multilingual data collected from the Internet, such as Wikipedia.
The context (or co-text), for a concordance, as in traditional, printed concordances, may be a whole sentence, a paragraph, or simply a given number of characters to the left and/or right of the search term.
In corpus linguistics, we are almost always dealing with nominal data.
In order to decide which language is most suitable for a particular corpus linguistics programming task, there are at least four important factors to consider at the outset.
While statistical significance testing has almost always been part of learner corpus studies, through the notions of over-and underuse, criticism has recently been voiced against this type of monofactorial statistics.
This is not yet standard in corpus linguistics, but it is a good idea to plan and document your research as though it already were.
By default, the algorithm returns the asymptotic p-values because the test statistics used in the algorithm are shown to tend to well-known distributions.
As described in the previous section, the computational n-grams method appears under various guises in corpus linguistics.
In contrast, the exploratory investigation that we present in our case study employs a corpus-driven approach to directly identify the important discontinuous frames in speech and writing.
Metadata can be used to limit searches to a particular subsection of the corpus, or can be used for examining variation due to some aspect of the individuals (i.e.
The following subsections will focus in turn on tools and methods related to the three key phases of corpus linguistics methodology that have already been highlighted, i.e.
Second, extremely short texts cannot be easily compared with other texts using many typical quantitative corpus-linguistic methods, such as normalization, because the normalization results become meaningless as the length of the text becomes increasingly short.
For example, if you want to generate a frequency list of a corpus or a concordance of a word in a corpus with R, you must write a small script or a little bit of code in a programming language, which is the technical way of saying you write lines of text that are instructions to R. If you do not need pretty output, this script may consist of just a few lines, but it will often also be longer than that.
In total, the WARD corpus contains 26,573,826 words, spread across 159,181 letters, written by 130,659 authors -which is a far larger sample than would be possible if traditional methods for data collection had been applied.
As this review of research illustrates, there has been a particular interest in lexical phrases in academic registers, perhaps because much corpus research on lexical phrases has been motivated by applied concerns related to language teaching and learning.
In short, the goal of the second edition of English Corpus Linguistics is to provide a comprehensive description of all aspects of corpus linguistics, ranging from how to build a corpus to how to analyze a finished corpus.
Judgment, or purposive, or expert choice sampling, a second type of sampling, was used to create the Brown Corpus.
The creators of the BROWN corpus are quite open about the fact that their corpus design is not a representative sample of (written) American English.
Consequently, it is automated linguistic annotation that we will be concerned with in this chapter (see Part III of this volume for discussion of manual annotation in certain kinds of corpora, e.g., annotation of errors in a learner corpus).
We argue in this chapter that bootstrapping is underused in corpus linguistics, and that quantitative corpus linguists would do well to add this tool to their repertoire.
First, we need to make sure that the dataset is organised according to the principles of the Linguistic feature research design (see Section 1.4).
If relevant to your research goal, is the corpus constructed so that specific sub-corpora are readily retrievable?
As a result, it is at least technically possible for this corpus to have conflicting constituent trees and sentence span borders for sentence type annotation.
One of the most common types of annotation is PoS tagging (cf.
This book is a practical introduction to statistical procedures in corpus linguistics, a discipline that uses computers to analyse language, organized according to linguistic topics.
By thus linking description to theory, Gries argues that his corpus-driven study not only describes, but explains and predicts the way the choice between option (5a) and option (5b) is made.
In most cases, this should not be a problem, though, because, as pointed out earlier, most corpus data is stored in some form of plain text, so you can always try opening these files in your editor first.
AntConc uses a different concept of 'word' from BNCweb, based on the token definitions you've chosen, and may thus throw an error when it encounters something that doesn't fit this definition, aborting the keyword list comparison.
Corpus-based approaches to phraseology, however, have uncovered the essential functions played in language by n-grams or lexical bundles, i.e.
The first step in creating an annotation scheme for a particular variable consists in deciding on a set of values that this variable may take.
Then, in Section 3 we take a step back to consider the state of the art of phraseological research in corpus linguistics, considering some of the core issues still being debated within the field.
Let us also determine the recall of neologisms from the OED (using the definition "first documented in the 20th century according to the OED"): the OED lists 31 of the 45 neologisms, so the recall is 31 /45 = 0.6889; this is much better than the recall of the corpus-based hapax definition, but it also shows that if we combine 9 Morphology commodify, desertify, extensify, geriatrify corpus data and dictionary data, we can increase coverage substantially even for moderately productive affixes.
By tracing these relationships, it is possible to extract features of corpus sentences-subjects of active and passive verbs, objects of prepositions, etc.-that would be difficult to retrieve from searches of unannotated text alone.
Thus, most of the approaches above are relatively easy ways in which we can try to make our co-occurrence-based studies more robust; there is no reason not to pursue those strategies if corpus linguistics as a whole wants to evolve in tandem with what happens in other disciplines.
Even generative linguists acknowledge the value of a speech corpus as a source of evidence in language acquisition studies.
There is no room in this chapter to attempt a comprehensive review of the impact that corpus research has had across the field of linguistics.
One of the criticisms of small corpus research is that a small corpus does not allow for generalization.
The key concepts here are the notions of the corpus as a sample and of balance and representativeness.
Corpus designers therefore need to reach out to different groups of contributors and use a range of incentives to obtain a representative sample.
The assumption is that it is possible to build a corpus covering a particular domain (in this case dogs) by using a commercial search engine to find web pages containing words likely to occur in that domain.
It contains some of the oldest French texts from 950 up to the present-day and, with currently about 300 million words of text, it decidedly qualifies as a large corpus.
Given that both production and perception of previous texts can influence people's behaviour, the perception of texts is important for considerations of representativeness and balance during corpus building.
As we will see below, in the field of stylistics, corpus linguistics tools do not seek to replace qualitative analysis, but only aim to guide the choice of themes and excerpts to analyze.
It would be surprising if corpus linguistics was an exception, and indeed, it is not.
For effective building of collocation networks specialized software is necessary that is able to run multiple comparisons of word co-occurrence and display the data in a visual form.
Thus, corpus linguistics needs to explore more adequate and conservative ways to extend AMs to n-grams.
The area of research/application where dispersion has gained most ground is that of corpus-based dictionaries and vocabulary lists.
On the one hand, POS tags can help you be more specific about the words you are searching if you are going through an already existing search engine and if you are searching a corpus that has been tagged for part of speech.
Such semi-structured elicitation techniques may also be used where a phenomenon is frequent enough in a typical corpus, but where the researcher wants to vary certain aspects systematically, or where the researcher wants to achieve comparability across speakers or even across languages.
It has been a decade since the book Corpus Linguistics 25 Years on (Facchinetti 2007) was published.
In corpus linguistics the most useful ones are: mean, median and 20% trimmed mean.
In the late 1990s, corpus-driven studies of recurrent lexical phrases in English registers began to appear.
In particular, LD corpora may come with severe limitations on corpus design, given that related projects have limited funding, and also because such projects are often undertaken by individual academics (though in collaboration with community members).
The findings from corpus-based typology also feed back into ideas about corpus building, composition, and annotation: more diverse languages require different considerations of register, representativeness, and potentially, require adaptations of annotation and querying strategies.
In this approach, linguists begin their research without an a priori and simply let hypotheses emerge from corpus data (this is called a corpus-driven approach).
Corpus work is therefore still rare; the databases that have been collected have mostly been small, and are perhaps best counted into the very generic category of "corpus" that in traditional philology was used to describe the language data investigated for a study.
The objective of this study is to investigate the rhetorical function of causality in scientific text on the basis of an expert corpus (taken from the MicroConcord Academic Corpus Collection) and a learner corpus (taken from the Hong Kong University of Science and Technology Learner Corpus).
Speech act studies represent function-to-form mapping, which is more difficult to deal with than the form-to-functions direction of fit with corpus-linguistic methods.
In our view, none of the parameters of authenticity, routineness, and spontaneity constitutes a demarcation criterion for corpus text and/or corpus linguistics.
These corpora range from multipurpose corpora, which can be studied to carry out many differing types of corpus-based analyses, to learner corpora, which have been designed to study the types of English used by individuals (from many differing first language backgrounds) learning English as a second or foreign language.
Lexical tagging is crucial because it will enable the factor analysis program to determine where the various parts of speech occur: e.g.
This approach to Corpus Linguistics is based on the observation of pattern in concordance lines, where a word or short phrase is the node of the line and the few words occurring before and after the phrase are shown for each occurrence.
For this reason, languages such as Perl, Python (which can run as a functional language), and R have been commonly used for corpus linguistics applications (e.g.
The fourth step consists in collecting data -in the case of corpus linguistics, in retrieving them from a corpus.
For three of the verbsencourage, support, and fear -use in one of the searched constructions is rare (less than 1% of occurrences of the verb) (continued) 7 Analyzing Co-occurrence Data 151 and not listed as a possible form in the corpus-based Collins Cobuild English Language Dictionary.
A third major challenge to meta-analysis in corpus linguistics and throughout many other fields is that of publication bias.
Strings of words are how texts are represented prior to any further corpus annotation (cf.
Blogs offer similar advantages when it comes to boilerplate removal and text dating (see Representative Corpus 2 for more information on the Birmingham Blog Corpus) and are one variety of so-called 'Web 2.0' content, which is being used increasingly in linguistic research.
We have found that offering students the opportunity to build and analyze their own corpora gives them valuable experience in corpus building and sometimes even encourages them to build other corpora for projects outside of the class.
I will develop this proposal by successively considering and dismissing alternative characterizations of corpus linguistics.
First, corpus-based studies have shown that dialect variation exists across a wide range of different varieties of language, including forms of written and standard language, where dialect patterns had never been sought or even believed to exist.
This is an example of an innovative study using the corpus-based method to gain new insights into the linguistic subdiscipline of pragmatics.
Creating a new POS tagging model is often an iterative process: small samples of text are manually annotated and used as training data to create a provisional POS tagger, which then automatically annotates more texts.
Introduction DOI: 10.4324/9780429269035-1 that linguists may have a hard time imagining, and corpus linguistics also has this kind of explorative data-driven facet.
Corpus-based studies have therefore shown that dialect variation is far more common than had previously been assumed.
This study illustrates how a corpus study can combine quantitative elements (the prevalence of different functions for a structure) with qualitative ones (the identification of semantic functions).
After that we store the results we obtain by sapplying functions to the list to count lengths and compute type-token ratios (with an anonymous/inline function); we compute a mean type-token ratio using all types and tokens, and a mean of all lengths of utterances with mean, but to avoid the effect that outliers might have we use the argument trim=0.05 to discard both the smallest and the largest 5 percent of the data.
We only need to consider the vast corpora like iWeb or corpora of the TenTen Family 3 to see this clearly: despite their massive sizes these corpora seem to not reflect well the large range of diverse instances of language use and, hence, fare relatively poorly in terms of representativeness.
For instance, we have not yet investigated how a poetic text should be annotated; and what kinds of textual, linguistic, stylistic, prosodic, and figurative annotation techniques we should design and apply to poetic texts.
As an additional practice assignment, now that this is all done, why don't you try to revise the script so that it uses the corpus files' XML annotation, i.e., try to use the packages XML and/or xml2.
I argue that the specific issues encountered with this database are due to fundamentally different ideas of what might constitute "good data" in the fields of Digital Humanities and corpus linguistics, respectively, and this in turn leads to different expectations regarding how the data is made available to researchers (e.g., Mehl 2021).
There are several types of resources, however, that it is good for any diachronic corpus linguist to be on the lookout for.
What are the modes, minimums, maximums, and ranges for token and type morpheme counts based on this sample?
An additional index that is somewhat particular to the context of corpus linguistics and that we would propose to include in this list is normed frequency.
In order to study the distribution and prevalence of this type of structure in the French spoken in Parisian suburbs, all the occurrences of indirect questions were collected in a small oral corpus of approximately 350,000 words.
Therefore, the metadata of a corpus will include a list of correspondences between the chosen phonetic alphabet and the IPA.
Elsness' (1997) long-term, corpus-based study of BrE and AmE shows that the PP increases over time but starts decreasing again from the second half of the eighteenth century, a development led by AmE.
The crawling for the WordPress and Blogger sub-corpus of the BBC took place without relying on commercial search engines.
The type/token ratio can only be used for comparing texts of similar length.
Like many other learner corpus-based studies, however, this one does not examine the wide range of variables that are available in learner corpora like LINDSEI and that may have an effect on the learners' use of DMs (such as the time spent in an English-speaking country or the knowledge of other languages), nor does it consider the general context in which the learners acquire English (e.g.
For corpus linguists, this involves determining how many text samples need to be included in a corpus to ensure that valid generalizations can be made about the genre, and what range of individuals need to be selected so that the text samples included in a corpus provide a valid representation of the population supplying the texts.
I uncovered some limitations of the current crop of computational tools and methods and reflected on whether corpus linguistics could be said to be becoming tool-driven.
First and foremost, the concordance view with one word of interest aligned vertically in the middle of the text and the left and right context justified in the middle, is a way of visualising the patterns of the context of a particular word, and is the main way that corpus linguists engage with corpora.
Also, the effects of demographic information on the use of specific linguistic constructions could be explored with the help of a corpus designed as a historical or diachronic corpus.
The number of words used per sentence can also be calculated, even on a corpus that has not been subjected to syntactic annotations.
The chapter finishes with a critical assessment and discussion of future developments in corpus linguistics programming.
Six words later, there is another occurrence of a, so type and hapax counts remain, respectively, at 12 and 11 as the token count rises to 14, and so on.
In fact, a corpus is a collection of texts or recordings specifically chosen in order to be representative of a language, of a certain register or even a language variety.
As you may have noticed, this editor is really not optimised for handling large files, so to just view the frequency list without the ability to concordance on it, a dedicated editor, such as Notepad++, would be far better.
But for others in the field, the insights into language that arise from the use of the corpus are so radically different from the traditional understanding of how language works that they consider corpus linguistics to constitute an independent field of study or theory of language.
At this age, his type/token ratio was 0.38.
For example, if you type in the word baby, you will see that it occurs over 60,000 times in the COCA corpus.
Now that we've discussed most of the preliminary issues in corpus design, and seen how we can actually collect and prepare our own data for analysis, we can soon move on to learning how to analyse linguistic data in various forms.
We also suggest that these challenges are ultimately not so different from the pitfalls discussed by Rissanen in his "philologist's dilemma": just like in the early days of historical corpus linguistics, today's challenges pertain to the linguist's knowledge and understanding of their data.
Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specific research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identification of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results.
Thus, where a corpus user is interested only in instances of matters that are plural forms of nouns they can search for a string <mat-ters_NNS> rather than just <matters>.
The definitions of (especially) the terms markup and annotation in the context of corpus design and encoding vary quite a lot in the literature; the term tagging is also sometimes used as a synonym for either or both of these terms.
These factors may have an impact on the accuracy of corpus annotation since the linguistic models underpinning off-the-shelf annotation tools are usually derived from standard written language.
At each step we consider and discuss both best practices as well as some of the many challenges unique to meta-analyzing research in corpus linguistics.
A monitor corpus is a corpus allowing researchers to track current changes synchronically in a particular language.
The latter can be observed thanks to the corpus design, which includes a parallel component.
Every summer, a large number of students from different parts of the world come to Lancaster to learn about corpora and statistics during a week of Lancaster summer schools in corpus linguistics.
Before performing descriptive statistics on the data obtained in corpus studies, it is necessary to make the number of occurrences comparable, and this can be achieved through the use of different sources.
As observed in other chapters in the volume, corpus linguistics stands to benefit from the innovative ideas, methods, and expanded possibilities developed in related fields such as natural language processing and computational linguistics, enriching its analytical toolkit and enhancing its capacity for nuanced linguistic analysis.
The annotation of syntactic dependencies can also be done automatically using computer tools, but so far these have not generally been included in the interfaces for creating corpora, and their use requires natural language processing (NLP) skills that go beyond this book.
This does not mean, however, that corpus linguists only deal with raw text files -quite the contrary: some corpora are shipped with sophisticated retrieval software that makes it possible to look for precisely defined lexical, syntactic, or other patterns.
Such variables are common in variationist corpus linguistics that focus on predicting linguistic choices when two alternating variants of a particular feature are possible (e.g.
However, this does not necessarily mean that the text must be excluded from the corpus, since there is annotation that can be included in a corpus indicating that certain sections of a sample are "extra-corpus" material; that is, material not considered part of the corpus for purposes of word counts, generating KWIC (key word in context), and so forth.
There are different ways in which the raw data -the audio and/ or video recordings of the speech event -can be made accessible to corpus users: in the SBC, the solution is to store two types of file for each corpus text, one audio file containing the text recording in WAV format and a text file containing the transcription thereof.
For example, we defined American English as "the language occurring in the BROWN and FROWN corpora", but we saw that the FROWN corpus contains at least one misclassified text by a British author, and we also saw that it is questionable to assume that all and only speakers of American English produce the language we would want to call "American English" (recall the uses of sidewalk by British speakers).
Because historical writings are the only source of direct empirical data about language from before the twentieth century, historical sociolinguistics has naturally adopted a corpus-based approach to data collection, showing that complex patterns of historical dialect variation can be observed in written sources.
Second, by integrating into the analysis a number of insights into how discourses function which have developed within the field of corpus linguistics.
The corpus-linguistic approach is the systematic study of language use represented by the texts in corpora, which targets both linguistic knowledge and language use.
Even more strikingly, the structure of the type then I appeared 10 times more frequently in the COBUILD corpus that the I then construction, repeatedly employed in the declaration.
The article is especially interesting for its discourse-oriented perspective, starting from a rhetorical function rather than from one or two isolated items, as well as for its focus on scientific prose and its plea for more corpus-based English for Academic/Specific Purposes textbooks.
If you are writing your own computer scripts to build your corpus, you can integrate (or merge in) your metadata, bringing in all the metadata for each participant and all the metadata for each file for every token in the corpus.
Chapter 4 by Marcus Callies discusses challenges related to the special characteristics of learner corpus data, emphasizing the importance of valid data representation for studying L2 production and development.
Finally, we need to cross-fertilize insights from natural language processing and corpus-based cross-linguistic studies.
To sum up, Open Corpus Linguistics can be a challenging endeavor, but given the "replication crisis", it is a necessary one.
It is also good practice in corpus linguistics to make corpora available to other researchers who can explore the same dataset further and thus advance knowledge in the field.
Metadata are also crucial in recontextualizing corpus data and in designing certain kinds of research projects, but they, too, depend on assumptions and choices made by corpus creators and should not be uncritically accepted by researchers using a given corpus.
In order to do so, in the past corpus compilers have frequently resorted to using samples of approximately 2,000 words from different texts and genres to achieve balance, but both that number and 'cutting out' part of a text appear fairly arbitrary, and may in fact make such samples somewhat unrepresentative of the texts they're supposed to represent as a whole.
This should ideally be done in the form of a text file that lists all the separate editing steps, and can later provide the basis for part of a manual of information to be distributed with your corpus if you ever plan to release or distribute it.
The proportions observed are calculated by taking into account the total number of occurrences of the word, whose distribution we are trying to determine in each portion of the corpus, and dividing it by the total number of occurrences of the word in the corpus.
When creating a spoken corpus, one also needs to think about whether an orthographic representation of the text will be sufficient, whether the corpus should be represented in phonetic transcription, or whether it should support annotation on various different levels (see Chapter 11).
For this reason, some corpora such as ICLE and Longman's Learner Corpus have been created to understand the systems of language learners and reveal their interlanguage systems.
The use of corpora in Western linguistics began in the late eighteenth century with the postulation of an Indo-European protolanguage and its reconstruction based on examination of numerous living languages and of historical written documents; after two centuries of development the importance of corpora in linguistics research has increased to the extent that a subdiscipline has come into being, corpus linguistics, whose remit is to develop methodologies for creating collections of natural language speech and text, abstracting data from them, and analysing that data with the aim of generating or testing hypotheses about the structure of language and its use in the world.
Most genres traditionally studied in quantitative corpus linguistics tend to comprise of longer texts, compared to the extremely short text lengths of up to only a few dozen words which are overwhelmingly common on, for example, social media.
Instead, the CHECL focuses on a critical discussion of the linguistic findings that have resulted from corpusbased investigations: what have we learned about language variation and use from corpus-based research?
From a parsed and glossed sub-corpus 2 of 8,961 words, verbs and nouns make up 5,375 word tokens.
For example, decisive advances have been made in the corpus-based description of the grammatical features of spoken language (e.g.
On the one hand, cross-linguistic research should take full stock of recent advances in natural language processing, for tasks such as automatic alignment and multilingual annotation.
Corpus-based studies also expand the range of data considered for analysis beyond the linguist's intuitions, and ensure the accuracy of any generalizations that are made.
A corpus is a representative collection of language that can be used to make statements about language use.
After evaluation, you may determine that additional texts or text types are necessary to achieve better representativeness.
The annotation process requires the prior identification of clear categories (see Chapter 7, section 7.3).
Several corpus techniques, for example, keyword and key-cluster tools, have the specific aim of facilitating comparison.
XML can also be used for encoding the metadata of a corpus (see Chapter 6, section 6.4).
There have almost been shadow developments occurring between corpus linguistics and the social sciences in this area.
Once these determinations are made, the corpus compiler can begin to collect the actual speech and writing to be included in the corpus.
However, the notion "hapax" is only an operational definition for neologisms, based on the hope that the number of hapaxes in a corpus (or sub-corpus) is somehow indicative of the number of productive coinages.
Finally, a recent trend in corpus-based research of World Englishes is the detailed statistical modeling of variation, often including ENL, ESL, and EFL varieties.
For example, the first concordance line in Fig.
Tools have been developed in order to make data annotation in XML format easier, such as the Nite XML tool, for example.
So far, the traditional approach to corpus design has been considered.
Given the broad sweep of focus in the various primary studies, it seems that corpora can be of benefit to L2 users for a range of purposes: learning and use of language anywhere on the lexico-grammatical continuum (including collocation and idiom) for both receptive and productive purposes, as well as in more extensive reading and writing tasks or in translation.
The tenet that lexicon and grammar form an integrated continuum of form-meaning pairing has gained ground to the point of probably forming the majority position in current corpus linguistics.
The results can then be displayed in the concordance format familiar to corpus linguistics with the search feature (e.g.
The first stage involves proofreading each individual sample in a corpus with samples being placed into a "proofread 1" directory.
Like its name suggests, the type-token ratio is the ratio of the number of different words in a text (types) to the number of all words in the text (tokens).
Furthermore, corpus linguistics has played a crucial role in the advancement of natural language processing (NLP) technologies.
We have shown that the main difference between corpus linguistics and experimental linguistics is the way in which empirical data are collected.
A popular standardised effect size measure is Cohen's d. It is used to express the difference between two groups.
This section focuses on the use of effect size (ES) measures in corpus linguistics.
Text selection is a non-trivial aspect of corpus building for general corpora: since we aim at a high degree of general representativeness, we need to consider carefully the composition of our corpus.
At this age, her type/token ratio was 0.37.
Semantic tagging is a harder task for a computer than POS tagging.
Language documentation shares with corpus linguistics the basic goal of representativeness.
On top of that, being a monitor corpus, it also makes it possible to track potential changes across different periods in the same way.
Thus, a frequency list of a corpus is usually a two-column table with all words occurring in the corpus in one column and the frequency with which they occur in the corpus in the other column.
Everybody who has ever worked in corpus linguistics knows that no corpus will ever be perfect, and that the quality of a corpus depends less on the competence of the researchers involved than on the resources they were able to put into it.
Tools for Corpus Linguistics provides an up-to-date list of software packages for corpus annotation and analysis, and includes information about their pricing and the operating systems that they support.
However, because it is virtually impossible for the creators of corpora to anticipate what their corpora will ultimately be used for, it is also the responsibility of the corpus user to make sure that the corpus he or she plans to conduct a linguistic analysis of is a valid corpus for the particular analysis being conducted.
Many of the classic and larger corpora of English contain PoS tagging, for example, COCA or the Brown family corpora.
Whilst corpus-based information on article usage might not add much to the treatment of articles in grammar books, we believe that some areas would really benefit from the inclusion of the results of corpus studies.
More specialized areas are currently booming, it seems: diachronic corpus linguistics, which needs to deal with the problem of how temporally-ordered corpus data are grouped into temporal stages for subsequent analysis; and learner corpus research, which needs to move on from decontextualized studies of over-and underuse to more comprehensive models of learner language and its differences to native language.
A little example of something I came across in an undergrad corpus linguistics course was that we generated a concordance of in and found that in one of the SGML-annotated files of the BNC that we looked at, the word in was tagged as VBZ, which means "third person singular form of to be".
Remember, however, that lexical category information will be contained in the PoS tagging that we have discussed above in its original Brown version, so that, for example, the NP-embedded PP would be fully annotated as in (7.8) (note that the separator in Penn tagging is forward slash / rather than underscore _): These searches will thus give us a count of NPs with and without recursive structures, that is, where an NP occurs embedded in a higher-order NP either as an initial possessor NP or as the complement of a preposition of an NP-embedded PP.
First, there are generally agreed-upon verbal descriptions for different ranges that the value of a correlation coefficient may have (similarly to the verbal descriptions of p-values discussed above.
In terms of quantitative analyses, we observe how there is much in common between corpus linguistics and the social sciences anyway.
Of course, we could theoretically check all examples, 3 Corpus linguistics as a scientific method as there are only 42 examples overall.
What scientific considerations do suggest for a general corpus is that we should include a large range of text varieties with different situational characteristics, including large proportions of spoken and/or signed text varieties in the interest of greater representativeness.
This chapter argues in favour of standardized reporting of effect sizes in corpus research and shows how meta-analysis can be carried out.
In this chapter we turn our attention to the process of corpus building (or corpus compilation) itself.
If you build a corpus out of language documentation data, you probably have a very good idea of what is in the corpus and the context for that information, much more so than with a multimillion-word corpus.
Over the years, scholars have noted and discussed the occasionally elusive nature of the challenges and suggested solutions to the problems, tackling broader and abstract notions such as representativeness and balance of corpora, but also more concrete questions like sampling, various levels of annotation, precision and recall of corpus queries, dispersion of search hits across a corpus, statistical evaluation, among other points.
Sharing annotated data also implies that the annotation process and the categories used are clearly documented in an annotation manual, which will be provided to future users together with the data.
To measure the style of a passage, the frequencies of its linguistic items of different levels must be compared with the corresponding features in another text or corpus which is regarded as a norm and which has a definite relationship with this passage.
On the one hand, this definition subsumes the previous two definitions: If we assume that corpus linguistics is essentially the study of the distribution of linguistic phenomena in a linguistic corpus, we immediately understand the central role of the techniques described above: (i) KWIC concordances are a way of displaying the distribution of an expression across different syntagmatic contexts; (ii) collocation tables summarize the distribution of lexical items with respect to other lexical items in quantitative terms, and (iii) frequency lists summarize the overall quantitative distribution of lexical items in a given corpus.
Then we fit a first model that, as above, contains all fixed effects -LOGLENGTH, TYPE, and their interaction LOGLENGTH:TYPE -as well as varying intercepts for each level of corpus sampling -(1|LEVEL1), (1|LEVEL2), (1|LEVEL3) -and for each verb (1|VERB) and each particle (1|PARTICLE).
One way is manual and largely qualitative: to undertake a hand-and-eye analysis of the concordance, as explained above, where the focus of analysis is to identify co-occurring elements, the collocates of the node, in the concordance lines.
Finally, we can express the overall quality of the automatic annotation by calculating a mean of F1 scores per category, by weighing every category according to its number of occurrences in the reference (micro versus macro mean).
This would allow the teacher to tailor future lessons to the needs of the students as the corpus would help highlight any common or frequent errors and, hopefully, aid in discovering in why this type of error was made.
In general, three main types of research design can be distinguished: (1) whole corpus design, (2) individual text/speaker design and (3) linguistic feature design.
Let us further assume that we can assign all other uses of pavement in the sample to the reading 'paved surface', and that two of the four examples of sidewalk in the British English corpus are genuine counterexamples.
Understanding the choice between possible variants is one of the main foci of corpus linguistics, especially variationist corpus linguistics.
Treebank-3 consists of a heterogeneous collection of texts totaling 100 million words, including one million words of text taken from the 1989 Wall Street Journal, as well as tagged and parsed versions of the Brown Corpus.
Linguists normally look askance at 'bag of words' studies, but I do think they offer interesting new ways of carrying out 'corpus-driven' research.
Unlike the more experimental data types often used in SLA, where learners are forced to produce a particular form (as in fill-in-the-blanks exercises or read-aloud tasks), the focus in learner corpus data is on message conveyance and the possibility for learners to use their own wording.
However, to discuss possible pitfalls associated with research at the interface of Digital Humanities and corpus linguistics, it is necessary to briefly review some basic assumptions associated with these respective areas of study.
The underlying theoretical idea of corpus linguistics is quite broad.
Interestingly, the importance of Python in corpus linguistics work might possibly grow as it is one of the most popular languages used to develop artificial intelligence (AI) and deep learning applications due to its rich number of natural language processing and machine learning libraries.
In the preceding chapter we saw that while collocation research often takes a sequential approach to co-occurrence, where any word within a given span around a node word is counted as a potential collocate, it is not uncommon to see a structure-sensitive approach that considers only those potential collocates that occur in a particular grammatical position relative to each other -for example, adjectives relative to the nouns they modify or vice versa.
Any given type will be more frequent in a bigger corpus; for that reason, corpus frequencies are often given in relative terms, as frequencies per thousand words or per million words.
The most obvious area in which the social sciences have shadowed developments in corpus linguistics is corpus markup.
Research papers and books have been published to introduce corpus linguistics and define how to use it or build a small corpus for pedagogic purposes.
This "piece of extra information concerning the data" included in the corpus is what we call metadata.
The book is intended for anyone interested in corpus linguistics and quantitative analysis of language.
In our discussion of file types, and in Section 2.3 when we discussed issues of encoding for diachronic/historical corpora briefly, we've already seen that not all forms of textual representation are equally useful for corpus analysis.
First of all, each text was assigned a number that designated a specific genre in the corpus in which the sample would be potentially included.
By performing a corpus study on spoken French using the Corpus de français parlé parisien des années 2000, the authors identified all the occurrences of "il y a" or "y a" (both forms meaning "there is") and then manually chose only the cases (98 in total) in which (il) y a was followed by a definite noun phrase and a pseudo-relative.
To do this, we have to look for the number of occurrences of the word huitante in each of these six cantons by means of the OFROM corpus, which compiles the French spoken in Switzerland (see Chapter 5).
In fact, the field of corpus linguistics is often conceptualised as being part of the Digital Humanities (e.g., Roth 2018; Zottola 2020; Mehl 2021) despite the fact that it has a longer history.
Once a draft version of a text contains "metadata" and "textual markup", it can then be placed into a "lexical (pending proofreading)" directory to indicate that the text will be ready for use as a lexical version of the corpus once it has been proofread.
We will discard empty character strings with nzchar and test whether each word token is "perl" with a simple logical expression testing for equality (i.e., ==) and then use plot (with type="h", for histogram) to plot the left panel (because FALSE = 0 and TRUE = 1).
Beyond the world of the corpus linguistic researcher, frequency lists can be used directly or indirectly to support language learners by providing a way to focus on the more frequent words in a text, or suggesting priorities for language teachers when preparing lesson materials.
Corpus linguistics is not a 'new area' of language study; it is a 'new approach' (or a new method) to language study.
This provides information such as register frequency, meanings, topics, collocates by word class, frequent clusters containing literally, concordance lines, and references to entire texts that contain the word.
This corpus also has an educational purpose in the area of language teaching.
KWIC) (see Section 5.1) concordancer: a software package that generates concordances, as well as possibly displays of other linguistic features, for corpora (see Section 5.1) corpus (pl.
In the simplest case (which we 4.1 Retrieval assumed to hold in the examples discussed in the previous chapter), a corpus will contain plain text in a standard orthography and the software will be able to find passages matching a specific string of characters.
The metadata of such a corpus should at least include information of where the recording took place, its date, the context in which it happened, the conversation topic, the number of participants, the gender of the person recorded, his/her age and profession.
The software compiles a list of words that occur in the context (either all words, or words which stand in some particular relationship to the node) and then applies statistical analysis to identify words which are more common in the vicinity of the node than elsewhere in the corpus.
So for instance, if we see on a frequency list for an English corpus that the is the most frequent word, occurring 60,000 times, then we could say that the single type the is represented in this corpus by 60,000 tokens.
An important notion in corpus linguistics is that of context.
This annotation is presumed to be completely correct, that is, it matches what has been deemed appropriate in the annotation scheme.
When studying words, we will normally be interested in how often they are used under a particular condition, so it is their token frequency that is relevant to us; but we could imagine designs where we are mainly interested in whether a word occurs at all, in which case all that is relevant is whether its type frequency is one or zero.
In Section 3, I discuss the main principles of Open Corpus Linguistics, taking potential challenges and pitfalls into account.
So while these authors have ample anecdotal evidence to suggest that bootstrapping methods need to be better understood, generally, it seems that their application in corpus linguistics research has gotten off to a great start.
Thus, during the last two decades, in addition to general linguistics research being consistently referenced, specialized academic journals on corpus linguistics, cognitive linguistics, pragmatics, sociolinguistics, and language education have also been actively cited.
It displays the number of times each preposition type is found in a certain context.
Many areas in psycholinguistics and computational linguistics have made interesting discoveries, have developed useful tools, have adopted great methods from neighboring fields, but corpus linguistics is unfortunately not leading the pack and must take care not to lose momentum either in terms of its own evolution or in terms of how it helps to shape linguistics as a whole.
Note that this entire corpus -let us call it A Historical Corpus of American News Writing -would consist of three sub-corpora related to each of the three time periods.
The paper then narrows in focus to look at a group of related areas in the social sciences that might have much to offer corpus linguistics.
As I mentioned above, there exists recurring confusion regarding the distinction between annotation and markup.
In the field of corpus linguistics and its neighboring field of language education, only a few attempts have been made to explore a comprehensive and bird's eye view of the interaction among published research articles on the subject.
On the one hand, corpus linguistics focuses on data observation as found in collections of texts, recordings, etc.
A corpus that has used this type of data to the best advantage is the Corpus of Early English Correspondence, compiled by Terttu Nevalainen, Helena Raumolin-Brunberg and a team of collaborators (Raumolin-Brunberg and Nevalainen 2007).
Thus, to be more explicit, we could in fact also allow more than one speech act to occur in our annotation, which would result in 'answer-state' for this particular example.
One of the first things many corpus linguists plot is the frequency of the words in a given corpus to see which words are most and least frequent and to examine the frequency distribution.
While the large monolingual reference corpus is an extraordinary source of lexicographic data, other types of corpora certainly deserve a more prominent place on the lexicographer's computer: specialized corpora, parallel corpora, and learner corpora.
In the field of language teaching, it is also possible to collect texts written by students having different levels, and to build a corpus of these writings in order to study the typical errors that students produce at different learning stages.
This chapter will focus on a few kinds of statistical analyses that are often used in corpus linguistics and will try to get you in a position to turn a research question into something specific and testable.
Moreover, these corpus practitioners started to compile their own personal corpora for analyzing and investigating linguistic features and examples of expressions by using corpus analysis software packages.
Finally, the CHECL concludes with a major section on applications of corpus-based research.
The term metadata refers to any additional information about the corpus compilers, the data collection (e.g.
Next, they look at concordances of the remaining words to determine first, which senses are most frequent and thus most relevant for the observed differences, and second, whether the words are actually distributed across the respective corpus, discarding those 10 Text whose overall frequency is simply due to their frequent occurrence in a single file (since those words would not tell us anything about cultural differences).
If corpus linguistics wants to retain its scientific status, it should not be content with statements such as 'this feature was found in a large corpus that is, however, not available'.
These data are typically not part of the corpus itself, but are listed in the metadata (see Chapter 6).
The mechanisms for this in BNCweb are sometimes misleadingly similar to the use of regular expressions we've learnt in Chapter 6, but the most basic forms employ a different system referred to as wildcards, whereas genuine regular expressions are in fact a feature of the CQP (Corpus Query Processor) syntax that BNCweb uses internally for its queries (without you necessarily noticing it), or that you can use to write more advanced and complex queries yourself.
This can be especially tricky, as there is often some variation in how these items appear across a corpus (or even within a text); for example, although the hyphenated form, well-being, is clearly the most common option found in the Corpus of Contemporary American English (COCA) (>8000 occurrences), the forms well being and wellbeing each occur several hundred times.
While the configuration of abhorment does occur more often than expected in the third corpus period (o = 75, e = 52.4), this difference is not large enough to be statistically significant, so that the configuration of abhorment is not a type.
Notions such as word type and word occurrence are both very important in corpus linguistics.
Indeed, corpus-based studies aim to test empirical hypotheses that are often formulated from the results of other studies, either more theoretical or empirical in nature, which have already been published in the scientific literature.
What corpus linguistics has contributed, in essence, is making the identification of those most common lexical items much easier and more data-informed than would have been possible without such a tool.
Although the focus of this handbook is on corpus-based investigations of English language texts, a few of the studies reviewed here have addressed the issue of phraseology in other languages.
Descriptive metadata provides classificatory information derived from internal or external properties of the corpus components.
There are 373 stem types occurring with -ic in the LOB corpus, with a mean length of 7.32 and a sample variance of 5.72; there are 153 stem types occurring with -ical, with a mean length of 6.60 and a sample variance of 4.57.
There are also personal names that differ across corpora; for example, the name Macmillan occurs 63 times in the LOB corpus but only once in BROWN; this is because in 1961, Harold Macmillan was the British Prime Minister and thus Brits had more reason to mention the name.
If you've studied the list fairly closely, you should have recognised a number of things: In terms of text type/category, the fairly high number of tokens for words, such as okay, so, uh, um, etc., clearly indicates that we're here dealing with a corpus of spoken texts.
A very general precaution against this possibility is to make sure that the corpus (or our sample) is balanced with respect to all potentially confounding variables.
At least with respect to a register functional approach, any feature that is identified through a corpus-driven approach will still merit closer scrutiny and analysis in the corpus.
The corpus builders have thus augmented the existing data by adding various types of information-about the speakers (sex, age, social class, region of origin, educational background), about the text (type of narrative; whether a stand-alone story or part of a 'narrative chain') and about the utterance (the roles of the participants visà-vis the narration; type of quotative verb used to signal who said what in a narrative; to what degree the discourse is represented as being verbatim or more or less indirect).
In Part I, we introduce the concept of a corpus and locate corpus linguistics as an approach to language study that is concerned with the analysis of authentic language, and a focus on language variation, using large amounts of texts (Chapter 1).
At the same time, it is worth noting that corpus-based work on lesser-documented languages is of particular value given how little we know about language use in many languages.
Some areas within corpus linguistics that may be ripe for meta-analysis include research on register variation (see, e.g.
All these reasons call for new visualisation techniques, or at least the adaptation of existing ones, in order to specifically address the particular needs of corpus linguistics in terms of scalability, and support for iterative exploration.
In this paper, a look will be taken at the area of corpus linguistics.
Apart from that, diachronic corpora differ widely in size, composition, scope, annotation, and the nature of their textual material.
Let us consider four of the most widely used kinds of corpus annotation -those often pre-encoded in general-purpose corpora prior to their being distributed.
Unlike the forms of annotation mentioned so far, parsing does not operate strictly at the word level; rather, what is annotated are grammatical phenomena at the phrase, clause and sentence level.
This definition is more specific with respect to the data used in corpus linguistics and will exclude certain variants of discourse analysis, text linguistics, and other fields working with authentic language data (whether such a strict exclusion is a good thing is a question we will briefly return to at the end of this chapter).
Conrad argued that three changes prompted by corpus-based studies of grammar had "the potential to revolutionize the teaching of grammar" (2000: 549): first, monolithic descriptions of English grammar would be replaced by register-specific descriptions; second, the teaching of grammar would become more integrated with the teaching of vocabulary; and third, the emphasis would shift from structural accuracy to the appropriate conditions of use for alternative grammatical constructions.
While it is certainly not the case that any particular linguistic item can be automatically retrieved instantlymany linguistic constructions are simply too complex for this type of "instant" retrievalnevertheless, the process of corpus analysis has been greatly automated.
WebCorp Live originally used a process known as 'web scraping': the extraction of useful information from the HTML code of a web page, in this case the 'hit' URLs from the Google results page and examples Few researchers would now claim that the web is a corpus in any meaningful sense, but the web as corpus approach can still be fruitful for certain kinds of research and it is particularly useful for introducing newcomers to the field.
The challenges that we encountered are related to several distinct issues: (i) a strong assumption on regional variation underpinning the methodological design -while some language use specific to Montreal is related to language contact, not all is; (ii) inherent limitations of the methods we used, with BERT occasionally capturing phenomena unrelated to lexical semantics; (iii) inherent limitations of the data we used, with a carefully filtered Twitter corpus representing an improvement on highly generic datasets, but still suffering from the 280-character limit and the limited ability to validate user descriptions, among other issues; (iv) the complexity of the phenomenon under study, which often involves very subtle -but nevertheless perceptible and socially meaningful -differences in language use.
Added to this is a layer of inadvertent 'noise' created along the way as a corpus text travels from historical manuscript or print to digital edition.
Absolute type frequencies are trivially related to corpus size, with larger corpora yielding larger type frequencies, all other things being equal.
On the other hand, there are textbooks that focus one or more specific corpus-based techniques, discussing very specific phenomena (often the research interests of the textbook authors themselves) using a narrow range of techniques (often involving specific software solutions).
There will then be a description of some of the methodologies behind corpus research, with an emphasis placed on the word-based approach.
Thus, the syntactic or prosodic annotation of a corpus might be based on a different theoretical tradition than the one preferred by the researcher or one type of annotation that is necessary for the current study might be missing altogether.
The literature draws a distinction between corpus-driven and corpusbased linguistics, where the former is taken to be a scientific paradigm in the sense that behaviourist and generative linguistics are paradigms, that is, ontologies in terms of which linguistic theories can be stated, and the latter to be a methodology, that is, a collection of techniques for the formulation and testing of hypotheses within some already-existing paradigm.
But as corpus linguistics developed alongside the dominant paradigm of generative grammar, obvious differences and disputes resulted.
Such information will then help other users of your data to understand better what to expect from your corpus, or allow yourself to refresh your memory if you should use the corpus data once more after an extended period of time of not working with it.
Furthermore, the corpus text should be linked to the recording media of the primary spoken/signed text in a time-aligned format.
Note that the balance should consider not only the number of texts in each sub-corpus but should also consider the word count of the sub-corpora.
Learner corpora, particularly those containing academic texts, often include instances of multilingual practices, code-switching, and borrowed content, presenting difficulties for annotation and analysis.
The reason for this is that, as mentioned above, type-token ratios and hapax-token ratios are dependent on sample size.
The model aims to predict possible cases of negative transfer (when the CA shows the target language and the mother tongue to differ in a certain respect) and seeks to explain problematic uses -misuse, overuse, underuse -in the learner corpus (by checking whether they could be due to discrepancies between the target language and the mother tongue).
Firstly, a brief outline of what corpus linguistics is will be given.
In the following we present the main steps and layers in corpus design and their respective challenges.
To make the sampling manageable, corpus designers often start with a set of categories within which they aim to collect an unbiased sample.
For those interested in more information on corpus building and copyright laws, there are some sources to consult at the end of this chapter.
This is why literary scholars working in stylistics have started to acknowledge the interest of incorporating quantitative methods from corpus linguistics into the analysis of literary texts, so as to have a more objective starting point for identifying interesting themes and relevant excerpts, which should be the subject of a book's qualitative analysis.
Word pairs are ranked according to their bare frequency of co-occurrence in the latter corpus, and according to their MI score, setting a cut-off point of MI>2 with FQ≥2 to exclude hapaxes and extremely low MI values.
It does mean, however, that you would have a hard time finding corpora on paper, in the form of punch cards or digitally in HTML or Microsoft Word document formats; the probably most widely used format consists of text files with a Unicode UTF-8 encoding and XML annotation.
For example, in order to create a corpus of French, speakers from different regions should be included in the sample.
In the Helsinki Corpus, text samples range from 2,000 to 10,000 words in length.
The following sections describe the three different ways of describing the content of a corpus, beginning with a brief overview of Metadata and more detailed descriptions of Textual Markup and Linguistic Annotation.
If attempts are made to secure permission to use copyrighted material in a corpus, it is best to over collect the number of texts to include in each part of the corpus: A written text may be gathered for possible inclusion in a corpus, and its author (or publisher) may not give permission for its use, or (as was a common experience gathering written texts for inclusion in the American component of ICE) contact is made requesting permission but no reply is ever received.
As we saw above, this is easy to remedy by simply determining the exact number of times that the phenomenon in question occurs in a given sample.
The exploration can be done in different levels: all corpus or by user type (news agencies or individuals).
It is crucial to always remember that a corpus is a sample of language (see Section 1.4).
Excluding such data from corpus linguistics will in a sense also deprive us of the possibility to thoroughly evaluate structures vis-à-vis certain production conditions, for example, the production of elicited texts not based on established routines.
And where the corpus text is in a language that is not known to the scientific community it needs to be translated to a more widely known language.
In this section, we will discuss a number of heuristic techniques that can be used to interrogate the syntagmatic-paradigmatic organization across the whole lexicogrammatical continuum, and we will illustrate their potential relevance to diachronic corpus linguistics.
For these 6 Analysing Keyword Lists 133 languages, it might be the case that comparing surface forms of words from the corpus works less well than comparing lemmas, because the frequencies are too widely dispersed across different word forms (in a similar way to historical spelling variants) to be comparable.
Consequently, we suggest that it would be prudent to think about the principle of "knowing one's data" from a new perspective: as the sheer size of the modern mega-corpora prevents scholars from engaging with either all or the majority of the original source texts in as much detail as in the early days of historical corpus linguistics, they should strive for an intimate understanding of the historical corpora as mediators of the original texts.
As researchers can understand the needs of students thanks to such studies, they can review the curriculum, materials, and even teaching methods thanks to learner corpus-based studies.
In order to promote and make better use of corpus enrichment, there is a need for collaborative work between linguists with a deep knowledge of the needs to different areas such as Second Language Acquisition or Historical Linguistics and experts in Computational Linguistics or Natural Language Processing.
It contains chapters on innovative approaches to phonological corpus compilation, corpus annotation, corpus searching and archiving and exemplifies the use of phonological corpora in various linguistic fields ranging from phonology to dialectology and language acquisition.
One of the novel software applications, the Keyword analysis, uses significance tests to distinguish words that are significantly more frequent or significantly less frequent than in a reference corpus; calculations are carried out automatically by the program and it is possible to gain valuable insights into the material that cannot be achieved with qualitative study.
These tags, of which there are many, are assigned as part of an "idiom tagging" step after the initial assignment of POS tags and involve matching the sequences of POS-tagged words in the corpus against templates of multiword sequences.
Relying on existing parallel corpus resources poses its own challenges as well, as present-day parallel corpora tend to be quite small and/or poorly meta-documented and typically cover relatively few text types.
In Chapter 3, we implicitly treated the second issue as a problem of retrieval, noting in passing that we queried our corpus in such a way as to capture all variants of the lemma PAVEMENT.
First, it vividly illustrates, through the application of a corpus-based, bottom-up methodology, the close interrelationship between pragmatic function and context.
In these days of web search engines and vast quantities of text that is available at our finger tips, the end user would be mildly annoyed if a concordance from a 1-billion-word corpus took more than five seconds to be displayed.
We will begin by looking at epistemology, arguing that this is a major driver of corpus linguistics' integration with, or separation from, the social sciences.
As corpora have become larger and larger, oftentimes containing millions of words of text, the feasibility of, for instance, proofreading a corpus or placing individual samples into neatly delineated sections becomes less viable: Such corpora are simply too large for any kind of proofreading to be done, or for a single text type (such as a press editorial from a particular newspaper) to be placed into a single directory.
Between 2007 and 2011, International Journal of Corpus Linguistics, which was first published in 1996 and covers the areas of linguistics, applied linguistics, and translation studies, began to be cited widely, ranking until even recent years.
This involves not only collecting data (speech and writing) but encoding it: transcribing recorded speech, for instance, as well as adding annotation to it, such as markup indicating in a conversation when one person's speech overlaps another speaker's, and in writing where such features as paragraph boundaries occur in written texts.
A more exhaustive list of learner corpora in many languages is provided on the Center for English Corpus Linguistics (CECL) website, from UCLouvain in Belgium.
As we've seen before, the very first electronic corpus, the Brown Corpus of written American English, contained 500 (written) text samples of 2,000+ words, and its size and composition set a standard for the compilation of new corpora for many years.
Now trade annotation schemas with a classmate or trial it on a different language or text type.
This type of information is highly useful, as it not only makes it possible to study and compare different types of narrative, but it also shows how the corpus is balanced (or not) with respect to type of narrative.
Let us compare British English (the LOB corpus) and Indian English (the Kolhapur corpus constructed along the same categories) instead.
Some corpus programs will highlight collocates of a word of interest with different colours depending on their parts of speech.
Metadata is what distinguishes a corpus from a random collection of texts by giving it explicit structure: minimally any user of a corpus will know what amount of text data is contained in it, what characteristics texts have, for example, whether they are written for a newspaper or spoken during a conversation with friends or colleagues, who produced the text, and so on.
This is the crucial component of any corpus building or compilation project (see Chapter 6) or of carefully using the metadata of existing corpora.
For example, there are more than 100 collocates in the LOB corpus with a Fisher's exact 𝑝-value that is smaller than the smallest value that a standard-issue computer chip is capable of calculating, and more than 5000 collocates that have 𝑝-values that are smaller than what the standard implementation of Fisher's exact test in the statistical software package R will deliver.
The reference corpus was further limited to include only materials produced between 1900 and 1960 to correctly match the time period.
Explanations and examples of these applications of boostrapping in corpus linguistics are included in Sect.
But as much research in corpus linguistics has demonstrated, empirical data can both enhance and support the claims made in linguistic analyses.
While preparing the instructions for the annotation process, it is important for each category to be clearly defined so that the annotators know how to use them, in cases where the annotation is performed by humans and not automatically.
While these problems have not received as much attention than they could have from quantitative corpus linguists (as evidenced by, e.g., the body of research on measures of lexical diversity), the difficulties caused by the confounding effects of text length are only going to become more central to many studies, as more and more research is being done on social media and web data.
Despite justified enthusiasm about the recent advances in diachronic corpus linguistics, such concerns must be taken into account in corpus design as well as in the development of heuristic techniques to crack the code of past language systems and to explain variation and change.
But corpus linguistics was not only developed thanks to the creation of such tools.
These differences notwithstanding, we opted for variety rather than homogeneity of material types as this variety is more representative of the impact of corpus research on teaching materials.
The MPC satisfies this criterion too: each sentence in the corpus was selected from some larger text of which it is a part.
It is necessary to decide if a corpus should contain all types of written text samples or specific sets of text samples.
There are five word tokens that contain this suffix (directly, briefly, wisely, truly, and wisely), so its token frequency is five; however, there are only four types, since wisely occurs twice, so its type frequency in this passage is four.
Markup has similar applications in that it allows analyses to take account of the structure of the corpus texts.
This difference is not due to the respective sample sizes: the fiction section in the BNC is much larger than the newspaper section; thus, the difference token frequency would suggest that the suffix is more important in newspaper language than in fiction.
As a way forward in trying to find solutions to improve the adaptability of databases for linguistic research, Hiltunen calls for more interdisciplinary discussions and collaboration between corpus linguistics, Digital Humanities, and related disciplines, which are still too far from each other because of the disparities in their research agendas.
As discussed above, an effect size that specifically quantifies the difference between two groups (rather than an omnibus effect size measure) is probably most useful to report.
The graph-based data model allows for completely independent annotation layers, united only by joint reference to the same primary text.
On the other hand, if we want to use lemma as the unit of analysis, we need to automatically process the corpus to assign each form its part-of-speech and lump together all inflectional forms related to the same basethis involves a certain percentage of errors.
In many cases, the analysis is done at the word level, so a single analytic label or tag is assigned to each word of the corpus.
The concepts that we normally apply for this purpose are representativeness and balance.
As with representativeness, the issues around balance are fraught with difficultly, but the critical thing for the user of any corpus is to be actively aware of how that corpus is balanced, to think carefully about what this means for the generalisability of results based on that corpus.
We need scan and tolower to load and prepare the corpus file, and we will need gsub to clean up characters we do not want to consider, and we will need paste and gsub again to create a clean version of a character vector with one element that contains the whole text.
In corpus linguistics, we are essentially interested in how various structures (sounds, words, constructions) are used in particular contexts characterised by internal and external features; how possible variation in usage and choices between alternative structures (the so-called variants of a variable) can be correlated with such features, and how the use of particular forms can then be explained in terms of specific (qualitative) mechanisms relating to such features (see 4.
Thus, POS tagging should be done simultaneously with or after glossing.
It is for this reason that other computing tools, specifically devoted to corpus linguistics, have been developed.
For example, journals such as Corpus Linguistics and Linguistic Theory, International Journal of Corpus Linguistics and Corpora are all three specialized in the publication of corpus studies, whereas the Journal of Language Resources and Evaluation publishes articles on methodological aspects related to the compiling and processing of corpus data.
Using an R script (R Core Team 2014), I extracted all instances of I and you (when tagged as PNP) from all 21 files of the British National Corpus World Edition (XML) whose names begin with 'KR'.
Finally, most learner corpora are cross-sectional corpora, including one sample per participant and representing a given moment during the acquisition process, since most of the time learners included in a corpus have a homogeneous level of competence in the foreign language.
One of the earliest corpus-driven studies of this type was Salem's (1987) analysis of repeated lexical phrases in a corpus of French government resolutions.
The type/token ratio cannot therefore be considered as a reliable measure of linguistic development.
Finally, we discussed the different ways in which the corpus linguistics methodology makes it possible to provide valuable tools for the stylistic analysis of texts, as well as for author identification in a legal framework.
Still less work has been undertaken which contrasts collocation and semantic prosody in different languages (but see Sardinha  2000; Tognini-Bonelli 2001: 131-156; Xiao and McEnery 2006; Ebeling,  Ebeling, and Hasselga º rd 2013); yet findings yielded by this kind of research can be particularly valuable in language typology, language education, contrastive linguistics, and translation studies.
Over time, the term came to be used for any natural-language dataset used by a linguist -so that, for example, a field linguist might refer to a set of sentences elicited from an informant as their 'corpus'.
Although meta-analysis has yet to take root in the field of corpus linguistics, we look forward to future applications of meta-analysis in this domain.
In fact, it would be hard to see how a register analysis could be achieved without the use of corpora.
The most widely used tools, and the specific areas of multimodal corpus research that is supported by these tools, focusing on corpus compilation, annotation and analysis, are presented below.
However, the 'web for corpus' approach utilises search engines in a different way, using them at the initial stage of corpus building only.
Even when the coding schemes are intended to be theoretically as neutral as possible, the POS tags always add an analytical layer to the corpus, which reflects a particular theoretical stance towards word class categorization.
As we saw in the preceding chapter, corpora fell out of favor just as linguistics grew into an academic discipline in its own right and as a result, corpus-based studies of language were relegated to the margins of the field.
In fact, it can be argued that intuitionbased linguistics developed as a reaction to corpus-based linguistics.
Even though the processes of collecting and encoding data for inclusion in a corpus are described above as separate processes, in many senses they are closely connected: After a conversation is recorded, for instance, it may prove more efficient to transcribe it immediately, since whoever made the recording will be available either to transcribe the conversation or to answer questions about the recording to aid in its transcription.
Before covering the steps in corpus building, we should acknowledge potential copyright issues.
This filtering process affects two dataset outputs which used for different purposes in the corpus.
These corpora are intended to cover all stages of the history of English, and as such, the annotation scheme has been designed to be backwards compatible all the way to Old English.
Although it's of course somewhat easier to confine our corpus-based investigation to single lexical items, single words and their frequencies aren't the only interesting things we may want to analyse with the help of corpora.
First of all, they should be linked to the corpus text data and their metadata so that corpus users can evaluate how annotators arrived at their annotation decisions; at the same time the annotations are ideally extractable in order to be analysed outside the corpus infrastructure itself, for example, by programmes for statistical analysis.
The majority of learner corpus studies are based on raw data, i.e.
We have pointed out that corpus files should contain plain text, in order to facilitate data analysis.
There are many excellent textbooks in print, providing thorough introductions to the methods of corpus linguistics, surveys of available corpora, and general reviews of previous research.
Based on metadata from the questionnaire, it is possible to select a subset of the ICLE corpus, for example to study systematically potential differences in language use between learners who have and who have not spent any time abroad in a country where the target language is spoken natively-and thus test a hypothesis from Second Language Acquisition research.
In corpus linguistics the procedure identifying and marking the boundaries of word tokens is called tokenisation.
To summarize, the text-corpus method is an immensely powerful tool in linguistic research that enables researchers to gain valuable insights into the rules and patterns of a language by analyzing a vast collection of texts.
The individual contributions discuss these issues and illustrate current practices in corpus design, data collection and annotation, as well as strategies for corpus dissemination and for increasing the interoperability between tools.
In this chapter, we will use the Corpus of Contemporary American English (COCA) to illustrate the most commonly identified units of language that researchers use for their analyses: words, collocations, n-grams/lexical bundles for lexical patterns, and part of speech (POS) tags for grammatical patterns.
As we will see later in this chapter, this method shares the use of a quantitative methodology with corpus linguistics.
While understanding variation and contextual differences is a goal shared by researchers in other areas of linguistic research, corpus linguistics describes language variation and use by looking at large amounts of texts that have been produced in similar circumstances.
As such, it has become an important practical challenge in corpus linguistics to determine how data annotation practices can evolve along with the needs of researchers (e.g.
Despite the large number of texts and the relative ease of obtaining numerous examples, a corpus analysis does not only involve counting things (quantitative analysis); it also depends on finding reasons or explanations for the quantitative findings.
We can also type out the p-values (although with large corpora with many files these are always very low) or specify the CIs.
Throughout our discussion of epistemological concerns, we will note debates within corpus linguistics that echo these debates in the social sciences.
As the results of corpus-based research on grammatical change accumulate and as the methods of analysis become more sophisticated, the corpus ceases to be merely a tool and becomes an active ingredient in the further development of usage-based theoretical models.
In addition to looking at p-values, we should also consider the size of the observed effect in the sample and the estimation of the size of the effect in the population, which can be expressed as a confidence interval (see Sect.
In Section 2.4, I will reflect on the question of whether corpus linguistics is now tooldriven, i.e.
In theory this process could run indefinitely but crawls run for corpus-building purposes tend to be restricted to a fixed time period.
In natural language processing (NLP), contextualized word embeddings generated with LLMs are often shown to perform impressively at "downstream tasks", like part-of-speech tagging, dependency parsing, or named-entity recognition (e.g.
Section 4 concludes the paper by bringing the two strands of the discussion together by showing how Open Corpus Linguistics can contribute to overcome several widely discussed problems in corpus linguistics.
In this respect, parallel corpora are clearly lagging behind compared with other corpus types, such as learner corpora, which are more richly documented (Chap.
However, we may not have thought of all potentially unclear cases before we start annotating our data, which means that we may have to amend our annotation scheme as we go along.
Since mainstream corpus linguistics focuses on English and other Western European languages, the Latinbased script used in these languages is most common, and relevant corpus software and query mechanisms are based on it.
The advantage is that very detailed information about a spoken text is provided to the user, thus ensuring that a broad range of studies can be conducted on the corpus without any doubt about the authenticity of the data.
This information can be considered comparable to metadata for texts in a corpus or to demographic data in a study with human participants.
Furthermore, the annotation itself is accompanied by a hermeneutic dimension, which reflects the annotators' point of view about the text.
An example of metadata based on a questionnaire from the International Corpus of Learner English (ICLE) is summarized in Fig.
Sections 2.1 and 2.2 contrast studies based on their major methodological approaches, distinguishing between corpus-based studies of pre-selected lexical expressions versus corpus-driven studies to identify the full set of important multi-word sequences in a corpus.
To distinguish this type of collection from other collections of texts, we will refer to it as a linguistic corpus, and the term corpus will always refer to a linguistic corpus in this book unless specified otherwise.
In the same way, the consultation interface for the CLAPI corpus (see section 5.3 for a description) does not currently provide information about the total number of words included in the portion of the corpus that is available for online consultation.
The following discussion is based on the full data from the fiction subcorpus and a subsample of the newspaper corpus that was arrived at by deleting every second, then every third and finally every 192nd example, ensuring that the hits in the sample are spread through the entire newspaper subcorpus.
This ensures the extracted sample corpus is reasonably clean and can therefore be expected to provide reliable output for standard corpus linguistic tasks.
Instead, I will provide an overview of the fundamental ideas of corpus linguistics and some key methods and practices -starting with how we approach corpus design and construction, moving on to a summary of the most widely used corpus methods, and finishing with a very brief survey of some applications and advanced forms of analysis.
The type/token ratio can also be used for comparing texts and their translations, only in cases of similar length.
This led, among other things, to the creation of new types of corpora, like longitudinal corpora representing different stages in the language learning process, to the collection of new types of metadata, such as information about the learner's mother tongue and exposure to the target language, and to the use of new methods to annotate or query the corpus, for example to deal with the errors found in learner corpora.
In this case, the annotation scheme should contain not just operational definitions, but also explicit guidelines as to how these definitions should be applied to the corpus data.
We consequently decided to investigate the frequency of sentence-initial as well in COHA, a corpus that provided us with enough material to examine its diachronic development in recent American English.
Chapters 10-16 in Part III review different corpus types (diachronic corpora, spoken corpora, parallel corpora, learner corpora, child language corpora, web corpora, and multimodal corpora), with each chapter focusing on the specific methodological challenges associated with the analysis of each type of corpora.
These developments have proven to be particularly beneficial to the emergent field of multimodal corpus linguistic enquiry.
It is important to stress that the decisions made at each stage of the web corpus building process will have a significant impact on the resulting corpus, in terms of size but also in terms of composition.
The most common type of annotation is the assignment of a grammatical category to each word in the corpus, as we have already mentioned.
Metadata capture properties of the (written) corpus text (text format, encoding, script, structure of annotations, etc.
Variationist corpus linguistics, which focuses on the proportions of variant items or constructions, is not as heavily affected by the issue.
This study is significant because it analyzes frequent conversational features that previously had not been systematically researched using corpus linguistics.
Unfortunately, there is simply no space here to give a full account of all the areas of linguistics, and of other fields of study, where the methods of corpus linguistics have been productively applied.
Included within the spoken part of the BNC is a sub-corpus known as the Corpus of London Teenage English (COLT).
These features have resulted in Python becoming one of the most popular languages used in corpus linguistics work today.
On the other hand, it might also be argued that the feature was not identified by previous corpus-based research so it is, by definition, not corpus-based.
For example, if we want to know whether learners of French as a foreign language at an advanced level are able to use collocations as native speakers do (collocations such as "prendre une décision" -to make a decision -or "pleuvoir à verse" -to pour with rain), we can search for occurrences of these expressions in text corpora produced by learners and compare the number of times these expressions appear -and their frequency -in a corpus of similar textual productions made by native speakers.
Taggers in XML thus provide a good way for associating annotations with corpus data.
Some work in corpus linguistics has drawn on such resources, including panel survey data (e.g.
This handbook provides a comprehensive overview of the different facets of learner corpus research, including the design of learner corpora, the methods that can be applied to study them, their use to investigate various aspects of language, and the link between learner corpus research and second language acquisition, language teaching and natural language processing.
In this section, I shall discuss one example each from two areas in which corpus research is currently booming.
As an alternative to some of the steps we modelled as regexes above, and for slightly more convenient manual annotation of the remaining XML structure, you could also use an annotation tool, such as my Simple Corpus Tool, which actually includes an editor that'll allow you to add these tags and attributes through the click of a button.
It is evidently impossible to collect a sample of the whole language, but in the same way that a general language dictionary aims to describe the common lexicon of a language, the general corpus seeks to offer a global image, including the main textual genres found in language.
It is intended as a monitor corpus yet it does not contain examples of the latest trends in language use, which tend to be found in blogs and other less formal text types.
Dataset is a series of corpus-based findings that can be statistically analysed.
This type of information is included in the "headers" of the text but will not be read by the concordance software.
Another option for effect size measure is to use probability of superiority (PS) discussed in Section 8.4.
By means of a corpus study, we will be able to identify all the types of errors produced and then quantify each of them: for example, 30 spelling mistakes, 12 lexicon errors, 20 syntax mistakes, etc., made every 100 words.
The perspective that we will take is, as you can tell from the title of the book, related to an area of language study called corpus linguistics.
How did its type/token ratio change in these two files, and what is the MLU in both files?
Despite the long-recognized importance of considering proper nouns and names in corpus annotation schemes, many contemporary linguistic corpora lack the capability to exclude these items effectively when setting up queries.
In each case, chapters assess what we have learned from corpus-based investigations to date, and provide detailed case studies that illustrate how corpus analyses can be employed for empirical descriptions, documenting surprising patterns of language use that were often unanticipated previously.
Part II deals with corpus methods: Chapters 4-9 provide an overview of the most commonly used methods to extract linguistic and frequency information from corpora (frequency lists, keywords lists, dispersion measures, co-occurrence frequencies and concordances) as well as an introduction on the added value of programming skills in corpus linguistics.
Software packages for corpus analysis vary in capability, but they all allow us to search a corpus for a particular (set of) linguistic expression(s) (typically word forms), by formulating a query using query languages of various degrees of abstractness and complexity, and they all display the results (or hits) of that query.
Although the focus in this chapter and the handbook is on tools and methods for English corpus linguistics, I highlight issues of support for other languages and corpora and tools that support multiple languages where they are relevant.
All of the files in a single time period would be available in a single folder so that each sub-corpus could be loaded separately.
In the context of corpus linguistics, the most obvious role for the Fisher exact test is to measure dependencies between collocations, or in the case of Stefanowitsch and Gries, dependencies between words and constructions.
Hyphenated tags also have a useful role to play in the tagging of diachronic corpora where a word may come to be associated with different parts of speech or different functions through time (cf.
Taking this into account, we can now posit the following final definition of corpus linguistics: Definition (Final Version) Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus.
This is exactly what the methodology of corpus linguistics offers to researchers in disciplines beyond linguistics.
As outlined in this chapter, multimodal corpus research is somewhat still in its infancy and as such we can expect a step-change in our description and understanding of language based on this research.
The concordance analysis also noted several contradictory (continued) representations, including the view that foreign doctors were desperate to work in the UK and were 'flooding' into the country (similar to the water metaphor used to describe refugees), appearing alongside other articles which claimed that foreign doctors 'ignore' vacancies in the UK.
Next, the chapter explains the basic building blocks of all software programs, and then provides a number of criteria that can be used to assess the suitability of a programming language for a particular corpus linguistics project.
In the field of written French language acquisition, the Littéracie avancée corpus produced by the Laboratoire linguistique et didactique des langues étrangères et maternelles (LIDILEM) of Grenoble Alpes University is made up of writings by undergraduate and master's degree students, covering the entire span of study.
And, of course, word frequency lists also provide the basis for many probabilistic approaches to language processing, such as establishing collocation measures or conducting probabilistic PoS tagging, some of which we've already discussed before, and others we'll turn to soon.
Both problems can be solved (or at least alleviated) by testing the annotation scheme on a smaller dataset using two annotators and calculating its reliability across annotators.
Thus, the pressing academic quest is to review past achievements as well as future directions of corpus linguistics.
In Chapter 2, we outline the basics of a register approach to language analysis and then ask students to refer to this same framework when building their corpus and doing their corpus study.
The classification tree resulting from the data can be interpreted as follows: Starting from the top, if the subordinate clause type is causal, go left and "predict" mc-sc; if the subordinate clause type is temporal, then go right and check the length difference of the main and the subordinate clause: if that difference is < -2.5, go left and also "predict" mc-sc, otherwise go right and check whether the conjunction is before.
Let us exemplify this by means of a character string that may be found like this in the BNC with SGML annotation: Every word is preceded by a POS tag in angular brackets, just as in the above examples.
And, of course, a simple listing of the single word form without context in a frequency list would (normally) not allow us to disambiguate our examples, as would for example be possible through sorting a concordance of the word by its left or right context.
Then, 95% confidence intervals were calculated for the two subcorpora and r was used as a standard effect size measure.
However, what you've just seen on the exercise page is not only an issue in corpus linguistics, but actually represents a much more prevalent problem on the internet.
Other kinds of corpus linguistics are interested in variation between many kinds of register and genre, and often in the differences between spoken and written modes.
Keyness in corpus linguistics is but the first statistical step in the analysis of texts.
Having each word in the corpus tagged according to its part of speech (i.e., word class) permits, for instance, the study of partiallyfilled constructions (e.g., N-PROP of N-PROP; "the Einstein of Italy") and of lexical items of a specific part of speech (e.g., love_V; "I love you").
Even more fundamentally, however, the corpus-linguistic study of language has great potential not only to show differences between literary and non-literary language but to shift the focus to the similarities between the two.
Proper names and multi-word proper names in particular pose challenges to the study of language use, and many currently available linguistic corpora are lacking in this kind of annotation.
In studying variation by gender in ICE-GB, the analyst will want to review the results with caution, since this corpus does not contain a balanced sample of males and females.
Given our definition of representativeness, the major issue with very small corpora is that they will hardly have a chance to achieve any reflection of a wider range of contexts of language use.
Also common in corpus linguistics is generalized linear regression.
For reasons of space, we won't look at further examples of other types of annotation here now, but will later work more extensively on a sample that combines some of the different levels.
Once again, the answers to these questions depend on the type of corpus the researcher has in mind.
Each corpus type has its own advantages and disadvantages.
As we will see in the following chapters, corpus linguistics may be of use in all areas of linguistics, for instance in fundamental (see Chapter 2) or applied (see Chapter 3) linguistics.
However, while the statistical properties of language are a worthwhile and actively researched area, they are not the primary object of research in corpus linguistics.
Finally, we presented some recommendations for the creation of an annotation manual, which documents both the content and the annotation process itself in order to enable other researchers to reuse it.
A concept that is intrinsically related to representativeness is balance, which has to do with the proportions of the different samples included in the corpus.
Exemplification throughout the discussion is based on data abstracted from the Diachronic Electronic Corpus of Tyneside English .
Therefore, in addition to the file describing the editing process, you'll probably want to keep at least one extra file that lists the contents of the corpus file-by-file if your data contains materials from different genres, text types or domains, or, as with our web page data, that lists information about where the file was retrieved from, when, what the original file name was if you've changed it), etc.
They typically allow the user to define a set of annotation categories with appropriate codes, import a text file, and then assign the codes to a word or larger textual unit by selecting it with the mouse and then clicking a button for the appropriate code that is then added (often in XML format) to the imported text.
Since overcoming the fallibility of introspective data is one of the central motivations for using corpora in the first place, the analysis of a given phenomenon must not be based on a haphazard sample of instances that the researcher happened to notice while reading or, even worse, by searching the corpus for specific examples.
Likewise, we also consider how social science theory is exerting influence on studies in corpus linguistics.
The preparation of samples to be included in the corpus poses two important methodological questions: on the one hand, the appropriate size for each sample, and, on the other hand, how to balance the portions of the corpus in such a way that the result is truly representative of the genre.
In this traditional corpus design, the aim of the corpus creators is to achieve an unbiased sample of texts in the categories from the sampling frame.
Again, corpus linguistics is a uniquely useful tool to investigate this.
Of course, just because only text samples are included in a corpus does not mean that sections of texts ought to be randomly selected for inclusion in a corpus.
Hence, corpus-based typologists need to carefully evaluate the specific properties of the corpora they use vis-à-vis the phenomena they are researching.
Dynamic corpora grow over time as more and more texts are included (monitor corpora are a type of dynamic corpus).
On the other hand, natural language processing researchers involved in parallel corpus compilation projects could try to document, whenever possible, meta-information that is of paramount importance to contrastive linguists and translation scholars, such as translation direction (from L X to L Y , or vice versa) and directness (use of a pivot language or not).
Accepting (or working around) the corpus creators' assumptions and decisions concerning POS tags and annotations of syntactic structure may seriously limit or distort researcher's use of corpora.
Likewise, a host of techniques that many corpus linguists would want to use are absent, including collocation analysis and keyword analysis.
After the project began, it was discovered that not all the regional groups involved in the project would be able to collect all of the text categories originally planned for inclusion in the corpus.
One area that reflects this complexity and that has efficiently adapted social media is Corpus Linguistics (CL).
Corpus-based dialectometry requires dialect corpora that are representative of multiple geographic areas associated with a particular language variety.
The goals of the CHECL are to complement, but not duplicate, the coverage of existing textbooks and handbooks on corpus linguistics.
In Section 2, an initial survey demonstrates the importance of the register factor in historical corpus linguistics and introduces a number of central matters that arise when the concept of register is applied to diachronic material.
If a word is repeated, it counts as a new token but not as a new type.
Metadata are data about the corpus files and the structure of the corpus as a whole and the compilation process (including design decisions), as well as data about the situational characteristics of texts.
Many of the considerations involved here are also relevant for the reverse perspective of a corpus user or analyst who needs to understand considerations of corpus compilation (both theoretical and practical ones) in order to properly evaluate their corpus findings.
Later, we will see that the use of the empirical methodology ingrained in corpus analysis can also work as a guide for the translator when it comes to making certain translation choices.
Rather than attempting to create a complete and exhaustive list, I focus on a handful of corpora (and related resources, such as text archives and the "Web as Corpus") that are representative of general classes of corpora.
Header elements may for instance be the page title (contained in the <title>…</title> tag, where the forward slash indicates the end of the tag in the closing part), which then appears in the title bar of your browser, or possibly meta-information (<meta>…</meta>), such as the text (content) type or encoding/character set (charset), as well as style (sheet) information/references (<style>…</style> or <link rel="stylesheet" href="./corpus.css" type="text/css" />, in our case) responsible for some of the page formatting.
In addition to lemmatization, words can be annotated into grammatical categories thanks to part-of-speech tagging, as we previously mentioned in relation to word annotations such as ferme and car.
Type in "say_spoken" under "Create new list" above the sample concordance lines and save the list.
However, at this point in time, quantitative corpus linguistics is becoming more and more established also in specific linguistic subdisciplines, which raise their own, more specialized problems.
One way to understand linguistic analysis and language is through corpus linguistics, which looks at how language is used in certain contexts and how it can vary from context to context.
Despite the similarity between the two corpora in their web-based content, the striking difference in size is clearly related to a difference in corpus design: while iWeb includes just about any text from almost any website, CORE has been created with the idea of identifying as much and as clearly as possible the situational characteristics of all texts included which constitute the web registers mentioned in the title.
Such a case would suggest a need to revise the original corpus design.
We have also seen that corpus linguistics often resorts to a quantitative methodology, studying a large sample of data which is representative of the phenomenon studied, with the aim of generalizing the observations to the whole of the language or to a language's register.
The confidence interval (CI) in inferential statistics is an attempt to move away from the dichotomous thinking that is often connected with NHST, statistical tests and p-values.
Variation often involves complex patterns of use that involve interactions among several different linguistic parameters but, in the end, corpus analysis consistently demonstrates that these patterns are systematic.
Metadata is a key component of any corpus: users need to know precisely what is in a corpus.
Tagsets are developed for specific languages, and often indeed for individual corpora, so that when you use a corpus, you need to consult the corpus metadata, manual, or other documentation.
A corpus is a principled collection of language data taken from real-life contexts.
Despite some limitations (gaps in the corpus for certain text types, as acknowledged by the authors, but also lack of reference to STs and parallel observations), this study is an example of best practice, nicely combining a careful corpus design, a linguistically motivated choice of patterns, solid grounding in theory, and sophisticated statistical techniques complemented by intuitive graphic representations.
In the case of affixes with low productivity, this will typically add little insight over studies based on dictionaries, but for productive affixes, a corpus analysis will yield more detailed and comprehensive results since corpora will contain spontaneously produced or at least recently created items not (yet) found in dictionaries.
The first step of annotation is called the structural markup, which gives descriptive information about the text; thus, structural markup is used to create the text's metadata.
Computational methods and tools for corpus annotation therefore take two forms.
The frequency list based on lemmas confirms our initial assumption (based on Zipf's law) about the number of words with the frequency of 30 and over: there are 3,196 such lemmas in the corpus.
The disadvantage of this approach is that the top collocates will typically be function words co-occurring with the node merely by the virtue of their frequency anywhere in the corpus.
If the corpus compiler or user has reason to suspect that a given register is linguistically heterogeneous, measures need to be taken to make period samples of the corpus comparable in this regard.
Therefore, this preparatory process should not be taken lightly, especially because corpus compilation and preparation, if done well, is a very time-consuming effort.
Many different statistics can be selected to determine the significance of the difference in the frequency of a word that occurs in close proximity to the node word against its frequency in the remainder of the corpus, e.g.
However, in the interest of overcoming compatibility problems that arose due to how different languages used different character encodings, the field of corpus linguistics has been moving towards using only one unified (i.e., not language-specific) multilingual character encoding in the form of Unicode (most notably UTF-8).
The following sections highlight key advancements within corpus linguistics on the study of speech, starting with characteristics that differentiate speech from writing (Section 2), and then moving to characteristics of particular spoken registers (Section 3), and specific individual features associated with speech (Section 4).
As corpus users we therefore need to think critically about the nature of the evidence that corpora provide in terms of their quality (representativeness and balance) as well as their quantity (corpus size).
That is to say, rather than telling you about the discipline of corpus linguistics -its history, its place in linguistics, its contributions to different fields, etc.
The Web as corpus, seen here through the lens of Google-based searches Finally, we will consider very large "hybrid" corpora, which take data from text archives or the Web, but which then deliver this data through powerful architectures and interfaces.
Between 2002 and 2006, although researchers still cited corpus-based grammar references for their studies (e.g., Cambridge Grammar of the English Language), one group of researchers made use of newly developed datasets, both large or small, such as The CHILDES Corpus, Wordnet, and A New Academic Word List.
The syntactic annotation scheme is a simplified and in some ways deliberately agnostic version of generative X-bar theory.
A novice student of linguistics could be excused for believing that corpus linguistics evolved in the past few decades, as a reaction against the dominant practice of intuition-based linguistics in the 1960s and 1970s.
The Oxford Collocations Dictionary for Students of English is the only one of the three collocation dictionaries which is corpus-based.
The Contemporary Historical Corpus of American English (COHA) is an example of a diachronic corpus.
Nearly all corpus analysis software permits the generation of a concordance -a listing of all the instances of a word or phrase in the corpus, together with some preceding co-text and some following co-text (usually anything from a handful of words to a couple of sentences).
If your project involves comparing the frequency findings of sub-corpora in your corpus (or to another existing corpus), you will need to employ a simple statistical procedure known as "normalization" to ensure that your results are comparable (see a brief mention of this in Chapter 6).
While word-class annotation is very well established in corpus linguistics, there are other types of annotation as well.
Corpus linguistics can be defined as an empirical discipline par excellence, since it aims to draw conclusions based on the analysis of external data, rather than on the linguistic knowledge pertaining to researchers.
The underlying principle of semantic tagging is the same as POS tagging, namely, the assignment to every word in a text or corpus of labels representing categories within a scheme of analysis.
CIA studies raised the issue of the norm (native vs. non-native; novice vs. expert) and pointed to the benefit of relying on an explicit corpus-based norm rather than the implicit and intuitive norm that underlies many SLA studies.
These two cases clearly represent issues related to automatic PoS tagging and the theoretical assumptions and rules behind it.
We can also observe that the frequency of words in a corpus decreases rapidly.
Given the methodological advantages of cluster analysis for hypothesis generation, the hope is that this book will foster its adoption for that purpose in the corpus linguistics community.
Second, corpus-based studies have shown that dialect variation can involve a much wider range of linguistic variables than is generally analyzed in dialectology and sociolinguistics.
The primary aim of corpus linguistics is to understand linguistic patterns and explore how and why they occur.
Platforms which do limit posting length, such as Twitter (now X), usually limit the maximum length, confining all of their content into the range of short texts, which is mathematically difficult to work with in quantitative corpus linguistics.
In principle, it is also possible (but not advisable, unless the user knows well what she or he is doing) to take the p-values without the Bonferroni correction (testtype = "Univariate") or to use the test statistics themselves instead of the p-values (testtype = "Teststatistic"); • 0.95 as the minimal 1p value needed to implement a split, defined by mincriterion = 0.95.
I then present what I take to be the methodological foundations that distinguish corpus linguistics from other, superficially Preface similar methodological frameworks, and discuss the steps necessary to build concrete research projects on these foundations -formulating the research question, operationalizing the relevant constructs and deriving quantitative predictions, extracting and annotating data, evaluating the results statistically and drawing conclusions.
The type of corpus required will determine what seeds should be chosen.
The PDEV is an ongoing corpus-driven project in which a procedure called Corpus Pattern Analysis (CPA) is applied to identify the various patterns in which a verb is used and then discover how exactly meanings arise from each of the patterns.
A specialized corpus is a corpus type represented by a collection of texts compiled from a particular genre (newspaper articles, agreement letters, academic articles, lectures, essays, etc.).
However, in light of the above discussion of representativeness, there is little reason to believe that any of these corpora, or the many others that fall somewhere between BROWN and ICE, even come close to approximating a random sample of (a given variety of) English in terms of the text categories they contain and the proportions with which they are represented.
In particular, spelling variation causes problems for POS tagging, concordancing, keywords, n-grams, and collocation techniques.
As in all research, there is much in corpus linguistics that is subjective, including the choice of research question and of the procedures and software to employ, not to mention the interpretation of the output data.
In particular, all of the above approaches only deal with the minimal amount of information one should include-the more comprehensive information regarding token and type frequency distributions and entropies still awaits first exploration.
Although each chapter includes a broad summary of previous research, the primary focus is on a more detailed description of the most important corpus-based studies in this area, with discussion of what those studies found and why they are especially important.
A typical example of this type of data is the corpus that contains newspaper archives or parliamentary debates.
Thus, the first step in a corpus study is to identify relevant sources that have so far explored the research subject under consideration.
Descriptive statistics should also be reported with care in research papers, making sure at least the following values are reported: sample sizes, number of cases by main groups (i.e.
There is also TEI-conformant Textual Markup to describe features occurring within a particular corpus.
There have been different ways of annotating named entities in corpus data, and the strategies tend to reflect both the various interests (e.g., translation and information retrieval, alongside general language study) as well as the practical possibilities of doing so.
In much of the code below -in particular in Chapter 5 -I will often use quite long names for data structures etc., which is really only in the interest of recoverability or ease of parsing and recognizing things: Sometimes, names such as qwe or asd will be used because they are superfast and virtually error-free to type on an American QWERTY keyboard, but in the long run, i.e., for your real research projects, names such as sentences.without.tags or cleaned.corpus.file are infinitely more revealing than the kind of names (e.g., qwe or aa) I have mostly used so far.
This is a problem not only for studies that are interested in linguistic variation but also for studies in core areas such as lexis and grammar: many linguistic patterns are limited to certain varieties, and a corpus that does not contain a particular language variety cannot contain examples of a pattern limited to that variety.
The most widely used XML schema for coding corpus data is the one provided by the TEI (Text Encoding Initiative).
The Brown Corpus is composed of just 500 texts, and it is very easy to achieve 100 percent accuracy in terms of metadata.
This might mean setting the effect size in advance and then calculating (a-priori) how big our corpora need to be, or at least being able to (post-hoc) calculate and compare the power of our corpus comparison experiments.
Corpus linguistics has had a major influence on such applications over the past two decades, so that it is now almost impossible to find a research journal in applied linguistics, language teaching, translation studies, or lexicography that does not regularly publish articles utilizing corpus research findings.
Since the notion of word is a little ambiguous here, it is useful to introduce a common distinction between (word) type and (word) token.
We will see that such variation can militate for, or against, interaction with corpus linguistics.
Thus, a concordance of a word w is a display of every occurrence of w together with a user-specified context; it is often referred to as KWIC (Key Word In Context) display.
Parsing can also be used while annotating a corpus, done through grammatical markup inserted by a software program called a parser that automatically assigns labels to forms beyond word level (phrase, clause, etc.).
In general, the repertoire of corpus-linguistic studies is fairly broad from corpus-based but mainly qualitative studies to advanced statistical methods and computer programs specially designed for the research question under investigation.
Considering the relationship among most sited publications and the salient academic research themes, it seems that the corpus linguistics has become a linchpin of certain academic disciplines.
This application has been more widely used by corpus linguistics researchers than the previous two applications.
It is therefore not surprising that the original corpus design included letters (both social and business) as a text category to be sampled for the written part of the corpus.
Many of them, for instance those just targeting words and their tags, could be done with regular expressions, but the kinds of searches exemplified here at the end show how useful some knowledge of how to use R for XML with XPath searches really is because they allow you to combine information from different levels of annotation relatively easily -all of the above is possible even when you treat an XML file as a text file -but trust me, it's painful.
Even if we are pursuing a well-motivated bivariate research design and find a significant influence with a strong effect size, it may be useful to take additional potential influencing factors into account: since corpus data are typically unbalanced, there may be hidden correlations between the variable under investigation and other variables, that distort the distribution of the phenomenon in a way that suggests a significant influence where no such influence actually exists.
If the text is annotated or corrected by hand then this could form the basis of a training corpus for an automatic tagging system which can then learn from the human annotators in order to attempt to replicate their coding later on larger amounts of data.
In this paper I shall present a view of Corpus Linguistics that conceptualises it in three phases, distinguished by the use made in each phase of quantitative data.
Given the considerable interest in utilizing the corpus linguistic approach, in addition to the dynamic and interdisciplinary nature of current studies involving partnerships among disciplines, a comprehensive and systematic overview of the development of and relationships among individual research in the fields of corpus linguistics is called for.
This definition is close to the understanding of corpus linguistics that this book will advance, but it must still be narrowed down somewhat.
We have also illustrated the notion of a corpus-driven study, as we extracted lexical items (n-grams) from a small corpus and showed what kinds of questions a keyword analysis can answer.
For example, one current issue for the study of phraseology in corpus linguistics concerns the best methods to be used for the identification of the most important lexical phrases in a corpus.
The final distinction I would like to mention at least briefly involves the encoding of the corpus files.
With Primary Sources this is only possible if the documents are individually downloaded and analysed using some other corpus tool.
Much corpus research can be characterized as searching for variables in corpora and analysing the relationship between them.
Future research should continue to investigate how different variables such as frequency profiles of target features, selection criteria used, or corpus design may affect the reliability of lists.
Random co-occurrence baseline ('shake the box' model): we compare the observed frequencies with frequencies expected by chance alone and evaluate the strength of collocation using a mathematical equation which puts emphasis on a particular aspect of the collocational relationship.
LCR as a field only emerged and became visible in the course of the 1990s in the context of the popularization of corpus linguistics at large but has rapidly evolved and grown in scope and sophistication over the past four decades.
As we will see in this section, the best solution may be to take the texts from a text archive or the Web (containing billions of words of data), and then combine this with a robust corpus architecture.
Such disambiguation remains both a theoretical and practical challenge for corpus-based frequency list research.
As has been observed, corpus linguistics is a fairly new and rapidly growing discipline.
In this type of corpus, development is inferred via between-group comparisons, i.e.
We will also discuss a newer method that is becoming popular in corpus linguistics and linguistics more widely, called recursive partitioning, which includes analyses like classification trees and random forests.
Specifically, we will discuss three types of data (or levels of measurement) that we might encounter in the process of quantifying the (annotated) results of a corpus query (Section 5.1): nominal data (discussed in more detail in Section 5.2), ordinal (or rank) data (discussed in more detail in Section 5.3, and cardinal data (discussed in more detail in Section 5.4.
The first chapter of this book started with a similar definition, characterizing corpus linguistics as "as any form of linguistic inquiry based on data derived from [...] a corpus", where corpus was defined as "a large collection of authentic text".
This example illustrates the influence of the methodological choices associated with data annotation and the conclusions that can be drawn from a corpus study.
As an attempt to consider how corpus research can be incorporated into materials to reflect authenticity we propose a number of ways in which corpus research can be used to inform materials.
This could be done using the measure of referential distance discussed in Section 3.2 of Chapter 3, which (in slightly different versions) is the most frequently used operationalization in corpus linguistics.
Finally, we will discuss briefly the features of the CLAN concordancer which makes it possible to explore data coded in CHAT format, the annotation standard used in the CHILDES database.
While this is a fairly trivial reflection of groups of languages with little specific relevance for corpus linguistics, a grouping that is more important for our purposes is that between wellstudied and lesser-studied languages.
Since we are most likely to deal with nominal variables in corpus linguistics, we will discuss in detail an example where both variables are nominal.
The first distinction we can make among all the existing corpora is the one that classifies them into a sample corpus and a monitor corpus.
We can avoid these problems by drawing our sample from the corpus itself.
We use lines to add the type-token ratio plot and add smoothers with lines(lowess(...)).
This brief overview of the current state of corpus-based research is explored in detail in this second edition of English Corpus Linguistics.
Both types of analysis have something to contribute to corpus-based language study.
However, would we say that this person is "doing" corpus linguistics?
Where the annotation scheme is to blame, it could be revised accordingly and re-applied to all unclear cases.
When naming files for the corpus, it is useful to have the file name in some way reflect what is in the file.
The first one basically gave you a means of designing your own ways of classifying your data in a sensible manner, which then enables you to use the corpus-based analysis methods we've discussed throughout the course in order to extract and count relevant information.
This is very much faster than most desktop corpus analysis tools, which have to deal with color highlighting and other display issues.
A more careful analysis will often attempt to quantify the number of concordance lines that exemplify a given function or contextual feature, and to make sure that every single concordance line has been inspected individually, to identify exhaustively all possible categories and patterns of usage.
While corpora will always be skewed relative to the overall population of texts and language varieties in a speech community, the undesirable effects of this skew can be alleviated by including in the corpus as broad a range of varieties as is realistic, either in general or in the context of a given research project.
As this textbook is more practical in nature than other textbooks on corpus linguistics, at the end of almost all chapters, I've also added a section entitled 'Sources and Further Reading'.
The question of the nativity of text producers is another crucial issue in corpus generation.
The relevance of both applications of register analysis relates closely to the definition of corpus linguistics discussed in Chapter 1.
However, we have to remember that the type/token ratio is very sensitive to the length of the text; it decreases as the text becomes longer and more words get used again (recycled).
Section 1 offers an overview of published studies, while Section 2 describes a study which illustrates how corpus research can inform our understanding of academic writing.
If the latter, you may need to be aware that that language feature is probably used in an idiosyncratic way; that is, it is used only by one or two participants or in only a few of the texts (depending on your unit of analysis).
This script does the exact same thing as the one just discussed and most of the code is in fact identical, but it does it not by essentially treating the corpus file as a simple flat sequence of character strings as we did above, but by utilizing the hierarchical XML annotation in ways discussed in Section 3.8.2.
Although this iterative process is often not reported in final publications, it is evident from the many textbook descriptions of corpus linguistics.
Counting word tokens in a text is far from trivial even in a language like English, where the question arises whether forms like It's or aren't should be counted as a single word token (taking the delimiter criterion, as we have done above) or two tokens.
Thanks to them, descriptive grammar will continue to have a role in corpus linguistics.
Static corpora have a fixed size (e.g., the Brown corpus, the LOB corpus, the British National Corpus), whereas dynamic corpora do not since they may be constantly extended with new material (e.g., the Bank of English).
Corpus linguistics is a particularly effective method for establishing the frequent contexts in which a word or an expression is used.
And if the data had to be annotated in any way more complex than "number of letters", that annotation will be difficult to reconstruct even if an annotation scheme is provided, and impossible to reconstruct if this is not the case.
A small corpus with wider text variations is a much better representative than a large corpus with fewer text variations.
Corpus composition and corpus types called corpus data, namely, the searchable text data of a corpus in a written form in digital format.
Furthermore, the procedure we presented has the added benefit that models used to annotated the data can be shared, and used to replicate the data annotation scheme in corroboration and follow-up studies.
The pattern of cited publications also suggests that corpus linguistics has been specialized and branched out.
Once these basics of corpus analysis and an analytical framework have been addressed, readers will be ready to build their own corpora and conduct their own research study.
The very fact that the corpus is a sample means that we need to think carefully about how our sample relates to the population, which means we need a detailed and critical awareness of the process of sampling that has been used in corpus construction.
As with our conclusions on corpus size, we take all different types of corpus composition to be viable options in corpus linguistics.
One monitor corpus that allows us to see changes in spoken corpora and also provides more current spoken data is COCA.
For mini-, the type-token ratio is much higher: it occurs in 382 different words, so its TTR is 382 /1702 = 0.2244.
In terms of how the information is conveyed, we see differences in the type-token ratio.
In the field of corpus linguistics, it is very common to hear that there are no good or bad corpora, rather there is only corpora which are more or less suitable to address a certain research question.
Corpus linguistics has long championed, and pioneered, markup schemes to permit the systematic encoding of metadata and interpretative analyses within corpora.
Development of a speech corpus involves issues and factors like purpose of use, selection of speakers, choice of settings, size of a corpus, use of recording instruments, manner of data sampling, manner of data elicitation, nature of transcription, types of data encoding, management of audio files, editing of input data, processing of spoken texts, annotation of speech, analysis of speech corpus, etc.
If there are fewer cases than required, no split will be made; • the minimum number of cases in a node after a split.
Without a reliable description of the data, generalizing from the sample (corpus) to the population (language as such) using sophisticated statistical tests and p-values lacks grounding.
However, since all texts in a normal-sized corpus are rarely close enough to each other in length, as a typical workaround, the ratio is calculated for a set number of words (such as 400 words) taken from the beginning of each text.
For more general info about how to report the results of a quantitative corpus-based study, see also Chap.
A key concern in corpus linguistics is to explain the variable use of wordforms and other structures in dependence on both types of contextual factors.
Corpus linguistics is concerned with understanding how people use language in various contexts.
However, automatic corpus analysis provides many examples of politeness routines, as the ones related to the opening of a conversation, to its closure or, to speech acts such as apologizing.
In this way the corpus text is searchable and users can grasp the meaning of specific passages.
The set of methods required to approach the quantitative and qualitative analysis of a collection of language data on a scale far larger than any human being could hope to analyse by hand -together with related areas such as the compilation and annotation of these corpora -constitute the modern field of corpus linguistics.
The current standard method for adding tags to a corpus is by means of XML (the eXtensible Markup Language).
In Chapter 3 (see Sections 3.2 and 3.4), we have seen contingency tables that were constructed for collocation and keyword analyses to show all possibilities (contingencies) of word (co)occurrence.
If a researcher is interested in these questions, it becomes necessary to undertake a corpus analysis that goes beyond the pointwise comparison of single examples.
Let us assume that the corpus targets a size of 200,000 words per genre, in order to have a representative sample of each of them.
The situation has changed a great deal since the 1990s, thanks to the arrival of corpus linguistics, which made it possible to empirically compare linguistic systems.
A typical corpus investigation would proceed with a large number of retrieval operations conducted through the corpus retrieval software (e.g.
A token is a single instance of any word at a particular place in the corpus; a type is an individual word-form, which can occur many times in the corpus: thus one type is usually instantiated by multiple tokens.
This is the typical constellation for corpus building today, and it is a major reason why only a few larger corpora are web-accessible to date.
We'll discuss options of how to actually mark (up) and distinguish larger units of text in Chapter 11, when we talk about annotation.
For the latter, you could opt for only investigating the speech of men or women, or speakers from the same or different social classes, or create subcorpora for both, and then compare the language in some way, which makes it possible to carry out corpus-based sociolinguistic analyses on the data, as do the options for restricting speakers to particular education levels, etc.
It also offers a starting point for corpus-driven investigations of academic corpora by generating list of items which can be further explored in more detail using concordance analyses.
In continental Europe the application of mathematical and statistical concepts and methods to analysis of corpus data for derivation of linguistic laws has continued to be developed.
The paraphrase of each sentence makes the MPC similar to a parallel corpusa type of corpus that typically contains sentences from a whole text, with each individual sentence translated into a different language.
Lemmatisation is conceptually the most straightforward of the word-level annotations; every word is, simply, annotated with the lemma to which it belongs (where each lemma is represented by its morphological base form, such as go for go, goes and went).
In addition to illustrating how a primarily qualitative corpus analysis is conducted, the analysis of Trump's speech provides a basis for describing the various steps involved in conducting a corpus analysis, such as how to frame a research question, find suitable corpora from which empirical evidence can be obtained to address the research question, and so forth.
Original text corpora are thus an option where the phenomenon of interest can be considered ubiquitous enough to be represented sufficiently in the corpus (cf.
As corpus linguistics is a methodology that allows us to develop insights into how language works by 'consulting' real-life data, it should be fairly obvious that we cannot learn how to do corpus research on a purely theoretical basis.
According to Chomsky, another problem related to corpus linguistics stems from the fact that corpora are not representative of the language as a whole.
Let us select a sample of 20 hits each for literal uses the singular and plural of flame(s) from the BNC (as mentioned above, Deignan's corpus is not accessible, so we must hope that the BNC is roughly comparable).
Section 5.3.2), and with complex scales combining a range of different dimensions, this is probably a good idea; but ordinal data also have a useful place in quantitative corpus linguistics.
We have seen that corpus analysis makes it possible to better characterize the language specificities of people suffering from language and communication impairments, by studying the different ways in which these patients interact in a natural environment.
Even though it is thought that corpus is quite helpful in language teaching, scholars still do not fully come to an agreement on this issue.
However, when it comes especially to the documentation and description of smaller languages that have not previously been investigated in much depth, you may be involved in corpus compilation to a considerable degree, including having to make relevant decisions on corpus design and structure.
But in addition to PoS tagging syntactic annotations also pick up more aspects of a syntactic structure.
The primary purpose of the annotation is to add additional information to a text so that future analysis and interpretation of a text becomes easy, accurate, and useful.
Brown corpus) whereas the alternative is to aim for a balance of text varieties (cf.
In doing so, we will outline our own epistemology and suggest a route that corpus linguistics may take in debates around epistemology in the social sciences.
The Brown, LOB corpus, and Survey of English Usage (SEU), for instance, contain a wide variety of text types, and therefore, they are considered much better representatives of English.
Wordforms are immediately observable and can be searched for; larger structures can only be detected through linguistic analysis and searching for these requires further corpus annotation (Chapter 7).
In conclusion, corpus-based research has shown that grammatical variation, like phonological variation, can be sociolinguistically conditioned.
For example, we may want to determine whether the two types of subordinate clause differ with regard to their mean word length.
Compared to, say, the 1980s or even the 1990s, there is now a huge and constantly growing number of studies in corpus linguistics that tackle phenomena of interest with methods that allow researchers to study the effect of multiple independent variables, or predictors, on the dependent variable, or response, of interest.
Spoken language that was produced without any involvement of the corpus compiler is often referred to as 'authentic' or 'natural' language, as it avoids the observer's paradox, i.e.
The only type of text we do not consider corpus text is mere mentions of structures, for example, intuited example sentences.
Indeed, it is no exaggeration to say that corpus linguistics using large computer-readable language data has established itself as the main methodology in historical pragmatics.
Audio recordings make it almost impossible to grasp this type of information, which later have to be added to the corpus so that the interactions can be understood and analyzed by the experts who will listen to these recordings.
Finally, each chapter includes an empirical case study illustrating the corpus analysis methods and the types of research findings that are typical in this area of research.
Topic bias is a problem especially in small corpora where each individual text forms a relatively large proportion of the corpus.
The subject of accessibility of data and its repercussions in corpus study is also covered by Stefan Hartmann (Chapter 6), who brings up the problem of replicability of corpus studies, often resulting from the limited accessibility of the corpora studied, as some corpora are only available behind a paywall.
Our main argument was that the recent trends and advances in the field, such as the publication of big-data corpora, the increased reliance on statistical approaches to linguistic data, and the exploitation of various kinds of metadata, require that the linguist should have a detailed understanding of the structure of the corpus, its sampling procedure, and the principles followed in the construction of the metadata.
Rights holders often agree to authorize a single researcher to use a reasonable amount of their data for research, but this type of corpus often cannot be later redistributed.
Anybody who wants the claims made based on a corpus to be accepted by the research community needs to show in some way that the corpus material is appropriate for the type of research done.
The remainder of the chapter focuses on more quantitatively based corpus research, such as Douglas Biber's work on multi-dimensional analysis and the specific statistical analyses he used to determine the register distributions of a range of different linguistic constructions, for instance, the higher frequency of pronouns such as I and you in spontaneous conversations than in scientific writing.
Written texts, in contrast, are now widely available in digital formats and can easily be incorporated in a corpus after permission has been received to use a given text.
Usually a tagger is trained on a smaller hand-annotated sample and then applied (tested) on a larger corpus.
More recent corpora at first glance appear to take a more principled approach to representativeness or balance.
The primary goal of semantic annotation is to identify the sense a word denotes when it is used in a text.
Part of the reason for this is that, as a practical matter, both can be inserted into the corpus using the same encoding system of tags inserted into the running text.
In order to automatically search for elements in a corpus, a surface feature such as a word or list of words should be associated with it.
A relatively small set of words accounts for a large proportion of tokens in a text (or corpus).
As the name suggests the annotation picks up the word class membership of word forms, traditionally called parts of speech.
But the first computer corpus for the computational analysis of language, as Hasko (2020: 952) comments, was the Brown Corpus, a corpus that paved the way for not only quantitative corpus research but qualitative studies of language as well, as demonstrated by the particular registers included in the Brown Corpus.
The central premise of corpus linguistics is that all of the language-internal andexternal contextual features are relevant to the way that people use language.
We will return to this in section 1.4, devoted to the connections between computer science and corpus linguistics.
In any corpus, the greater the number of unique text samples, the higher the reliability of the analysis of linguistic variables.
It also depends on the decision of how many text categories are kept in a corpus, how many text samples are put in each category, and how many words are kept in each sample.
Following the works of American linguist Noam Chomsky, it changed from the essentially rationalist discipline it had been since the middle of the 20th Century, and gradually (re)opened up the empirical approaches represented by corpus linguistics and experimental linguistics.
Social scientists have developed mathematical formulas that enable a researcher to calculate the number of samples they will need to take from a sampling frame to produce a representative sample of the frame.
This is both the exciting and frustrating part of corpus linguistics.
Examples for synchronic English data include the Open American National Corpus for spoken and written American English, as well as the ENCOW corpus for World Englishes as used on the web.
Finally, note that our final definition does distinguish corpus linguistics from other kinds of observational methods, such as text linguistics, discourse analysis, variationist sociolinguistics, etc., but it does so in a way that allows us to recognize the overlaps between these methods.
However, if one is creating a historical corpus and thus working with texts from earlier periods of English, converting a text into an electronic format can be a formidable task and, in addition, raise methodological concerns that the corpus linguist working with modern texts does not need to consider.
It involves the notion of lexical frequency profiles, which is a corpus-based approach in applied linguistics concerned with the amount and the kind of vocabulary second/foreign-language learners use in their speaking and writing (see Laufer & Nation 1995 for an introduction; Meara 2005 for a critique; and Laufer 2005 for a response).
However, if you persist, I can promise you that soon you'll not only be able to understand the value of regexes for achieving even highly complex tasks in linguistic analysis very efficiently, but will also learn to hone your analysis skills related to morphology and morpho-syntax, something you'll definitely be able to profit from in your work in corpus linguistics, as these two areas often form the basis for further linguistic analyses.
Read in (i.e., upload) your corpus through the "File" menu ("Open files") and type any search word in the search box that you would like to find out about in your text(s) and hit the "start" button to get a KWIC concordance line.
Second, every column but the first represents either the data in question (e.g., (parts of) a concordance line to be investigated) or one and only one variable with respect to which the data are coded.
More generally, static and full text representations do not sit well with the iterative and data-driven nature of the corpus linguistics methodology.
Here, too, corpus linguistics provides useful tools, for example to determine whether the choice between affixes is influenced by syntactic, semantic or phonological properties of stems.
That said, our analysis identified clear trends regarding the use of semantic shifts based on a large amount of data, further confirming the potential that corpus-based analyses have in understanding the patterns behind complex linguistic behaviors.
Secondly, Corpus Linguistics attempts to make contributions to linguistic theory that are informed by quantitative information.
If, for example, a word list shows that one particular word is more frequent in one sub-corpus than in another (a corpus-driven method), then the researcher will still need to look at the distribution and use of this feature more closely in the corpus by investigating its use in some more detail.
To start out, we clarify briefly what we mean by the terms grammar, grammatical change, and corpus-based analyses.
We have also shown that corpus analysis methods can be useful for uncovering recurring patterns in a source text and to better adapt the strategies used for its translation.
Corpus linguists deal with a small fraction of all texts that have been produced in a language, so any corpus is always only a small sample drawn from the real-world population of texts (cf.
Second, we would have to choose a list of words to search within the corpus, representative of the vocabulary related to emotions, for example verbs such as to annoy, adjectives like furious or nouns like anger.
First, corpus tools can be applied to individual texts, in helping decide whether a text is appropriate and what elements to focus on.
Particularly if a researcher is oriented towards certain "bottom-up" approaches to language analysis (as in some kinds of corpus-driven linguistics; cf.
During the process of reviewing the corpus-based research that uses bootstrapping, we have seen no evidence of bootstrapping results being misinterpreted or misrepresented.
However, lemmatization refers to the act of associating every word occurrence in a corpus with its basic morphological form.
With corpus linguistics the basis has broadened and the focus has shifted to common features and everyday practices.
However, due to the nature of a concordance analysis -the number of lines the analyst is presented with, along with the fact that they only contain a few words of context either side of a search term, it is possible that these more nuanced cases may be overlooked.
As can be seen from the equation, there are three factors that have an effect on whether the test will be significant: (i) size of the mean difference, (ii) variance in each of the two groups and (iii) sample size (number of cases, i.e.
Changing legislation in these areas might pose further difficulties for corpus-based research in the future.
This does not mean, however, that considerations of corpus design become irrelevant on the ground, and we will point out what can be done better even under difficult circumstances.
Metadata may also pertain to the structure of the corpus itself, like the file names, line numbers and sentence or utterance ids in the examples cited above.
Each of these properties should then be mirrored by the corpus sample proportionally to its prevalence in the population.
As we have seen, annotation always requires a certain type of interpretation and this may vary slightly from one annotator to another.
The main reason to use a corpus is to find real examples of language use in context and count how frequent they are.
As already noted, multimodal corpus analysis is essentially a mixed methods approach, one which seeks to combine quantitative techniques with qualitative textual analyses, as utilised in conventional corpus enquiry.
Of course, such an annotation scheme is especially important in cases where interpretative judgments are involved in categorizing the data.
The Web is used as a corpus in ten studies, whether through a generalpurpose search engine (e.g.
The third step, discussed in detail in the next section, consists in testing the reliability of our annotation scheme.
This bears some consequences for corpus building in language documentation because it means that our corpora are mostly haphazard and opportunistic.
In Chapter 8, Daniel Ocic Ihrmark explores the challenges of categorizing fiction genres in corpus compilation, especially when catering to both linguistic and literary research fields.
An additional strain of corpus-based research in linguistic typology focuses on diversity in language use.
As a consequence, it is usually impossible to extract a complete sample of a given phenomenon manually, and this has lead to a widespread use of computers and corpus linguistic software applications in the field.
From all the problems we've seen above, it may seem as if single word frequency lists are actually best avoided, but nevertheless, they may provide us with at least some superficial information as to lexical density or makeup/type of a text/corpus.
The book is aimed at a variety of readers, ranging mainly from linguistics students at senior undergraduate, Masters, or even PhD levels who are still unfamiliar with corpus linguistics, to language teachers or textbook developers who want to create or employ more real-life teaching materials.
Provided that you don't forget to change the option for creating a new corpus, there should be no issues in completing this exercise and creating a suitable frequency list.
It is written for someone doing corpus linguistics perhaps even for the first time and for someone who has limited or no basic statistical knowledge.
For example, the type/token ratio in the Sciences Humaines written corpus that contains around 260,000 word occurrences is 0.06, but it drops to 0.007 for the 2012 installment of the Le Monde corpus, which has over 26 million word occurrences.
Phonetic corpus research is concerned with the acoustic measurements of spoken language in context.
CITs and CRFs can be particularly useful in the situations of small n, large p. This may be the case in many subfields of corpus linguistics, where data are small and costly, e.g.
When considering the term 'web as corpus', the first question we must ask is whether the web can actually be classed as a corpus according to the criteria set out in Chap.
In gathering written texts for inclusion in a corpus, the corpus compiler will undoubtedly have a predetermined number of texts to collect within a range of given genres: twenty 2,000-word samples of fiction, for instance, or ten 2,000-word samples of learned humanistic writing.
The crucial question to ask in this context therefore is: what kind of language do the corpus of interest (C) and the reference corpus (R) represent and how is the composition of each corpus reflected in the comparisonthe keyword procedure?
This article offers a critical view on using frequency data in corpus linguistics in the context of language learning; this critique is, however, applicable to other contexts as well.
One advantage is that examples may help the annotators understand the annotation scheme.
POS=NOUN); in some complex architectures, annotations can potentially include more complex data types, such as hierarchical feature structures, in which annotations can contain not only simple values, but also further nested annotations (see ISO 24612 for a standardized representation for such structures in corpus markup).
It's thus well worth bearing the above-mentioned factors in mind when conducting any kind of statistics-based keyword analysis, and especially when reporting on the presumed importance of particular keywords for a given text/corpus.
Corpus building not only allows you to answer a specific research question, but it also gives you experience in corpus construction.
This seems like an unnecessarily complicated way of representing the kind of co-occurrence design used in the examples above, but I have chosen it to show that in this case sentences containing a particular word are used as the condition under which the occurrence of another word is 7.2 Case studies investigated -a straightforward application of the general research design that defines quantitative corpus linguistics.
At 3 years and 5 months old, the most frequent word was ça with 54 occurrences, and her type/token ratio was 0.21.
Answering either of these questions involves answering the other, as the notion of a 'corpus' in contemporary linguistics is inextricably tied up with the ideas and techniques of corpus linguistics.
For example, the online interface on the OFROM corpus (see section 5.3) only offers the possibility of looking for productions by women or men, but for the moment does not indicate the total number of participants of each genre, or the number of words produced by each of them.
For instance, a concordance can be produced for a certain part-of-speech tag, a frequency list of lemmas, key semantic tags, and calculate collocation statistics for which semantic tags relate to a given word.
They may, however, inform corpus-based syntactic argumentation (cf.
However, as the example corpus analysis in the final section of the chapter demonstrates, conducting "interesting" linguistic research based on "real" samples of language are not necessarily mutually exclusive activities: a corpus can serve as a test bed for theoretical claims about language; in turn, theoretical claims can help explain patterns and structures found in a corpus.
By contrast, in many ways, software tools in the social sciences that facilitate textual analysis are oriented towards qualitative researchers and focus squarely on providing support for manual text annotation.
First, any text, spoken or written, will lose not only its communicative context (the discourse of which it was originally a part), but also some of its linguistic and paralinguistic properties when it becomes part of a corpus.
Thus, such cases have to be treated separately so that they can be excluded from search results and word counts to not distort the data in learner corpus studies.
In the years before 2000 and the early 2000s, corpus linguistics tended to be studied by using enormous empirical datasets.
These figures are essential for performing lexical diversity calculations on corpus data, such as the type/token ratio (see Chapter 8).
The overload in corpus linguistics is symptomatic of a more general trend.
One comment here: We could just use Adam Kilgarriff's frequency list file or the results from the word-tag combination exercise in Section 5.2.8 to get adjectives to tag, and one could just use a tagged version of the ICLE or Brown corpus, but we will pretend we don't have access to any such resources and write a script from scratch just so you get some more practice that'll help you do these things when those additional resources are really not available; unlike several scripts above, we will write this one again such that it uses the BNC's XML annotation and, thus, the packages XML and xml2.
In order to compare the two genres in terms of the type-token and hapaxtoken ratios, they need to have the same size.
Research of this typereferred to as a "corpus-driven" approach -identifies strong tendencies for words and grammatical constructions to pattern together in particular ways, while other theoretically possible combinations rarely occur.
For example, the token frequency of the suffix -icle is higher in the BROWN corpus (269 tokens) than in the LOB corpus (225 tokens).
I have suggested that multilingual practices, metalinguistic language use and instances of intertextual reference should be identified and annotated so that they can be dealt with in or be excluded from corpus analysis.
Otherwise, the corpus itself, without the metadata, will be useless since the researcher will not be able to know the background information of the corpus.
It has highlighted that while multimodal corpus research is gaining some momentum, there are still some areas where further development is required.
The easiest option is to find an archive for the corpus, such as The Oxford Text Archive or CLARIN.
For example, if a corpus-based analysis contains the length of a phrase as an independent variable, and we introduce the pronominality of that phrase as a second independent variable, these two variables might correlate significantly, since full lexical phrases are by necessity longer than single pronouns.
But even more basic aspects of this example are not directly extractable from the corpus text; for example, the grouping of words into phrases: how do we know which words belong together, for example, her living room, and form what kind of relationships with other words and phrases?
Corpus-based grammatical research (following the principle of total accountability) cannot ignore such areas, and often finds that they are by no means uninterestingrather the opposite.
Thus, the following sections will try to provide you with a rough overview of what exactly PoS tagging is and how it can be carried out, where its strengths and weaknesses lie, and how you may be able to use it with your own data.
We have structured the main body of CHECL around these two domains of inquiry: chapters dealing with "Corpus analysis of linguistic characteristics" in Part II and chapters dealing with "Corpus analysis of varieties" in Part III.
If this is the only information that a corpus analysis could provide, then corpus linguistics would have at best a marginal role in the field of linguistics.
The main objective is to test a limited number of variables, in a highly controlled environment whenever possible and on a language sample that can be representative of the phenomenon studied.
This can be measured thanks to the type/token ratio (see Chapter 8).
These tools also make it possible to establish the list of words contained in the corpus, together with their frequency, and to generate a list of keywords matching the content of a corpus.
Given the recent overarching knowledge-building practices and the methodological roles of corpus linguistics, it is necessary to review how a certain body of knowledge has been created according to the common denominator of corpus linguistics.
Let us consider another, even more important example of why a qualitative concordance analysis is important.
Like semantic tagging, automated parsing has a much higher error rate than POS tagging, because the task is inherently more difficult.
To test this hypothesis by means of a corpus study, we should first make sure that we are comparing records of men and women produced in the same context, for example, in the context of friendly discussions around a topic, or a face-to-face interview with a researcher.
In this small sample, we not only see differences in nouns ("English" is the third most frequent word in the individual corpus and "students" is the fourth most frequent word in the collaborative corpus) but also differences in function words ("in" is the fourth most frequent word in the individual texts and "of" is the fifth most frequent words in the collaborative texts).
If you want to compare results produced on a corpus analysis in AntConc with those drawn from a subcorpus of BNCweb, you should definitely select either the 'Mutual Information' or 'T-score' options there, depending on the size of your samples.
The first study that we present to illustrate the use of corpora for the study of language acquisition concerns the acquisition of the French verbal system for expressing temporal references.
That being said, good practices in corpus linguistics and experimental linguistics require obtaining information about participants beforehand, which can eventually eliminate this type of bias.
Second and more interestingly, the above two models were fitted with user-defined orthogonal contrasts-something else that happens way too rarely in corpus linguistics-to see easily (i) whether the speakers of an unknown sex are different from those where the sex is known, and (ii) whether female and male speakers behave differently.
Alongside the concordance method, a further four methods have emerged as central to the work of the corpus user: frequency lists, keywords, n-grams, and collocations.
In corpus linguistics, bootstrapping has also been proposed as a method for quantifying the degree of homogeneity in a corpus sample, for validation of statistical results, and as a methodological step in random decision forests, an advanced classification method.
Thus, more standardization, also linking to standards existing in other corners of corpus linguistics, seems an indispensable step for exploiting the full range of developmental data that we have.
To sum up, in this section, we have defined corpus linguistics as an empirical discipline, which observes and analyzes quantitative language samples gathered in a computerized format.
Annotation of semantic categorisation is useful, for example, for various investigations of text content.
On the other hand, the perception of errors is contingent on the acuity of the researcher while, with corpus research, the corpus compilation would not be contingent on a particular person's perceptual skills.
New directions and challenges for spoken corpus research is the focus of Section 5, including research on fluency, prosody, and non-verbal behavior in spoken corpora, dialect studies, and discourse-level investigations.
However, for a corpus file to be used as a sample representing a certain type of language, metalinguistic information (which is not part of the text or of the dialogue) should be accessible to the researchers who will analyze it.
Adding further levels of annotation to corpus data almost always leads to added complexity in the data, although, for instance, striking the right balance between using an appropriate number of container elements, empty elements, and suitable attributes can already help us to go a long way, as hopefully Exercise 84l has shown you.
These methodological considerations are generally disregarded in corpus-driven studies of phraseology.
Next, the frequency of each word in the target corpus is compared to its frequency in the reference dataset in order to calculate a keyness value.
When you count how often each type occurs in a particular corpus, you usually get a skewed distribution such that 1 a few types -usually short function words -account for the lion's share of all tokens (for example, in the Brown corpus, the ten most frequent types out of approximately all 41,000 different types (i.e., only 0.02 percent of all word types) already account for nearly 24 percent of all tokens); and 2 most tokens occur rather infrequently (for example, 16,000 of the tokens in the Brown corpus occur only once).
First, because texts to be included in a corpus will be edited with some kind of text editing program, it may be tempting to save computerized texts in a file format used by a word processing program (such as files with the extension .doc in Microsoft Word).
The fact that the type/token ratio decreases with age in the two children reflects that the total number of occurrences they produce increases a lot as recordings progress (e.g.
In modern corpus linguistics, transcriptions are linked to audio or audio-visual recordings of texts so that a spoken and signed language corpus consists of transcription text files as well as audio and/or video files.
These non-primed comparatives are fairly evenly distributed -194 are analytic comparatives and 187 are synthetic, which is not significantly different from a chance distribution (𝜒 2 = 0.13, df = 1, 𝑝 = 0.7199), suggesting that our sample of adjectives does indeed consist of cases that are fairly evenly distributed across the two comparative strategies (see Supplementary Online Material for the complete annotated concordance).
Worse, developers might even discontinue the development of a tool altogether -and let us not even consider how sorry the state of the discipline of corpus linguistics would be if a majority of its practitioners was dependent on not even a handful of ready-made corpus tools and websites that allow you to search a corpus online.
The ultimate length of a corpus might therefore be better determined not by focusing too intently on the overall length of the corpus but by focusing more on the internal structure of the corpus: the range of genres one wishes to include in the corpus, the length and number of individual text samples required to validly represent the genres that will make up the corpus, and the demographic characteristics of the individuals whose speech and writing will be chosen for inclusion in the corpus.
The sample used here is the Diachronic Electronic Corpus of Tyneside English (henceforth DECTE), a collection of interviews with Tyneside English speakers that will be fully described in due course.
Like the SBC, a corpus created with ELAN will have two components, namely, a set of media files with the raw audio and/or audio-visual data and a corresponding set of ELAN document files (a special kind of XML file).
Corpus-based typology language community (being all from the same geographic area) and can therefore not account for observed cross-linguistic differences.
However, if the design had been widely felt to be completely off-target, 2 What is corpus linguistics?
Second, whenever possible, we should go beyond the inclusion of translated novels, news, and international organizations' legal and administrative texts, and strive for the inclusion of more genres and text types, especially those that are dominant in today's translation market, to which corpus compilers have had limited access to date, for obvious reasons of confidentiality and/or copyright clearance.
If they are not, it is very easy to bracket corpus linguistics together with approaches to language data which, very often, are free of any serious reflection upon the nature of language in the social world.
As a result, corpus-driven studies of lexical phrases (both continuous and discontinuous) have shown that lexical patterning is ubiquitous in English, and basic to the discourse structure of both spoken and written texts.
Metadata and Textual Markup are more descriptive, noting, respectively, what is in a corpus and where, for instance, paragraph boundaries in a written corpus occur or speakers pause in a spoken corpus.
For example, in order to study causal relations in French, part-of-speech tagging makes it possible to only look for occurrences of car working as conjunctions and eliminating those which are nouns (a type of vehicle).
A final consideration relates to the type of corpus that you will build to conduct your project.
A large type of corpus comprising a large number of different speakers would be desirable.
A frequent exploratory method to answer the first question, namely to discern sub-structure(s) in corpus data, is hierarchical cluster analysis, a statistical tool that groups data points into clusters on the basis of the points' pairwise similarity (such as the differences between MLU values or differences between percentages of (e)s).
We fed each tweet in its raw text form as a single sequence into BERT, which then produced context-informed vectors for each token in the tweet.
If there is not enough evidence, or the evidence is biased towards, for example, a certain genre or certain types of speakers/writers (see 'diachronic representativeness' above), the patterns we observe are ambiguous.
For most of the questions addressed by corpus linguistics, it would be impossible to search through a paper database, and that is why having computerized corpora becomes essential.
Because register variation considers how linguistic features co-occur in a given context, a corpus linguistic approach is well-suited to register analysis because corpora provide large amounts of authentic texts for analysis.
Let us use the adjective-noun sequence good example from the LOB corpus (but horse lovers need not fear, we will return to equine animals and their properties below).
In order to overcome certain limitations pertaining to parallel corpora, the ideal would be to work with a bi-directional corpus, where both languages are alternately source and target, since these corpora make it possible to combine the two types of multilingual data discussed above (comparable and parallel).
The Child Language Data Exchange System, or CHILDES Corpus, includes transcriptions of children engaging in spontaneous conversations in English and other languages.
However, these frames can be identified through direct corpus-driven analysis.
These two corpora inaugurated the modern age of corpus linguistics, although both suffered from limitations: the Brown Corpus was restricted to written, printed material; and the SEU corpus remained on paper until the mid 1970s, when most of the spoken part of it was computerized by Jan Svartvik and became the London-Lund Corpus (LLC).
Once generated, the concordance was saved and then a special command -"delete to N" -was used to reduce the concordance lines to a random sample of just 100.
Integrating corpus studies with other theoretical background or other research techniques would make register variation studies more applicable to other fields.
Progress has been rather slow, compared with monolingual corpus collection initiatives, but in recent years we have witnessed a boom in the collection of parallel corpora, which are increasingly larger and multilingual.
Even for this type of corpus, several months of work are often necessary for collecting the data, and may take even longer if the latter are enriched with linguistic annotations (see Chapter 7).
To a large extent, these are reasons why the corpus-based approach is not more popular in dialectology and sociolinguistics today.
In this section, we will go through a number of scenarios and conditions that pose restrictions on corpus design as desirable from a purely scientific point of view.
A reference corpus cannot in any sense represent the language, unless it is subdivided into text categories or subcorpora representing a broad range of registers, as in the BNC or the Bank of English.
A book that discusses the history and epistemology of corpus linguistics only to the extent necessary to grasp these methodological issues and that presents case studies of a broad range of linguistic phenomena from a coherent methodological perspective.
Furthermore, there are some new trends in corpus-based research that Rissanen could not have anticipated in his article, and these introduce challenges of their own.
Within the transcripts, it might be useful to include an annotation of the non-standard words used, for example, in verlan, with their equivalent standard so that they can be found in a concordancer.
In the context of corpus linguistics, this means annotating them according to an annotation scheme containing the operational definitions.
To turn a number of occurrences into a relative frequency, we need to apply a rule of three, by dividing the number of occurrences found in the corpus by the total number of words in the corpus, then multiplying by the base of normalization, as shown in the example below, which has a base of normalization equal to 10,000.
Some corpus practitioners use scripts, often in Python or R, to evaluate their data, without using a dedicated search engine (see Chap.
In what follows, I provide a very brief overview of a few additional ways in which the XML package can help you search XML corpus data in sophisticated ways.
Retrieval methods and tools are those most commonly and prototypically associated with the corpus user's toolbox because many linguists use pre-existing corpora and so can skip the first two stages.
The corpus includes images of handwritten texts, raw transcriptions in text format and an annotated transcription, also in text format.
The text should be free from any annotation that carries linguistic and extralinguistic information.
Two possibilities: First and as briefly mentioned above, you can loop over each file, identify all word tokens in it, use table to generate a frequency table of it, and then save that frequency list into a separate file, which means you end up with 4,049 very small frequency list files, none of which will take up a lot of memory on its own.
As a result, in the twenty-first century, corpus linguistics has become -in the words of Tony McEnery -a 'killer method' in linguistics, applied to a hugely diverse array of types of linguistic research.
Due to many such advantages, criticisms against corpus linguistics, although partly succeeded at the initial stage, have failed to stop corpus linguistics.
Corpus linguistics, however, has become an indispensable methodology throughout the field of linguistics and its neighboring disciplines.
As we mention above, the applications we propose here represent just two of the many potential applications of bootstrapping in future corpus-based research.
This paper explores these issues, looking, at an abstract level but illustrated through examples, at the interaction between corpus linguistics and the social sciences.
According to this guideline, a corpus is a type of text that is regarded "as composite, that is, consisting of several components which are in some important sense independent of each other" (www.tei-c.org/release/doc/tei-p5-doc/en/html/DS.html).
Finally, in Section 4, we briefly present a case study illustrating the application of large-scale corpus analysis to investigate the types of discontinuous lexical frames found in spoken and written registers of English.
As corpus research into academic genres continues to grow, therefore, we can anticipate an ever increasing broadening of studies beyond texts to the talk and contexts which surround their production and use, beyond the verbal to the visual, and beyond tertiary to school and professional contexts.
Having discussed different aspects of corpus building, one basic question still remains to be answered: how large should a corpus be?
Over the years, corpus linguistics has made progress in its home discipline by being results-focused and by spawning new theories that use the data made available by corpus analyses.
Type in 'corpus linguistics filetype:doc', and hit 'Enter'.
Sample clusters of tweets are presented in the keyword-in-context format, for ease of reading as well as to illustrate the effect of this approach on manual perusal of corpus data, as observed during the manual annotation.
P is expressed as Fc NÀF n and E as P F n S, where F n and F c are the frequency counts of the node and collocate while N and S stand for the size of the corpus (i.e.
During that time, corpora enabled researchers to conduct studies on grammar using corpus-based parsers, such as the Penn Treebank and its parts of speech tags.
To turn these definitions into operational definitions, we need to provide the specific queries used to extract the data, including a description of those aspects of corpus annotation used in formulating these queries.
Working with corpus linguistics therefore implies being in contact with linguistic data in the form of texts, and also in the form of recordings, videos or any other sample containing language.
The purpose here is obvious: with our interest in modern languages encompassing thousands of different languages no corpus user can be expected to have command over every language under study.
By way of conclusion, we would like to offer a list of ordered stages, making it possible to implement the concepts discussed in this book step by step, and to carry out a corpus study.
In the example given above, within a 1L, 1R collocation window around each occurrence of the node, we can observe the words that co-occur with love, namely, in the order of appearance, my (three times), is (twice), in (once), am (once), will (twice), thee (twice), only (once), and (once) and thou (once).
Now that you already have a pretty good idea of what types of features you might encounter in a text and how this relates to aspects of its organisation, as well as issues that may affect its analysis, let's move on to finding and processing some data in electronic form as a first step in laying the foundation for creating your own corpus and analysing it.
In a matter of seconds, a concordancing program can count the frequency of words in a corpus and rank them from most frequent to least frequent.
The importance of a corpus is acknowledged because it contributes to making new findings, helps in utilizing data in applications, provides scopes for modifying old observations, generates opportunities for formulating new theories, and prepares new fields for applying language data in direct service to society.
This is also why the solutions to, and discussions of, the exercises may often represent those parts of the book that cover some of the more theoretical aspects of corpus linguistics, aspects that you'll hopefully be able to master once you've acquired the more practical tools of the trade.
However, the field of corpus linguistics is not well-established and methodologically mature enough yet to have yielded uncontroversial and widely applicable annotation schemes for most linguistic phenomena.
Yet beginning corpus users might benefit from learning about what may be regarded as tacit knowledge in corpus linguistics, and even the more advanced scholar may encounter issues new to them that have been addressed earlier.
If 95% of the corpus is made up of Parisian speakers aged between 20 and 30 years, such a corpus will not be a representative sample of the population, even if this means including the other speakers among the remaining 5%.
Annotation, by contrast, encodes into the corpus the results of some linguistic analysis which we have undertaken on the corpus text.
For instance, we saw that if we want to treat a Word or PDF document as a corpus file, we first have to extract its textual contents to a plain-text file in order to be left with any amount of usable text.
As with any other effect size measure, we also need to look at the 95% confidence interval for Cohen's d, which, in our example, is 0.18 to 1.21, as calculated automatically by statistical packages such as Lancaster Stats Tools online.
Now we will consider the defining characteristics of corpus linguistics as they will be used in this book.
For instance, many corpus-based analyses of media discourse are based on examining media texts that address some particular topic.
Conversely, a corpus study focuses on linguistic productions without manipulating the data before collecting them.
The translational component of the comparable corpus of narrative texts contains a higher proportion of high-frequency words and its list head covers a greater percentage of text with fewer lemmas than the nontranslational component.
While the purpose of the analysis of texts may vary between corpus linguistics and studies interested in style, the methods, however, can still be similar.
This is the function of concordance analyses, which provide information about users' preferred meanings by displaying repeated co-occurrence of words, allowing us to see the characteristic associations and connections they have, and how they take on specific meanings for particular individuals and in given communities.
For instance, many of the corpus-based reference grammars of English, such as Quirk et al.
Furthermore, even if they are obtainable, there may be a number of issues that make it difficult to handle them for the average corpus user.
I cannot think of any other scientific discipline whose textbook authors would feel compelled to begin their exposition by defending the use of observational data, and yet corpus linguistics textbooks often do exactly that.
The first type is accessible through the 'File→Save Output to Text File…' menu option, which saves the hit number, the concordance line, plus the file name for each hit.
Returning to corpus linguistics, it is instructive to compare its methodological challenges and trajectories to those of psycholinguistics.
In particular, we wish to show first how the concordancer's ability to collect examples of a similar linguistic phenomenon, as contained in repeated word strings or clusters, can lead to insights into the intentions of discourse participants, second how corpus techniques can enable the tracking of discourse features over time, and third how, contrary to charges from some quarters, corpora can shed light on what is absent from a dataset under examination and what this might signify.
Finally, they resume problems in connection with the application of corpus linguistics in the classroom since knowledge of the restrictions of corpus linguistics is necessary for its coming prosperity.
All this can make a full concordance analysis a major undertaking; in compliance with the principle of total accountability, concordances too long to analyse in full should be reduced (or thinned) randomly to a manageable size.
Yet, a number of directions should be further pursued before LCR can be said to meet the methodological requirements that are expected of corpus research and empirical research in general.
Unfortunately, most of the best-known taggers that have been developed over the years, such as, for example, the CLAWS system, for which a number of tagsets, such as the C7 discussed above, have been developed, are generally not freely available to the public or may require the purchase of a commercial licence for tagging suitable quantities of text.
To study press reportage, for instance, it is only necessary to take from a given corpus all samples of press reportage, and to study within this sub-corpus whatever one wishes to focus on.
Through the inclusion of 'metadata'-data about the data-about the type of discourse represented in the corpus, the corpus user can keep track of or investigate different factors that may influence language use, which may explain differences observed in different types of data.
While the focus in statistical textbooks and in the field in general is on statistical techniques, interpretations of p-values etc., low-level operations such as getting data from a corpus tool into a spreadsheet and then into a statistical package often remain in the background.
While there are many different corpora available (see a selection at the end of the book), as an example, we will use the Corpus of Contemporary American English (COCA), which is a publicly available collection of over 1 billion words of text of American English (freely available at www.english-corpora.org/coca/), to investigate the use of two words: equal and identical.
Logistic regression examples are used throughout this section, and we begin with the fictional corpus study of the dative alternation introduced in Sects.
First, it must not be misunderstood to suggest that studying the distribution of linguistic phenomena is an end in itself in corpus linguistics.
To perform comparative studies, we apply multivariate statistical techniques (e.g., Factor Analysis, Multidimensional Scaling, Cluster Analysis, Log-linear Models, and Pearson Correlation) to extract hidden patterns of language use from frequency data obtained from a corpus.
As this functionality is so common, the term concordancer is an often-used synonym for 'corpus software tool'.
A very large set of questions in corpus linguistics can be answered without going beyond standard, widely available corpora.
However, there is a potential problem that we need to take into account: the values of the variables Sex and 6.6 Complex research designs Age and their intersections are not necessarily distributed evenly in the subpart of the BNC used here; although the makers of the corpus were careful to include a broad range of speakers of all ages, sexes (and class memberships, ignored in our study), they did not attempt to balance all these demographic variables, let alone their intersections.
In this more specific sense, collocation refers to the co-occurrence of particular word-forms with the node, and three other terms are used to refer to grammatical, semantic or discourse-pragmatic or affective co-occurrence pattern: colligation, semantic preference and semantic prosody.
It was by means of this type of comparison that the annotations offered for the Google Books corpus in section 7.2 were considered as suitable tagging and parsing systems.
In sum, there are compelling arguments to include type frequencies from theoretical considerations as well as from neighboring disciplines such as psycholinguistics or computational linguistics, and there are promising first results within corpus linguistics proper, but more exploration is definitely required.
On the other hand, there is the area of diachronic historical corpus linguistics, in which corpus data are-given the relevant time spans-usually cross-sectional, covering, for instance, several centuries of the history of a language.
At the same time, whenever appropriate, I've also tried to point out other potential applications for such data, for example, in the development of teaching materials/textbooks, grammars, or direct application in the classroom, but of course such a list will always be incomplete as there are too many applications of corpus linguistics to be listed exhaustively.
The chapter also features representative corpus-based studies of learner language, representative learner corpora, tools and resources related to learner corpora, and annotated references for further reading.
Third, corpus research has provided more reliable empirical evidence than intuition that facilitates the identification of collocational behavior and semantic prosody of an extensive range of lexical items that have until recently been hidden from intuition.
Obvious possibilities include cases that are not covered by the annotation scheme at all, cases where the definitions in the annotation scheme are too vague to apply or too ambiguous to make a principled decision, and cases where one of the annotators has misunderstood the corpus example or made a mistake due to inattention.
The degrees of freedom is a complex concept, which is used when dealing with calculations based on a sample (corpus) rather than a population; in corpus linguistics, this is the case most of the time.
Given the cost and effort of creating a corpus of speech, it is understandable why corpora of this type exist, and while they do not contain spontaneous face-to-face conversations, they do provide a substantial amount of spoken language occurring in other speechbased registers.
If the p-value is equal to or is larger than 0.05 (or 5%) we conclude that there is not enough evidence in the corpus to reject the null hypothesis.
In the book, it is noted that these two approaches are not necessarily mutually exclusive: the generative linguist, for instance, could very well work with examples that are corpus-based rather than examples that are made up.
As we have discussed many times throughout the book, one can follow a) a corpus-driven or b) a corpus-based (often called corpus-informed) approach to linguistic analysis.
The one type of annotation that all spoken corpora share is an orthographic transcription (see also Chap.
To get those concordance lines for your sample, go to the main page of COCA, click on "chart" and type in "sa*_v" in the search box.
However, as also discussed in Chapter 3, many if not most hypotheses in corpus linguistics have to be formulated in relative terms -like those introduced in Chapter 5.
To use the corpus for analysis, the researcher needs to frame a research question that will help the researcher set the parameters of the process of the corpus analysis.
This is because with each test that uses a p-value we are willing to accept that in a small number of cases (5%) the result will be statistically significant, even if the null hypothesis is true (there is no effect of the explanatory variable).
The corpus studies are crucial since they either test the validity of a language theory or hypothesis or help researchers create a language theory based on corpus analysis.
In Chapter 9, we discuss some of these issues with the hope that this book has taught you enough about corpus research to pursue a more advanced study of the field.
Corpus linguistics 185 convenience samples (cf.
On one hand, it normally supplies a preface to the domain of corpus linguistics and its high relevance to language teaching and learning.
For instance, in the Littéracie avancée corpus described above, a search for the French word avis (opinion) by means of a concordancer indicates that the students have used this word 61 times.
You can continue to look for lexico-grammatical patterns that you find interesting, or you can carry out corpus-based studies that rely on the results of previous, corpus-driven studies.
In these cases, quantitative corpus linguistics is essentially a variant of sociolinguistics, differing mainly in that the linguistic phenomena it pays most attention to are not necessarily those most central to sociolinguistic research in general.
These introductory publications advanced the usage of the corpus for language teaching.
They are generally used in a span of French text within a codeswitched tweet where most tokens are in English; this explains why the tweets were tagged as written in English and retained in the corpus.
One thing to watch out for with PoS tagging is that it is often automated.
In the second part, we use character and numeric to create empty collector vectors to merge the hyphenated forms and their frequencies from all over the corpus, a for-loop to load each frequency list file, and, if there are hyphenated forms in the file, we merge them with subsetting, incrementing the vector counter on each iteration (as in Section 5.2.8).
It is worthwhile to explore various tools whether you are new to corpus linguistics or are a seasoned veteran.
We then detailed the different stages that make up an annotation process and stressed the importance of good methodological practices, so that the annotation is as valid and reusable whenever possible.
Comparability is sometimes complicated by rather technical reasons such as different choices of file formats, corpus formats and syntax, and coding schemes.
Other fields have had long and intense discussions about these things -corpus linguistics, unfortunately, has not.
The chapter also contains discussions of corpus tools, such as concordancing programs, that can facilitate the analysis of corpora by retrieving relevant examples for a given corpus analysis, indicating their overall frequency, and specifying (depending upon the program) the particular genres in which they occur.
The International Corpus of English (ICE) Project provides a number of examples of logistical realities that forced changes in the initial corpus design for some of the components.
Note that in each case, the frequency of co-occurrence was provided in the brackets; we call this value the observed frequency of collocation.
If we consider the future of corpus building from the perspective of the loss of complex information, it is interesting to note that few existing corpora reflect a feature which many present-day types of discourses exhibit: that of multimodality.
Current conceptions of corpus linguistics started with the creation of the Quirk Corpus (which contained print samples of spoken and written English) in 1955 at the Survey of English Usage at University College London.
Thus, the hopefully not too high-flying goal of this paper is to become for corpus linguistics what the abovequoted papers have become for psycholinguistics: a first go-to resource that explains to corpus linguists what (generalised) linear mixed-effects/multilevel modelling ((G)LMM/MLM) has to offer and that provides them with a concrete example and some instructions on how to perform such analyses.
As most operating systems recognise the extension .txt and will automatically open an appropriate built-in editor when a plain-text file with this extension is clicked, I'd strongly recommend you to use this for your own corpus data, at least for data that doesn't contain any special annotations, even if some operating systems, such as Linux or Mac OS X, may not require it, and default installations of Windows will unfortunately also hide known extensions from the user.
Beginning in the 1990s, most research on phraseological patterns has been empirical, utilizing corpus analysis.
This means that the probability of a given word occurring at some point in a text is influenced by the words that have occurred before and thus the words in a corpus are not random.
Only in the case of certain historical data, where the amount of text that has survived from a particular period is finite, can we have a complete corpus of some language or language variety.
Until relatively recently, corpus-based studies have tended to focus almost exclusively on the English language, and on British and American English in particular.
Finally, we use abline and text to add some annotation into the plot; we use lines(lowess(...)) to add a trendline, and I introduce mtext to add colored axis labels.
To potentially distinguish between the three options if they should cooccur in the same KWIC window, you can also make use of the columns labelled A, B, or C for each concordance line.
The most popular crawler in web for corpus research is the open-source Heritrix system (used by Kehoe and Gee 2007 amongst others).
