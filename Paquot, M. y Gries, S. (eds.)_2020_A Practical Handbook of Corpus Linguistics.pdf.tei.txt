Introduction

Corpus linguistics is "a whole system of methods and principles"

The tasks of corpus linguists are manifold and complex. They can be grouped into minimally three different, though of course interrelated, areas:

• Corpus design, which requires knowledge about corpus compilation (e.g., the notions of sampling and/or representativeness), data processing for corpus annotation (e.g., tagging, lemmatizing, parsing), and corpus architecture (e.g., representing corpus data in a maximally useful way); then, once there is a corpus, • Corpus searching/processing, which requires knowledge of, ideally, general data processing (e.g., file management, dealing with different annotation formats, using regular expressions to define character strings that lead to good search results) as well as corpus query tools and methods (from off-the-shelf tools to programming) to address the specificities of various data types (e.g., time alignment of spoken and multimodal corpora or bitext alignment of parallel corpora); then, once there are results from a corpus, • Statistical analysis to get the most out of the corpus data, which requires knowledge of statistical data wrangling/processing (e.g., establishing subgroups in data or determining whether transformations are necessary), statistical analysis techniques (e.g., significance testing or alternative approaches, regression modeling, or exploratory data analysis), and visualization (e.g., representing the results of complex statistical analysis in ways non-quantitatively minded readers can understand).

This handbook aims to address all these areas with contributions by many of their leading experts, to be a comprehensive practical resource for junior and more v vi Introduction senior corpus linguists, and to represent the whole research cycle from corpus creation, method, and analyses to reporting results for publication. It is divided into six parts. In Part I, the first three chapters focus on corpus design and address issues related to corpus compilation, corpus annotation, and corpus architecture.

Part II deals with corpus methods: Chapters 4-9 provide an overview of the most commonly used methods to extract linguistic and frequency information from corpora (frequency lists, keywords lists, dispersion measures, co-occurrence frequencies and concordances) as well as an introduction on the added value of programming skills in corpus linguistics. Chapters 10-16 in Part III review different corpus types (diachronic corpora, spoken corpora, parallel corpora, learner corpora, child language corpora, web corpora, and multimodal corpora), with each chapter focusing on the specific methodological challenges associated with the analysis of each type of corpora.

Parts IV-VI aim to offer a user-friendly introduction to the variety of statistical techniques that have been used, or have started to be used, more extensively in corpus linguistics. As each chapter under Part IV-VI uses R for explaining and exemplifying the statistics, Part IV starts with an introductory chapter on how to use R for descriptive statistics and visualization. Chapters 18 and 19 focus on exploratory techniques, i.e., cluster analysis and the multidimensional exploratory approaches of correspondence analysis, multiple correspondence analysis, principal component analysis, and exploratory factor analysis. Part V focuses on hypothesistesting (classical monofactorial tests, fixed-effects regression modeling, mixedeffects regression modeling, generalized additive mixed models, bootstrapping techniques and conditional inference trees and random forests). It is important to note that the chapters on mixed effects regression modeling and generalized additive mixed models in particular are primarily meant for readers to get a grasp of what these techniques are (more and more corpus linguistic papers rely on such methods and it is important for corpus linguists to understand the current literature). However, a single chapter can of course not provide all that is required to get started with statistics of such a level of complexity. If the reader is interested to know more and wants to use these statistics for their own purposes, they will necessarily need to read more on the topic.

Part VI aims to pull everything together by providing guidelines for how to write a corpus linguistic paper and how to meta-analyze corpus linguistic research.

Chapters in Parts IV and V as well as Chaps. 7, 9 and 27 come with online additional material (R code with datasets).

It is our hope that this handbook will serve to help students and colleagues expand their methodological toolbox. We certainly learned a lot while editing this volume! Introduction vii

Introduction

Given that linguistics is descriptive at its core, many linguists study how language is used based on some linguistic sample. Finding the right material to use as the basis for a study is a key aspect of the research process: we are expected to use material that is appropriate for answering our research questions, and not make claims that go beyond what is supported by the material. This chapter covers the basics of compiling linguistic material in the form of a corpus. Corpus compilation involves "designing a corpus, collecting texts, encoding the corpus, assembling and storing the relevant metadata, marking up the texts where necessary and possibly adding linguistic annotation"

Fundamentals

Representativeness

The most basic question to consider when compiling a corpus involves representativeness: what type of speakers/variety/discourse is the corpus meant to represent? In many of the well-known corpora of English, the ambition has been to cover a general and very common type of discourse (such as 'conversation in a variety of English') or a very large population (such as 'second-language learners of English'). However, such a comprehensive aim is beyond the scope for most researchers and should be reserved for large groups of researchers with plenty of resources at their disposal (see e.g.

The general sense of the word 'sample' is simply a text or a text extract, but in its more specific and statistical sense it refers to "a group of cases taken from a population that will, hopefully, represent that population such that findings from the sample can be generalised to the population"

The corpus builder needs to consider very carefully how to collect samples that maximally represent the target discourse or population. One of the ways of selecting material for a corpus is by stratified sampling, where the hierarchical structure (or 'strata') of the population is determined in advance. For example, a researcher who is interested in spoken workplace discourse could document demographic information about speakers' job titles and ages and whether interactions involve peers or managers/subordinates, and then include in the corpus a predetermined proportion of texts from each category. In the detailed sampling process, it is decided exactly what texts or text chunks to include.

There is a range of possible considerations to take in deciding about sampling procedures for a corpus, one of which concerns to what extent to organize the overall design around text production or text reception. For illustration, this is what the compilers of the British National Corpus

In selecting texts for inclusion in the corpus, account was taken of both production, by sampling a wide variety of distinct types of material, and reception, by selecting instances of those types which have a wide distribution. Thus, having chosen to sample such things as popular novels, or technical writing, best-seller lists and library circulation statistics were consulted to select particular examples of them.

A concept that is intrinsically related to representativeness is balance, which has to do with the proportions of the different samples included in the corpus. In a balanced corpus, "the relative sizes of each of [the subsections] have been chosen with the aim of adequately representing the range of language that exists in the population of texts being sampled"

The notions of representativeness and balance are scalar and vague (see e.g.

However, if the available types of discourse are not already classified in some reliable way, as in the case of spoken language, it means that the corpus builder will have to dedicate a great deal of time to researching the characteristics of the target discourse in order to develop valid and acceptable selection criteria.

With a definition of representativeness as the extent to which a corpus reflects "the full range of variability in a population"

The literature on corpus design sometimes contrasts 'principled' ways of building a corpus to 'opportunistic' ones. An opportunistic corpus is said to "represent nothing more nor less than the data that it was possible to gather for a specific task", with no attempts made "to adhere to a rigorous sampling frame"

1 Corpus Compilation 7

Issues in Collecting Data for the Corpus

Corpus compilation involves a series of practical considerations having to do with the question 'Given the relative ease of access, how much data is it feasible to collect for the corpus?'. Indeed, this needs addressing before it is possible to determine fully the design of a corpus. Relevant spoken or written material may of course be found in many different places, and the effort required to collect it may vary considerably. Some types of discourse are meant to be widely distributed, and are even in the public domain, while others are relatively hidden, and are even confidential or secret. In an academic setting, for example, written course descriptions and spoken lectures target a large audience, while teacher feedback and committee discussions about the ranking of applicants for a post target a restricted audience.

Once the data have been collected, varying degrees of data management will be required depending on the nature and form of the data. If spoken material is to be included in the corpus, it needs to be transcribed, that is, rendered in written form to be searchable by computer. The transcription needs to be appropriately detailed for the research question (see

Nowadays there are massive amounts of material on the web, which are already in an electronic format. As a consequence, it has become popular among corpus builders to include material from online sources (see Chap. 15), which represent a great variety of genres, ranging from research articles to blogs. It is important, however, to make the relevance of the material to the research question a priority over ease of access, and carefully consider questions such as "How do we allow for the unavoidable influence of practicalities such as the relative ease of acquiring public printed language, e-mails and web pages as compared to the labour and expense of recording and transcribing private conversations or acquiring and keying personal handwritten correspondence?"

Even if material is available on the web, it does not necessarily mean that it is easy to access-at least not in the way texts need to be accessed for corpus work. Online newspapers are a case in point. While they often make it possible to search A. Ädel the archive, they may not make the text files downloadable other than one by one by clicking a hyperlink. The work of clicking the link, copying and saving each individual article manually is then left to the user. This is no small task, but it tends to be underestimated by beginner corpus compilers. Fortunately, there are ways of speeding up and automatizing the process in order to avoid too much manual work; Chap. 15 offers suggestions.

Corpus compilers who are able to collect relevant material in the public domain still need to check the accuracy and adequacy of the material. Consider the case of a research group seeking the answer to the question 'To what extent is (a) the spoken dialogue in the fictional television series X (dis)similar to (b) authentic non-scripted conversation?'. They may go to the series' website to search for material, following the logic that an official website is likely to be a more credible source for transcripts than a site created by anonymous fans. Before any material can be included in the corpus, however, each transcript needs to be checked against the recorded episode to ensure that the transcription is not only correct, but also sufficiently detailed for the specific research purposes. When collecting material from the web, there may also be copyright restrictions to take into account; see e.g. the section on Ethical considerations below and Section 3.2 in

Beginner corpus researchers often find themselves confounded by the question 'How much data do I need in order for my study to be valid?'. There is no rule of thumb for corpus size, except for the general principle 'the more, the better'. That said, it requires more data to be able to make valid observations about a large group of people and a general type of discourse than a small group of people and a specific type of discourse. It also requires more data to investigate rare rather than common linguistic features. Thus, the appropriate amount of data depends on the aim of the research. Each study, however, needs to be considered in its context. There are always going to be practical restrictions on how much time a given researcher is able to put into a project. Researchers who find themselves in a situation of not being able to collect as much data as planned will need to adjust their research questions accordingly. With less data-a smaller sample-the claims one is able to make based on one's corpus findings will be more modest. But most importantly, as discussed above, the issue of representativeness needs to be addressed before a corpus, regardless of size, can be considered appropriate for a given study.

Ethical Considerations

Corpus compilation involves different types of ethical considerations depending on the type of data. For data in the public domain, such as published fiction or online newspaper text, it is not necessary to secure consent. However, such data may be protected by copyright. For data that is collected from scratch by the researcher, it is necessary to obtain the informants' informed consent and it may be necessary to ask for institutional approval.

In the case of already published material, permission may be needed from a publisher or some other copyright holder. There are grey areas in copyright law and copyright infringement is looked at in different ways in different parts of the world, so it is difficult to find universally valid advice on the topic, but generally speaking copyright may prove quite a hindrance for corpus compilation. To a certain extent, restrictions on copyright may be alleviated through concepts such as 'fair use', as texts in a corpus are typically used for research or teaching purposes only, with no bearing on the market.

In the case of collecting data from informants, approval may be needed from an institutional ethics review board before the project can begin. Even if institutional approval is not needed, consent needs to be sought from the informants in order to collect and use the data for research purposes. Asking for permission to use material for a corpus is often done by means of a consent form, which is signed by each informant, or by the legal guardians in the case of children (see

Once permission has been obtained to use data for a corpus, the informants' integrity needs to be protected in different ways, such as by anonymizing the material. An initial step may be to not reveal the identity of the informants by not showing their real names, for example through 'pseudonymisation', whereby personal data is transformed in such a way that it cannot be attributed to a specific informant without the use of additional information, which is kept separately. A second step may be to manipulate the actual linguistic data (that is, what the A. Ädel people represented in the corpus said or wrote) by changing also names and places mentioned which could in some way give away the source. In the case of image data, this would involve masking participants' identity in various ways.

Confidential data needs to be stored in a safe way. Sensitive information may have to be destroyed if there is a risk that others may access information which informants have been promised will not be revealed. For further reading on ethical perspectives on data collection, see e.g. BAAL's Recommendations on Good Practice in Applied Linguistics.

Documenting What Is in the Corpus

As language use is characterized by variability, factors which may have an impact on the way in which language is used should be recorded in some way-these may include demographic information about the speakers/writers, or situational information such as the purpose of the communication or the type of relationship between the discourse participants. Even if the corpus compilers are deeply familiar with the material, it is still the case that memory is both short and fallible, so if they want to use the corpus in a few years' time, important details of the specific context of the data may well have been forgotten. In addition, if the corpus is made available to others, they need to know what is in it in order to make an informed decision about whether the design of the corpus is appropriate for answering their specific research questions.

Anybody who wants the claims made based on a corpus to be accepted by the research community needs to show in some way that the corpus material is appropriate for the type of research done. With incomplete description of the corpus, people will be left wondering whether the material was in fact valid for the study. There are several different ways in which information about the corpus design can be disseminated. It can be done through a research publication, such as a research article or an MA thesis, which includes a section or chapter describing the material (for more on this, see Chap. 26). Corpus descriptions are sometimes published in peer-reviewed journals, especially if the corpus is breaking new ground (as is the case in Representative Study 2 below), so that the research community can benefit from discussions on corpus design. It can also be done by writing a report solely dedicated to describing the corpus (and possibly how to use it), which is made available either as a separate file stored together with the corpus itself, or online. Corpora often come with "read me" files where the corpus design is accounted for. Some large corpus projects intended to attract large numbers of users, such as the British National Corpus (BNC) and the Michigan Corpus of Academic Spoken English (MICASE), provide relatively detailed reports online.

Another reason for documenting what is in the corpus is to enable researchers to draw on various variables in a systematic way when analyzing data from the corpus. As an example, see Chap. 8 and the subsection on quantitative analysis of concordance lines. In a study of that-complementation in English, for each hit in the corpus, the researchers considered external variables such as the L1 of the speaker who had produced the hit and whether the hit came from a written or spoken mode.

Through the inclusion of 'metadata'-data about the data-about the type of discourse represented in the corpus, the corpus user can keep track of or investigate different factors that may influence language use, which may explain differences observed in different types of data. Metadata can consist of different types of information. For example, the corpus compiler may include information based on interviews with participants or participant observation. A common way of collecting metadata is by asking corpus participants to fill out a questionnaire which has been carefully designed by the corpus compiler so as to include information likely to be relevant with respect to the specific context of the discourse included and the people represented. An example of metadata based on a questionnaire from the International Corpus of Learner English (ICLE) is summarized in Fig.

Based on metadata from the questionnaire, it is possible to select a subset of the ICLE corpus, for example to study systematically potential differences in language use between learners who have and who have not spent any time abroad in a country where the target language is spoken natively-and thus test a hypothesis from Second Language Acquisition research.

Formatting and Enriching the Corpus

There is a great deal to be said about how best to format corpus material, but this section will merely offer a few hints on technicalities. (More detailed information is found in Chaps. 2 and 3.) Researchers' computational needs and programming skills vary. Those who are reasonably computer literate and whose corpus needs are relatively simple are likely to be able to do all the formatting themselves. However, those who wish to compile a corpus involving complex types of information or do advanced types of (semi-)automatic corpus searches would be helped by collaborating with a computational linguist or computer programmer (see Chap. 9).

A plain text format (such as .txt) is often used for corpus files. MS Word formats are avoided, as these add various types of information to the file and do not work with corpus tools such as concordance programs. When naming files for the corpus, it is useful to have the file name in some way reflect what is in the file. For example, the file name 'BIO.G0.02.3' in a corpus of university student writing across disciplines and student levels (Michigan Corpus of Upper-level Student Papers, MICUSP; see

It may be necessary, or just a good investment of time, to add markup, that is, "codes inserted into a corpus file to indicate features of the original text rather than the actual words of the text. In a spoken text, markup might include utterance breaks, speaker identification codes, and so on; in a written text, it might include paragraph breaks, indications of omitted pictures and other aspects of layout"

Markup allows the corpus builder to include important information about each file in the corpus. Various types of metadata can be placed in a separate file or in a 'header', so that a computer script or web-based tool for example will be able to use the information in systematic ways when counting frequencies, searching for or displaying relevant data. If we consider the metadata from the ICLE (Fig.

Another way of adding useful information to a corpus is through annotation, or "codes within a corpus that embody one or more linguistic analyses of the language in the corpus"

(a) But like, I was thinking this is gonna be so embarrassing like in P E! The contemporary standard for corpus markup and annotation is XML (eXtensible Markup Language), where added information is indicated by angle brackets <>, as illustrated in Fig.

XML is ideal "because of its standard nature" and "because so much corpus software is (at least partially) XML-aware"

Even more fundamental than markup or annotation is encoding, which refers to "the process of representing a text as a sequence of characters in computer memory"

There are many considerations for formatting corpus material in ways that follow current standards and best practice. An authoritative source is the Text Encoding Initiative (TEI),

Sharing the Corpus

One of many ways in which corpora vary is in how extensively and long-term they are intended to be used. A corpus can be designed to be the key material for many different research projects for a long time to come, or it can be created with a single project in mind, with no concrete plan to make it available to others. In the former category, we find 'standard' corpora, which are widely distributed and which form the basis for a large body of research. This type of corpus is designed to be representative of a large group of speakers, typically adopting "the ambitious goal of representing a complete language", as

Even in the context of a small-scale corpus project, it is considered good practice in research to make one's data available to others. It supports the principle of replicability in research and it fosters generosity in the research community. Our time will be much better invested if more than one person actually uses the material we have put together so meticulously. Certain types of data will be of great interest to not only researchers or teachers and students, but also the producer community itself, as in the case of sign language corpora (e.g.

Even if open access is not a requirement, in a case where a researcher is applying for funding to compile a corpus for a research project, it may be a good idea to include an entry in the budget for eventually making the corpus available. If, say for various reasons related to copyright, it is not possible to make the complete set of corpus files available to others, the corpus could still be made searchable online and concordance lines from the corpus be shown.

Another consideration in sharing corpus resources involves how to make these accessible to others and how to preserve digital data. The easiest option is to find an archive for the corpus, such as The Oxford Text Archive or CLARIN.

Corpus Comparison

Corpus data are typically studied quantitatively in some capacity. This means that the researcher will have various numbers to which to relate, which typically give rise to questions such as 'Is a frequency of X a lot or a little?'. Such questions are difficult to answer in a vacuum, but are more usefully explored by means of comparison-for example by studying the target linguistic phenomenon not just in one context, but contrasting it across different contexts. Statistics can then be used to support the interpretation of results across two or more corpora, or to assess the similarity between two or more corpora (see e.g.

The researcher may go on to ask qualitative questions such as 'How is phenomenon X used?' and systematically study similarities and differences in (sub-)corpus A and (sub-)corpus B. Even if frequencies are similar in cross-corpus comparison, it may be the case that, once you scratch the surface and do a qualitative analysis of how the individual examples are actually used, considerable differences emerge. In order for the comparison to be valid, however, the two sets ((sub-)corpus A and (sub-)corpus B) need to be maximally comparable with regard to all or most factors, except for the one being contrasted. Some corpora are intentionally constructed for comparative studies (this includes parallel corpora, covered in Chap. 12). In contrastive studies of different languages or varieties, for example, it is useful to have a so-called comparable corpus, which "contains two or more sections sampled from different languages or varieties of the same language in such a way as to ensure comparability"

Having considered some of the fundamentals of corpus compilation, we will next turn to the two sample studies, which will illustrate further many of the concepts mentioned in this section. Jaworska (2016:84) makes the point that "corpus tools and methods

1. How are hosts represented in tourism promotional materials produced by Western versus local tourist industries? 2. To what extent do these representations differ? 3. What is the nature of the relationship between the representations found in the data and existing stereotypical, colonial, and often gendered ideologies?

To answer these questions, two corpora were created, consisting of written texts promoting tourist destinations that have a history of being colonised.

The two corpora represent, on the one hand, a Western, 'external' perspective and, on the other, a local, 'internal' perspective, which are contrasted in the study. They are labelled the External Corpus (EC) and the Internal Corpus (IC).

To create the EC, texts were manually taken from the websites of "some of the largest tourism corporations operating in Western Europe". A selection of 16 destinations was made, based on the most popular destinations as identified by the companies themselves during the period of data collection-however excluding Southern European destinations, as the focus of the research was on post-colonial discursive practices. To create the IC, official tourism websites were sourced from the 16 countries selected in the process of creating the EC. All of the websites are listed in an appendix to the article.

A restriction imposed on the data selection for both corpora was to include only "texts that describe the countries and its main destinations (regions and towns)" rather than specific resorts or hotels or information on how to get there. This was to make the two corpora as comparable as possible. However, one way in which they differ is with respect to size, with the IC being three

The corpora were compared by extracting lists of the most frequent nouns (cf. Chap. 4). From these lists were identified the most frequent items used to refer to local people (e.g. people, locals, man/men, woman/women, fishermen). Careful manual analysis was required in order to check that each instance was relevant, that is, actually referring to hosts/local people. The word people, for example, was also sometimes used to refer to tourists. It was found that the IC had not only more tokens of such references, but also more types (F = 68) compared to the EC (F = 20). The tokens were further classified into socio-semantic groups of social actors based on an adapted taxonomy from the literature, for example based on 'occupation' (fisherman, butler), 'provenance' (locals, inhabitants), 'relationship' (tribe, citizens), 'religion' (devotees, pilgrims), 'kinship' (son/s, child/ren) and 'gender' (man/men, woman/women).

The corpora were compared qualitatively as well, by identifying patterns in the concordance lines and analysing the context ("collocational profiles") of the references to hosts, specifically of people and locals, which occurred in both corpora. The pattern found for locals was that local people were represented "on an equal footing with tourists" in the IC, while in the EC they were portrayed as "docile, friendly and smiley servants

Representative

The authors hope that the use of the corpus "will advance the linguistic theory of narrative as a primary mode of everyday spoken interaction" (315).

(continued)

A. Ädel Previous work on this type of discourse has been based not on corpus data, but on elicited interviews or narratives told by professional narrators. The corpus comprises selected extracts of narratives, 153 in all, for a total of around 150,000 words, taken from the demographically sampled 'casual conversations' section of the BNC, which is balanced by sex, age group, region and social class, and which totals approximately 4.5 million words. This example is somewhat unusual in that the authors do not collect the data themselves, but instead use a selection of data from an existing corpus. However, given that the intended audience of this handbook is expected to have limited resources for corpus compilation, it seems useful to provide an example of a study where it was possible to use part of an already existing corpus. The NC is only about 3% of the original collection from BNC, so the authors have put a great deal of effort into selecting the data, which is done in a transparent and principled way. In the article, they describe (i) the extraction techniques, (ii) selection criteria and (iii) sampling methods used in constructing the corpus. In order to (i) retrieve narratives, they (a) read the files manually and (b) used a small set of lexical forms (e.g. it was so funny/weird; did I tell you; reminds me) that tend to occur in narratives based on the literature or based on analysis of their own data. In

The NC is not only a carefully selected subset of the demographically sampled BNC, but it is also annotated. The corpus builders have thus augmented the existing data by adding various types of information-about the speakers (sex, age, social class, region of origin, educational background), about the text (type of narrative; whether a stand-alone story or part of a 'narrative chain') and about the utterance (the roles of the participants visà-vis the narration; type of quotative verb used to signal who said what in a narrative; to what degree the discourse is represented as being verbatim or more or less indirect). The authors stress that all of the annotation is justified in some way by the literature on conversational narrative, so the rationale for

The corpus design makes it possible to use the demographic information about the speakers-such as sex-and consider how it is distributed in relation to the number of words uttered by the speakers who are involved in the narratives, as exemplified in Table

Each narrative in the corpus is classified also based on a taxonomy of narrative types. This type of information is highly useful, as it not only makes it possible to study and compare different types of narrative, but it also shows how the corpus is balanced (or not) with respect to type of narrative. The classification is justified by an observation from the literature that "we are probably better off [] considering narrative genre as a continuous cline, consisting of many subgenres, each of which may need differential research treatment"

At the time of creation, the NC was the first corpus of conversational narratives to be annotated, so there was no established practice to follow regarding what analytical categories to annotate. However, the authors were able to follow some general guidelines, for example

A. Ädel

Critical Assessment and Future Directions

The representation of a group of language users/variety of language/type of discourse in a corpus inevitably involves simplification and loss of complex contextual information. If we consider the future of corpus building from the perspective of the loss of complex information, it is interesting to note that few existing corpora reflect a feature which many present-day types of discourses exhibit: that of multimodality. It represents information of a kind that many corpus creators have expressed an interest in, but which few corpus projects have included (see Chap. 16 for more information). If we take the two sample studies as an example, they would both have benefitted from multimodal data. In

Regarding the inevitable loss of contextual information in the making of a corpus, it is important to attempt to compensate for this by means of rich metadata that describe the material. With better metadata about individual texts and speakers, we will be in a better position to understand the data, not only to correlate metadata to variation, but also to see more precisely how corpora differ in the case of comparison. Corpus enrichement is an important way forward, and this applies not only to metadata but also to linguistic annotation. Some of the possibilities of corpus annotation are presented in the next chapter. In order to promote and make better use of corpus enrichment, there is a need for collaborative work between linguists with a deep knowledge of the needs to different areas such as Second Language Acquisition or Historical Linguistics and experts in Computational Linguistics or Natural Language Processing. most typically takes the form of adding linguistically relevant information about words, phrases, and clausal/sentential units, though other linguistic units can also be annotated, e.g., morphemes, intonation units, conversational turns, and paragraphs. The reality of contemporary corpus linguistics is that the corpora we rely on, in most cases, are simply too large for manually adding annotation, and the automated annotation of electronic texts has become the primary focus in the development of annotation methods. Consequently, it is automated linguistic annotation that we will be concerned with in this chapter (see Part III of this volume for discussion of manual annotation in certain kinds of corpora, e.g., annotation of errors in a learner corpus). In order to simplify the discussion that follows, we illustrate our points about the fundamentals of annotation using primarily English data.

While the raw text of an unannotated corpus has its own unique value, (wisely) annotated corpora offer great advantages over the raw text when it comes to the investigation of linguistic phenomena. Most linguistic phenomena of interest to linguists are couched in terms of linguistic constructs (the plural morpheme, the passive construction, time adverbials, the subject of a verb, etc.), rather than orthographic words. A corpus that has been annotated with the needs of linguists in mind can greatly facilitate the exploration of such phenomena by reducing the time and effort involved. Even if an automatically annotated corpus is unlikely to meet all the expectations of a researcher in terms of its categories of annotation, it can still be an invaluable resource.

Fundamentals

In the following sub-sections, we deal with a few main types of annotation (partof-speech tagging, lemmatization, syntactic parsing, and semantic annotation), the accuracy of annotation, and the practicalities of carrying out annotation of texts.

Part-of-Speech Tagging

Part-of-speech (POS) tagging is a common form of linguistic annotation that labels or "tags" each word of a corpus with information about that word's grammatical category

There are many POS tagsets currently used in English corpus analysis, varying in degree of differentiation of POS categories and in the nature of the categories themselves (see Atwell 2008 for an overview of English POS tagsets). One commonly used tagset is CLAWS (Constituent Likelihood Automatic Word-tagging System), available in different versions (e.g., CLAWS 5 contains just over 60 tags, while CLAWS 7 contains over 160). (

(1) a. If_CS the_AT government_NN1 continues_VVZ to_TO behave_VVI in_II this_DD1 way_NN1 ,_, it_PPH1 will_VM find_VVI itself_PPX1 facing_VVG opposition_NN1 from_II those_DD2 who_PNQS have_VH0 been_VBN supporting_VVG it_PPH1 many_DA2 years_NNT2 . Contracted forms of English show some peculiarities when tagged with CLAWS. For example, in the British National Corpus, tagged with CLAWS 5, gonna is segmented into gon (VVG) and na (TO), tagged just like the unreduced equivalent going to would be. Similarly, the reduced form of isn't, inn't, appears as the sequence in (VBZ, a tag otherwise reserved for the is form of the verb be), n (XX0, a negative particle), and it PNP (personal pronoun) (see Chap.

Sometimes, it is useful to allow multiple tags to be associated with the same word. The CLAWS tagger, for example, assigns a hyphenated POS tag, referred to as an "ambiguity tag", when its tagging algorithm is unable to unambiguously assign a single POS to a word. For example, singing in the sentence She says she couldn't stop singing, even if she wanted to try is tagged in the British National Corpus by CLAWS 5 as VVG-NN1. The hyphenated tag in this case indicates that the algorithm was unable to decide between VVG (the -ing form of a verb) and NN1 (the singular of a common noun), but the preference is for the VVG tag which appears as the first element of the hyphenated tag. In some cases, genuine multiple readings of a sentence may be possible, e.g., the sentence the duchess was entertaining from the Penn Treebank, where entertaining could justifiably be tagged either as an adjective or as a present participle. Hyphenated tags also have a useful role to play in the tagging of diachronic corpora where a word may come to be associated with different parts of speech or different functions through time (cf.

POS tags may also be attached to sequences of words. So, for example, in the British National Corpus, sequences such as of course, all at once, and from now on are tagged as adverbs, while instead of, in pursuit of, and in accordance with are tagged as prepositions. These tags, of which there are many, are assigned as part of an "idiom tagging" step after the initial assignment of POS tags and involve matching the sequences of POS-tagged words in the corpus against templates of multiword sequences.

Each POS tagset, either explicitly or implicitly, embodies some theory of grammar, even if the theory is simply traditional grammar. It would be unrealistic, therefore, to expect that different POS-tagging algorithms for the same language will produce identical results. Consider the tags assigned to rid in the three sentences in Table

Lemmatization

Another common kind of annotation found in modern-day corpora is lemmatization. In lemmatization, each orthographic word encountered in a corpus is assigned a lemma, or 'base form', which provides a level of abstraction from any inflection that might appear in the original orthographic word. If we were to lemmatize this paragraph up to this point, for instance, we would see that the resulting nouns would appear without plural marking, and many verbs without agreement with their subjects, as in (2). Here, the lemmas have replaced the original words, but lemmas could also be added as additional information to the inflected forms (see

(2) another common kind of annotation find in modern -day corpus be lemmatization . in lemmatization , each orthographic word encounter in a corpus be assign a lemma , or ' base ' form , which provide a level of abstraction from any inflection that may appear in the original orthographic word . if we be to lemmatize this paragraph up to this point , for instance , we will see that the resulting noun will appear without plural marking , and many verb without agreement with their subject , as in ( 2 ).

These kinds of lemmas often resemble the headwords found in dictionaries. Like the lemmas found in corpora, dictionary headwords often aim to represent a base word form (e.g., toaster, shine, be), rather than provide separate entries for each distinct inflected word form (e.g., toasters, shines / shone / shining, is / am / are / were / was / been / being). Both the headwords used in dictionaries and the lemmas found in corpora serve a similar purpose: they allow researchers to locate information more readily, particularly when searching by individual inflected word forms might otherwise make it difficult to find features of interest.

In corpus linguistic studies, lemmas can be particularly useful in making complex searches more tractable, especially when a single word appears in many forms. Searches based on lemmas can be invaluable when working with corpora of languages with rich inflection, such as Romance languages like French or Spanish, where a single regular verb may have dozens of distinct inflected forms. While searches such as these can also often be conducted using regular expressions, lemmas generally make these searches more straightforward, and help to ensure that no relevant surface forms are inadvertently overlooked. English is not as richly inflected as some languages, but the variation found in inflected forms of, say, lexical verbs is still considerable (cf. the inflected forms of bring, sing, drive, send, stand, etc.) and being able to search for all instances of such verbs on the basis of the lemmas will save time and effort.

The choice of lemmatization software often depends on the kinds of language found in the corpus materials. Lemmatization can be an extremely useful tool in the corpus builder's toolkit, especially when searches of the annotated corpus may need to be run over many inflected forms. Although current English-language lemmatization tools make this process much easier to carry out on large bodies of text, it is often worth bearing in mind that even the most sophisticated lemmatization software will inevitably run into cases that are not entirely clear-cut (e.g., should the lemma of the noun axes be AXE or AXIS?) and where the resulting lemmas are not necessarily what one might expect. Different analyses can thus lead to different lemma assignments; accordingly, many lemmatization tools are careful to document the language-specific lemmatization guidelines that they follow. As when using other corpus annotations that have been produced by automatic or semi-automatic procedures, understanding the limitations of automatic lemmatization and treating its outputs accordingly with a degree of circumspection is often a necessary part of the corpus annotation and analysis process.

Syntactic Parsing

The preceding sections have focused on adding annotations to corpus sources that focused on the properties of individual words-in the case of part-of-speech tagging, on their membership in particular grammatical classes, or in the case of lemmatization, on their association with particular headwords. When annotating a corpus, we may also be interested in adding information about the relationships that exist between elements in our texts above the level of the individual word that may help shed light on larger patterns in our corpus. Identifying particular multiword expressions, as mentioned in the preceding section, is an example of this. Another kind of higher-level annotation is syntactic parsing, which aims to provide information about the grammatical structure of sentences in a corpus.

While syntactic structures can be assigned to sentences manually (and may be necessary when developing a corpus for a language for which few syntacticallyannotated corpus resources have already been developed; see Sect. 2.3), it is more common for syntactic annotations to be added automatically by a syntactic parser, a program that provides information about different kinds of syntactic relationships that exist between words in a given text (parses). In the past, syntactic parsers were typically developed following deterministic approaches, often applying sets of carefully crafted syntactic rules or constraints to determine syntactic structure in a fully predictable way. In the 1990s, a new wave of syntactic parsers emerged that adopted a range of novel, probabilistic approaches (see Collins 1999 for an overview). These parsers began by analyzing large numbers of syntactically annotated sentences (a treebank), attempting to learn how syntactic structures are typically assigned to input sentences. Once trained on a particular sample of sentences, a probabilistic parser could then use that information to determine what the most likely parses would be for any new sentences it encountered, weighing the likelihood of different possible analyses in consideration. These probabilistic parsers have become increasingly common in corpus and computational linguistics, and generally work quite well for annotating arbitrary corpus texts in many languages. One such parser is the Stanford Parser

(3) Parse of She ran over the hill.

(ROOT (S (NP (PRP She)) (VP (VBD ran) (PP (IN over) (NP (DT the) (NN hill)))) (. .)))

Another common form of syntactic information in corpora are dependency annotations, which indicate the grammatical relationships between sets of words. The online Stanford Parser is able to provide dependency parses for input sentences, as well. Example (4) shows the dependency parse for our previous example sentence, She ran over the hill. In this representation, each word in the input is numbered according to its position in the original sentence: the first word, She, is marked with -1, while the fifth word, hill, is marked as -5. Each word appears in a three-part structure that gives the name of the grammatical relationship, followed by the governing and dependent elements (e.g., the nominal subject (nsubj) of the second word in the sentence, ran, is the first word in the sentence, She; the determiner of the fifth word, hill, is the fourth word, the, etc.). By tracing these relationships, it is possible to extract features of corpus sentences-subjects of active and passive verbs, objects of prepositions, etc.-that would be difficult to retrieve from searches of unannotated text alone.

(4) nsubj(ran-2, She-1) root(ROOT-0, ran-2) case(hill-5, over-3) det(hill-5, the-4) nmod(ran-2, hill-5)

As informative as these compact, textual representations of constituency and dependency structures can be, it can also be useful to be able to visualize syntactic annotations, especially whenreviewing annotated corpus materials for accuracy. Several freely available tools are able to produce graphical representations of the kinds of dependency and constituency structures seen here. In Fig.

Semantic Annotation

Semantic annotation refers to the addition of semantic information about words or multiword units to a corpus. An example of an annotation model like this is the UCREL Semantic Analysis System (USAS).

(6) a. The_Z5 ending_T2-of_Z5 the_Z5 poem_Q3 may_A7+ seem_A8 to_Z5 be_A3+ contradictory_A6.1-because_Z5/A2.2 both_N5 girls_S2.1f marry_S4 and_Z5 have_A9+ children_S2mf/T3-;_PUNC thereby_Z5 filling_N5.1+ the_Z5 traditional_S1.1.1 female_S2.1 role_I3.1 ._PUNC b. at_T1.

Annotation Accuracy

Automated annotation is subject to errors and consequently the accuracy of annotation models is a key consideration for researchers either choosing a model to apply to a corpus or working with a corpus already annotated. A common measure of (token) accuracy is to report the number of correct tags in a corpus as a percentage of the total number of tags in the corpus, or more likely in some sample of the whole corpus extracted for the explicit purpose of checking accuracy (cf.

The concepts of precision and recall (cf.

The accuracy rates reported above give some sense of what is possible with stateof-the-art automatic annotation. For one thing, the more fine-grained an annotation system, the more difficult it will be to achieve high accuracy. Clearly, the accuracy of a model will also vary depending on the type of text it is applied to, and even within formal written genres of English, accuracy of annotation can vary quite a bit, as seen in Table

Practicalities of Annotation

While the preceding discussion has focused on introducing different kinds of automatic annotation that are commonly assigned to linguistic corpora, it is also reasonable to ask how these methods can be applied in practice when creating a new corpus. At the outset, this involves making decisions as to what kinds of annotations should be added, what conventions should be followed for representing that information consistently, and what tools will be used to apply those conventions to corpus source materials. All these decisions and the availability of existing conventions and annotation tools can make a significant difference to the overall process of annotation that follows. In the case of relatively well-resourced languages like English, for which many corpus annotation standards and tools exist, many common annotation tasks can be accomplished with 'off-the-shelf' software tools and minimal customization of annotation standards or procedures. In contrast, for many lesser-studied languages and varieties, these same tasks may require the development of annotation conventions that 'fit' the linguistic features of the source materials, as well as the implementation of these conventions in existing annotation tools, adding additional complexity to the overall corpus annotation workflow (see

In this section, we consider the example of applying POS tagging to a collection of unannotated corpus source materials. As mentioned in Sect. 2.2.1, this is a common task in corpus development, and one on which other forms of linguistic annotation (e.g., lemmatization, syntactic annotation) often rely. In the most straightforward case, it may be possible to use existing annotation software to automatically apply a given set of POS tagging conventions to corpus source materials. If one were aiming to create an English-language corpus of the novels of Charles Dickens (1812-1870), one might begin by retrieving plain-text copies of these works from Project Gutenberg,

(8) for i in $ * ; do tree-tagger-english "$i" > "TAGGED-$i"; done For many larger-scale or more complicated corpus construction projects, it is common for annotation to be implemented in custom software, which may in turn be integrated into larger 'pipelines' of natural language processing (NLP) tools that feed corpus source documents through successive stages of annotation and analysis (cf. Chap. 3).

While this discussion has assumed that appropriate tagsets and tagging models exist for the language or variety represented in your corpus, in many cases, existing sets of POS categories may not be suitable for the data, or may not exist at all. In situations like these, it is possible to train some kinds of annotation software to apply new tagging conventions to your corpus sources. This involves manually annotating samples of corpus text as examples of how the new annotation categories should be assigned. These examples are then provided to training software that attempts to learn how these categories should be assigned to other texts, often by assessing the probability of particular sequences of words and annotations occurring together (e.g., is dog more likely to be a noun or a verb when preceded by the words the big?).

Creating a new POS tagging model is often an iterative process: small samples of text are manually annotated and used as training data to create a provisional POS tagger, which then automatically annotates more texts. These texts are then reviewed and their POS tags corrected, creating more data for the next round of training and annotation (cf.

Once POS annotations have been added to a collection of texts, there are several ways in which they can be used in queries. Some corpus search tools, such as AntConc

Representative

The analysis relies on the collostructional analysis approach pioneered by

As an initial attempt to establish noun preferences in the two constructions, one could simply retrieve all instances of EXPERIENCE N and EXPERIENCE OF N from the corpus and identify the most frequently occurring nouns in each construction. We say "simply", but this task is only simply carried out when the corpus is POS-tagged and one is able to search for the verb EXPERIENCE immediately followed by a noun. In the COCA interface the relevant search expression is:

Instead of relying solely on raw frequency of occurrence of nouns in each construction, however, it has become commonplace to invoke more sophisticated measures of attraction of words to a construction (see Chap. 7 for more information on collostructional analysis). Typically, these measures take into account frequencies in the whole corpus. The relevant statistic that we rely on for this more nuanced investigation into the preferences for the nouns (collexemes) in each of these constructions is called collostructional strength. Put simply, collostructional strength is a measure of the overuse or underuse of a word in a construction in light of expected frequencies. The higher the values, the greater the attraction of the collexeme to the construction.

Looking at the top 20 collexemes of EXPERIENCE N in Table

(continued) This case study illustrates ways to refine search results from a corpus study using the notions of precision and recall. The focus of the investigation is what

Part of Granger's study is concerned with passives in a native-speaker corpus, the Louvain Corpus of Native English Essays (LOCNESS), consisting of samples of American university students' essays

(9) a Children in orphanages were not taken from their families, but voluntarily placed there by parents who couldn't adequately care for them. b The white students are expected to be reverent and respectful. c The reader is lead to believe that he is hiding something.

In (9a), there is ellipsis of were before the past participle placed in the second conjunct and since the passive tag attaches only to the be auxiliary in this tagging system, the passive structure in the second conjunct is not identified automatically.

Critical Assessment and Future Directions

While annotation facilitates linguistic research and enables more immediate access to certain kinds of patterns in a corpus, one should acknowledge the potential for valuable linguistic research to be carried out even on unannotated corpora (cf. Chap. 8). Particularly if a researcher is oriented towards certain "bottom-up" approaches to language analysis (as in some kinds of corpus-driven linguistics; cf.

Looking ahead to the future, we see two areas that present particular challenges, reflecting newer trends in the field of linguistics. One of these trends is the rise of interest in the multimodality of language, combining audio, visual, and textual components of communication, as presenting particularly exciting new challenges when it comes to annotating corpora consisting of such material (see also

Conceivably, researchers might wish to have such corpora annotated for elements such as hand gestures, head movement, gaze, motion, speed of body movements, facial expressions, and bodily stance aligned with audio and textual elements. With the increased attention now given to multimodal analysis of language, establishing agreed upon standards and good practice for creating and working with annotated multimodal corpora will be a priority (cf. the discussion of multimodal annotation in

A second significant challenge for future corpus annotation efforts concerns smaller and endangered languages. Over the past two decades, researchers in corpus and computational linguistics have made efforts to include a wider range of the world's languages in their scope, viewing this as one of the standing challenges for these fields in the twenty-first century

Tools and Resources

1. TagAnt

2. TreeTagger

3. Natural language processing pipelines provide one means of applying separate annotation tasks to texts in user-defined sequences (e.g., applying tokenization, then part-of-speech tagging, then syntactic parsing to unannotated materials), which can greatly assist in constructing annotated corpora of all sizes. Many such frameworks exist today, including Apache OpenNLP (

5. Tools for Corpus Linguistics provides an up-to-date list of software packages for corpus annotation and analysis, and includes information about their pricing and the operating systems that they support.

Accessed 25 May 2019.

Introduction

The architecture chosen for a certain corpus refers to the conceptual division of different types of objects contained in a corpus, such as texts, annotations and metadata. As presented in Chaps. 1 and 2, annotations and metadata are sometimes relatively simple: markup can be added to set them apart from the text and control their inventories. However once annotations become more complex they often form a hierarchical architecture of added information which can be represented in multiple ways, with varying consequences. This chapter presents some of the key 50 A. Zeldes characteristics distinguishing different corpus architectures. The focus is on abstract data models and the ways in which they are realized in concrete formats for corpus representation, as well as consequences for the usability of the resulting corpora.

Fundamentals

The fundamentals of corpus architecture begin with conceptualizing corpora as collections of documents, possibly arranged in subcorpora, and often carrying metadata. Within each document, we must consider how primary data is represented, such as textual data, transcribed dialogue with or without multiple overlapping speakers, and aligned multimodal data. For example, although spoken language is considered 'primary' in many senses, corpus architectures usually treat aligned audio/video information (A/V for short) as a type of annotation, and this can have consequences for corpus architecture. As we will explore below, cases breaking the idea of text as a sequence of adjacent tokens, such as overlapping data and multiple or conflicting tokenization, can arise, which will have complex effects and different possible solutions (see

Corpus Macro-structure

If a corpus is "a collection of pieces of language that are selected and ordered according to explicit linguistic criteria in order to be used as a sample of the language",

The term virtual subcorpus is also used sometimes to refer to querying subsets of earlier queries, i.e. one can dynamically design a 'subcorpus' containing all documents matching an arbitrary query (e.g. the subcorpus of documents containing the word 'snow'), and work further with these documents (see

A. Zeldes While the structures in Fig.

Primary Data and Text Representation

As collections of textual data (in the broad sense, whether written, or transcribed from speech, see Wichmann 2008, or even multimodal corpora of sign language utterances, see

(2) Mark/NNP agreed/VBD. /SENT This/DT was/VBD,/, then/RB,/, the/DT end/NN ./SENT But/RB I/PRP can/MD not/RB accept/VB it/PRP./SENT For many linguistic research questions, the representation in (2) is adequate, for example for vocabulary studies: one can extract type/token ratios to study vocabulary size in different texts, find vocabulary preferences of certain authors, etc.

However for many other purposes, the loss of information about the original text from (

• Tokens with ambiguous spacing: both 'can not' and 'cannot' are usually tokenized as two units, but to study variation between these forms, one needs to represent whitespace somehow.

• Training automatic sentence/document/subsection splitters: Position and number of spaces, as well as presence of tab characters are very strong cues for such programs. For example TextTiling, a classic approach to automatic document segmentation, makes use of tabs as predictors

Whitespace and other features of the original primary data can therefore be important, and some corpus architectures employ formats which preserve and separate the underlying data from processes of tokenization and annotation, often using 'stand-off' XML formats. In stand-off formats, different layers of information are stored in separate files using a referencing mechanism which allows us, for example, to leave an original text file unchanged. One can then add e.g. POS annotations in a separate file specifying the character offsets in the text file at which the annotations apply (e.g. marking that a NOUN occurred between characters 4-10; see 'Tools and Resources' for more details).

A second important issue in representing language data is the tokenization itself, which requires detailed guidelines, and is usually executed automatically, possibly with manual correction (see

(3) I'll do it (4) I won't do it then In order to make all instances of the lexical item will findable, some corpora rely on lemmatization (the lemma of all of these is will), while other corpora use explicit normalization. This distinction becomes more crucial in corpora with non-standard orthography, as in example (5), featuring the contraction I'm a (frequent in, but not limited to African American Vernacular English,

(5) I'm a do it (i.e. I'm going to do it)

This last example clearly shows that space-delimited orthographic borders, tokenization, and annotations at the word form level may not coincide. To do justice to examples such as (5), a corpus architecture must be capable of mapping word forms and annotations to any number of tokens, in the sense of minimal units. In some cases these tokens may even be empty, as in the position following a in the 'tok' layer for (

The example shows several issues: towards the end, two speakers overlap with word forms that only partially occur at the same time, meaning that borders are needed corresponding to these offsets; in the middle of the excerpt, there is a moment of silence, which has a certain duration; and finally, there is an extra linguistic event (a phone ringing) which takes place in part during speaker A's dialogue, and in part during the silence between speech acts.

An architecture using the necessary minimal units can still represent even this degree of complexity, provided that one draws the correct borders at the minimal transitions between events, and add higher level spans for each layer of information. In cases like these, the concept of minimal token is essentially tantamount to timeline indices, and if these have explicit references to time (as in the seconds and milliseconds in the 'time' layer of Fig.

Data Models for Document Annotations

The central concern of annotations is 'adding interpretative, linguistic information to an electronic corpus'

At its most general formulation, a graph is just a collection of nodes connected by edges: for example an ordered sequence of words, each word connected to the next, with some added nodes connected to multiple words (e.g. a sentence node grouping some words, or smaller phrase nodes). Often these nodes and edges will be annotated with labels, which usually have a category name and a value (e.g. POS=NOUN); in some complex architectures, annotations can potentially include more complex data types, such as hierarchical feature structures, in which annotations can contain not only simple values, but also further nested annotations (see ISO 24612 for a standardized representation for such structures in corpus markup). Additionally, some data models add grouping mechanisms to annotation graphs, often referred to as 'annotation layers', which can be used to lump together annotations that are somehow related.

Basic annotation graphs, such as syntactically annotated treebanks, can be described in simple inline formats. However, as the corpus architecture grows more complex or 'multilayered', the pressure to separate annotations into different files and/or more complex formats grows. To see why, one can consider the Penn Treebank's

This syntax tree defines a hierarchically nested annotation graph, with nodes corresponding to the tokens and bracketing nodes, and annotations corresponding to parts of speech and syntactic category labels

Because of the complexity inherent in annotation graphs, complex tools are often needed to annotate and represent multilayer data, and the choice of search and visualization tools with corresponding support becomes more limited (see Tools and Resources). In the case of formats for data representation, the situation is somewhat less critical, since, as already noted, different types of information can be saved in separate files. This also extends into the choice of annotation tools, as one can use separate tools, for example to annotate syntax trees, typographical properties of source documents, or discourse annotations. The greater challenge begins once these representations need to be merged. This is often only possible if tools ensuring consistency across layers are developed (e.g. the underlying text, and perhaps also tokenization must be kept consistent across tools and formats).

As a merged representation for complex architectures, stand-off XML formats are often used (see

Representative Corpus 1

The GUM corpus

The Georgetown University Multilayer corpus (GUM, Zeldes 2017), is a freely available corpus of English Web genres, created using 'classsourcing' as part of the Linguistics curriculum at Georgetown University. The corpus, which is expanded every year and currently contains over 129,000 tokens, is collected from eight open access sources: Wikinews news reports, biographies, fiction, reddit forum discussions, Wikimedia interviews, wikiHow how-to guides and Wikivoyage travel guides. Its architecture can therefore be considered to follow the common tree-style macro-structure with eight subcorpora, each containing simple, unaligned documents. The complexity of the corpus architecture results from its annotations: as the data is collected, student annotators iteratively apply a large number of annotation schemes to their data using different formats and tools, including document structure in TEI XML, POS tagging, syntactic parsing, entity and coreference annotations and discourse parses in Rhetorical Structure Theory. The complete corpus covers over 50 annotation types (see

For example, there is a dependency edge connecting the two tokens and carrying a label (func = nsubj, since I is the nominal subject of know), belonging to a layer 'dependencies'. The single node in the layer 'sentence types' above the tokens is annotated as s_type = decl (declarative sentence), and is attached to both tokens, but the edges attaching it are unannotated (no labels). Finally, some layers, such as 'constituent syntax', contain a complex subgraph: an NP node is attached to the token I, and a VP node is attached to know, and together they attach to the S node denoting the clause. Similarly, the 'discourse' layer, of which we only see one incoming edge, is the entry point into the discourse annotation part of the graph, which places multiple tokens in segments, and then constructs a sub-graph made of sentences and clauses based on Rhetorical Structure Theory (RST,

Note that it is the corpus designer's decision which elements are grouped in a layer. For example, the constituent S representing the clause has a similar meaning to the sentence annotation in the 'sentence types' layer, but these have been modeled as separate. As a result, it is at least technically possible for this corpus to have conflicting constituent trees and sentence span borders for sentence type annotation. If these layers are generated by separate automatic or manual annotation tools, then such conflicts are in fact likely to occur over the course of the corpus. Similarly, a speaker annotation ('sp_who') is attached to both tokens, as is the sentence annotation, but it is conceivable that these may conflict hierarchically: a single sentence annotation may theoretically cover tokens belonging to different speakers, which may or may not be desirable (e.g. for annotating one speaker completing another's sentence). The graph-based data model allows for completely independent annotation layers, united only by joint reference to the same primary text.

A. Zeldes

Representative Corpus 2

MERLIN

The MERLIN project (Multilingual Platform for European Reference Levels: Interlanguage Exploration in Context,

To see how the MERLIN corpora take advantage of their architecture in order to expose learner data across levels we must first consider how users may want to access the data, and what the nature of the underlying primary textual data is. On one level, researchers, language instructors and other users would like to be able to search through learner data directly: the base text is, trivially, whatever a learner may have written. However, at the same time the problems discussed in Sect. 3.2.2 make searching through non-native data, which potentially contains many errors,

(continued)

This data is invaluable to learners and educators interested in article errors. However users interested in finding all usages of da in the L2 data will not be able to distinguish correct cases of da from cases that should have dal or dalla. At the same time, less obvious errors may render some word forms virtually unfindable. For example, the word pomeriggio 'afternoon' is misspelled in this example, and should read pomerrigio (the 'r' should be double, the 'g' should not be). As a result, users who cannot guess the actual spelling of words they are interested in will not be able to find such cases.

In order to address this, MERLIN includes layers of target hypotheses (TH, see

A. Zeldes

One consequence of using a TH layer for the architecture of the corpus is that the data may now in effect have two conflicting tokenizations: on the 'learner' layer, the first '?' and the second 'Da' stand at adjacent token positions; on the TH1 layer, they do not. To make it possible to find '?' followed by 'Da' in this instance, while ignoring or including TH layer gaps, MERLIN's architecture explicitly flags these annotation layers as 'segmentations', allowing a search engine to use either one for the purpose of determining adjacency as well as context display size (i.e. what to show when users request a windows of +/-5 units).

One shortcoming of TH annotations is that they cannot generalize over common error types which are of interest to users: for example, they do not directly encode a concept of 'article errors'. To remedy this, MERLIN includes a wide range of error annotations, with a major error-annotation category layer (EA_category, e.g. G_Morphol_Wrong for morphological errors), and more fine grained layers, such as G_Morphol_Wrong_type. The latter indicates a 'gender' error on prossima 'next (feminine)' in Fig.

Critical Assessment and Future Directions

At the time of writing, corpus practitioners are in the happy position of having a wide range of choices for concrete corpus representation formats and tools. However, few tools or formats can do 'everything', and more often than not, the closer they get to this ideal, the less convenient or optimized they are for any one task. To recap some important considerations in choosing a corpus architecture and a corresponding concrete representation format:

• Is preservation of the exact underlying text (e.g. whitespace preservation) important? • Are annotations very numerous or involve conflicting spans to the extent that a stand-off format is needed? • Are annotations arranged in mutually exclusive spans? Are they hierarchically nested? Are discontinuous annotations required? • Are complex metadata management and subcorpus structure needed, or can this information be saved separately in a simple table? • Does the data contain A/V signals? If so, are there overlapping speakers in dialogue? • Is parallel alignment needed, i.e. a parallel corpus?

A. Zeldes These questions are important to address, but the answers are not always straightforward. For example, one can represent 'discontinuous' annotations slightly less faithfully by making two annotations with some co-indexed naming mechanism (cf. MOVS and MOVT in Representative Corpus 2). This may be unfaithful to our envisioned data model, but will greatly broaden the range of tools that can be used.

In practice, a large part of the choice of corpus architecture is often dictated by the annotation tools that researchers wish to use, and the properties of their representation formats. Using a more convenient tool and compromising the data model can be the right decision if this compromise does not hurt our ability to approach our research questions or applications. For example, many spoken corpora containing dialogue do not model speaker overlap, instead opting to place overlapping utterances in the order in which they begin. This can be fine for some research questions, for example for a study on word formation in spoken language; but not for others, e.g. for pragmatic studies of speech act interactions in dialogue. Table

CoNLLU is a popular format in the family of tab-delimited CoNLL formats, which is used for dependency treebanks in the Universal Dependencies project (

A. Zeldes Penn Treebank bracketing (PTB), TigerXML and tiger2 are formats specializing in syntax annotation (treebanks). The PTB format is the most popular way of representing projective constituent trees (no crossing edges) with single node annotations (part of speech or syntactic category). It is highly efficient and readable, but has some limitations (see the 'crocidolite' example above). TigerXML is a more expressive XML format, capable of representing multiple node annotations, crossing edges, edge labels and two distinct types of edges. The tiger2 format

TCF

An important trend in corpus building tools looking forward is a move away from saving and exchanging data in local files on annotators' computers or private servers. Corpora are increasingly built using public, version-controlled repositories on platforms such as GitHub. For example, the Universal Dependencies project is managed entirely on GitHub, including publicly available data in multiple languages and the use of GitHub pages and issue trackers for annotation guidelines and discussion. Some tools (e.g. the online XML and spreadsheet editor GitDox, Zhang and Zeldes 2017) are opting for online storage on GitHub and similar platforms as their exclusive file repository. In the future we will hopefully see increasing openness and interoperability between tools which adopt open data models and best practices that allow users to benefit from and re-use existing data and software.

Tools and Resources

An important set of tools influencing the choice of corpus architecture is NLP pipelines and APIs, which allow users to construct automatically tagged and parsed representations with complex data models (and these can be manually corrected if needed). Some examples include Stanford CoreNLP

Finally, corpus architecture considerations also interact with the choice of search and visualization facilities that one intends to use. Having an annotation tool which supports a complex data model may be of little use if the annotated data cannot be accessed and used in sensible ways later on. Some corpus practitioners use scripts, often in Python or R, to evaluate their data, without using a dedicated search engine (see Chap. 9). While this approach is very versatile, it is also labor intensive: for each new type of information, a new script must be written which traverses the corpus in search of some information. It is therefore often desirable to have a search engine that is capable of extracting data based on a simple query. For corpora that are meant to be publically available to non-expert users, this is a necessity. In public projects, a proprietary search engine tailored explicitly for a specific corpus is often programmed, which cannot easily be used for other corpora. Here I therefore focus on generic, freely available tools which can be used for a variety of datasets.

The Corpus Workbench

Further Reading

Introduction

The use of corpora and corpus-based tools allows for increased efficiency in the identification of linguistic features that are characteristic of a language variety. A fundamental statistic in assessing the saliency of any linguistic feature is frequency of occurrence, or, simply, the number of times a feature of interest occurs in a data set.

Probably the most widely known frequency lists are lists of frequently occurring lexical items, such as

But the use of frequency lists extends well beyond pedagogically oriented applications, helping to address a wide variety of language-related research questions across numerous fields of inquiry.

Fundamentals

Zipf's Law

A key to understanding both the efficacy and limitations of frequency lists is Zipf's law

This phenomenon is perhaps best visualized via a log-log plotting of rank vs. frequency such as in Fig.

In practical terms, the distribution identified in Zipf's law has two important implications:

1. A relatively small set of words accounts for a large proportion of tokens in a text (or corpus). For example, the 100 most frequently occurring words in the BNC account for approximately 45% coverage of the corpus

Since the vast majority of words in a language are highly infrequent, very large corpora are required to capture and understand the distribution and behaviour of rare lexical features.

D. Miller

Unit of Analysis

As noted above, perhaps the most widely known frequency lists are lists of individual words. Even at this level, some basic decisions have to be made in the operationalization of the construct of a "word." In languages using the Roman alphabet, for example, it may seem obvious that a word can be identified as one or more letters surrounded by spaces. However, this is not so simple. In English, for instance, decisions must be made regarding whether to treat compounds, either (a) open (e.g., school bus), closed (e.g., weekend), or hyphenated (e.g., well-being), (b) transparent (e.g. school bus) or opaque (e.g., deadline), and (c) as one word or two words. This can be especially tricky, as there is often some variation in how these items appear across a corpus (or even within a text); for example, although the hyphenated form, well-being, is clearly the most common option found in the Corpus of Contemporary American English (COCA) (>8000 occurrences), the forms well being and wellbeing each occur several hundred times. Researchers also need to account for different spellings across different varieties of a language (e.g., behaviour vs. behavior) or even commonly misspelled words (e.g., mispelled). Whatever decisions are made, they should be principled, consistent, and clearly explained (cf.

In profiling lexical frequencies, researchers also have to decide which related forms should be counted as a single lexical item. Operationalizations of the 'word' construct can range from treating all orthographic word forms as distinct words (e.g., book and books are two separate words), to treating all word forms sharing a classical root as members of a word family

Yet another robust research paradigm has widened the concept of "lexical item" beyond single words to include multi-word units (MWUs), including collocations, "phrasal verbs (call on, chew out), idioms (rock the boat), fixed phrases (excuse me), and prefabs (the point is)"

A great deal of research in this paradigm has focused on identifying and analyzing prefabricated 'chunks' of formulaic language (e.g., if you look at)

In the past decade, increasing attention has been given to non-contiguous chunks of frequently co-occurring language. These "discontinuous sequences in which words form a 'frame' surrounding a variable slot (e.g. I don't * to, it is * to)"

Beyond Raw Frequency

As noted above, frequency of occurrence can be used in understanding the relative salience of linguistic features in a text or discourse domain. It can also be used for comparison across texts or discourse domains. For example, Table

The frequency counts in Table

Normalising Frequency Counts

As can be seen from the ranked lists in Table

Normalising frequencies is simply a matter of dividing a raw frequency by the total number of words in a text (or corpus) and, optionally, multiplying the result by a meaningful common denominator that is somewhat comparable to the length of a corpus (or texts within a corpus). Thus, as can be seen in Table

Range and Dispersion

In practice, frequency alone-even normalized frequency-is of limited use for understanding the salience of a feature throughout a discourse domain. This point can be illustrated by considering two different content words, each occurring 42 times (per 100,000 words) in the corpus of Donald Trump's campaign speeches: military (n.), and Virginia (n.). While frequency appears to suggest some sort of parity in salience, these occurrences had very different distributions throughout the corpus. Closer analysis reveals that the word military occurred in 80% of his speeches (24 of 30), suggesting it was one of Donald Trump's frequent talking points throughout the campaign, whereas the word Virginia occurred in just 7 speeches, illustrating its use was limited to a much narrower set of contextsperhaps to speeches given in this state. If the goal is to identify Donald Trump's pet talking points throughout his campaign, it should be clear that frequency alone would be insufficient.

For this reason, researchers typically include measures of dispersion in order to better understand the distributional characteristics of a target feature. Measures of dispersion provide a more comprehensive picture of frequency distributions, allowing researchers to determine whether features are generally representative of a target discourse domain or, alternatively, limited to certain contexts or idiosyncrasies of certain language users. The simplest measure of dispersion is range, and it is typically operationalized in terms of the number of texts and/or sections in which a feature occurs. Many frequency lists have been designed based on a combination of frequency and range criteria. For example, the words included in

Though range may be the mostly commonly employed measure of dispersion in frequency list design, contemporary researchers are increasingly employing measures of dispersion that take into account how evenly a feature occurs throughout a target discourse domain -not simply the number of texts or sections of a corpus that a feature occurs in, but whether the occurrences are evenly distributed or particularly frequent or infrequent in any of these sections (see Chap. 5 for further discussion). For example, the words in

The variety of measures and criteria evidenced in these examples illustrate an important point: there are no hard and fast rules regarding the best frequency or dispersion criteria for constructing frequency lists. Rather, the criteria used have often been somewhat arbitrary, reflecting researcher's intuitions about what seems reasonable. Often times, this has meant that researchers simply employ criteria used in previous research. Alternatively, they might experiment with different criteria, observing resulting lists to gauge whether the criteria used lead to lists with desired characteristics (see Representative Study 2). Gardner and Davies' study exemplifies a contemporary approach to constructing and validating a frequency list of academic vocabulary-one based on lemmas rather than word families-by taking advantage of technological advances (e.g., part of speech tagging; the ability to compile substantially larger corpora) and the application of statistics to building frequency lists.

Representative

(continued)

D. Miller

The list that Davies and Gardner designed, the 3000-word Academic Vocabulary List (AVL), was culled from a 120+ million word subcorpus of written academic English from the Corpus of Contemporary English (COCA). According to Gardner and Davies, the AVL improves upon previous efforts in a number of key ways. First, the AVL is based on a contemporary, balanced corpus that is considerably larger than the 3.5 million word corpus from which its most popular predecessor, the AWL

Secondly, the AVL is a list of lemmas rather than word families. Gardner and Davies argue that using the lemma as the unit of analysis will allow list users to more accurately target the most frequently occurring forms and meaning senses of academic vocabulary.

Third, the authors included a more comprehensive set of statistical measures related to ratio and dispersion than were employed in previous efforts for identifying academic vocabulary (esp. the AWL). Specifically, words in the AVL had to meet the following criteria:

1. Ratio: Selected words had to occur at a rate 50% higher (i.e., at 1.5 the 'expected' rate of occurrence) in their academic corpus than in a nonacademic corpus (the rest of COCA). 2. Range: Selected words had to occur with more than 20% of the expected frequency (cf. Chap. 20) in at least 7 of the 9 disciplines in their academic corpus 3. Dispersion: In order to demonstrate evenness of distribution, selected words had to achieve a Juilland's D of at least .80. 4. Discipline measure: Selected words could not occur at more than 3 times the expected frequency in any one discipline, in order to avoid including discipline-specific vocabulary Gardner and Davies arrived at each cut-off rate via experimentation, as no guidance was available in previous frequency list research. For example, the 1.5 ratio rate was chosen because the researchers determined that lower ratios allowed too many general words (e.g. different, large), while higher ratios disallowed many words that intuitively belong in a high-frequency academic word list (e.g., require, create).

(continued)

In addition to the methodology used for corpus design and list extraction, two coverage-based analyses were used to provide evidence of the AVL's validity. First, to demonstrate the specialized nature of the list, Gardner and Davies compared the coverage of the AVL in two academic corpora to that in two non-academic corpora. Results of this analysis demonstrate that the list performs considerably better coverage-wise in academic writing than in other written genres, in both in the academic portion of the corpus from which the list was constructed (13.8%), and a different academic sub-corpus from the BNC (13.7%) (compared to 8.0% for newspaper articles and 3.4% for fiction).

In the second analysis, Gardner and Davies compared the performance of the AVL to that of the AWL in order to demonstrate that it is indeed a more robust list. Because the AWL is a list of 570 word families, they built word families based on the most frequent 570 lemmas in the AVL for this comparison. The resulting 570 word families based on the AVL indeed provided considerably better coverage than the AWL in the two academic corpora-in fact, almost twice the coverage: nearly 14% by the AVL compared to approximately 7% by the AWL.

The researchers acknowledge a key concern that may be raised about the fairness of the AVL vs. AWL coverage-based comparison in this second analysis. Many specialized lists that have been designed over the past several decades have relied on

Nevertheless, because Gardner and Davies used a different methodologyspecifically, not using the GSL as a stoplist-their AVL includes many high frequency items that are also found on the GSL and so were not included in the AWL. While this does not detract from the efficacy of the AVL, it does limit the strength of conclusions that can be drawn from this coverage-based comparison. In this study, Brezina and Gablasova detail their development of an updated general service list, the new-GSL. Of particular note in their study is an additional step that they employ for addressing reliability: comparing frequency list items across different corpora. An important difference between their methodology and

Representative

(1) it is beginners, i.e., learners without wide derivational morphological knowledge, who are the most likely users of a general service list; (2) using lemmas-rather than a more inclusive category like word family-can lessen the number of different word senses contained in a single lexical unit.

Another key difference is that they selected words for their list by using distributional criteria only: words were ordered according to a composite score based on frequency and range, i.e. Average Reduced Frequency

To address and better understand the issue of diachronic change in frequency profiles, they extracted and compared lists from the four corpora. Differences among the lists were found primarily in content words, often reflecting changes in technology. For example, the BE06 includes a number of words that its earlier counterpart, the LOB, designed 40 years earlier, did not, e.g.,

4 Analysing Frequency Lists 89 Notable differences were also found between the BE06 and EnTenTen12 corpora, despite their being so close in age. The list from the EnTenTen12 included a higher proportion of words related to the internet and technology

The authors then compared the frequency lists culled from each corpus to identify an overlapping, "stable" core of general service words-a list not demonstrating bias toward any of the four corpora, and thus exhibiting stability across time. The four lists had 78 to 84 percent overlap, with correlations between ranks at r s = .762 to r s = .870, all p < .001. 2116 lemmas were shared by each of the four lists, thus, according to the authors, constituting a stable core. In order to further modernize their new-GSL, they also added 378 lemmas which were identified in the two recent corpora, the BE06 and the EnTenTen12, completing the final, 2494-word list.

Finally, Brezina and Gablasova compared the coverage of their new-GSL with that of

Critical Assessment and Future Directions

Three critical issues require attention as frequency list research goes forward: (1) dealing with homoforms and multi-word units, (2) the application of dispersion (and other) statistics to frequency list creation, and (3) addressing reliability/stability in validation of frequency lists.

Dealing with Homoforms and Multi-word Units

Two critical obstacles that need to be addressed in frequency list research is the handling of homoforms (e.g., river bank vs. investment bank vs. bank as a verb) and multi-word units (e.g., I didn't care for the movie at all vs. The product is D. Miller carried at all locations.)

Identification of these units is the first complex hurdle. Take for example a word from Nation's first BNC 1000 list, course. Among its many meanings are (1) a certain path or direction (e.g., a circuitous course); (2) an academic class or program (e.g., a biology course); (3) a part of a meal (e.g., first course, second course . . . ); (4) to move (e.g., blood coursing through the veins). As

Such differentiation would no doubt have some effect on frequency lists.

The task of identifying and sorting homoforms and multi-word units in corpora and incorporating them into frequency lists is clearly a significant one, and this is only part of the challenge. If we then want to use these lists to profile lexis in other texts or corpora, the same procedure is necessary for each new text. Such disambiguation remains both a theoretical and practical challenge for corpus-based frequency list research.

Application of Dispersion (and other) Statistics

As discussed in Sect. 4.2.3.2, contemporary operationalizations of lexical item salience typically include some measure of dispersion. The most frequently used measure in contemporary frequency list research is Juilland's D (e.g.

Further investigating the limitations raised by

Addressing Reliability in the Validation of Frequency Lists

Over the past two decades-and as exemplified in the two representative studies in this chapter-it would appear that the greatest efforts in designing and validating frequency lists have gone into three areas: corpus design, item selection criteria, and-in the case of word lists-coverage-based demonstrations of list robustness.

Corpora are now often much larger and better balanced and, as a result, perhaps more representative than ever before. The application of additional distributional statistics allows for better targeting of items with desired distributions (e.g.,

D. Miller

In the midst of these important developments, one issue that deserves more attention is frequency list generalizability. That is, to what degree are corpus-based frequency lists generalizable to target discourse domains?

At present, evidence of frequency list generalizability tends to come in one or two forms, both of which are indirect. The first comprises primarily corpus-external evidence, focused on corpus design (i.e., Do samples represent the diversity of text types, topics, etc. in a proportion reflecting the target language use domain?).

In word list research, a second form of evidence of generalizability is also typically employed: post-hoc assessment of a list's coverage in other, similarly purposed corpora. As discussed above,

Very rarely do studies include a direct assessment of list generalizability -of the extent to which items on a frequency list produced from one corpus overlap with items on a list extracted from a similarly purposed corpus made up of different texts. Manuals on lexical frequency list research, however, do recommend such comparisons.

For this particular reason, Nation (2016) recommends checking lists "against competing lists not just for coverage but also for overlapping and non-overlapping words" (p. 132), as was done in

More research is needed to better understand the effects of different methodologies employed on the ability to capture reliable, generalizable frequency lists. Current practice has been to design new lists by adopting previously used methodologies or to update lists by manipulating several variables (e.g., compiling larger, more principally balanced corpora; changing the unit of measurement; employing different measures of dispersion). But is it unclear whether newly employed methodologies are leading to (more) reliable lists, and, if so, which (combination of) methodological adjustments account for seeming improvements. Future research should continue to investigate how different variables such as frequency profiles of target features, selection criteria used, or corpus design may affect the reliability of lists. What combination of selection criteria ensure the greatest list reliability

Tools and Resources

This section highlights three tools that are highly useful for lexical frequency profiling and the construction of frequency lists, including single words and ngrams. AntConc is very easy-to-use yet powerful freeware which allows users to quickly construct frequency lists of word forms, lemmas, or n-grams based on their own uploaded texts. A nice feature is that AntConc allows users to build frequency lists based on lemmas, by using lists of lemmas provided in the software for English, French, or Spanish or by uploading their own. AntWordProfiler is an update of

AntConc

Further Reading

Nation, I.S.P. 2016. Making and using word lists for language learning and testing. John Benjamins, Amsterdam.

This text provides a comprehensive, practical, and very accessible discussion of important issues in frequency list design and evaluation. Nation provides useful recommendations for researchers through all steps in frequency list development, from designing (or choosing) a corpus to choosing an appropriate unit of analysis (including dealing with homoforms and multi-word units), to determining criteria for word selection and ordering. He also provides a helpful list of questions that can guide the analysis of pedagogically oriented frequency lists and walks readers through specific examples of evaluations via reflections on the merits and shortcomings of the BNC lists he designed.

Baayen, H. 2001. Word frequency distributions. Kluwer Academic, New York.

Whereas

Introduction

Imagine a corpus linguist looking at a frequency list of the Brown corpus, a corpus aiming to be representative of written American English of the 1960s that consists of 500 samples, or parts, of approximately 2000 words each. Imagine further that corpus linguist is looking at that list to identify verbs and adjectives within a certain frequency range -maybe because he needs to (i) create stimuli for a psycholinguistic experiment that control for word frequency, (ii) identify words from a certain 100 S. Th. Gries frequency range to test learners' vocabulary, or (iii) compile a vocabulary list for learners, or some other application. Imagine, finally, the frequency range he is currently interested in is between 35 and 40 words per million words and, as he browses the frequency list for good words to use, he comes across an adjective and a verbenormous and staining -that he thinks he can use because they both occur 37 times in the Brown corpus (and are even equally long) so he notes them down for later use and goes on. This is not an uncommon scenario and yet it is extremely problematic because, while that corpus linguist has indeed found words with the same frequency, he has probably not even come close to do what he actually wanted to do. The frequency range of the words he was interested in -35-40 -or the actual frequency of the two words discussed -37 -may have been an operationalization for things that might have to do with how fast people can identify the word in a psycholinguistic experiment (as in a lexical decision task) or with how likely a learner would be to have encountered, and thus hopefully know, a word of that kind of rarity. However, chances are that this choice of words is highly problematic: While both words are equally long and equally frequent in one and the same corpus, they could hardly be more different with regard to the topic of this chapter, their dispersion, which probably makes them useless for the above-mentioned hypothetical purposes, controlled experimentation, vocabulary testing, or vocabulary lists. This is because

• the word enormous occurs 37 times in the corpus, namely once in 35 corpus parts and twice in 1 corpus part; • the word staining occurs 37 times in the corpus, namely 37 times in 1 corpus part.

In other words, given its (relatively low) frequency, enormous is pretty much as evenly dispersed as a word with that frequency can possibly be while, given its identical frequency, staining is as unevenly dispersed as a word with that frequency can possibly be: enormous is characterized by even dispersion, staining is characterized by a most uneven dispersion, clumpiness, or, to use

Fundamentals

An Overview of Measures of Dispersion

Corpus linguistics is an inherently distributional discipline: Virtually all corpuslinguistic studies with at least the slightest bit of a quantitative angle involve the frequency or frequencies with which • an element x occurs in a corpus or in a part of a corpus representing a register or variety or something else, . . . or • an element x occurs in close proximity (however defined) to an element y in a corpus (or in a part of a corpus).

Also, any kind of more advanced corpus statistic -for instance, association measures (see Chap. 7) or key words statistics (see Chap. 6) is ultimately based on the observation of, and computations based upon, such frequencies. However, just like trying to summarize the distribution of any numeric variable using only a mean can be treacherous (especially when the numeric variable is not normally distributed), so is trying to summarize the overall 'behavior' (or the co-occurrence preferences or the keyness) of a word x on the basis of just its frequency/frequencies because, as exemplified above, words with identical frequencies can exhibit very different distributional behaviors.

On some level, this fact has been known for a long time.

To discuss how these statistics and some other competing ones are computed, I am following the expository strategy of

• the words b and e are equally frequent in each corpus part (two times and one time per corpus part respectively), which means that their dispersion measures should reflect those even distributions;

• the words i, q, and x are attested in one corpus part each: i in the first corpus part (which has 9 elements), q in the second corpus part (which has 10 elements), and x in the third corpus part (which has 11 elements), which means these words are extremely clumpily distributed, but slightly differently so (because the corpus parts they are in differ in size); • the word a, whose dispersion we will explore below and which is highlighted in bold, is attested in each corpus part, but with different frequencies.

To compute the measures of dispersion to be discussed here, a few definitions are in order; we will focus on the word a:

(1) l = 50 (the length of the corpus in words) (2) n = 5 (the length of the corpus in parts) (3) s = (0.18, 0.2, 0.2, 0.2, 0.22) (the percentages of the n corpus part sizes) (4) f = 15 (the overall frequency of a in the corpus) (5) v = (1, 2,

The most important dispersion measures -because of their historical value and evaluation studies discussed below -are computed as discussed in what follows; see

(7) range: number of parts containing a = 5 Then, there are two traditional descriptive statistics, the standard deviation of the frequencies of the element in question in all corpus parts (sd, see (

A maybe more useful variant of this measure is its 'normalized version, the variation coefficient (vc, see (

The version of Juilland's D that can handle differently large corpus parts is then computed as shown in

Carroll's D 2 is essentially a normalized version of entropy of the proportions of the element in each corpus part, as shown in (11) (see also

(11) Carroll's D 2 :

The version of Rosengren's S that can handle differently large corpus parts is shown in

(12)

Finally,

The final measure to be discussed here is one that, as far as I can tell, has never been proposed as a measure of dispersion, but seems to me to be ideally suited to be one, namely the Kullback-Leibler (or KL-) divergence, a non-symmetric measure that quantifies how different one probability distribution (e.g., the distribution of all the occurrences of a across all corpus parts, i.e. v / f ) is from another (e.g., the S. Th. Gries corpus part sizes s); the KL-divergence is computed as shown in (

137 with log 2 0 : = 0 Table

In sum, corpus linguists have proposed quite a few different measures of dispersion, most of which are generally correlated with each other, but that also react differently to the kinds of distributions one finds in corpus data, specifically,

• the (potentially large) number of corpus parts in which an element is not attested; • the (potentially large) number of corpus parts in which an element is attested much less often than the mean; • the range of distributions a corpus linguist would consider to be different but that would yield the same dispersion measure(s); • the number of different corpus parts a corpus linguist would assume and their (even or uneven sizes).

Areas of Application and Validation

There are at least a few areas where dispersion information is now considered at least occasionally, though much too infrequently. The area of research/application where dispersion has gained most ground is that of corpus-based dictionaries and vocabulary lists.

It is worth pointing out in this connection that, especially in this domain of dictionaries/vocabulary lists, researchers have often also computed what is called an adjusted frequency, i.e. a frequency that is adjusted downwards depending on the clumpiness/unevenness of the distribution. In the mathematically simplest case, the adjusted frequency is the observed frequency of the word in the corpus times the dispersion value; for instance, Juilland's usage coefficient U is just that: the frequency of the word in the corpus f times Juilland's D, a measure that, for instance,

Another area where dispersion information has at least occasionally been recognized as important is psycholinguistics, in particular the domain of lexical decision tasks. Consider, for instance,

Given a certain number of exposures to a stimulus, or a certain amount of training, learning is always better when exposures or training trials are distributed over several sessions than when they are massed into one session. This finding is extremely robust in many domains of human cognition.

S. Th. Gries

More empirically, there are some studies providing supporting evidence for the role of dispersion when it comes to lexical decision tasks. One such study is in fact

• they only use the crudest measure of dispersion possible (range) and do not relate to previous more psychological/psycholinguistic work that also studied the role of range (such as Ellis 2002a, b); • they do not establish any relation to the notion of dispersion in corpus linguistics and, somewhat worse even, refer to range with the misleading label contextual diversity, when in fact the use of a word in different corpus parts by no means implies that the actual contexts of the word are different: No matter in how many different corpus parts hermetically is used, it will probably nearly always be followed by sealed.

Nonetheless, they do show that dispersion is a better and more unique predictor of word naming and lexical decision times than token frequency and they, like

In spite of all the effort that has apparently gone into developing measures of dispersion and in spite of uneven dispersion posing a serious threat to the validity of virtually all corpus-based statistics, it is probably fair to say that dispersion is still far from being routinely included in both (more) theoretical research and (more) practical applications. One early attempt to study the behavior of these different measures is

Biber et al. then proceed with two case studies. The first one explores D-values of a set of words in the British National Corpus, which, for the purpose of testing what effect the numbers of corpus parts n one assumes, was divided into n = 10, 1000, and 1000 equal-sized parts; crucially, the words explored were words for which theoretical considerations would lead an analysis to expect fairly different D-values, contrasting words such as at, all, or time (which should be distributed fairly evenly) with words such as erm, ah, and urgh (which, given their preponderance in spoken data, should be distributed fairly unevenly). Specifically, they analyzed 153 words in 10 categories emerging from crossing (i) several different word frequency bands and (ii) expected distribution

In this first case study, they find the expected high D-values for higherfrequency words that would be uniformly-distributed or skewed towards writing (i.e. the 90% majority of the BNC) regardless of n. However, they also discover that the D-values for lower-frequency writing-skewed words are quite sensitive to variations of n. Their concern that these results are not due to the larger sampling sizes reflecting the dispersions more accurately is supported by what they find for the speech-skewed words, namely "extremely large discrepancies even for the most frequent speech-skewed words" (p. 450). More precisely, D-values for high-frequency speech-skewed words can vary between very high (e.g. 0.885 for yeah with n = 1000) and very low (e.g.

(continued) 0.286 for yeah with n = 10). Even more worryingly, "[t]hese discrepancies become even more dramatic as [they] consider moderate and lower-frequency words" (p. 452), with differences in D-values frequently exceeding 0.5 just because of varying n, which on a scale from 0 to 1 of course corresponds to what seems to be an unduly large effect. Their main conclusion of the first case study is that "D values based on 1,000 corpus parts completely fail to discriminate among words with uniform versus skewed distributions in naturalistic data" (p. 454).

In their second case study, Biber et al. created different data sets with, therefore, known distributions of target words across different numbers of corpus parts, but the bottom line of this more controlled case study is in fact the same as that of the first. Their maybe most extreme, and thus worrying, result is that the exact same distribution of a target word -a uniform distribution across 10% of a corpus -can result in a D value of 0.0 when the computation is based on a corpus split into 10 parts, versus a D value of 0.905 when the computation is based on a corpus split into 1000 parts. (p. 457)

As a more useful alternative, they propose to use

Theoretically, we would expect more conservative estimates of dispersion based on a large number of corpus parts. For example, it is more likely that a word will occur in 6 out of 10 corpus parts than for that same word to occur in 600 out of 1000 corpus parts. The values for 1-DP seem to reflect this fact, resulting in consistently lower values when computations are based on a large number of corpus parts. In summary, DP is clearly more effective than D at discriminating between uniform versus skewed distributions in a corpus, especially when it is computed based on a large number of corpus-parts.

Representative

As for the first case study, he extracted all word types from the spoken component of the BNC that occur 10 or more times -there are approx. 17,500 such types -and computed all 29 dispersion measures and adjusted frequencies cataloged in the most recent overview article of

The results from both analyses revealed several relatively clear groupings of measures. For instance, the following clusters/components were well established in both the cluster and the principal components analysis:

• Rosengren's S, range, and a measure called Distributional Consistency

In fact, the principal components analysis revealed that just two principal components capture more than 75% of the variance in the 16 dispersion (continued) measures explored: many measures behave quite similarly and fall into several smaller groups. Nevertheless, the results also show that the groups of measures also sometimes behave quite dissimilarly: "different measures of dispersion will yield very different (ranges of) values when applied to actual data"

With regard to the adjusted frequencies, the results are less diverse and, thus, more reassuring. All measures but one behave relatively similarly, which is mostly interesting because it suggests that (i) the differences between the adjusted frequencies are less likely to yield very different results, but also that (ii) the computationally very intensive distance-based measures that have been proposed (see in particular Savický and Hlaváčová 2002 as well as

The second case study in this paper involves correlating dispersion measures and adjusted frequencies with response time latencies from several psycholinguistic studies, specifically with (i) data from young and old speakers from

Gries concludes with some recommendations: Many dispersion measures are relatively similar, but if one is uncertain what measure to trust, it would be useful to compute measures that his cluster/principal component analyses considered relatively different to get a better picture of the diversity in the data; at present and until more data have been studied, it seems as if the computationally more demanding measures may not be worth the effort. Trivially, more analyses (than Lyne's really small study) are needed, in particular of larger data sets and, along the lines of what

5 Analyzing Dispersion 111

Critical Assessment and Future Directions

The previous sections already touched upon some recommendations for future work. It has hopefully become clear that dispersion is as important an issue as it is still neglected or even completely ignored. While every corpus linguist with only the slightest bit of statistical knowledge knows to never present a mean or median without a measure of dispersion, the exact same advice is hardly ever heeded when it comes to frequencies and dispersions in corpus data: There are really only very few studies that report frequency data and dispersion or, just as importantly, report frequencies and association measures and dispersion, although

Second, we need more studies of the type discussed in the representative studies boxes so that we better understand the different measures' behavior in actual but also controlled/designed data. One issue, for instance, has to do with how corpora are divided into how many parts and how this affects dispersion measures (see for example Biber et al.'s 2016 discussion of the role of the denominator in Juilland's D, which features the number of corpus parts). Another is how dispersion measures relate to issues outside of corpus linguistics such as, again, psycholinguisticallyor cognitively-informed approaches. This is particularly relevant for measures that are advertised as having certain characteristics. To discuss just one example,

However, the advantage is just asserted, not demonstrated, and in

That being said, a certain frequent trend in corpus linguistic research should be resisted and this is best explained with a very short excursus on association measures (see Chap. 7), where the issue at hand has been recognized earlier than it has in the little existing dispersion research. For several decades now, corpus linguists have discussed dozens of association measures that are used to rank-order, for instance, collocations by the attraction of their constituent words. Some of these measures are effect sizes in the sense that they do not change if the co-occurrence tables from which they are computed are increased by some factor (e.g., the odds ratio), others are based on significance tests, which means they conflate both sample size/actual 112 S. Th. Gries observed frequencies and effect size (e.g., the probably most widely-used measure, the log-likelihood ratio). This is relevant in the present context of dispersion measures because we are now facing a similar issue in dispersion research, namely when researchers and lexicographers also take two dimensions of information -frequency and the effect size of dispersion -and conflate them into one value such as an adjusted frequency (e.g., by multiplication, see above Juilland's U). To say it quite bluntly, this is a mistake because, frequency and dispersion are two different pieces of information, which means conflating them into a single measure loses a lot of information. This is true even though frequency and dispersion are correlated, as is shown in Fig.

Second, a relatively 'specialized' word like council is in the same (6th) frequency bin (freq = 4386, DP = 0.72, range = 292 out of 905) as intuitively more 'common/widespread' words like nothing, try, and whether (freqs = 4159, 4199, 4490; DPs = 0.28, 0.28, 0.32; ranges = 652, 664, 671 out of 905); in both plots, the positions of council and nothing are indicated with the c and the n respectively plotted into the graph. S. Th. Gries Also, even just in the sixth frequency band, the extreme range values that are observed are 85 / 905 = 9.4% vs. 733 / 905 = 81% of the corpus files, i.e. huge differences between words that in a less careful study that ignores dispersion would simply be considered 'similar in frequency'.

Finally, these graphs also show that forcing frequency and dispersion into one value, e.g. an adjusted frequency, would lose a huge amount of information. This is obvious from the visual scatter in both plots, but also just from simple math: If a researcher reports an adjusted frequency of 35 for a word, one does not know whether that word occurs 35 perfectly evenly distributed times in the corpus (i.e., frequency = 35 and, say, Juilland's D = 1) or whether it occurs 350 very unevenly distributed times in the corpus (i.e., frequency = 350 and, say, Juilland's D = 0.1). And while this example is of course hypothetical, it is not as unrealistic as one might think. For instance, the products of observed frequency and 1-DP for the two words pull and chairman in the spoken BNC are very similar -375 and 368.41 respectively -but they result from very different frequencies and DP-values: 750 and 0.5 for pull but 1939 and 0.81 for chairman. Not only is it the dispersion value, not the frequency one, that reflects our intuition (that pull is more basic/widely-used than chairman) much better, but this also shows that we would probably not want to treat those two cases as 'the same' as we would if we simply computed and reported some conflated adjusted frequency. Thus, keeping frequency and dispersion separate allows researchers to preserve important information and it is therefore important that we do not give in to the temptation of 'a single rank-ordering scale' and simplify beyond necessity/merit -what is needed is more awareness and sophistication of how words are distributed in corpora, not blunting our research tools.

In all fairness, even if one decides to keep the two dimensions separate, as one definitely should, there still is an additional unresolved question, namely what kind of threshold value(s) to choose for (frequency and) dispersion. It is unfortunately not clear, for instance, what dispersion threshold to adopt to classify a word as 'evenly dispersed enough for it to be included in a dictionary': DP = 0.4/D = 0.8? DP = 0.45/D = 0.85? In the absence of more rigorous comparisons of dispersion measures to other kinds of reference data, at this point any cut-off point is arbitrary (see

Tools and Resources

Dispersion is a corpus statistic that has not been implemented widely into existing corpus tools and arguably it is in fact a statistic that, unlike others, is less obvious to implement, which is why all implementations of dispersion in such generalpurpose tools probably leave something to be desired. This is for two main reasons. First, most tools offer only a very small number of measures, if any, and no ways to implement new ones or tweak existing ones. Second, most existing dispersion measures require a division of the corpus into parts and the decision of how to do this is not trivial. While ready-made corpus tools such as WordSmith Tools or AntConc might assume for the user that the corpus parts to be used are the n (a user-defined number) equally-sized parts a corpus can be divided into or the separate files of the corpus, this may actually not be what is required for a certain study if, for instance, sub-divisions in files are to be considered as well (as might be useful for some files in the BNC) or when groupings of files into (sub-)registers are what is of interest.

To mention a few concrete examples, WordSmith Tools offers a dispersion plot as well as range and Juilland's D-values (without explicitly stating that that is in fact the statistic that is provided) while AntConc offers a version of a dispersion plot separately for each file of a corpus, which is often not what one needs. The COCA-associated website

By far the best -in the sense of most versatile and powerful -approach to exploring issues of dispersion is with programming languages such as R or Python (see Chap. 9), because then the user is not dependent on measures and settings enshrined in ready-made software but can customize an analysis in exactly the way that is needed, develop their own methods, and/or run such analysis on data/annotation formats that none of the above tools can handle. This chapter comes with some companion code for readers to explore as well as an R function to compute a large number of dispersion measures for data provided by a user. This function is an update of the function provided in

Further Reading

Savický, P., and Hlaváčová, J. 2002. Measures of word commonness. Journal of Quantitative Linguistics 9(3):15-31.

Introduction

As we have seen from Chap. 4, frequency lists are an essential part of the corpus linguistics methodology. They allow us to see what words appear (and do not appear) in a text, and give an indication of their prominence if we sort the list in frequency order. Beyond the world of the corpus linguistic researcher, frequency lists can be used directly or indirectly to support language learners by providing a way to focus on the more frequent words in a text, or suggesting priorities for language teachers when preparing lesson materials. Lexicographers use frequency lists indirectly when constructing traditional printed dictionaries. Frequency lists have also been allied with other kinds of grammatical information (such as major word class) and translational glosses (both for the words themselves and example sentences) and turned directly into frequency dictionaries for learners and teachers in a number of languages (e.g.

When used in simple form, frequency lists can sometimes be misleading, and care needs to be taken when making generalisations from them. For example, if frequency counts are derived from a large representative corpus such as the British National Corpus (BNC), we may reasonably claim that high frequency words in the corpus may also have a similarly high usage in the language. However, a word may have a high frequency count in the BNC not because it is widely used in all sections of the corpus, but because it has a very high frequency in only certain parts of the corpus and not in others, e.g. conversational speech rather than newspaper articles. Hence, as described in Chap. 4, and more particularly in Chap. 5, we need to pay careful attention to dispersion or range measures, which can help us estimate, for instance, how widely represented a word is across the various texts, domains, or genres within a written corpus, or across speakers within a spoken corpus.

Using computer software to automate the creation of frequency lists from texts saves the researcher significant amounts of time, but the results can be overwhelming in terms of the amount of information to analyse. One option to reduce the wealth of information is to compare a frequency list from one corpus with another in order to highlight differences in word rank or frequency, since significant changes to the relative ordering of words can flag points of interest

Another major milestone in the development of the keywords approach was the inclusion of the method by Mike Scott in his WordSmith Tools software. A number of authors had used significance tests to determine the importance of differences of specific words or linguistic features between corpora, but

Fundamentals

The keywords method is conceptually simple, relying on the comparison of (normally) two word frequency lists. The complexity of the method lies in the choice of statistics and frequency cut-offs to appropriately filter the results (for further discussion of this, see Sect. 6.4). As part of the corpus linguist's toolbox, the keywords method is most appropriate as a starting point to assist in the filtering of items for further investigations, rather than an end in and of itself.

As a first step, two frequency sorted word lists are prepared, one from the corpus being studied (the 'target') and one from a reference corpus. Each word list contains a list of tokens and associated frequencies. The reference dataset in corpus linguistics studies is usually a general corpus, representative of some language or variety of language. However, depending on the research question and aims, a suitable comparison set may also be used, e.g. from a corpus representing a different variety, time, or genre. Rather than using two different corpora entirely, some researchers use various subcorpora from the same (reference) corpus for the 'target' and 'reference' sets, and this approach is further exemplified in the two representative studies summarised below. Next, the frequency of each word in the target corpus is compared to its frequency in the reference dataset in order to calculate a keyness value. Finally, the word list for the target corpus is reordered in terms of the keyness values of the words. The resulting sorted list contains two kinds of keyword: positive (those which are unusually frequent in the target corpus relative to the reference corpus) and negative (words which are unusually infrequent in the target corpus). It is also common to describe these two groups of words as overused (for positive) and underused (for negative), particularly in the learner corpus literature (cf. Chap. 13).

In order to perform the keyness calculation for each word in the list, corpus tools set up a 2 by 2 contingency table, as shown in Table

Here, N1 = c, and N2 = d. Hence, for E1 = c*(a + b)/(c + d) and E2 = d*(a + b)/(c + d). Note that the calculation of expected values takes account of the sizes of the corpora, so the raw frequency figures should be used in the table.

The log-likelihood score in this case is LL = 2*((a*ln(a/E1)) + (b*ln(b/E2))).

Representative

Previous studies of people with cancer found gender differences which align broadly with findings in more wide-reaching sociolinguistic analyses, namely that women's style is more expressive compared to men's style, which is more instrumental

The corpora utilised contained two types of data: research interviews and Internet-based support groups. Qualitative interviews were adopted for secondary analysis from the Database of Individual Patient Experiences project; these were conducted in the UK, 2000-2001, with 97 people with cancer (45 women with breast cancer; 52 men with prostate cancer), totalling 727,100 words. Posts were inspected to extract only those written by people with cancer (as opposed to family members, carers, or those experiencing symptoms that may be associated with cancer), which resulted in a final corpus comprising 12,757 posts and 1,629,370 words.

Often, stylistic, grammatical, or syntactical features of the target corpus are highlighted through keyness comparison with a general reference corpus. However, the comparative keyword analysis reported here did not involve a general reference corpus (which may conflate or obscure gender differences in the specific data sets. Instead, the breast cancer texts were compared to the prostate cancer texts to facilitate analysis of meanings made by their female and male authors, respectively. Keywords were calculated using WordSmith Tools

Analysis of the interviews and web fora led to findings broadly aligned with previous research on gender differences in the experience of serious illness. Qualitative analysis of the interview corpus also supported findings from previous studies: women were more likely to claim to seek social support on the Internet, whereas men said that they use the Internet to look for information. Quantitative analysis using keyness helped to identify further areas of interesting difference. Compared to women with breast cancer, men with prostate cancer had a much greater number of keywords pertaining to

Other limitations not acknowledged by the researchers were also present in this study. The use of WordSmith's 'keyness measure' was used to rank results, with the 'top 300' skimmed from each corpus for further analysis. However, with all p-values nearing zero and no thresholds of log-likelihood included, it is unclear whether the most salient semantic categories were, in fact, included and populated. Finally, while we acknowledge that the creation of ad hoc semantic categories is useful for thematic content analysis, (continued) 6 Analysing Keyword Lists 125 particularly when the researchers are very well-versed in the content of their corpora, we wonder about the accompanying detriment that this brings, particularly in relation to replicability. A number of other studies make use of further computational methods to undertake semantic annotation and categorisation. Below, we summarise one such work. In this study, the speech of the six characters with the highest frequency of speech in Romeo and Juliet was isolated to create subcorpora varying in length from 1293 to 5031 words. Culpeper posited that studying these subcorpora would expose the differing speech styles of characters (which, in turn, contribute to reader perception of characterisation). In this case, the very small sizes of data under analysis also allowed for full consideration of all results.

Representative

Earlier literary studies have described (contextualised) frequency of features as indicators of an author's or character's style. Culpeper's aim was to "produce key items that reflect the distinctive styles of each character compared with the other characters in the same play, rather than ... stylistic features relating to differences of genre . . . or aspects of the fictional world

This study also made use of WordSmith Tools

Key parts-of-speech and key semantic categories are calculated by applying the keywords procedure to part-of-speech and semantic tag frequency (continued) lists in the same way as described in Sect. 6.2 for word frequency lists. Keyword analysis can be illuminating, but can also be misleading, because semantic similarity is not explicitly taken into account during analysis. While many (or even most) researchers who use keyword analysis end up grouping or discussing words in semantic categories, this can be a somewhat flawed method, as these categories are subjective, and words which are too infrequent to appear as key on their own will be discounted. In small corpora (such as the Romeo and Juliet corpus), many content words of interest would necessarily be low-frequency. The incorporation of a computational annotation system allows for systematic, rigorous annotation followed by statistical analysis.

In this study, semantic domains were annotated and analysed using UCREL's (University Centre for Computer Corpus Research on Language, based at Lancaster University) Semantic Analysis System (USAS) in Wmatrix. The input is part-of-speech tagged text produced by CLAWS, which is then run through SEMTAG, which uses lexicons to assigns semantic tag(s) to each lexical item or multiword unit (for details, see

In the analysis section of the paper, a number of key semantic categories and constituent items (words and set phrases) were presented for Romeo, Nurse, and Mercutio. An indicative selection of Romeo's key semantic categories appears in Table

The first two semantic categories (RELATIONSHIP: INTIMATE/SEXUAL and LIKING) are clearly linked semantically and metonymically. The keyness of these themes was predictable, but did provide empirical and statistical evidence for topicality of Romeo's speech and a sense of his role as a lover. The third category in Table

Interestingly, very few items within key semantic domains were also identified as keywords in this study. In Table

(continued) Adapted from

Critical Assessment and Future Directions

In this chapter so far, we have described the origins and motivations for the development of the keywords method in corpus linguistics, and shown two representative studies using the technique. In the next few sub-sections, we will undertake a critical assessment describing some of the pitfalls and misconceptions about the use of the technique, along with a summary of criticisms, concluding with future directions for the approach.

Corpus Preparation

Given that the input to the keywords method is word frequency lists, then the specific details of the preparation of the lists are important-but often overlooked-in the corpus linguistics literature. What counts as a word in such lists (on the basis of tokenisation, punctuation, capitalisation, standardisation, etc.) can potentially make a large difference to the results. Usually, corpus software tools tokenise words by identifying boundaries with white space characters and removing any punctuation characters from the start and end of words. However, different texts and authors might use hyphenation differently; for instance, "todo", "to-do" and "to do", must be carefully cross-matched or standardised, or else the frequencies in contingency tables will not compare word types consistently. Most corpus tools avoid the capitalisation issue completely by forcing all characters to upper-or lowercase, but this can cause issues with different meanings, e.g. "Polish" versus "polish". Wmatrix makes an attempt to preserve capitalisation if words are tagged as proper nouns but this, in turn, relies on the accuracy of the POS tagger software.

Corpus methods are increasingly being applied to historical corpora, as we have seen in

Focus on Differences

One of the central drawbacks to keyness analysis is the innate focus on difference (and obfuscation of similarity).

Another issue in keyword analysis highlighted by

Applications of Statistics

There have been a number of criticisms of the keywords approach in relation to the application and interpretation of the significance test statistics used in the procedure. The method described in Sect. 6.2 can be seen as a goodness-of-fit test, where the null hypothesis is that there is no difference between the observed frequencies of a word in the two corpora. If the resulting metric (log-likelihood in our case) exceeds a certain critical value, then the null hypothesis can be rejected. After choosing a degree of confidence, we can use chi-squared statistical tables to find the critical value, e.g. for the 5% level (p < 0.05) the critical value is 3.84, and for 1% (p < 0.01), it is

It was

More recent papers have also investigated similar issues of statistical validity and appropriateness of the keywords procedure as currently envisaged for comparing corpora with specific designs.

Many misconceptions about statistical hypothesis testing methods are observable in the corpus linguistics literature and beyond; for further details, see

Clusters and N-Grams

Both

Researchers may also be interested in (semantic) meaning beyond the single word. The USAS semantic tagger can be used to identify semantically meaningful multiword expressions (MWEs) since these chunks need to be analysed as belonging to one semantic category or are syntactic units e.g. phrasal verbs, compounds, non-compositional idiomatic expressions. Wmatrix then treats these MWEs as single elements in word lists, allowing key MWEs to emerge alongside keywords. Consider an example MWE 'send up'. If this were not identified in advance as a semantically meaningful chunk meaning 'to ridicule or parody', then separate word counts for 'send' and 'up' would be observed and merged with the other occurrences of those words in the corpus, potentially incorrectly inflating their frequencies.

Without the benefit of a semantic tagger,

Future Directions

Many current studies using the keywords method are on English corpora. As this method is readily available in software such as WordSmith and AntConcwhich work well in most languages-more thought should be given to how well keywords work in languages other than English, especially where much more complex inflectional and derivational morphology occurs, e.g. Finnish. For these 6 Analysing Keyword Lists 133 languages, it might be the case that comparing surface forms of words from the corpus works less well than comparing lemmas, because the frequencies are too widely dispersed across different word forms (in a similar way to historical spelling variants) to be comparable.

In terms of future directions for keyness analysis, we recommend that more care is taken in the application of the technique. Rather than blindly applying a simple method to compare two relative frequencies, more thought is required to consider the criticisms and shortcomings that have been expressed in the preceding sections. Any metadata subdivisions present within a target corpus or reference corpus should be better explored via comparison so that they are not hidden; the corpora should be carefully designed and constructed with the aim of answering specific research questions and facilitating comparability; issues such as tokenisation, lemmatisation, capitalisation, identification of n-grams and multi-word expressions, and spelling variation should be considered; and differences as well as similarities should be taken into account when undertaking the analysis of the keyword results. As a corpus community, we need to agree on better guidelines and expectations for filtering results in terms of minimum frequencies and significance and effect size values rather than relying on ad hoc solutions without proper justifications.

In the future, we recommend investigating the use of statistical power calculations in corpus linguistics. Power calculations can be used alongside significance testing and effect size calculations and are increasingly employed in other disciplines, e.g. psychology. Statistical power allows us to calculate the likelihood that an experiment will detect an effect (or difference in frequency in our case of comparing corpora) when there is an effect to be detected. We can use higher statistical power to reduce the probability of a Type-2 error, i.e. concluding that there is no difference in frequency of a word between two corpora, when there is in fact a difference. This might mean setting the effect size in advance and then calculating (a-priori) how big our corpora need to be, or at least being able to (post-hoc) calculate and compare the power of our corpus comparison experiments. This might help us answer the perennial question, 'How big should my corpus be?' and help researchers determine comparability and the relative sizes of sub-corpora defined by metadata such as socio-linguistic variables. Finally, related to the experimental design and interpretation of results, issues of corpus comparability, homogeneity and representativeness are highly important to consider alongside reliability of the statistical procedure

Tools and Resources

Tools

Keyness was arguably one of the later additions to the quiver of corpus linguistic tools; many papers published between 2001-2008 (including the two representative studies summarised in this chapter) discussed the infancy of its adoption. Now, however, this is considered one of the standard five methods of the field, alongside frequency, concordance, n-gram and collocation. As a result, nearly all concordancers and corpus linguistic tools will offer some assistance in the calculation of keyness. Distinguishing features, then, are:

1. the incorporation of more sophisticated taggers, allowing for calculation of key lemmas, parts-of-speech (POS), or semantic domains; 2. the inclusion of built-in reference corpora, often general corpora or subsections thereof, allowing for immediate calculation against a known 'benchmark' without the necessity of sourcing or collecting a comparable corpus; 3. the selection of measures of keyness available (see Sect. 6.3.3 for discussion).

We have provided an overview of popular tools and these features in Table

Resources (Word Lists)

Generation of keywords in a target corpus necessitates some point of comparison, usually either a second target corpus, a reference corpus, or a word list from a large, general corpus. Selection of a reference corpus will impact the results, and some care should be taken to select an appropriate 'benchmark' to highlight differences aligned with a given research question. Many research questions necessitate the collection of specialised reference corpora, or entail comparison of subcorpora.

Those wishing to answer more general questions (e.g. 'aboutness') may choose to make use of a general reference corpus. Word lists of many of the largest general corpora are readily available online; we have provided a sample of some Englishlanguage resources in Table

Further Reading

Bondi, M. and Scott, M. (Eds.). 2010. Keyness in texts. Amsterdam: John Benjamins.

This is quite a comprehensive guide for scholars with particular interest in keywords and phrases. The collection is divided into three sections: (1) Exploring keyness;

(2) Keyness in specialised discourse; and (3) Critical and educational perspectives. Section one deals with a number of the issues that we have touched upon here in

Introduction

General Introduction

One of the, if not the, most central assumptions underlying corpus-linguistic work is captured in the so-called distributional hypothesis, which holds that linguistic elements that are similar in terms of their distributional patterning in corpora also exhibit some semantic or functional similarity. Typically, corpus linguists like to cite

[i]f we consider words or morphemes A and B to be more different in meaning than A and C, then we will often find that the distributions of A and B are more different than the distributions of A and C. In other words, difference of meaning correlates with difference of distribution.

That is, a linguistic expression E -a morpheme, word, construction/pattern, . . . -can be studied by exploring what is co-occurring with E and how often.

Depending on what the elements of interest are whose co-occurrence is studied, different terms have been used for such co-occurrence phenomena:

• lexical co-occurrence, i.e. the co-occurrence of words with other words such as the strong preference of hermetically to co-occur with, or more specifically, be followed by, sealed, is referred to as collocation; for collocations, it is important to point out the locus of the co-occurrence and

The simplest possible way to explore a linguistic element (such as hermetically or regard) would be by raw co-occurrence frequency -how often do I find the collocation hermetically sealed in my corpus? -or, more likely, conditional probabilities such as p(contextual element(s)|E) -how likely is a verbal construction to be an as-predicative when the verb in the construction is regard?

While obtaining sorted frequency lists that reveal which collocates or constructions occur most often or are most likely around an element E is straightforward, much corpus-linguistic research has gone a different route and used more complex measures to separate the wheat (linguistically revealing co-occurrence data) from the chaff (the fact that certain function words such as the, of, or in occur with everything a lot, qua their overall high frequency. Such measures are often referred to as association measures (AMs) simply because they, typically, quantify the strength of mutual association between two elements such as two words or a word and a construction. In the following section, we discuss fundamental aspects of the computation of some of the most widely-used AMs.

Fundamentals

For decades now, AMs have typically been explained on the basis of co-occurrence tables of the kind exemplified in Table

A large number of AMs has been proposed over the last few decades, including (i) measures that are based on asymptotic or exact significance tests, (ii) measures from, or related to, information theory, (iii) statistical effect sizes, various other measures or heuristics;

( that there is a strong mutual association between X (the as-predicative) and E (regard); if one computed the actual p-value following from this G

• instances of collocations/collostructions where X attracts E but E does not attract X (or at least much less so); • instances where E attracts X but X does not attract E (or at least much less so); • instances where both elements attract each other (strongly).

Based on initial discussion by

( P X|E = a a+b -c c+d = 80 99 -607 138565 ≈ 0.804 As is obvious from the equations, P is essentially an adjusted conditional probability. In this case, and this is not atypical, the difference between the conditional probabilities and the corresponding P-values is quite small and may even seem to be negligible. However, P appears more useful for theoretical reasons (its exact form has proven useful in research on associative learning

Another relevant issue is concerned with the fact that some AMs -in particular those ultimately related to significance tests such as G 2 , chi-squared, z, . . .conflate frequency (how often are the elements X and E observed together and in isolation?) and effect size (how strong is the attraction between X and E?). From a seemingly moderate to high G 2 -value in isolation, it is not obvious whether that value reflects medium frequencies of (co-)occurrence and high association or very high frequencies of (co-)occurrence and a medium degree of association. This also means that AM-values involving different overall frequencies (n in Table

As mentioned above, a fully-fledged application of AMs to co-occurrence can involve computing one or more AMs for each element E co-occurring with a fixed element X at least once and rank-ordering them. Often, studies focus on either the top t elements (with t taking on different number such as 20 or 100 depending on the application) or focus on all elements that meet a particular threshold value for the AM and/or also just the observed frequency value (e.g. a researcher might study only collocations whose MI-score is ≥3 and whose observed co-occurrence frequency a is ≥5; see e.g.

We now discuss two representative studies in which many of the above methodological decisions are reflected. A prominent theme in recent second language research has been the learning of collocations. Historically, there has been a perception that second language learners find collocations difficult to acquire. This led

Representative

The study focuses in particular on how different measures of frequency and association differ in their ability to predict learner knowledge. The predictors assessed differed in five key aspects:

• the choice of corpus: frequency data were retrieved separately from the BNC and COCA, and from each of the main register-based sub-corpora of each, i.e. the five written sub-corpora titled academic, fiction, magazine,

newspaper and non-academic (this last appears in BNC only) and the spoken sub-corpus; • the choice of measure: collocations were quantified in terms of raw frequency, t, MI, and conditional probability; • the span within which words had to appear to be counted towards an item's frequency of collocation. Two spans were used: four words either side of the node and nine words either side of the node; • whether counts were based on lemmatized or non-lemmatized counts. In the former, for example, arguing strongly and argued strongly would both count as cases of the collocation argue strongly. In the latter, these counts would be kept separate; • to account for possible effects of the evenness of dispersion of a collocation within the corpus, each item was also quantified with a DP value (see Chap. 5).

A number of key findings emerge from these data. First and overall, frequency and association data were found to be reliable predictors of learners' knowledge of collocation. Frequency and t-values from COCA achieved correlations with knowledge of between ρ = 0.24 and ρ = 0.27. Second, there was a large difference in predictiveness between frequency and t, on the one hand, and MI and conditional probability on the other. For these foreign language learners, it seems that the number of times a collocation occurs is a far more important factor than the strength of association between components. This tallies with the psycholinguistic work of

Third, frequency data derived from COCA were substantially better predictors of knowledge than those from the BNC. Participants in the tests analyzed came from Denmark, France, Japan, Jordan, Saudi Arabia, Spain and Sweden. While it is possible that students in these settings are more influenced by US than by British English, the impact of this is likely to have been marginal: Given the widespread view that learners' overall knowledge of collocation is weak (see, e.g.,

Fourth, within the two national corpora, there were also substantial differences in the predictiveness of data from different registers. In both the BNC and COCA, fiction showed the strongest correlation and academic writing the weakest.

Finally, the dispersion of a collocation across a reference corpus had only a weak relationship with knowledge (more widely spread collocations were better recognized), and this relationship was significant only in the BNC. Hampe and Schönefeld's study asks how such instances should be accounted for in linguistic theory, comparing in particular two possible accounts. One, attributed to

Representative

Hampe and Schönefeld evaluate the plausibility of these models through a detailed description, first, of the typical verbal associates of a complextransitive ASC and, second, of the syntactically creative uses of four verbs.

(continued) AMs -in particular, p Fisher-Yates exact test (a measure very highly correlated with G 2 , see Sect. 7.2) -are central to both analyses. These analyses are particularly useful for illustrating the uses to which AMs can be put in that, though they rely on the same test, each makes use of a rather different type of association and for different purposes. The first analysis looks at associations between an abstract construction and the verbs which instantiate it in order to understand the range of meanings which the construction can carry. The second looks at associations between verbs and their accompanying collocations in order to understand restrictions on the use of particular syntactically creative forms.

The first analysis focuses on the ASC illustrated above in (7a), i.e. constructions in which the verb is followed by a direct object and an adjectival phrase acting as object predicate (usually referred to in construction grammar as the resultative construction). Retrieving all cases of this construction from the syntactically parsed ICE-GB corpus, Hampe and Schönefeld use the Fisher-Yates exact test to identify verbs which are associated with it. These verbs are taken to indicate the meanings in which the ASC is most characteristically used. The associated verbs are classified into three semantic groups. The first is most centrally characterized by make (the strongest associate of this ASC), which is used to indicate causation (as in He made John angry). Other associated verbs of this type are render, get, and set. Closely related to these are verbs most centrally represented by keep (as in She kept it safe), which indicate maintenance of a given state. Other examples include leave, hold, and have. The third group of verbs has find (after make, the second mostly strongly associated verb of the ASC) as its central example (as in He found her arrogant). The group is also represented by consider (as described above). This group is rather different from the first two in that it cannot be classified under a broad 'resultative' meaning by which the ASC has been characterized. These are cognition verbs which, Hampe and Schönefeld observe, can be described as 'attributive', rather than 'resultative'. They argue that these should be treated as distinct constructions, pointing out that the generic ASC described by

The second part of Hampe and Schönefeld's analysis explores syntactically creative uses of four verbs: encourage, support, bore and fear. In particular, they look at cases of these verbs in complex transitive patterns in which the direct object noun is followed by either a prepositional phrase (to create a 'caused motion' construction, e.g. encourage tourists into the area) or an adjective phrase (to create a 'resultative' or 'attributive' construction, e.g. the subject bores them stiff ).

For three of the verbsencourage, support, and fear -use in one of the searched constructions is rare (less than 1% of occurrences of the verb) (continued) 7 Analyzing Co-occurrence Data 151 and not listed as a possible form in the corpus-based Collins Cobuild English Language Dictionary. The resultative use of bore (as in He bore her stupid) is more common (accounting for around 7% of uses of the verb) and is listed in the dictionary. Importantly for our current focus on AMs, each verb's syntactically creative use appears to come with collocational restrictions. That is to say, they are strongly associated with specific accompanying words. As with the ASC-verb associations described above, strong collocates are identified using the Fisher-Yates exact test.

Hampe and Schönefeld argue that the apparent restriction of these creative syntactic forms to particular lexical contexts cannot be accounted for in terms of the properties of the relevant ASCs alone. Taking as an example the case of fear and its strong association with dead and (to a lesser extent) with terms related to death

Regardless of whether we ultimately accept Hampe and Schönefeld's conclusions, their paper demonstrates well how AMs can be used to identify two different types of patterning -attractions between abstract constructions and the verbs which instantiate them, and attractions between verbs and the collocates which accompany them. The former provides a way of understanding the meaning potential of a construction; the latter provides a way of understanding restrictions on use. As Hampe and Schönefeld acknowledge, the purely textual nature of this analysis means that strong inferences about the nature of psycholinguistic representations and processes cannot be drawn. What these analyses do provide, however, is a clearer picture of the language use for which any linguistic model would need to account. This picture provides us with a basis both for forming linguistic hypotheses and for evaluating the prima facie plausibility of existing models. Most pertinently to our current topic, their use of AMs provides a granularity of description which cannot be convincingly provided through consideration of abstract syntactic forms or of vocabulary items alone, revealing additional levels of complexity in linguistic patterning and hence in the models that are required to explain it.

Critical Assessment and Future Directions

Given its nature as a distributional discipline, the discussion of how to best approach the quantification and exploration of co-occurrence is likely to continue for the foreseeable future, in particular as corpus-linguistic methods are used in a wider range of theoretical frameworks and with a wider range of other kinds of data, be they observational, experimental, or simulation data. In this section, we are discussing a few areas that we feel should be on corpus linguists' radar; they involve.

• the recognition that much current discussion of AMs is more fragmented than it needs to be (Sect. 7.3.1); • candidates for measures that have so far not been explored but rather than just being yet even more different ways to crunch the same numbers, that offer additional advantages that current measures do not provide (Sect. 7.3.2); • additional pieces of information that virtually no current AM includes

Unifying the Most Widely-Used AMs

The above discussion presented AMs as they are typically discussed, namely based on seemingly unrelated mathematical formulae in turn based on 2 × 2 co-occurrence frequency tables such as Table

The results of binary logistic regression models are similar to those of the maybe more straightforward linear regression models and include the following:

• an intercept: log odds of the predicted level of the dependent variable (the second, i.e. regard) when the predictor is the first level (i.e. any construction); • a coefficient: the change in log odds of the predicted level of the dependent variable (regard) when the predictor becomes the second level (i.e. as-predicative); • the intercept and the coefficient can then be used to compute predicted probabilities of the two levels of the dependent variable;

• a significance test of the overall regression model, which, in the case of a model with only one predictor, is also the significance test of that predictor (see also Gries 2013b: Section 5.3).

Space does not permit a detailed discussion and exemplification here in prose; for detailed code, computations, and results in R, see the companion code file. Suffice it to say here, that • G 2 is the difference between a regression model that predicts the use of E (any verb vs. regard) given X (any construction vs. the as-predicative) and a null model that predicts the use of E (any verb vs. regard) given no other information; • the odds ratio is the exponentiated coefficient in the regression model; • MI is log 2 of the predicted probability of E being regard happening when X is the as-predicative divided by the probability of E being regard in general; etc.

More interestingly, P Construction → Verb , the adjusted conditional probability measure from

To reiterate, while corpus-linguistic research into the association between elements has produced dozens of AMs, the frequencies of their use is as Zipfiandistributed as that of words: While there is still a lively discussion of which measure(s) is/are most useful for which specific purpose, a mere handful of (symmetric) measures are used in the vast majority of studies. However, there is now more recognition that at least the symmetry-of-association assumption built into most AMs used is problematic and more uni-directional/asymmetric measures are being explored now. The still intense discussion of AMs notwithstanding, it is instructive to realize that all the most frequent measures -uni-and bi-directional ones -are really only different parts/facets of a simple binary logistic regression trying to predict the realization of one element based on another: Once that is realized, all the seemingly different AMs can be captured under one and the same approach (which is of course part of the reason why many AMs are very highly correlated with each other); not only does that facilitate their teaching, it also naturally bridges the gap between AMs on the one hand and hundreds of regression-based studies of alternation phenomena in sociolinguistics or usagebased linguistics or over-/underuse studies in learner corpus research (see

Additional (Different) Ways to Quantify Basic Co-occurrence

As mentioned above, the number of AMs that have been proposed is vast and, ironically speaking, inversely proportional to the number of rigorous and comparative evaluations of many of AMs, which is why it may seem futile to add new measures to the mix. However,

The first of these is to use as an AM another general information-theoretic measure, namely the Kullback-Leibler (KL) divergence. The KL divergence is written as D KL (posterior/data || prior/theory), which refers to how much a posterior/data percentage distribution of an element (e.g., E) in the presence of another element (e.g., X) diverges from the overall/theoretical overall percentage distribution of E; it is computed as in (8). Eq. (

Baayen's second proposal is to use the varying intercepts of the simplest kind of mixed-effects model (see

In sum, AM research requires more exploration of measures that allow for elegant ways to include more information in the analysis of co-occurrence phenomena.

Additional Information to Include

Another kind of desiderata for future research involves the kind of input to analyses of co-occurrence data. So far, all of the above involved only token frequencies of (co-)occurrence, but co-occurrence is a more multi-faceted phenomenon and it seems as if the following three dimensions of information are worthy of much more attention than they have received so far (see

• type frequencies of co-occurrence: current analyses of co-occurrence based on tables such as Table

To conclude, from our above general discussion and desiderata, one main takehome message should be that, while AMs have been playing a vital role for the corpus-linguistic analysis of co-occurrence, much remains to be done lest we continue to underestimate the complexity and multidimensionality of the notion of co-occurrence. Our advice to readers would be • to familiarize themselves with a small number of 'standard' measures such as G 2 , MI, and t; but • to also immediately begin to learn the very basics of logistic regression modeling to (i) be able to realize the connections between seemingly disparate measures as well as (ii) become able to easily implement directional measures when the task requires it; • to develop even the most basic knowledge of a programming language like R to avoid being boxed in into what currently available tools provide, which we will briefly discuss in the next section.

Tools and Resources

While co-occurrence is one of the most fundamental notions used in corpus linguistics, it is not nearly as widely implemented in corpus tools as it should be. This is for two main reasons. First, existing tools offer only a very small number of measures, if any, and no ways to implement new ones or tweak existing ones. For instance, WordSmith Tools offers MI and its derivative MI3, t, z, G 2 , and a few less widely-used ones (from WordSmith's website) and AntConc offers MI, G 2 , and t (from AntConc's website). While this is probably a representative section of the most frequent AMs, all of these are bidirectional, for instance, which limits their applicability for many questions. Second, these tools only provide AMs for what they 'think' are words, which means that colligations/collostructions and many other co-occurrence applications cannot readily be handled by them. As so often and as already mentioned in Chap. 5, the most versatile and powerful approach to exploring co-occurrence is with programming languages such as R or Python, because then the user is not restricted to lexical co-occurrence and dependent on measures/settings enshrined in ready-made software black boxes, but can customize an analysis in exactly the way that is needed; some very rudimentary exemplification can be found in the companion code file to this chapter; also, see

Further Reading

Wiechmann, D. 2008. On the computation of collostruction strength: testing measures of association as expression of lexical bias. Corpus Linguistics and

Linguistic Theory 4(2):253-290.

Wiechmann (

Chapter 8

Analyzing Concordances Stefanie Wulff and Paul Baker

Abstract In its simplest form, a concordance is a list of all attestations (or hits) of a particular search word or phrase, presented with a user-defined amount of context to the left and right of the search word or phrase. In this chapter, we describe how to generate and manipulate concordances, and we discuss how they can be employed in research and teaching. We describe how to generate, sort, and prune concordances prior to further analysis or use. In a section devoted to qualitative analysis, we detail how a discourse-analytical approach, either on the basis of unannotated concordance lines or on the basis of output generated by a prior quantitative examination of the data, can help describe and, crucially, explain the observable patterns, for instance by recourse to concepts such as semantic prosody.

In a section devoted to quantitative analysis, we discuss how concordance lines can be scrutinized for various properties of the search term and annotated accordingly. Annotated concordance data enable the researcher to perform statistical analyses over hundreds or thousands of data points, identifying distributional patterns that might otherwise escape the researcher's attention. In a third section, we turn to pedagogical applications of concordances. We close with a critical assessment of contemporary use of concordances as well as some suggestions for the adequate use of concordances in both research and teaching contexts, and give pointers to tools and resources.

Introduction

In this chapter, we describe how to generate, manipulate, and analyze concordances.

In its simplest form, a concordance is "a list of all the occurrences of a particular search term in a corpus, presented within the context that they occur in; usually a few words to the left and the right of the search term"

The term concordance usually refers to the entire list of hits, although sometimes researchers refer to a single line from the list as a concordance. In this chapter, to avoid confusion, we refer to a concordance as the list of citations, distinguishing it from a concordance line, which is a single citation. We have used a range of concordancing tools to create this chapter, but for consistency, we have formatted the concordances in a standard way. Figure

The typical layout of concordances-the search term in the middle; some context around it; the left-hand context aligned flush right and the right-hand context aligned flush left-is meant to make it easier to inspect many examples at one glance in order

The  to identify patterns around the search term that might escape one's notice if one were reading the examples as one running text. In Fig.

Similarly, looking at the verb predicates following refugee(s), we get a glimpse of the actions associated with them: walking, fleeing, clambered, and crammed are four examples.

Fundamentals

Sorting and Pruning Concordances

While concordances are typically formatted slightly differently from regular running text, it can be difficult to see patterns because the attestations in a simple concordance as shown in Fig.

Depending on what your end goal is, you may want to not only sort (and sort in different ways to obtain different perspectives on your data), but also prune a concordance. By pruning, we here mean one or more of the following: deleting certain concordance lines and keeping others; narrowing down the context window; or blanking out the search term and/or collocates. Most typically, we delete concordance lines and/or clip the context window in the interest of saving space. Spatial restrictions apply to a handbook article like this one (hence the 15line snippets as opposed to displaying the concordance in its entirety) as much as to the use of concordances for classroom use (not many students would want to inspect thousands of concordance lines). In a research context, in contrast, especially when researchers want to make a claim for exhaustive data retrieval and/or when the sequencing of attestations in the corpus matters for the research question at hand, one would only delete concordance lines that contain false hits.

Another typical reason for deleting certain concordance lines is that depending on your search term (as well as the functionality of the software you are using and the make-up of the corpus data), the resulting concordance may contain a sizeable share of false hits. Imagine, for example, that you have a (untagged) corpus from which you want to retrieve all mentions of former and current Presidents of the United States. If you (can only) enter the simple search terms Bush, Clinton, Obama, and Trump, chances are that you will retrieve a number of hits that do not refer to the Presidents, but other people by the same name, or that do not refer to people at all, but instead, say, plants (Steffi is hiding behind the bush) or card games (Paul played his trump card).

Similarly, you may want to blank out either the search term or the collocates surrounding the search term to create a fill-in-the-blank exercise for teaching purposes. There are countless applications of this; to give but one example, imagine you wanted to teach non-native speakers of English the difference between nearsynonymous words such as big, large, and great. You could create a worksheet in a matter of minutes by creating a concordance of these three adjectives, deleting any false hits or hits you deem too confusing or complex for your students, blanking out the search terms themselves, and printing out the formatted concordance

Concordance lines form the basis for both qualitative and quantitative analysis. We define the term quantitative analysis here to refer to analyses that focus either exclusively, primarily, or in the initial stages of analysis, on the distributional patterns and global statistical trends underlying a given phenomenon, while we define the term qualitative analysis to refer to analyses that focus either exclusively, primarily, or in the initial stages of analysis, on in-depth scrutiny of individual attestations of a phenomenon. As we see it, the distinction between the two kinds of analysis is more a matter of degree than a categorical choice, and which form of analysis dominates in a research project depends on the phenomenon under investigation and the researcher's goals. Ideally, both forms of analysis are employed to provide complementary evidence: a qualitative analysis may be very thorough, yet leave open questions of generalizability and robustness that can be addressed in a more quantitative study. Conversely, even the most sophisticated quantitative analysis typically entails item-by-item annotation of each data point, which equates to a qualitative analysis of sorts; likewise, the results of a quantitative analysis demand interpretation, which typically requires a qualitative approach. In any case, concordance lines serve to provide the lexical and/or grammatical context of a search term and thus can be needed at all stages of an analysis, from data coding to interpretation.

Qualitative Analysis of Concordance Lines

In order to show the benefit of a qualitative analysis of concordance lines, we stick with the subject of constructions of refugees in the BNC. One way that concordance lines can be used fruitfully is to identify semantic prosody

While an approach which considers collocates is useful in obtaining a general sense of a word's semantic prosody, we would argue that such an approach works best when complemented by concordance analyses which may identify more nuanced uses. For example,

The qualitative concordance analysis is also helpful in indicating that not all of the co-occurrences of water-related words are used in metaphorical ways. For example, the first concordance line in Fig.

Let us consider another, even more important example of why a qualitative concordance analysis is important. Looking again at the collocates of refugee(s), the word bogus is the 10 th strongest adjectival collocate of the term, occurring 14 times. Another collocate, genuine, appears with the search term 41 times. This entails a contrast between genuine and refugee(s) that calls into question the veracity of people who are identified as refugees. Figure

At this point it would be reasonable to conclude that one way that refugees are constructed in the BNC is negatively, as 'bogus refugees' who require regulation to stop them from illegally obtaining money from the British government. The fact that the terms bogus and genuine appear as collocates of refugee(s) in the corpus suggests that this occurs quite frequently. However, there is a danger of presenting an over-simplified analysis if we stop here. It is often wise to look at expanded concordance lines before making a strong claim, in order to consider more context. Take for example the line "SCANDAL OF THE BOGUS REFUGEES-80% cheat their way into Britain and the good life". The use of quotes at the start and end of this line perhaps indicates an intertextual use of language (where a text references another text in some way), and it is worth expanding the line to see whether this is occurring, and if so why. A fuller example of this is below:

(1) Intending to increase sensitivity to the supposed threat, the right-wing tabloids have been regaling the public with anti-refugee stories during 1991 and 1992. For example, the Sunday Express of 20 October 1991 headlined its front-page lead article: 'SCANDAL OF THE BOGUS REFUGEES -80% cheat their way into Britain and the good life'.

Consulting the header information from this particular file, we see that it is from an article in the New Statesman. Importantly, this article references constructions of refugees in 'right-wing tabloids' like the Sunday Express by quoting from them. Reading the whole article, the New Statesman's tone is critical of such constructions -the article is titled 'Racist revival'. A similar case is found in the ninth line in Fig.

(2) Does my right hon. Friend agree that the opportunity for this country to help support genuine refugees abroad through various aid programmes is not helped by the fact that, according to a headline in The Times today, bogus refugees bleed Britain of £100 million through benefit fraud? Has he seen the comments of a DSS officer in the same article that benefit fraud is now a national sport and that bogus asylum seekers think that the way in which this country hands out so much money is hilarious?

Again, the example of bogus refugees is cited in this text in order to be critical of it, arguing that such representations do not help to support genuine refugees, although the speaker still makes a distinction between genuine and bogus refugees. However, the speaker does appear to be critical of the Times's reference to bogus refugees, and so these two examples indicate that not every mention of a bogus refugee should be seen as uncritically contributing towards the negative semantic prosody. Had our analysis involved a close examination of a small number of full texts, this point would have quickly been obvious. However, due to the nature of a concordance analysis -the number of lines the analyst is presented with, along with the fact that they only contain a few words of context either side of a search term, it is possible that these more nuanced cases may be overlooked. Before making claims, it is important to consider a sample of expanded concordance lines and to maintain vigilance in terms of spotting lines that potentially may be functioning differently to a first glance. In Fig.

Quantitative Analysis of Concordance Lines

Many quantitative corpus analyses are based on concordance data (though not necessarily all: one could think of, for example, a study that is based on frequency or collocation lists instead, see Chaps. 4-7). This is particularly true for multifactorial studies, that is, studies that try to explain a linguistic phenomenon with recourse to not one, but several variables (see Chaps.

(3) a. I thought (that) Steffi likes candy.

b. The problem is (that) Steffi doesn't like candy. c. I'm glad (that) Paul likes candy.

The variables that govern speakers' choices to realize or omit the complementizer have been studied intensively (e.g. Jaeger 2010;

In a second step, after false hits had been pruned from the raw data sheet, the remaining true hits were coded for different variables known to impact native speakers' choices. The result was a spreadsheet with one row for each concordance line and one column for each variable considered for analysis. A snippet of that spreadsheet is shown in Fig.

The conversion of the concordance lines into a spreadsheet like in Fig.

We do not have enough space to discuss the statistical evaluation and detailed results of this study here; suffice it to say that applying a multiple regression approach to the entire data sample of over 9,000 attestations, the authors found that intermediate-advanced German and Spanish learners rely on the same factors as native speakers do when they choose to realize or omit that. The main difference between the two speaker groups is that learners are overall more reluctant to omit the complementizer, especially if complexity-related variables increase the cognitive cost associated with processing the sentence (for instance, if the complement clause is quite long). For more information regarding regression-type approaches, see Chaps.

Pedagogical Applications of Concordance Lines

There are two ways in which concordances (or any other corpus-based output) can be used in the classroom: either the students generate concordances themselves in class, or the instructor provides materials that include concordance lines. There is a growing strand of research that explores the efficacy of so-called datadriven learning (DDL) approaches, which give students access to large amounts of authentic language data (Frankenberg-Garcia et al. 2011), and corpus-based materials naturally lend themselves to use in such an approach. Space does not permit a comprehensive review of that literature here; for a good point of departure, see

Another example is

Finally, for a recent study that explored the effect of concordance-based teaching not only in terms of learners' performance on a vocabulary test, but its potential effect on learner production, see

Representative

Concordance analyses of the two other newspaper corpora were carried out in similar ways in order to compare representations of doctors, foreigners and foreign doctors. The analysis found that 41% of the references to foreign doctors directly represented them as incompetent (particularly in terms of not being able to speak English), with a further 16% implying incompetence by calling for tighter regulation of them. There was frequent reference to foreign doctors who had accidentally killed patients, labelling them as killers. The concordance analysis also noted several contradictory (continued) representations, including the view that foreign doctors were desperate to work in the UK and were 'flooding' into the country (similar to the water metaphor used to describe refugees), appearing alongside other articles which claimed that foreign doctors 'ignore' vacancies in the UK. As well as being regularly described as incompetent and bungling, foreign doctors were also characterized as 'sorely needed in their own countries' and the UK was seen as amoral for 'stripping poorer countries of professionals'. Foreign doctors were thus negatively represented, no matter what position they were seen to take. Representations of doctors were different to those of foreign doctors, with few mentions of the need for tighter regulation and only a small number of references to incompetent doctors. A common phrase in this corpus was 'see your doctor', which implied that journalists placed trust in doctors (as long as they were not foreign). Representations of foreigners were largely concerned with political institutions like the foreign office, although a sample of 21 out of 100 concordance lines taken at random (using an online random number generator) showed negative constructions of foreigners involving stereotyping, implying they were taking up British resources or jobs, or controlling British interests. Overall, the analysis indicates that foreign doctors tend to be viewed negatively, as foreigners first and doctors second, with individual stories of negligence being generalized and used as evidence of a wider problem. Gries and Wulff (2013) examined data obtained from the British sub-section of the International Corpus of English and the Chinese and German subsections of the International Corpus of Learner English in order to determine what factors govern learners' choice of either the s-genitive (as in the squirrel's nest) or the of -genitive (the nest of the squirrel), and how learners' choices align with those of native speakers. They annotated 2,986 attestations captured as concordance lines for 14 variables that were previously shown to impact native speakers' choices, including the semantic relation encoded by the noun phrases, the morphological number marking on the noun phrases, their animacy, specificity, complexity, and, crucially, the L1 background of the learners, among others. The data sample was analyzed with a binary (continued) logistic regression (Chap. 21) in which the dependent variable was the choice of genitive (-s vs. of ) and the 14 variables were the predictor variables. The final model suggested that learners generally heed to the same variables that native speakers consider in their choice of genitives. The most important variable across speakers' groups was segment alternation: native and nonnative speakers alike strongly preferred to opt for the genitive variant that represented the more rigid alternation of consonants and vowels. Overall, the Chinese learners were much better aligned with the native speaker's choices than the German learners were, yet showed a stronger tendency to overuse the s-genitive across different contexts.

Representative

Critical Assessment and Future Directions

What are the limitations and drawbacks of concordance analysis? For one, it can be time consuming, particularly if we are using a large corpus or searching on a frequent item. This is a valid concern especially in the contexts of using concordances in the classroom, or for self-study. Secondly, human error and cognitive bias can creep in, meaning that we may over-focus on patterns that are easy to spot, such as a single word appearing frequently at the L1 position, while more variable patterns may go unnoticed. It can be mentally tiring to examine hundreds or thousands of lines, so there is a danger that what we notice first may receive the most attention (which stresses the importance of trying different sorting patterns to yield different perspectives on the data). One option would be to use multiple analysts to carry out coding of concordance lines, with an attendant measure of inter-coder agreement

Where is concordance analysis headed? To date, most concordancing research has been carried out on corpora of plain text. However, with moves towards multimodal corpora, it is possible to combine concordancing with analysis of sound or visuals (see

In summary, concordance analysis is one aspect of corpus linguistics that sets it apart from other computational and statistical forms of linguistic analysis. It ensures that interpretations are grounded in a systematic appraisal of a linguistic item's typical and atypical uses, and it guards against interpretative positivism. The inspection of dozens of alphabetically sorted concordance lines enables patterns to emerge from a corpus that an analyst would be less likely to find from simply reading whole texts or scanning word lists. By bridging quantitative and qualitative perspectives on language data, concordance analysis is and will remain a centerpiece of corpus-linguistic methodology.

Tools and Resources

Table

• some are tailored more towards research, others were designed primarily with classroom use in mind; • some can only query corpus files that contain data in Latin alphabet format, while others are Unicode-compatible, i.e. can accommodate any writing system; • some can handle regular expressions (cf. Chap. 9), while others only allow simple searches; • some tools are simple concordancers, others include many other functions such as generating frequency lists, collocate and n-gram lists, and visualization tools, to name but a few.

The list below is not comprehensive in at least three ways: firstly, the tools listed below are all for offline use-there are web-based concordancers such as the Sketch Engine that either allow access to specific corpora, or let the user upload data for online examination. 1 Secondly, we only list monolingual concordancers, i.e. tools that let the user examine text from one corpus representing one language. There are also multilingual concordancers specifically designed to query parallel corpora, i.e. corpora that contain data from multiple languages in their direct translations (see Chap. 12). Furthermore, it is worth pointing out that in research, there is a growing trend away from ready-made concordance tools and towards writing and adapting scripts written in programming environments like Python or R (e.g.

Further Reading

Partington, A. 1998. Patterns and meanings: using corpora for English language research and teaching. John Benjamins, Amsterdam and New York.

Partington presents a series of case studies that illustrate how corpus methods can shed light on diverse areas like synonymy, cohesion, and idioms; analysis of concordances plays a major role throughout.

Sinclair, J. 1991. Corpus, concordance, collocation. Oxford University Press,

Oxford.

An early introduction to corpus linguistics written for students in language education.

Stubbs, M. 2001. Words and phrases: corpus studies of lexical semantics.

Blackwell, Malden MA.

Stubbs outlines how the meanings of words depend on their contexts, and how the connotations of words arise from their recurring embedding in larger phrases.

Chapter 9

Programming for Corpus Linguistics

Laurence Anthony

Abstract This chapter discusses the important role of programming in corpus linguistics. The chapter opens with a history of programming in the field of corpus linguistics and presents various reasons why corpus linguists have tended to avoid programming. It then offers some strong counter arguments for why an understanding of the basic concepts of programming is essential to any corpus researcher hoping to do cutting-edge work. Next, the chapter explains the basic building blocks of all software programs, and then provides a number of criteria that can be used to assess the suitability of a programming language for a particular corpus linguistics project. To illustrate how the building blocks of programing are used in practice, two case studies are presented. In the first case study, basic programming concepts are applied in the development of simple programs that can load, clean, and process large batches of corpus files. In the second case study, these same concepts are applied in the development of a more advanced program that can replicate and, in some ways, improve on the tools and functions found in ready-built corpus tools. The chapter finishes with a critical assessment and discussion of future developments in corpus linguistics programming.

Introduction

Computer programming has played a key role in corpus linguistics since its growth in the 1960s. Early researchers did not have access to any existing corpusanalysis software. As a result, they had to build simple tools from scratch in programming languages such as Fortran and COBOL. This work led to the creation 182 L. Anthony of concordancers, such as those of

On the other hand, when corpus linguists are asked what they would want to do with corpora (ignoring what is possible with existing ready-built tools), it is clear that a much wider range of functions are desired (see

• the automatic creation of clean, annotated corpora • the comparison of two or more texts across multiple lexical/grammatical/rhetorical dimensions • the batch counting of words, phrases, and n-grams (lexical bundles) to give pertext frequency and dispersion information • the calculation of lexical diversity scores for each file in a learner corpus • the measurement of distance between priming words and target words • the identification and counting of collocates of certain target word types in target and reference corpora • the extraction of complex meaning units such as definitions • the counting of interesting lexical phenomena, such as disfluencies, in a tagged corpus • the automated analysis of the rhetorical structure of texts (see

Corpus linguists may also choose not to learn to program for other, more mundane, reasons. One is that corpus linguists, especially those working in academic institutions, are likely to be pressured for time (see

Although there are several reasons why a corpus linguist may not want to learn to program, there are also important reasons why knowledge of programming can benefit a corpus linguist greatly.

Continuing from the ideas expressed in

Fundamentals

The Basic Building Blocks of Software Programs

Software programs are built using computer programming languages, i.e., simple, artificial languages that are designed to enable humans to issue instructions to computers in a non-ambiguous way. The instructions written in a computer language consist of statements formed from a vocabulary of concepts recognized by the computer (e.g. variable names, variable values, mathematical operators, string operators, data types, and so on) arranged in a particular order as defined by the syntax of the language. If the statements are correctly formed, they can be parsed by a compiler or interpreter built into the computer language and converted into a machine language used internally by the computer. Complex instructions can be created by combining individual statements into larger blocks of code following structures that are also predefined by the computer language (e.g. flow structures, functions, methods, objects, and classes).

Computer languages can be considered as members of particular language families if they share a similar vocabulary and syntax rules. The most popular languages in the world today are all part of a family of computer languages that evolved from the C language, which was developed towards the end of the 1960s. 'C-like' languages include C, C++, C#, Objective-C, Java, JavaScript, Perl, PHP, Python, and many more. Other well-known languages that have given rise to families of related languages include BASIC, Fortran, Pascal, Lisp, Prolog, Smalltalk, and S, the modern equivalent of which is R.

As with human languages, learning one computer language in a family can be hugely beneficial when learning other languages in that same family. This is one reason why many degree courses in computer science first focus on the C language (or a related 'C-like' language). However, all computer languages share many common vocabulary and syntax features. So, learning any computer language will be hugely beneficial to a corpus linguist if and when it becomes necessary to learn another computer language later. Some common features of computer languages are listed in Table

Although all computer languages are similar in many ways, they are each designed with certain features and idiosyncrasies that offer advantages (as well as disadvantages) over other languages in particular settings. For example, C is a lowlevel language, i.e., one that closely resembles the computer's core instruction set. This feature allows it to run very quickly and efficiently, but also makes it quite difficult to read and understand. In contrast, JavaScript, Perl, Python and R are highlevel languages (ones that more closely resemble normal human languages), making them easier to read and understand, but also making them slower and less efficient than C.

Table 9.1 Common features of computer languages

Data types

Boolean, number, string, list, key/value pairs (often termed "dictionary" or "hash") Operators arithmetic ("+", "-", . . . ), comparison ("<",">", . . . ), condition ("if", "when", . . . ), logical (e.g. "OR", "NOT", . . . ), string (".", "+", "eq", . . . ) Control and loop flow if, while, for/foreach Modularization functions or classes/objects or both Input/output print, read, write L. Anthony

Perhaps the most important feature that distinguishes one programming language from another is the way that it handles the grouping of statements into larger units. Here, there are two main approaches. One is a Functional Programming (FP) approach, where statements that are designed to instruct the computer to perform an action (e.g. sorting) are grouped into a self-contained "function" that does not rely on any external state (e.g. variable values). The other is an Object-Oriented Programming (OOP) approach, where the instructions to perform an action as well as the variables on which that operation are performed are all grouped into a single "object class". FP programming generally allows for the easy addition of new functions to a program but keeping this growing set of functions organized and working well with existing and new variables can become confusing and error prone. OOP programming, on the other hand, requires more thought when deciding which variables and actions (methods) should be included in objects. However, creating and organizing new objects is relatively simple and because each object class is independent of the others, error checking is easier.

Computer languages are designed from the ground-up to facilitate programming using one or both of these design approaches. C, JavaScript, Perl, and R are examples of languages designed for functional programming, whereas Java is an example of a language designed for object-oriented programming. Python is an example of a language that was designed to accommodate both programming approaches. In practice, current software engineering practices tend to favor an OOP approach as it allows programs to scale well and lends itself more easily to the modularization of code that is developed in a team. As a result, many functional programming languages, such as C, Perl, and R, have been adapted or extended to allow for some kind of object-oriented coding, although the designs that have been employed are not always elegant or efficient.

Choosing a Suitable Language for Programming in Corpus Linguistics

All modern programming languages can be used to develop programs that will fulfill the needs of corpus linguists, whether they are MA students, PhD students or seasoned experts in the field. However, some languages are more suited to the tasks commonly carried out by corpus linguists, i.e., reading large text files into memory, processing text strings, counting tokens, calculating statistical relationships, formatting data outputs, and storing results for use with other tools or for later use. In order to decide which language is most suitable for a particular corpus linguistics programming task, there are at least four important factors to consider at the outset. The first consideration is the design of the language. High-level, functional languages are well suited for writing short, simple, "one-off" programs for quick cleaning and processing of a corpus, e.g. renaming files, tokenizing a corpus, cleaning noise from a particular corpus, and calculating a particular statistic for corpus data. For this reason, languages such as Perl, Python (which can run as a functional language), and R have been commonly used for corpus linguistics applications (e.g.

A second consideration is whether to use a language that is converted into machine language 'on the fly' during runtime (usually referred to as an 'interpreted' language) or one that is converted (or compiled) into machine language prior to runtime (usually referred to as a 'compiled' language). Interpreted languages, such Perl, Python, and R allow for programs to be "prototyped", meaning that developers can create working programs with placeholders marking yet-to-bewritten or unfinished code. They can also be used in a 'live', interactive way, with new lines of code written in response to the output generated by the interpreter. This makes them particularly suited to many MA and PhD student projects, where development speed and flexibility are important factors. They are also useful for carrying out some of the 'quick and dirty' programming tasks that a corpus linguist might need to do in order to get their data into a form that can be analyzed. Compiled languages, such as C and Java, on the other hand, require a slower "write-compilerun" design. This process can reduce the number of bugs in the final code (as they can be detected at compile time) and allow the code to run faster than equivalent code written in an interpreted language. For these reasons, compiled languages are often chosen for very large projects, where speed or accuracy are required, such as the engine of the IMS Open Corpus Workbench (CWB) (

A third consideration is whether the programming language is designed primarily for creating web-based tools, traditional desktop tools, or mobile platform tools. Some languages, such as PHP and JavaScript, have many features designed specifically for the web, with CQPweb using PHP extensively on the server side, and SketchEngine using JavaScript heavily for its interface. In contrast, languages such as Java, Perl, Python, Ruby, and R have features tailored for both web and desktop environments, making them a common choice for many corpus tools. As an example, AntConc

The final and perhaps most important consideration is the size and vibrancy of the community that supports a computer language. 'Popular' languages such as C, C++, PHP, Python, and R have large, active groups of core developers that continually develop and improve the code base. These languages also have a large group of active users who are willing to answer questions and offer advice on coding through forums such as StackOverflow (

Representative Study 1

Edberg, J., and Biber, D. 2019. Incorporating text dispersion into keyword analyses. Corpora.

In this paper, the authors aim to determine which keyness measure best identifies words that are distinctive to the target domain(s) present in a corpus (cf. Chap. 6). To achieve this goal, they compare various keyness measures that are primarily based on the frequency of words in a corpus together with their own "text dispersion keyness" measure, which is based on the number of texts in which a word appears.

In order to carry out the analysis, the authors use Python scripts to calculate a traditional keyness measure based on log-likelihood, as well as two variations, each with different minimum dispersion cut-off values. They also use a Python script to calculate their own "text dispersion keyness" measure and compare all four of these measures against the "Key keyword analysis" measure available in WordSmith Tools

(continued)

9 Programming for Corpus Linguistics 189 In this project, the authors are working in a small team and are probably using one-off scripts that are unlikely to be extended further. Therefore, any scripting language would probably be sufficient for the purpose with Python being an excellent choice. Using Python scripts as part of their analysis, the authors are able to quickly and accurately compare a range of possibilities for calculating keyness, including their own "text dispersion keyness" measure, which is not yet available in ready-built tools. The results from the study not only show the relatively merits of using dispersion as part of a keyness measure, but also suggest an important extension that can be added to readybuilt tools.

First Steps in Programming

The following case studies are designed to contextualize the previous discussion and show how programming languages can be used to carry out some of the most common and important tasks carried out by corpus linguists. Although the majority of examples presented in these case studies mimic the functionality of existing ready-built corpus tools, they will generally perform much faster because they are designed to carry out a specific task and they remove the overhead of creating and updating an interface. It should also be noted that they can be easily extended or adapted to carry out tasks that are quite difficult or impossible to achieve with the most popular tools used in corpus linguistics today. Script 4 in Case Study 1 illustrates this point.

The programming language used in the case studies is Python. As described earlier, Python is a high-level, object-oriented, interpreted programming language that can be used to build web applications and also desktop applications for Windows, Macintosh, and Linux. It has a very large, vibrant community of core developers and users, and was ranked the second most popular programming language in the world in 2019 (StackOverflow 2019). In the survey, only JavaScript ranked higher, perhaps due to its common use for building web-based applications. Python also has a huge number of user-and company-developed extensions that add important features on top of its rich core functionality. For example, the Pandas extension allows Python to carry out advanced statistical measures that can be visualized using one of many visualization packages, such as Matplotlib or Seaborn. These features have resulted in Python becoming one of the most popular languages used in corpus linguistics work today. Interestingly, the importance of Python in corpus linguistics work might possibly grow as it is one of the most popular languages used to develop artificial intelligence (AI) and deep learning applications due to its rich number of natural language processing and machine learning libraries.

L. Anthony

For reference, equivalent scripts written in the R language are also provided by the second editor, St. Th. Gries, at

In order to follow the case studies and implement the code samples presented, the following steps should first be completed:

1. Set up the target computer to run Python scripts This is a trivial task that simply involves downloading the latest version of the software and running the installer with default settings (

Case Study 1: Simple Scripts to Load, Clean, and Process Large Batches of Text Data

We will start by assuming that a very simple, UTF-8-encoded, plain-text text file called "text_1.txt" is needed to be processed. It contains a single line of text: "The cat sat on the mat."

Loading a Corpus File and Showing its Contents

Script 1 is a short script that loads "text_1.txt" and shows its content in the console window (on Windows) or terminal (on Macintosh/Linux).

Script 1: Load a File and Show Its Content

Script 1 is just four lines long. However, it illustrates some interesting and useful features of a modern, object-oriented language. First, the script shows that the most important components of the language are stored in classes that are loaded when needed. Here, on line 1, the program imports the Python "Path" object class from the pathlib library. This is used to create "Path" objects that automatically adjust the string parameter to match the formatting rules of the operating system. The actual "Path" object is created on line 2 and given the name "corpus_file" (for convenience). Second, the script shows that objects, such as "Path", have associated methods that can be accessed using a dot notation. As an example, on line 3, the "read_text" method of the "Path" object is called using "corpus_file.read_text()" in order to read the content of the file into memory. Then, the content of the file is printed to the screen in line 4 using a Python core "print" function. A third interesting feature is that the user-defined variables in the script (i.e., "corpus_file" and "file_content") have long, meaningful names making the script easy to read and understand. The variables could just as easily be named "cf" and "fc" as long as they did not clash with reserved names used by the Python core. However, such names would be confusing and easily forgotten, especially if the script was not used for several weeks. Importantly, the Python core object classes and functions also have long, meaningful names, which is one reason why it is a commonly recommended first language to learn.

While Script 1 can be said to "work", it includes some deliberate weaknesses that should be avoided when creating programs any longer than this. First, the code contains no comments (lines of code often prepended with a hash character that are ignored by the interpreter but can be read by humans). These are useful for "self-documenting" the code (i.e. adding documentation directly within the code) so that anyone reading the code later (including the original coder!) will understand its design choices. The code also contains no whitespace to divide up the different parts of the code, which again improves readability. Third, the code is written as a series of commands rather than grouped together in a well-formed function (or L. Anthony object class). As a result, the code cannot easily be recycled and used in other scripts. This writing style also leads to confusing code that is prone to include bugs.

Script 2 shows a more useful version of Script 1 written as a function with comments and whitespace to improve readability and its likelihood of future use: Script 2 is comprised of three parts: First, the necessary object class is loaded (line 2). Second, the main function is defined (lines 5-11); Third, the function is launched with the path to the file as a parameter (line 14), and the returned result is shown (line 15). Importantly, the function "get_file_contents" is completely encapsulated (e.g. no outside parameter values are hard-coded into the function), so it can be used with any corpus file in any program. As an example, to show the contents of a file called "text_2.txt" only the one line of code that launches the function (line 14) needs to be changed: L. Anthony

The following output produced by Script 3 is shown in the command window or terminal:

The Cat Chapter 1 Once upon a time, a cat sat on a mat. Script 3 differs from Script 2 in only four places:

• The "BeautifulSoup" object class from the bs4 Python extension library is loaded in line 3. This library is used to parse HTML files and extract plain text. • An additional parameter, "file_type", is created for the function with a default value of "text". This is introduced so that the script can handle both plain-text and HTML files (line 6) • A conditional expression is added to the function to parse files using a "Beauti-fulSoup" object if they are signaled to be HTML files (lines 12-18) • The extended function is now called with two parameters: (1) the path to the file, and (2) the file type (line 21)

Loading a Web Page, Cleaning it, and Showing Its Contents

Script 3 can be easily adapted further to download an HTML webpage, clean it, and show its text contents. To do this, a "request" object from the urllib Python core library needs to be utilized. This object essentially serves as a mini Web browser, which can access servers and webpages, and download the content for later processing. The adapted script is shown as Script 4, with the statements in lines 9 and 10 serving to download and read the contents of a webpage into memory for processing. Line 19 is then used to call the new function with the specific URL address of the desired webpage. 17 18 # launch the function 19 file_contents = get_url_contents(url = '

Loading an Entire Corpus and Showing its Contents

We can extend Script 3 in a different way to process an entire corpus. To do this, we create a new function ("show corpus") that finds the paths of all the corpus files in a folder (e.g. "target_corpus"), calls the "get_file_content" function on each file path, and then prints out the output of each file (lines 21-28). Of course, this script is not very useful on its own, but it can be extended to form the foundation of a complete corpus toolkit, as described in Sect. 9.3.2.

Case Study 2: Scripting the Core Functions of Corpus Analysis Toolkits

For this case study, imagine that a toy corpus that comprises just three UTF-8encoded, plain-text files needs to be processed. Each corpus file contains a single, short sentence and the whole corpus is stored in a folder called "target_corpus" in the project folder. The details of the "target_corpus" are given in Table

Analyzing such a small corpus can perhaps be done by hand or with a calculator. However, when developing computer programs that can analyze corpora of many thousands, millions, or even billions of words, it is often useful to test the scripts being developed with these simple examples that can be calculated exactly. This makes it possible to check if the code is running correctly and ensure that no bugs have been inadvertently introduced.

Creating a Word-type Frequency List for an Entire Corpus

To create a script that produces a word-type frequency list for a set of plain-text corpus files, we can first utilize the "get_file_contents" function of Script 2 (or Script 3 if we want to process HTML files). Then, we only need to adapt the "show_corpus" function from Script 5 to process each file and count all the words in the corpus. Script 6 shows the complete program. The following output produced by Script 6 is automatically saved in a file named "word_type_frequency_list.txt".

Word

Frequency Word Frequency the 5 dog

Script 6 introduces two new object class imports: "Counter" from the Python Core collections library, and "findall" from the regex Python extension library. The "Counter" object is a very fast and memory efficient "key-value" data structure that is used to store the word types as keys and their growing frequencies as values as each corpus file is processed. The "findall" function is used to find all the tokens in each file. This function uses a widely used and powerful search language called "regular expressions", which is used to define very precise search conditions based on four core concepts listed in Table

From Table

At the end of the script, the "create_word_type_frequency_list" function is run with suitable parameter values (line 45-49). Here, the regular expression [\p{L}]+ is used for the token definition, which translates as "a series of one or more characters in the Unicode "Letter" character category" (

Script 6 is a very fast, efficient, and fully-featured program that can process and produce a word-type frequency list for the 1-million-word Brown corpus

Creating a Key-Word-In-Context (KWIC) Concordancer

To create a script that produces a classic Key-Word-In-Context (KWIC) concordancer, again, we can utilize the "get_file_contents" function of Script 2 (or Script 3 if we want to process HTML files). We then adapt the "show_corpus" function from Script 5 to process each file and output a KWIC result for every search hit that is found. Script 7 shows the complete program. The following output produced by Script 7 is automatically saved in a file named "kwic_results.txt".

The cat sat on The cat sat on the at sat on the mat. The cat chased ed at the cat.

Script 7 does not require the "Counter" object class from the Python Core collections library or the "findall" from the regex Python extension library. However, it does use the "finditer" and "sub" functions of the regex library (line 3). The "finditer" function performs similarly to the "findall" function, but produces results iteratively, one-by-one, allowing them to be saved to a file immediately. The "sub" function, on the other hand, is used to find and replace (substitute) strings using a regular expression. This function is used to clean the KWIC results by removing unwanted line-break characters from the results. The call to the regex library also includes an "IGNORECASE" flag which tells the regex library to ignore case in searches.

The main, "create_kwic_concordance" function is designed to accept five parameters: (1) the corpus location, (2) an option to ignore case when searching, (3) the search term, (4) a context size that defines how many tokens to the left and right of the search term will be shown in the KWIC results, and (5) a file path for the results file (lines 15-20). The function then performs just three main actions. As in the previous scripts, it first defines a "target_corpus_reader" Path object (line 22). Next, it creates a file handle that is used to output the results as they are generated and a variable to process the case option (lines 28-33). Finally, it performs the main action of the function, i.e., locating search term hits in the corpus files (via "finditer") and generating KWIC results for each of them (lines 35-51).

At the end of the script, the "create_kwic_concordance" function is run with suitable parameter values (lines 54-59). Here, the "search" parameter is given as 203 r"\b[\p{L}]+at\b", which is a regular expression that translates to "any string of Unicode letter characters that is immediately followed by "at" and starts and ends with a word boundary". This leads to results that include "cat", "mat", and "sat". The "context_size" parameter is set to 10, which leads to KWIC results with 10 characters of context to the left and right of the search term.

Script 7 is again a very fast, efficient, and fully-featured program. It can create the nearly 70,000 KWIC results for the word "the" in the 1-million-word Brown corpus (a notoriously slow search) in under 1 sec on a modest computer. This is very much faster than most desktop corpus analysis tools, which have to deal with color highlighting and other display issues. It can also search for words, phrases, or full regular expressions with case-sensitivity, and produces hits with any amount of surrounding context desired. Perhaps surprisingly, the script comprises just 59 lines of code, and again, most of these are in the form of whitespace and comments. One limitation of the program, however, is that it does not sort the results. Adding this functionality would require an additional sorting function. Or the sorting could be carried out later in a spreadsheet software tool, such as Excel.

Creating a "MyConc" Object-Oriented Corpus Analysis Toolkit

Script 8 recreates the functionality of Scripts 6 and 7 in a single object class called "MyConc". As discussed earlier in this chapter, object-oriented programming offers numerous advantages over functional programming, especially for largerscale projects. In this case, we could use the MyConc class as a foundation for a more complete corpus toolkit that could be released as an open source project allowing it to be used and extended by others.

Critical Assessment and Future Directions

Learning to program with a computer language is certainly not an easy task. As with learning to use a human language, it requires study, practice, and perhaps most importantly, a genuine need. There is also an aspect of creativity and beauty in computer language use that mimics that of human languages. Some programs may "work" but they are short, abrupt, and difficult to understand. Others may be long and overly complex. This raises an aspect of programming that is often forgotten: Computer programs must be understood by humans. The first human that needs to understand the code is the developer, especially when they return to the code months after the original project in order to fix a bug or add a new feature. Other humans are likely to see the code, too. If the program is written as part of a funded project, at some point, the developer might leave requiring others to take over the work. If the program is part of an open-source project, many people might want to contribute to the code. There is also a growing requirement by journals and funding agencies to make programs open access in order to facilitate the replicability and reproducibility 204 L. Anthony of research results

The scripts presented in this chapter are designed to illustrate good programming habits. However, they are limited in terms of scope (e.g., only two core functions of corpus analysis toolkits are presented) and functionality (e.g., the KWIC tool does not include a sorting function). Fortunately, corpus linguists who are interested in further developing their programming skills have an abundance of learning resources available to them. There are numerous MOOCs (Massive Open Online Courses) offered online, as well as specially prepared web-based courses and tutorial guides. One notable course for Python, for example, is the tutorial offered by the w3resource team (see Sect. 9.5). The main site for asking specific questions about programming, as well as seeing code samples that have been posted in response to questions, is StackOverflow (again, see Sect. 9.5). This is a truly vital resource for anyone seriously considering to entering the world of programming.

It is highly likely that at some point in a corpus linguist's career, they will need to develop custom scripts to investigate their unique research questions. As discussed here, one strategy is to write these scripts directly. However, another possibility is to work with an expert programmer. In the latter case, it is important that the language of programming does not get in the way of communicating what the task should be. Corpus linguists should avoid trying to explain to the programmer how the task should be completed, e.g., saying that they want the programmer to create a program that opens each file, tokenizes the content, and then counts the frequencies of each word. Rather, they should explain what they want, e.g. an ordered list of important words in the corpus. Through discussions, the precise meaning of "important" can be clarified, as well as the best way to order the list.

One danger when working with computer programmers is becoming overwhelmed by the amount of programming terminology that tends to appear in their conversations. To some extent, the discussions on different programming languages and the functional and object-oriented programming paradigms given in this chapter should help to demystify some of the terminology that may be used. Of course, most programmers are very willing to explain what they mean, so the corpus linguist should always ask for clarification where necessary.

Tools and Resources

Numerous tools and resources exist to help novice programmers download, install, setup, and use a programming language. The list that follows targets the Python and R programming languages, but a simple Internet search will produce resources that can fill the gaps for other languages

Downloading, Installing, and Setting Up the Programming Language

• Getting started with Python:

• Getting started with R:

Online Communities for Programming (Including Corpus Linguistics)

• StackOverflow:

• Python Software Foundation:

• StatForLing with R:

Accessed 31 January 2020.

Packages to Allow Python and R to Interact with Each Other

• The "rpy2" Python package to access R scripts from Python:

Introduction

Ever since its philological beginnings, diachronic linguistics has relied on corpus data. Past lexicogrammatical patterns are not accessible through speaker intuition or experimentation, but have to be reconstructed on the basis of the written historical record. Historical linguists therefore have always had to take recourse to collections of texts or collections of quotations. However, with the advent of electronic corpora, the speed and systematicity with which diachronic -like synchronic -records can be queried has increased tremendously, opening up new research possibilities by dramatically facilitating at least some aspects of data collection. This is, in fact, a continuing trend, as historical corpora continue to grow in number and size, and as the techniques for interrogating them become both more efficient and more sophisticated. At the same time, barring corpora of very recent history, diachronic data will always present us with important problems, failing to represent all the variation associated with the users and contexts of use, with especially spoken language as the perennial gap. Despite justified enthusiasm about the recent advances in diachronic corpus linguistics, such concerns must be taken into account in corpus design as well as in the development of heuristic techniques to crack the code of past language systems and to explain variation and change.

In the most general terms, our plea here is one for informed use of diachronic resources. For end users to make appropriate use of a corpus it is instrumental that they understand how it has been compiled. The corollary of this is that compilers should set out their compilation procedures accessibly and explicitly. At the same time, it would be beneficial for the research community as a whole to consider ways in which historical resources can be built and enriched in a more dynamic and bottom-up way, to ensure they are maximally adaptable to specific research needs as well as being more responsive to newly gained insights

Fundamentals

Issues and Challenges of Diachronic Corpus Compilation

There is one thing that all diachronic corpora have in common: the usage data they contain is at least organized along the temporal dimension, such that comparison across earlier and later manifestations of a language becomes possible. Apart from that, diachronic corpora differ widely in size, composition, scope, annotation, and the nature of their textual material. Because of the limitations of the historical data available, and because research goals are very diverse, there is no such thing as an ideal historical corpus. Specific requirements of diachronic research simply need to be met in different ways. Nevertheless, there are recurrent challenges that both compilers and end users of diachronic corpora have to confront.

As diachronic corpora are typically used to study language change, and language change is generally understood to arise from and give rise to language variation, it is something of a bitter irony that one of the greatest difficulties diachronic corpora face lies precisely in capturing historical variation. This holds for both major dimensions of variation, which -following

Identifying the Lectal and Diatypic Properties of Texts

In addition to being organized along the temporal dimension, diachronic corpora often include information on other lectal and diatypic properties of the texts they contain. Identifying these properties, however, may be difficult for historical texts. The challenges start with the identity of historical authors, which more often than not is something of a mystery. Added to that there are the complexities of textual transmission. Think, for instance, of scribal interference in mediaeval texts or editorial interference in more recently published materials. Tellingly, when creating the Helsinki Corpus of English texts, which was the first diachronic electronic corpus of any language, its compilers found themselves forced to provide many of the older texts with multiple period labels, as the best way to indicate both a text's (approximate) manuscript date and its creation date

Of course, not all historical texts pose these difficulties, and some offer special opportunities. Letters, for instance, have several advantages as a source of historical text material. They are often precisely located in time and space, can often be unambiguously assigned to a single author, have in their addressee a clearly identifiable target audience, and may represent speakers who have otherwise left no written records. A corpus that has used this type of data to the best advantage is the Corpus of Early English Correspondence, compiled by Terttu Nevalainen, Helena Raumolin-Brunberg and a team of collaborators (Raumolin-Brunberg and Nevalainen 2007). Not only does the corpus contain letters from the sixteenth to eighteenth century, it also covers four major English regions, as well as giving information on the social rank of the letter writers and how they relate to their addressees. Providing this type of social background information of course poses new challenges, requiring a thorough knowledge of the social structure of past societies. The compilers eventually arrived at a very elaborate coding scheme to describe the letter writers in their corpus, using 27 different parameters, some with very open information. Parameters include, for instance, the letter writer's gender, year of birth, occupation, rank, their father's rank, their education, religion, place of residence, any history of migration, their careers and social mobility, the general type of contents of their letters, and how well their letters can be authentified. Additional information on a letter writer, if potentially relevant, is included in open comment boxes. In providing all this information, the compilers clearly chose to collect as much metadata as possible. As a result, it is to an important extent also up to the end user to interpret the complexities of historical reality.

On the whole, however, detailed background information on historical texts is often impossible to come by directly. While in such cases healthy agnosticism remains a sensible default option, it is good to be aware that avenues towards possible solutions are currently being explored. Particularly in the domain of authorship, automated stylometric techniques now allow probabilistic identification. An example is the authentication of the writings of Julius Caesar by

Another recent development promising new insights into lectal variation is the increasing access offered by very large corpora to individual variation. One example is the Hansard Corpus, compiled by Jean Anderson and Marc Alexander, which contains the proceedings of the British Houses of Parliament from 1803 to 2005 and represents nearly 40,000 individual speakers. Corpora built from Parliamentary proceedings may become immensely valuable to historical linguists and sociolinguists for a number of reasons

Redressing Historical Bias

While classifying and contextualizing available text material will always pose difficulties, many of the problems diachronic corpora face do not come from the 10 Diachronic Corpora 215 texts they have, but from the texts they do not have. Depending on the historical period at issue, the historical record is patchy to a greater or lesser degree, but in practically all cases it is severely biased. The voices of the less powerful and/or less literate strata of the population are as a rule unrecorded, and the texts that do reach us are -prior to the large-scale employment of modern recording equipmentbiased to the written mode. Considering that direct spoken interaction and patterns of social stratification are believed to be crucial to the emergence and transmission of linguistic variants, this is of course a frustrating situation. Moreover, this is one area where, despite their merits, current big data projects may (for now) be exacerbating the problem by their tendency to go where the data is and harvest text materials indiscriminately. Typically, it is corpora making the most of unique but limited and less accessible resources that are best placed to redress the biases in the historical record. A good example is the Corpus of Early English Correspondence, already discussed above. Another example is the data set used by

It is instructive here to consider in some more detail another substantial effort to create a diachronic corpus that contains -at least by proxy -socially stratified spoken interaction. The Old Bailey Corpus (OBC2.0), compiled by Magnus Huber and his team

Though limited to recent periods, another way to counter the biases in the historical record is of course the compilation of diachronic corpora containing actual audio-recorded speech (see Chap. 11 for more information about spoken corpora). There are several ways to pursue this goal. First, corpora of spoken usage began to be compiled in the second half of the twentieth century. Initially intended as representative of contemporary usage, these corpora are now gradually becoming historical corpora and it is to be hoped that current and future researchers will be willing to repeat the efforts of their predecessors to create new contemporary and comparable spoken corpora. One such effort is the recent creation of the Spoken BNC 2014, whose structure echoes the spoken component of the British National Corpus, originally released in 1994

Diachronic Comparability

The problem of gaps and biases in the historical record is further complicated by the temporal dimension of diachronic corpora. Not only is it difficult to approximate the full range of synchronic variability for a language at a given point in time, there is a further difficulty in doing so without compromising diachronic comparability. For example, given a rich historical record and a long tradition of high-quality text editions, it is perfectly possible to create a sizeable corpus of Old French, as shown by the FRANTEXT database. Yet it is impossible to create a corpus of Old French that is comparable in any straightforward way to a corpus of Present-day French. The reason is, put simply, that there is no Old French equivalent to a Presentday French newspaper, just as there is no Present-day French equivalent to an Old French epic poem. Similarly, it is possible to create a sizeable corpus of Latin, with a time-depth of over 2000 years, as shown by the 13-million-word LatinISE corpus, built by Barbara McGillivray. However, even despite efforts to preserve genre balance, any diachronic comparison across this timespan has to take into account that the language used in such a corpus changes from a relatively standardized written language with more or less direct roots in a contemporary vernacular to a functionally impoverished and much more writing-dependent language used mainly in religious contexts and as a lingua franca among European scholars and scientists.

More generally, because the conditions under which language is produced are themselves subject to change, it is fundamentally impossible to compare linguistic 10 Diachronic Corpora 217 material only along its temporal dimension. In this respect, historical linguists are always comparing apples and oranges. Arguably, all major diachronic reference corpora, though often striving to produce stratified samples of language use across time, suffer from this problem.

One possible response is to ignore the issue and simply include material as exhaustively as possible. This approach prioritizes coverage of synchronic variability, to the best level achievable and with no prior assumptions made. Especially where the body of historical data is finite, disparate and severely biased, or where a corpus is to be used to study change over very long time periods, this is a perfectly defendable strategy. An example is the Dictionary of Old English Corpus (compiled by Antonette di Paolo Healey and colleagues), which exhausts all Old English texts available down even to the odd Runic inscription. Another example is the Oxford Corpus of Old Japanese (compiled by Bjarke Frellesvig and colleagues).

A completely different response to the issue of comparability is to create singlegenre diachronic corpora that cover relatively short time spans and draw their data from a single historical source. The OBC2.0 or the Hansard Corpus, both already discussed above, are good examples. The single-genre approach has the advantage of improving diachronic comparability, but comes at the expense of coverage, both with respect to synchronic variability and time-depth. Moreover, although diachronic comparability improves, it may still not be optimal, because genres themselves tend to be moving targets. Consider again trial proceedings, as represented in the OBC2.0. The recording of trial proceedings in the Old Bailey started outside the actual control of the court, as publishers were commercially interested in the more sensational cases and sent out their scribes to record them. But in the course of the eighteenth century the proceedings gradually developed into official true-to-fact records. Another example is found in De Smet and Vancayzeele (2014), who show that eighteenth-century English narrative fiction contains far fewer action sequences and has longer descriptive passages than later narrative fiction. In a diachronic corpus of narrative fiction this inevitably affects the frequencies of specific grammatical patterns associated with either descriptive or more action-driven narrative passages. Especially over longer time spans, it is virtually impossible to keep genre -or, for that matter, any other diatypic parameters -constant.

Besides awareness of these difficulties, a more bottom-up and data-driven approach to describing diatypic text properties may, in the long term, provide the more satisfactory solutions. Dimensions such as 'spokenness' or 'formality' can be operationalized and measured on a text-by-text basis from linguistic properties

To apply such methods more systematically and across long time-spans will require further research, but it at least allows researchers to position texts relative to one another and to the contemporary norm on one or more dimensions of interest. The implication is again that responsibility for interpreting the structure of a corpus moves from the corpus compiler to the researcher. It also means that it may be necessary for corpora themselves to become the object (rather than just a means) of study.

Issues and Challenges of Text-Internal Annotation

Turning from the level of the texts that make up a corpus to the internal properties of those texts, perhaps the most fundamental question compilers and users of diachronic corpora must ask is to what extent they can rely on methods devised for the annotation and analysis of contemporary data in handling data from older periods. Older texts are in principle somewhat alien. Their writing conventions differ from present-day practices and, obviously, the very language they represent is different from any present-day variety. Added to this is a layer of inadvertent 'noise' created along the way as a corpus text travels from historical manuscript or print to digital edition. All of this complicates even the most basic analyses, including the identification of lexical items, grammatical classes and grammatical structures. Nevertheless, corpora whose texts have been annotated with lexical and grammatical information can of course be extremely valuable tools for research.

The most straightforward problems are the strictly technical issues. The development of new techniques of corpus analysis is often spearheaded by research on contemporary performance data. To extend such techniques to older data requires both circumspection and additional technical know-how. For example,

Solving technical problems of course pertains to only one side of the issue. More fundamental are matters of linguistic analysis proper, which present all the problems associated with lemmatization, tagging and annotation of synchronic data to a much higher degree (see Chap. 12 for a general introduction to corpus annotation; see again Piotrowski 2012 for discussion of the various techniques, e.g. for part-ofspeech tagging and syntactic parsing). For this reason, rich annotation is best seen only as a means to facilitate querying a corpus.

Consider, for example, the York-Toronto-Helsinki Parsed Corpus of Old English Prose (YCOE), developed by Ann Taylor and collaborators

While exciting and impressive, it is nevertheless good to bear in mind that the syntactic annotation in YCOE is indeed a tool. That is, it is not meant as a final analysis of all the sentences in the corpus but as a means to retrieve with greater ease data that is of potential research interest and that is otherwise nearly impossible to collect. The syntactic annotation scheme is a simplified and in some ways deliberately agnostic version of generative X-bar theory. Optimal precision and recall are certainly not guaranteed (see Chap. 2 on precision and recall). As regards precision, results collected automatically from the corpus should be manually checked to make sure they actually include what the researcher is looking for. As Taylor herself points out, there may be "a strong temptation to skip this relatively time-consuming step", but it "must be manfully resisted"

Finally, any effort at creating richly annotated corpora runs the risk of obscuring existing patterns in the data. It is not obvious, for instance, that Old English authors had a concept of sentences that is exactly comparable to the notion of sentence assumed by the formal theory underlying the parsing in YCOE -Old English punctuation, in any case, suggests otherwise

Issues and Challenges Specific to the Analysis of Diachronic Corpora

From the design properties of corpora and their texts, we move to the actual use of diachronic corpora for research. Methods of corpus interrogation will be affected by how linguistic organization is conceived. This holds a fortiori for the complex interrogation of diachronic corpora. The tenet that lexicon and grammar form an integrated continuum of form-meaning pairing has gained ground to the point of probably forming the majority position in current corpus linguistics. In this section, we will discuss a number of heuristic techniques that can be used to interrogate the syntagmatic-paradigmatic organization across the whole lexicogrammatical continuum, and we will illustrate their potential relevance to diachronic corpus linguistics.

It is generally recognized that, at the lexical end of this continuum, major methodological progress has come from

With collostructional analysis

Further towards the grammatical end is colligational analysis, which is now mostly implemented in

(1) While you were in the middest of your sport [ . . . ] (OED, a1548) (2) [ . . . ] when you are in the middle of loving me. (CLMETEV, 1873)

Firstly, predicative and existential matrices are distinguished from each other by the different diachronic realization of the position syntactically enclitic with the finite matrix verb. In predicative matrices, this paradigmatic distribution is zero

(3) Micele mare wundor is þaet he wolde beon mann on þisum life 'Much greater wonder it is (lit: is) that he wanted to be a human in this life' (YCOE, 950-1050) (4) þaet is wundor, þaet ðu swa raeðe forhaefdnisse & swa hearde habban wilt.

'that is wonder, that you want to have fierce and harsh abstinence' (YCOE, 850-950) (5) Full mycel wundor hit waes þaet þaet maeden gebaer cild.

'Full great wonder it was that that maiden bore a child' ( 'there was no doubt then that it drew near to the death of them who were named there' (YCOE, 1050-1150)

Secondly, predicative and existential matrices historically used the same pronouns to refer to the complement clause. In Old English, the most commonly used pronoun was demonstrative that, which occurred as subject (nominative) in predicative matrices like (4) and as adjunct (genitive) in existential matrices like

(9) There is no doubt about it that he is in discomfort all the time (WB)

The fact that this colligation with its changing realization (that, it) occurs in both predicative and existential matrices suggests that the so-called it-extraposition construction is part of a larger class of evolving complementation constructions, even though, because of the different matrix syntax, reference to the complement is obligatory in predicative and optional in existential matrices.

Finally, at what is arguably the most distinctively grammatical end of the lexicogrammar, we find syntactic paradigms based on relations between constructions. Relations between constructions have been studied mainly from a variationist perspective, in which examples of variants are annotated in terms of various predictors (language internal, language-external, information theoretic), and processed statistically

As pointed out by

(10) The pregnant woman which hath tenasmum, for the moste parte aborteth [L. abortit] (OED, 1540) (11) Hee wrote a large Discourse..which he intended to send to her Maiestie..but that death preuented him; and (he dying) that worke aborted with him. (OED, 1620) (12) It [sc. the Parliament] is aborted before it was born.

(13) I don't think I would abort a baby. (WB) Alternations that are not dependent on the lexical verb differ from the verbspecific ones in fundamental ways. They are not selectively but generally available to all clauses with internal constituent structure. Examples are subject-finite inver-sion, anteposition of non-subjects in the clause, etc. With these alternations, each syntagmatic variant is meaningful in its own right. From the perspective of functional frameworks such as

To sum up, in this section, we have discussed and illustrated a number of heuristic techniques that can be used to interrogate lexicogrammatical patterning in diachronic data from various perspectives: collocational, collostructional, colligational and variational. In their study,

Representative

Their case studies focus on changes in the semantic range of verbs found in the 'V the hell out of ' construction and the 'V one's way' construction through the various decades represented in the Corpus of Historical American English (COHA), for which they develop an alternative to the collostructional approach (see Sect. 10.2.3). They argue that the meaning of lexical items can best be revealed by their association with mid-to high-frequency content words, which are semantically specific and co-occur with a wide range of target words in non-random ways, and therefore "yield robust measurements of meaningful lexical associations"

Perek and Hilpert find that only the 'V one's way' construction has undergone qualitative semantic changes. Initially it was associated mainly with verbs expressing the creation of a material path, e.g. carve, break, rip, fight, but from the 1880s it also accommodated verbs of perception, cognition and communication, e.g. smell, guess, joke, talk, expressing the creation of metaphorical paths. Perek and Hilpert then compare these results with the findings obtained by collostructional analysis, which does not filter out highly frequent and semantically neutral collocates that do not contribute much to the meaning of the construction. The collexemes that in the early stages of the 'V one's way' construction score highest for being more frequent than expected are take one's way and find one's way, which arguably are barely instances of the 'V one's way' construction.

Perek and Hilpert then take on the intrinsic periodization of changes as opposed to relating them to language-external historical landmarks. For this, they use variability-based neighbour clustering (VNC), a variant of an agglomerative hierarchical clustering algorithm which allows only periods that are temporally adjacent to be merged. VNC was proposed as a method for inductive periodization by

Representative Study 2

Buyle, A., and De Smet, H. 2018. Meaning in a changing paradigm:

The semantics of you and the pragmatics of thou. Language Sciences 68:42-55.

For present purposes, Buyle and De Smet's study shows that there is insight to be gained from small and closely annotated purpose-built corpora, particularly when it comes to some of the more elusive domains of grammatical analysis such as paradigmatic meaning and interactional pragmatics in earlier stages of a language.

10 Diachronic Corpora 227

Representative Corpus 1

Base textuelle FRANTEXT is in between a text archive and a typical reference corpus that strives to represent the history of a language -in this case, French. It contains some of the oldest French texts from 950 up to the present-day and, with currently about 300 million words of text, it decidedly qualifies as a large corpus. In its present version, it intends to cater to a great variety of researchers, including literary scholars and historians, as well as linguists. It started life, however, as a corpus for lexicographic research which explains its great time-depth and wide coverage in terms of genres. Part of the corpus has been part-of-speech tagged. One striking feature is that the corpus comes with various predefined subcorpora, varying in size or in the period that is represented, so as to meet different research needs. Indeed, the corpus has such a flexible online interface that it allows the user to dynamically select a working corpus precisely tailored to their specific objectives. Another distinguishing feature is that it is open to contributions by third parties, who can submit new material for inclusion in the corpus.

Representative Corpus 2

The York-Toronto-Helsinki Parsed Corpus of Old English Prose (YCOE) is a 1.5-million-word syntactically annotated corpus representing Old English prose, with texts dating from before 850 up to 1150. YCOE is a member of a bigger family of corpora. With the Helsinki Corpus of English Texts

Representative Corpus 3

The Old Bailey Corpus (OBC2.0) consists of trial proceedings from London's Central Criminal Court, the Old Bailey, published between 1720 and 1913. With its 24.4 million words it is a sizeable corpus, but what really sets it apart is the kind of text material it contains, with published transcripts of the spoken interactions in court. It was social historians Tim Hitchcock and Robert Shoemaker who started the process of digitizing the proceedings of the Old Bailey, annotating the texts and making them available online with a dedicated search engine

Critical Assessment and Future Directions

In this chapter, we have seen how the increase in the variety and overall size of corpus data allows researchers to broach new horizons in diachronic linguistics. Looking ahead, at least two ongoing trends can be expected to continue.

The first trend pertains to corpora themselves. The quantitative turn is strong, and with it the idea that bigger data are better data. Sound generalizations are increasingly expected to be based on large datasets, while at the same time, large data sets are bringing within reach the possibility of studying change in the language system of individual users over their own lifetime. Even so, there are some risks involved that critics of big data will not hesitate to point out. First, using large data sets may make it impossible for analysts to familiarize themselves in any detail with the texts that make up the empirical basis of their research. Second, for research to remain feasible it must be increasingly automated, again increasing the risk that researchers lose touch with their data. Third, a bigger data set is not necessarily more balanced or representative of historical usage.

As we have seen, however, strategies are emerging to avoid some of the potential pitfalls. More bottom-up text-based approaches to text classification, for instance, can reduce the need to rely on more aprioristic classifications. Computational techniques may even begin to supply information -such as author identity -that traditional philological work could not definitively determine. Another strategy lies 10 Diachronic Corpora 229 not in automation but in team work. Complex questions, involving large data sets and requiring specialized knowledge of diverse domains such as language history, text traditions, and computational techniques, can be handled if information passes between specialists efficiently. Therefore, it is to be hoped that the future will see corpora that can support and incorporate end user input, as well as researchers making richly annotated data sets available to colleagues.

The second trend pertains to the kind of research corpora are used for. As we have seen, particularly strong advances have been made with bottom-up and datadriven approaches to historical patterns and changes in these patterns, that "mak[e] the study of grammar more similar to the study of the lexicon"

Tools and Resources

What is useful to diachronic corpus linguists depends obviously on the languages they intend to study. Indeed, there are few resources that are truly languageindependent. There are several types of resources, however, that it is good for any diachronic corpus linguist to be on the lookout for.

First, there are several websites dedicated to cataloguing or collecting corpora, such as the Linguistic Data Consortium (

Second, anyone wanting (or needing) to compile their own corpora can benefit from digitized texts in online repositories, such as Project Gutenberg (

Third, corpus compilers may choose to optimally adapt their corpus to various further research needs. It is best to be aware of the Text Encoding Initiative, which seeks to standardize XML annotation for digital text editions (

Further Reading

Introduction

Compared to written corpora, spoken corpora are still few in number and are typically much smaller. This is chiefly due to the greater costs and challenges in terms of technology and time that are connected with the compilation and annotation of spoken corpora. However, interest in spoken corpora has been on the increase in the past two decades (e.g.

There is no doubt that the advent of spoken corpora has opened up new avenues for studying spoken language properties and use that have resulted in some fundamental reshapings of linguistic theories. For example, decisive advances have been made in the corpus-based description of the grammatical features of spoken language (e.g.

Fundamentals

There are two largely distinct but not exclusive types of corpora that contain spoken language, usually referred to as speech corpora (or speech databases) and spoken corpora respectively. Speech corpora or databases such as the Multi-Language Conversational Telephone Speech 2011-Slavic Group database (Jones 237 et al. 2016) typically contain large amounts of spoken language, often recorded under experimental conditions, and are used for industrial and technological applications such as assessing automatic speech recognition systems and developing human-machine communication or text-to-speech systems (see e.g.

Raw Data and Different Types of Spoken Corpora

The types of raw data that can be found in spoken corpora differ in terms of the corpus compilers' influence and control over the communicative context, ranging from 'no control at all' to 'highly controlled data elicitation methods'. Spoken language that was produced without any involvement of the corpus compiler is often referred to as 'authentic' or 'natural' language, as it avoids the observer's paradox, i.e. the fact that the presence of a researcher and the speakers' awareness that they are being recorded influence the nature of the language produced

Attempting to avoid the observer's paradox, the compilers of the London-Lund Corpus of English

Most spoken corpora contain language productions that were purposefully elicited by the corpus compilers. By controlling the situation in which language is produced corpus compilers increase the probability that the phenomena they are interested in are actually included in the corpus. Especially rare linguistic phenomena might not occur in sufficient numbers in a corpus that contains data U. Gut exclusively produced in uncontrolled situations. A wide range of speaking styles with varying degrees of corpus compilers' influence can be elicited, including unplanned and pre-planned as well as scripted and unscripted speech: In interviews carried out with speakers of different British dialects such as those recorded for the Freiburg English Dialect (FRED) corpus

The degree of control that is exerted over the communicative situation in which the raw data is produced determines the characteristics of the language that is being produced. For example, under highly controlled conditions speakers cannot choose their own words but have to produce language that is visually or aurally presented to them by the researcher. Thus, it is typically monologues rather than dialogues that are recorded in very controlled situations. In contrast, data produced in uncontrolled conditions comprises many types of spontaneously produced, unplanned, preplanned, scripted and unscripted language in monologues and dialogues. The researcher's control over the raw data production moreover influences the degree of variation that is represented in the corpus. While in uncontrolled data social, situational and genre-related variation in language use is typically present, it is increasingly restricted in the different types of elicited language data.

As the selection of the corpus data is determined by the intended uses of the corpus, the spoken corpora that are collected in the various linguistic subdisciplines differ sharply in terms of the raw data they contain. Spoken corpora that comprise 'authentic' raw data are typically compiled for the study of spoken language morphosyntax, pragmatics, discourse, conversations and sociolinguistics as they contain a breadth of different types of language (registers) and a sufficient amount of language variation. Spoken corpora that were assembled for the study of phonological and phonetic phenomena, on the other hand, tend to contain highly controlled raw data in the form of scripted monologues that ensure the occurrence of sufficient tokens of the features under investigation (they have thus been classified as peripheral corpora, e.g. by

Many spoken corpora contain several types of raw data, thus combining more and less controlled recording scenarios. This is true for all reference corpora, which aim to constitute representative samples of a language or language variety. Apart from written language, they contain a wide range of types of spoken language to guarantee the representation of variation across registers in this language (see e.g. the BNC). Other spoken corpora that combine raw data types include the IvIE corpus

Corpus Annotation

A collection of spoken language recordings does not constitute a linguistic corpus unless linguistic annotations are added to it. The one type of annotation that all spoken corpora share is an orthographic transcription (see also Chap. 14). Whether any further annotations are added to the corpus and what type is again largely determined by the intended use of the corpus (see also

Orthographic Transcription

All spoken corpora contain orthographic transcriptions. However, they can vary considerably in terms of the orthographic conventions chosen (some languages like English and German have different standard spellings in the different countries in which they are spoken, e.g. British English colour vs. American English color), spelling conventions concerning capitalisation rules (e.g. Ladies and Gentlemen vs. ladies and gentlemen) and hyphenisation (ice-cream vs. icecream). Likewise, they differ in the transcription of communicative units such as sounds of hesitation (erm) or affirmative sounds (mhm), for which no agreed spelling standard exists. Moreover, some corpora contain transcriptions for word forms typical of spoken U. Gut but not written language, for instance contractions like wanna, while others do not and corpora can differ in whether and how mispronunciations of words by individual speakers and non-speech events such as laughter and background noises are transcribed. It is important for corpus users to be aware of such potential differences in the orthographic transcriptions as they may lead to the missing of tokens in a word-based corpus search: in a search for 'because', for instance, transcribed forms such as 'coz' and 'cos' will not be found. Options to avoid the time-consuming manual orthographic transcription are beginning to materialise: for example, it is possible to train speech-to-text software such as the one included in GoogleDocs on one's voice and then record oneself repeating the content of the raw data file. Moreover, youtube offers a free automatic transcription service for videos. While these systems do not work very reliably yet, especially on recordings with background noise or overlapping speech, they will no doubt improve dramatically over the next few years.

POS-Tagging and Lemmatisation

Some spoken corpora have POS-tagging, which indicates the grammatical category of each transcribed word and which is essential for the quantitative analysis of the grammatical properties of spoken language (see Sect. 11.2.4; Chap. 2). A number of different taggers with different tag sets are in use that were originally developed for automatically annotating written corpora: these include the freely available CLAWS tagger,

Parsing

Parsing, i.e. the automatic annotation of the syntactic structure of a language (cf. Chap. 2), is still tremendously challenging for spoken corpora. Due to the characteristics of spoken language such as constructions with word order patterns not found in written language and incomplete utterances, parsers that were developed for written language usually yield poorer output when applied to spoken corpora. Yet, first advances have been made with the Constraint Grammar parser PALAVRAS that was successfully adapted to handle the structures of spoken language and which was used for parsing the C-ORAL Brazil corpus with 95% accuracy for syntactic function assignment

Phonemic and Phonetic Transcription

Some spoken corpora that were compiled for the study of phonological and phonetic phenomena (so-called 'phonological corpora'; see Gut and Voormann 2014) further contain phonemic or phonetic transcriptions. In a phonemic transcription, the phonological form of a word is transcribed while a phonetic transcription represents the actual pronunciation by a speaker. For a phonemic transcription, each phoneme is transcribed using either the symbols of the International Phonetic Alphabet

Prosodic Transcription

Spoken corpora that were compiled in order to study prosodic phenomena in speech can contain various types of prosodic transcriptions. Unlike for phonemic transcriptions, several transcription conventions exist side by side for prosody: the most commonly used are the transcription system ToBI

Multi-layered and Time-Aligned Annotation

Spoken corpora do not only differ in terms of the number and type of annotations they contain but also in the fundamental question of how these annotations are represented. Annotations of existing spoken corpora differ in two ways: annotation layering and time-alignment. The term 'annotation layer' refers to the issue of whether different types of annotation are integrated together in one linear transcription or whether they are represented individually on separate layers. Many spoken corpora compiled by researchers working in the field of conversation analysis and interactional linguistics use transcription systems that integrate various linguistic aspects in one linear transcription. The GAT-2 system

Most modern spoken corpora, however, use multi-layered annotations, where only one type of linguistic annotation is contained per layer (or tier) (see also

Data Format and Metadata

The large number of corpus annotation tools that are in use determines that spoken corpora also differ widely in their data format, ranging from txt files to XML formats. While nearly every tool produces its proprietary output format, many of them have import/export functions that allow data exchange across tools. However, U. Gut the interoperability between tools is still one of the major challenges for spoken corpus use and re-use across linguistic subdisciplines as discussed in Sect. 11.3. One major problem, for example, is that linear annotations cannot be converted to multi-layered ones. The data format of the corpus also restricts the range of tools that can be used for corpus searches (see

Further heterogeneity across spoken corpora exists in terms of metadata. The term metadata refers to any additional information about the corpus compilers, the data collection (e.g. the procedure and date and place of recordings), the speakers that were recorded (e.g. their age, gender, regional background, further languages) and the data annotation process (e.g. the transcription conventions, tagset used, see Chap. 1). While several international initiatives have attempted to standardise metadata formats (see Broeder and van Uytvanck 2014 for an overview), its divergent use across existing spoken corpora is pronounced. As is true for any corpora, when the metadata is provided in a separate file without using standoff techniques (e.g. IDs pointing from annotations to corresponding speaker metadata), it requires great manual effort to make use of it for automatic corpus searches (see also Chap. 3).

Corpus Search

As existing spoken corpora vary greatly in the type and number of annotations as well as their data format, possibilities for corpus search diverge significantly. In general, corpora with rich multi-layered annotations lend themselves to largescale automated quantitative analyses and statistical exploitation (e.g. Moisl 2014), while others that contain only orthographic transcriptions can be used mainly for manual corpus inspection (see also

Representative Study 1 Aijmer, K. 2002. English discourse particles -evidence from a corpus.

Amsterdam: John Benjamins.

This is an example of an innovative study using the corpus-based method to gain new insights into the linguistic subdiscipline of pragmatics. She analysed the use of the English discourse particles now, oh/ah, just, sort of, actually, and some phrases such as and that sort of thing in the London-Lund corpus and compared this to the Lancaster-Oslo/Bergen Corpus of written English and the COLT Corpus. Since these corpora are not pragmatically annotated, she used a classic KWIC search method to find these words in the corpora and carried out subsequent qualitative analyses of the use of these discourse particles and phrases. Her findings show that these discourse particles have multiple functions and differ in their use with respect to the level of formality. This is an example of a study based on a corpus with integrated linear annotations. For investigating the interplay between grammatical stance expressions and prosody they carried out a KWIC search (using AntConc)

Representative

Representative Corpus 1

The Spoken Dutch Corpus

The first major obstacle for the reusability of spoken corpora is the often insufficient documentation of the corpus creation process, the type of raw data and metadata in the corpus and the annotation schemes applied. If a potential spoken corpus user interested in the grammatical variation between older and younger speakers cannot find information on the age of the speakers represented in the corpus, and if a researcher interested in the interplay between prosody and syntax in a language cannot interpret the transcription symbols used for prosody, re-use of corpora is impossible. It is therefore essential for corpus compilers to make available to future corpus users ample metadata and a corpus manual detailing the corpus compilation and annotation process. Equally, it would be of great benefit to the research community if the metadata for already existing spoken corpora could still be made available, for example for some of the older ICE corpora.

Yet, even well documented spoken corpora are often not immediately (re-)usable to the wider research community, especially if the intended linguistic use is not the original one of the corpus compilers. Thus, the syntactic or prosodic annotation of a corpus might be based on a different theoretical tradition than the one preferred by the researcher or one type of annotation that is necessary for the current study might be missing altogether. Adding new annotations to a spoken corpus, however, can still constitute a major challenge for the simple lack of suitable tools although great advances have been made in the last decade in terms of the interoperability of the major tools in use for spoken corpus construction: ELAN, Praat, EXMARaLDA and ANVIL (see Sect. 11.4 below) now all have import and export functions for their respective file formats so that it is possible to add new annotations with one of these tools to a spoken corpus that was compiled with another tool (for this Transformer by Oliver Ehmer can also be used). Yet, the conversion of linear corpus annotations into multi-layered ones still constitutes an unsolved challenge. Moreover, many older spoken corpora could be opened up for entirely new directions of research if their annotations were time-aligned.

Apart from re-using and enriching existing spoken corpora, future corpus-based research might also increasingly make use of combining existing corpora for U. Gut linguistic hypothesis testing in order to overcome size limits of individual corpora or in order to allow diachronic studies of phenomena (see also the SPADE project

The third impediment for the (re-)use of some spoken corpora sometimes is the lack of suitable automatized corpus search tools. Corpora that can only be searched with specialised tools might prove inaccessible to some linguists as discussed in Sect. 11.2.4. Some international initiatives such as CLARIN in Europe have the aim of overcoming this challenge by providing an infrastructure for corpus users that includes easy-to-use standardised tools. Many more similar efforts are necessary to make more spoken corpora accessible to researchers from all linguistic subdisciplines.

The last challenge for the future of spoken corpora is their continued availability and accessibility. While an increasing number of corpus compilers are eager to make their spoken corpora available to the research community, technological and ethical difficulties have to be met as discussed below. For corpus data stored in nondigital form such as analogue tapes (there is still a lot of historical data that has not been digitised yet) every access means loss of quality. Moreover, many older data formats will not be accessible anymore in the near future. The archiving and dissemination of spoken corpora, even in digital form, thus implies the constant pressure of keeping up with technological advances. For instance, raw video data encoded in one format such as MPEG1 will have to be regularly updated to new encoding schemes. Furthermore, new strategies for corpus dissemination are being proposed to ensure long-term access to spoken corpora. Some corpora that were made available and searchable via websites are 'lost' due to lack of maintenance. As an alternative, corpora can be stored in external data services such as clouds with corpus access for example by compressed media streaming. Yet another option is the storage of spoken corpora at large data centres such as the MPI Archive

Further challenges for research based on spoken corpora are ethical issues such as privacy rights and copyrights. Typically, privacy laws require corpus compilers to ask for the formal written authorisation by each speaker to be recorded for the corpus and to allow the transcription, sharing and re-use of his or her data. Only when corpus compilers have received the written consent of the speakers recorded for the corpus that their data can be used for research and be disseminated can corpora be used and shared. Moreover, privacy rights require corpus compilers to anonymise the disseminated data (cf. Chap. 1): while this is easily achieved in the transcriptions, where references to people and places can be removed, complete anonymization in audio files, i.e. the changing of the voice quality, would run counter and make impossible many research purposes of the corpus. Legislation on copyright and privacy issues often changes and can differ widely across nations. In some national laws the speaker can withdraw his or her consent at any later point 11 Spoken Corpora 251 in time, which poses serious challenges for corpus dissemination. The European General Data Protection Regulation, which became enforceable in May 2018, for example, states that personal data may only be processed when the data subjects have given their consent for specific purposes. Changing legislation in these areas might pose further difficulties for corpus-based research in the future.

In conclusion, both the compilation of new spoken corpora and the reuse of older ones remain exciting and challenging tasks for the future. I have no doubts, however, that they will help to provide many more important insights into human language use. The most comprehensive volume on spoken corpora to date that covers all aspects of the construction, use and archiving of spoken corpora with a focus on phonological corpora. It contains chapters on innovative approaches to phonological corpus compilation, corpus annotation, corpus searching and archiving and exemplifies the use of phonological corpora in various linguistic fields ranging from phonology to dialectology and language acquisition. Furthermore, it contains descriptions of existing phonological corpora and presents a wide range of popular tools for spoken corpus compilation, annotation, searches and archiving. This collection of papers focuses on questions of standards for the construction, annotation, searching, archiving and sharing of spoken corpora used in conversation analysis, sociolinguistics, discourse analysis and pragmatics. The individual contributions discuss these issues and illustrate current practices in corpus design, data collection and annotation, as well as strategies for corpus dissemination and for increasing the interoperability between tools.

Tools and Resources

Raso

Introduction

This chapter gives an overview of parallel corpora, which are widely used in corpusbased cross-linguistic research (here understood as an umbrella term for contrastive linguistics and translation studies) and natural language processing. Parallel corpora (also called translation corpora) contain source texts in a given language (the source language, henceforth SL), aligned with their translations in another language (the target language, henceforth TL). It is important to point out from the outset that the term parallel corpus is to some extent ambiguous, because it is sometimes used to refer to comparable original texts in two or more languages, especially texts that 258 M.-A. Lefer belong to comparable genres or text types and deal with similar topics (e.g. Italian and German newspaper articles about migration or English and Portuguese medical research articles). Here, the term will only be used to refer to collections of source texts and their translations. The compilation of parallel corpora started in the 1990s. Progress has been rather slow, compared with monolingual corpus collection initiatives, but in recent years we have witnessed a boom in the collection of parallel corpora, which are increasingly larger and multilingual. Parallel corpora are highly valuable resources to investigate cross-linguistic contrasts (differences between linguistic systems) and translation-related phenomena, such as translation properties (features of translated language). They can also be used for a wide range of applications, such as bilingual lexicography, foreign language teaching, translator training, terminology extraction, computer-aided translation, machine translation and other natural language processing tasks (e.g. word sense disambiguation and cross-lingual information retrieval).

This chapter is mainly concerned with the design and analysis of parallel corpora in the two fields of corpus-based contrastive linguistics and corpus-based translation studies. Contrastive linguistics (or contrastive analysis) is a linguistic discipline that is concerned with the systematic comparison of two or more languages, so as to describe their similarities and differences. Corpus-based contrastive linguistics was first pioneered by Stig Johansson in the 1990s and has been thriving ever since. Corpus-based translation studies is one of the leading paradigms in Descriptive Translation Studies

Fundamentals

Types of Parallel Corpora

Parallel corpora can be of many different types. They can be bilingual (one SL and one TL), such as the English-Norwegian Parallel Corpus (ENPC; Johansson 2007), or multilingual (more than one SL and/or TL), such as the Oslo Multilingual Corpus, which is fully trilingual (English, Norwegian and German), with some texts available in Dutch, French and Portuguese as well

Main Characteristics of Parallel Corpora

The majority of parallel corpora used in contrastive linguistics and translation studies are characterized by two key features. First, the source and target languages are clearly identified. In other words, the translation direction is known (from Language X to Language Y or from Language Y to Language X ). In cross-linguistic research, it is of paramount importance to know, for instance, whether a given text was translated from Spanish into German or vice versa, because corpus studies have shown that translation direction influences translation choices, and hence the linguistic make-up of translated text (e.g. Dupont and Zufferey 2017). Second, only direct translation is included, i.e. no pivot (intermediary, mediating) language is used between the source and target languages. In texts produced by the European Union (EU), for example, English has been systematically used as a pivot language since the early 2000s. In practical terms, this means that a text originally written in, say, Slovenian or Dutch is first translated into English. The English version is then translated into the other official languages of the EU. In other words, English acts as a pivot language and most target texts originating from EU institutions are in fact translations of translations (see Assis

Directional parallel corpora typically (i) contain written texts (ii) translated by expert translators (iii) working into their native language (L1), and (iv) cover a M.-A. Lefer rather limited number of text types or genres. Each of these typical features will be discussed in turn:

(i) Directional parallel corpora mainly cover written translation (e.g. the ENPC), to the detriment of other translation modalities, such as interpreting and audiovisual translation. In recent years, however, efforts have been made to include other forms of translation. A case in point is the compilation of several parallel corpora of simultaneous interpreting (see

Methods of Analysis in Cross-Linguistic Research

Parallel corpora are widely used in corpus-based contrastive linguistics and translation studies and they are starting to emerge as a useful source of data in typology as well

Figure

Figure

In the two examples mentioned above, we started with a given SL item (no kidding, sustainability) and examined its translation equivalents in the TL (Italian and French, respectively), i.e. going from source to target. Interestingly, this source- to-target approach is also used in monolingual corpus linguistics to examine the semantic, discursive and pragmatic features of source-language items

An alternative method is to start off from a given item or structure in translated texts and examine its corresponding source-text items or structures, i.e. from target to source. Taking the same example as above, this would entail analyzing all occurrences of sul serio in Italian subtitles and identifying the English source items that have triggered their use. This target-to-source approach is quite common in translation studies. Delaere and De Sutter (2017), for example, rely on an English-to-Dutch parallel corpus to find out whether the English loanwords found in translated Dutch stem from their corresponding trigger words in the English source texts.

Naturally, these two approaches (source to target and target to source) can be combined if a more comprehensive picture of cross-linguistic correspondences is required. Indeed, many new insights can be gained by investigating a given item or structure in both source and target texts, so as to find out how it is commonly translated and which items in the other language have triggered its use in translation (e.g.

It is also possible, on the basis of parallel corpora, to work out what Altenberg has termed mutual correspondence (or mutual translatability), i.e. "the frequency with which different (grammatical, semantic and lexical) expressions are translated into each other"

If, say, a lexical item A is always translated with an item B, and vice versa, then items A and B have a mutual correspondence of 100%. If, on the contrary, A and B are never translated with each other, they display a mutual correspondence of 0%. In other words, this index makes it possible to assess the extent to which items are equivalent across languages: "the higher the mutual correspondence value is, the greater the equivalence between the compared items is likely to be"

So far, we have outlined different methods of parallel corpus analysis (from source to target, from target to source, mutual correspondence). However, it should be stressed that several types of corpora can be combined to reveal and disentangle cross-linguistic contrasts and translation-related phenomena

Bilingual (or multilingual) comparable corpora are "collections of original [i.e. non-translated] texts in the languages compared"

Issues and Methodological Challenges

Issues and Challenges Specific to the Design of Parallel Corpora

This section presents an overview of some of the main challenges specific to the design of parallel corpora (for a detailed discussion of more general issues, such M.-A. Lefer as representativeness and balance, copyright clearance,

The first issue is text availability. As mentioned above, parallel corpora, especially bidirectional ones, tend to be modest in size and are often restricted to a small number of text types. One of the reasons for this is that for any given language pair (L X and L Y ), there is often some kind of asymmetry or imbalance between the two translation directions (L X > L Y and L Y > L X ). This imbalance can take several forms: either there are simply fewer texts translated in one direction than in the other (especially when the language pair involves a less "central", or more "peripheral", language), or certain text types are only (or more frequently) translated in one of the two directions. For example, as noted by

[t]ourist brochures in Portuguese translation are practically non-existent: Portuguesespeaking tourists abroad are expected to get by in other, more widely known languages. In contrast, almost all material destined to be read by tourists in Portuguese-speaking countries comes with an English translation.

To sum up, "translations are heavily biased towards certain genres, but these biases are rarely symmetrical for any language pair"

Obtaining detailed metadata is another challenge facing anyone wishing to compile a parallel corpus. In this respect, parallel corpora are clearly lagging behind compared with other corpus types, such as learner corpora, which are more richly documented (Chap. 13). Ideally, the following metadata should be collected (this list is non-exhaustive):

• Source text and target text: author(s)/translator(s), publisher, register, genre, text type, domain, format, mode, intended audience, communicative purpose, publication status, publication date, etc. • Translation direction, including SL and TL (and their varieties) • Translation directness: use of a pivot language or not • Translation directionality: L2 > L1 translation, L1 > L2 translation, L2 > L2 translation, etc. • Translator: translator's status (professional, volunteer/amateur, student, etc.), translator's occupation, gender, nationality, country of residence, translation expertise (expert vs. novice), translation experience (which can be measured in many different ways, e.g. number of years' experience), language background (native and foreign languages), etc. • Translation task: use of computer-aided translation tools (translation memories, terminological databases) and other tools and resources (dictionaries, forums, corpora, etc.), use of a translation brief (set of translation instructions, including, for instance, use of a specific style guide or in-house terminology), fee per word/line/hour, deadline/time constraints, etc. • Revision/editorial intervention: self-and other-revision, types of revision (e.g. copyediting, monolingual vs. bilingual revision), etc.

It is also important to stress here that the concepts of source language and source text are becoming increasingly blurred. In today's world, some "source" documents are simultaneously drafted in several languages. In multilingual translation projects, there are also cases where there is no single "source" text, as translators translate a given text while accessing some of its already available translations (e.g. when confronted with an ambiguous passage).

Third, there is the issue of alignment, i.e. the process of matching corresponding segments in source and target texts (see

As pointed out by

Finally, yet another major challenge relating to the compilation of parallel corpora (or any other type of multilingual corpus) is multilingual linguistic annotation (e.g. lemmatization, morphosyntactic annotation, syntactic parsing, semantic tagging; Chap. 2).

If corpora are annotated independently for each language, to what extent is the analysis comparable? If they are provided with some kind of language-neutral annotation (for parts of speech, syntax, etc.), to what extent do we miss language-specific characteristics?

At present, no definite answers have been found to these questions. As a matter of fact, issues related to multilingual annotation (e.g. whether it should be languagespecific or language-neutral, or, more generally, how cross-linguistic comparability can be achieved) have received relatively little attention in contrastive linguistics and translation studies (one notable exception is Neumann 2013). The languagespecific and language-neutral approaches are both used in parallel corpora, the former being more common. In the language-specific approach, researchers rely either on separate annotation tools (one per language involved) or on one single tool that is available for several languages, such as the TreeTagger

The multilingual annotation of existing parallel corpora is still very basic, being mostly limited to lemmatization and POS tagging. Syntactic annotation will probably become more standard in years to come, given recent advances in multilingual parsing (e.g.

Issues and Challenges Specific to the Analysis of Parallel Corpora

Clearly, compared with monolingual corpora, parallel corpora are lagging behind in terms of size (representativeness is also an issue, as small corpora tend to represent relatively few authors and translators/interpreters). Low-frequency linguistic phenomena may be hard to analyze on the basis of parallel corpora, for sheer lack of sufficient data that would allow reliable generalizations. Researchers in contrastive linguistics and translation studies are therefore often forced to combine several parallel corpora to extract a reasonable amount of data, but this approach raises a number of problems. One is that several confounding variables may be intertwined in the various corpora used, which in turn hinders the interpretability of the results. In

Another issue, also directly related to the interpretability of the results, is the cross-linguistic comparability (or lack thereof) of genres and text types in bidirectional parallel corpora (such as the ENPC, the DPC and CroCo) (see

It is also worth pointing out that most parallel corpora are poorly metadocumented (source and target texts and languages, translator, translation task, editorial intervention, etc.), which, unfortunately, can lead researchers to jump to hasty conclusions as regards both cross-linguistic contrasts ("this pattern is due to differences between the two language systems under scrutiny") and features of translated language ("this is inherent in the translation process").

One final point to be made in this section is that parallel corpora (even those whose texts have all been translated by highly-skilled professionals) contain infelicities and even translation errors (to err is human, after all). Researchers may therefore feel uncomfortable with some of the data extracted from parallel corpora. Rather than sweeping erroneous items under the carpet, when in doubt it is probably safer to acknowledge these seemingly infelicitous or erroneous data explicitly. Moreover, looking on the bright side, these infelicities and errors can prove to be highly valuable in applied fields such as bilingual lexicography, foreign language teaching or translator training. In

Representative

Loanword use is related to a widely investigated topic in translation studies, viz. the normalization hypothesis, which states that translated text is more standard than non-translated text. The starting-point hypothesis of Delaere & De Sutter's study is that overall, translators make more use of endogenous lexemes (a conservative option compared with the use of loanwords), than

M.-A. Lefer do non-translators (writers). Relying on the Dutch Parallel Corpus, the authors combine two approaches in their study: monolingual comparable (Dutch translated from English and French, alongside original Dutch) and parallel (English to Dutch). As is often the case in corpus-based translation studies, parallel data are used with a view to identifying the source-text items/structures that have triggered the use of a given item/structure in the translations (in this case, the presence of a trigger term in the English source texts, such as unit, job, or team). The authors apply multivariate statistics (profile-based correspondence analysis and logistic regression analysis) to measure the effect of the three factors investigated on the variability of English loanword use. The logistic regression analysis reveals that the effect of register is so strong that it cancels out the effect of source language. Their study convincingly illustrates the need to adopt multifactorial research designs in corpus-based translation studies, as these make it possible to go beyond the monofactorial designs where, typically, only the "translation status" variable is considered (translated vs. non-translated).

Representative Corpus 1

The Dutch Parallel Corpus

Representative Corpus 2

To date, Europarl

Critical Assessment and Future Directions

As shown above, anyone wishing to design and compile a directional parallel corpus faces a number of key issues, such as parallel text availability (especially in terms of text-type variety), access to source text-, translator-and translation taskrelated metadata, automatic sentence alignment, and linguistic annotation. Relying on existing parallel corpus resources poses its own challenges as well, as present-day parallel corpora tend to be quite small and/or poorly meta-documented and typically cover relatively few text types. Notwithstanding these issues and challenges, parallel corpus research to date has yielded invaluable empirical insights into cross-linguistic contrasts and translation.

There are many hopes and expectations for tomorrow's parallel corpora. There are three ways in which headway can be made in the not too distant future. The first M.-A. Lefer two are related to the design of new parallel corpora, while the third is concerned with a rapprochement between natural language processing and cross-linguistic studies.

First, it is high time we started collecting richer metadata, notably in terms of SL/TL, source and target texts, translator, translation task, and editorial intervention. This will make it possible to adopt multifactorial research designs and use advanced quantitative methods in contrastive linguistics and translation studies much more systematically, thereby furthering our understanding of cross-linguistic contrasts and of the translation product in general.

Second, whenever possible, we should go beyond the inclusion of translated novels, news, and international organizations' legal and administrative texts, and strive for the inclusion of more genres and text types, especially those that are dominant in today's translation market, to which corpus compilers have had limited access to date, for obvious reasons of confidentiality and/or copyright clearance. This also entails compiling corpora representing different translation modalities (e.g. audiovisual translation, interpreting) and translation methods, such as computer-aided translation and post-editing of machine-translated output, as translation from scratch is increasingly rarer today (one notable exception is literary translation). Including different versions of the same translation would also prove to be rewarding (e.g. draft, unedited, and edited versions of the translated text).

Finally, we need to cross-fertilize insights from natural language processing and corpus-based cross-linguistic studies. This "bridging the gap" can go both ways. On the one hand, cross-linguistic research should take full stock of recent advances in natural language processing, for tasks such as automatic alignment and multilingual annotation. Significant progress has been made in recent years in these areas, but parallel corpora, especially those compiled by research teams of corpus linguists,

have not yet fully benefited from these new developments. At present, for instance, very few parallel corpora are syntactically parsed or semantically annotated. On the other hand, natural language processing researchers involved in parallel corpus compilation projects could try to document, whenever possible, meta-information that is of paramount importance to contrastive linguists and translation scholars, such as translation direction (from L X to L Y , or vice versa) and directness (use of a pivot language or not). In turn, taking this meta-information into account may very well help significantly improve the overall performance of data-driven machine translation systems and other tools relying on data extracted from parallel corpora.

Even though it is quite difficult to predict future developments with any certainty, especially in view of the fact that translation practices are changing dramatically (e.g. human post-editing of machine-translated texts is increasingly common in the translation industry), it is safe to say that compiling and analyzing parallel corpora will prove to be an exciting and rewarding enterprise for many years to come.

Tools and Resources

Query Tools

Resources

• OPUS project

M.-A. Lefer

Surveys of Available Parallel Corpora

A large number of parallel corpora have been mentioned or discussed in this chapter, but it was outside the scope of the present overview to list all available parallel corpora. As a matter of fact, there is as yet no up-to-date digital database documenting all existing parallel corpora (be they bilingual or multilingual, directional or non-directional, developed for cross-linguistic research and/or natural language processing). However, there are some promising initiatives in this direction, such as

Chapter 13

Learner Corpora

Gaëtanelle Gilquin

Abstract This chapter deals with learner corpora, that is, collections of (spoken and/or written) texts produced by learners of a language. It describes their main characteristics, with particular emphasis on those that are distinctive of learner corpora. Special types of corpora are introduced, such as longitudinal learner corpora or local learner corpora. The issues of the metadata accompanying learner corpora and the annotation of learner corpora are also discussed, and the challenges they involve are highlighted. Several methods of analysis designed to deal with learner corpora are presented, including Contrastive Interlanguage Analysis, Computeraided Error Analysis and the Integrated Contrastive Model. The development of the field of learner corpus research is sketched, and possible future directions are examined, in terms of the size of learner corpora, their diversity, or the techniques of compilation and analysis. The chapter also features representative corpus-based studies of learner language, representative learner corpora, tools and resources related to learner corpora, and annotated references for further reading.

Introduction

Learner corpora are corpora representing written and/or spoken 'interlanguage', that is, language produced by learners of that language. Typically, the term covers both foreign language and second language situations, that is, respectively, situations in which the target language has no official function in the country and is essentially confined to the classroom (and, possibly, international communication), and situations in which the target language is learned by immigrants in a country where it is the dominant native language. It is normally not used to refer to corpora of child language, which are made up of data produced by children acquiring their first language (see Chap. 14), nor corpora of institutionalized second-language varieties, 284 G. Gilquin which are collected in countries that have the target language as an official, though not native, language (cf. 'New Englishes' like those represented in the International Corpus of English), although their data may also reflect a process of learning or acquisition.

While the first corpora were compiled in the 1960s, it took some 30 years before the first learner corpora started to be collected, both in the academic world (International Corpus of Learner English (ICLE)) and in the publishing world (Longman Learners' Corpus). Initially, they were corpora of written learner English, keyboarded from handwritten texts. Gradually, however, learner corpora representing other languages as well as spoken learner corpora made their appearance, while written learner corpora were increasingly compiled directly from electronic sources, which facilitated the compilation process. The nature of learner language made it necessary to rethink and adapt some of the general principles of corpus data collection and analysis. This led, among other things, to the creation of new types of corpora, like longitudinal corpora representing different stages in the language learning process, to the collection of new types of metadata, such as information about the learner's mother tongue and exposure to the target language, and to the use of new methods to annotate or query the corpus, for example to deal with the errors found in learner corpora. These specificities, and others, will be considered in Sect. 13.2.

Fundamentals

Types of Learner Corpora

Like other corpora, learner corpora can include written, spoken and/or multimodal data; they can be small or large; and they can represent any (combination of) languages. The 'Learner Corpora around the World' resource (see Sect. 13.4) reveals that the majority of learner corpora are made up of written data, and that these data often correspond to learner English. Other types of corpora, however, including spoken learner corpora and corpora representing other target languages, are becoming more widely available. As for size, many of the learner corpora listed in the 'Learner Corpora around the World' resource are under one million words, with some of them not even reaching 100,000 words and a couple just containing some 10,000 words. It is likely that among those learner corpora that are not listed but exist 'out there', most can be counted in tens of thousands rather than in millions of words. Yet, there are also learner corpora that are much larger, especially those that have continued to grow over the years (like the Longman Learners' Corpus, which now comprises ten million words) and those that come out of the testing/assessment world, such as EFCAMDAT

One of the defining features of corpora is that they should be made up of authentic texts. This concept of authenticity, however, tends to be problematic in the case of learner corpora. Learner language, most of the time, is not produced purely for communicative purposes, but as part of some pedagogical activity, to practise one's language skills. Writing an argumentative essay or role-playing with a classmate, for example, may be natural tasks in the classroom, but they are not authentic in the sense of being "gathered from the genuine communications of people going about their normal business"

One type of learner corpus that is worth singling out, because it is specific to varieties that are in the process of being learned or acquired, is the longitudinal learner corpus. In such a corpus, data are collected from the same subjects at different time intervals, so as to reflect the development of their language skills over time.

Metadata

Given the "inherent heterogeneity of learner output"

Despite the wealth of metadata that accompany most learner corpora and despite the facilities that some of these corpora provide to access them, it must be recognized that metadata are not used to their full potential in learner corpus research. One variable that is regularly taken into account is that of the learner's L1 background (e.g.

Annotation

Learner corpora can be enriched by means of the same types of annotation as all other corpora, including part-of-speech (POS) tagging, parsing, semantic annotation, pragmatic annotation and, for spoken learner corpora, phonetic and prosodic annotation (see Chaps. 2 and 11). One issue to bear in mind, however, is that, with very few exceptions, the tools that one has to rely on to annotate learner corpora automatically are tools that have been designed to deal with native data.

Applying them to non-native data may therefore cause certain difficulties. For POS tagging, for example, the many spelling errors found in written learner corpora have been shown to lower the accuracy of POS taggers (de Haan 2000; Van Rooy and Schäfer 2002). As for parsing, punctuation and spelling errors in written learner corpora have the highest impact according to

(2010) for POS tagging), it must be underlined that some attempts to automatically annotate learner corpora with off-the-shelf tools have been quite successful.

Methods of Analysis

In addition to the application of well-established corpus linguistic methods, like the use of concordances (Chap. 8), frequency lists (Chap. 4) or collocations (Chap. 7), a number of techniques have been developed to deal specifically with learner corpora. Among these, we can mention Computer-aided Error Analysis

Contrastive Interlanguage Analysis (CIA) consists of two types of comparison: a comparison of learner language with native language and a comparison between different learner varieties

The Integrated Contrastive Model (ICM) is partly based on CIA, but it also integrates a contrastive analysis (CA), comparing the target language and the mother tongue thanks to comparable or parallel corpora (cf. Chap. 12). The model aims to predict possible cases of negative transfer (when the CA shows the target language and the mother tongue to differ in a certain respect) and seeks to explain problematic uses -misuse, overuse, underuse -in the learner corpus (by checking whether they could be due to discrepancies between the target language and the mother tongue). It thus has both predictive and diagnostic power. By combining careful analyses of learner, native and bilingual corpora, the model avoids the trap of misattributing certain phenomena to transfer simply because intuition seems to suggest that this is a plausible interpretation.

The last few years have witnessed a general refinement of the methods of analysis in learner corpus research. One major change is the increasingly prominent role of statistics in the field. While statistical significance testing has almost always been part of learner corpus studies, through the notions of over-and underuse, criticism has recently been voiced against this type of monofactorial statistics.

Representative

In addition, the potential role of the mother tongue is examined, and some possible cases of transfer are highlighted, as well as strategies that appear to be common to the two groups of learners (e.g. a "decompositional" strategy which results in constructions like make the family live instead of support the family). Interestingly, the article also discusses the methodological issue of how accurate and useful an automatic extraction of collocates is. More generally, it demonstrates the benefits of combining an automatic and manual analysis, as well as a quantitative and qualitative approach. This study focuses on German as a foreign language, and how advanced learners acquire morphological productivity for German complex verbs, that is, prefix verbs (like verstehen 'to understand') and particle verbs (like aufstehen 'to get up'). Looking at the treatment of morphological productivity in different acquisition models, including generative and usage-based models, the authors put forward a number of hypotheses, which are then tested against a learner corpus. The corpus is Falko (see Sect. 13.2.3) and its (continued)

Representative

G. Gilquin L1 equivalent. The study combines Contrastive Interlanguage Analysis and Computer-aided Error Analysis. First, it compares the frequency and uses of complex verbs in learner and native German. Second, it relies on the error tagging of Falko to identify grammatical and ungrammatical uses of complex verbs and to determine error types. The results show that learners tend to underuse prefix verbs and, especially, particle verbs, and that the variance between individual learners is greater than that between individual native speakers. Learners also appear to use complex verbs productively, although the new forms they produce sometimes result in errors. The paper illustrates some of the latest developments in learner corpus research, such as a solid grounding in theories and a combined aggregate and individual approach.

It also makes the interesting methodological point that, through corpus annotation, categorization of the data can be made explicit and available to other researchers. This study is based on one of the large learner corpora coming out of the testing/assessment world (see Sect. 13.2.1), namely EFCAMDAT, the EF Cambridge Open Language Database. EFCAMDAT is made up of 33 million words, representing 85,000 learners and spanning 16 proficiency levels.

Representative

Although the corpus includes longitudinal data for certain individual learners, this study adopts an aggregate approach, considering each proficiency level as a 'section', but with the acknowledgment that "combining the cross-sectional perspective with an analysis of individual learner variation is a necessary next step" (p. 126). The paper investigates the development of learners' use of relative clauses. Like

(continued)

This focus on tasks echoes

Representative

Critical Assessment and Future Directions

Over the last few years, learner corpora have grown in number, size and diversity. Written learner corpora are already quite numerous and large. In the near future, we should see the release of more and bigger spoken learner corpora, like the (still growing) Trinity Lancaster Corpus

In

More and more learner corpora nowadays come with an equivalent L1 corpus representing the target language (cf. CEDEL2 and PAROLE). This is a welcome development, as it makes it possible to carry out contrastive interlanguage analyses on the basis of fully comparable data. Such target language data are likely to be included in the mega databases of the future. What would also be desirable is input data, which should strive to represent the language that learners get exposed to, so that correlations between input and output can be measured. While in the past learners' input has been approximated by means of textbook corpora (cf.

At the same time as we should witness an exponential growth in the size of learner corpora/databases, we should also observe the creation of new types of learner corpora, some of which have already started to be collected. The PROCEED corpus (Process Corpus of English in Education),

In addition to an expansion and diversification of learner corpora, we can also expect these corpora to come with more additional information than ever before, in the form of metadata and annotation. Starting with metadata, although learner corpora have included a large variety of them from the very beginning, there is G. Gilquin also a growing recognition that these may not be enough to reflect the complexity of the second language acquisition process. Limiting target language exposure to the 'time abroad' factor, for example, means neglecting other possible sources of exposure like the Internet, TV series or songs, all of which have become omnipresent in the lives of many young people. Proficiency is another case in point. While typically it has been evaluated on the basis of external criteria such as age or number of years of English instruction, scholars like

The availability of more, more diverse, bigger and more richly annotated learner corpora will have an impact on the way we conduct learner corpus research.

Tools and Resources

Learner Corpus Bibliography: this bibliography is made up of references in the field of learner corpus research. The bibliography can be found on the CECL website (

Learner Corpora around the World (

Université Catholique de Louvain Error Editor (UCLEE; Hutchinson 1996): this program facilitates error tagging thanks to a drop-down menu that makes it possible to select an error tag. It also facilitates the insertion of a corrected form. A new version of the software is currently in preparation.

Compleat Lexical Tutor (Lextutor;

Further Reading

Granger, S. 2012. How to use foreign and second language learner corpora.

In Research methods in second language acquisition: A practical guide, eds. Mackey, A., and Gass, S.M., 7-29. Chichester: Blackwell Publishing.

After briefly introducing learner corpora, this paper clearly presents the different stages that can be involved in a learner corpus study: choice of a methodological approach, selection and/or compilation of a learner corpus, data annotation, data extraction, data analysis, data interpretation and pedagogical implementation. This handbook provides a comprehensive overview of the different facets of learner corpus research, including the design of learner corpora, the methods that can be applied to study them, their use to investigate various aspects of language, and the link between learner corpus research and second language acquisition, language teaching and natural language processing.

Díaz-

Introduction

Corpora in language development research are collections of naturalistic interactions of children and their surrounding environment. They usually comprise several subcorpora corresponding to different target children. The underlying purpose of developmental corpora is to learn about children's proficiency and to understand how they use language in their natural environment. Thus, corpora allow the researcher to find out what children do in natural interaction, in contrast to experiments, which test what children can do. Ideally, a language development corpus presents an ecologically valid and representative picture of the linguistic development of language learners. To capture how children learn language, the change in their development of vocabulary, grammar (e.g. morphology, syntactic constructions) and pragmatic understanding are monitored over a predefined time window. Thus, developmental corpora are necessarily time series data, whose internal structure is important. To model the child's proficiency at a specific point in time or over a period of time, a session by session comparison of the child's and surrounding adults' constructions is key. The raw data in developmental corpora are audio-visual recordings, which are transformed into transcriptions, either phonetic or, more usually, orthographic (see also

Two main types of developmental corpora are used in the field: cross-sectional and longitudinal corpora. In cross-sectional corpora, specific age points of interest are identified and a number of children with the respective ages are recorded. Each child is recorded individually in either naturalistic or semi-structured contexts, depending on the purpose of the study. In this type of corpus, development is inferred via between-group comparisons, i.e. by averaging over a large number of participants each of which is recorded only at one point in their development.

In longitudinal corpora, on the other hand, the development of individual children is estimated based on temporally ordered samples of the same child. Each child is recorded at regular intervals at a number of consecutive time points, usually stretching over several months or even years. Thus, longitudinal corpora portray the individual development of a few children and thereby capture individual differences in developmental curves rather than inferring a general but averaged profile of productivity as it is the case in cross-sectional studies.

Since recording over several years is time-consuming, a staggered design of several longitudinal studies of children including different age spans is often applied (cf. e.g. the Chintang corpus in Box 1). In this design, several children are observed over a predetermined time span but recordings start and end at different ages for different groups of children. Instead of recording, for instance, two children over two full years, four children of two different ages (e.g. two 2-year-olds and two 3-year-olds) are recorded over one year so that the same age spans are covered.

Cross-sectional and longitudinal studies both have advantages and disadvantages. Cross-sectional studies require large numbers of children at each interval because the individual variation across children is huge (e.g.

In the following we focus on longitudinal designs, which are more prevalent in research on language development, but many issues such as annotation layers or metadata are relevant for both corpus types.

Fundamentals

Longitudinal corpora imply regular recordings over at least several months, transcriptions of each utterance (ideally not only of the target child, but of all interlocutors), and a multitude of different annotation levels, depending on the questions of the respective project. This makes the development of longitudinal corpora a logistically difficult, time consuming, and ultimately very expensive endeavor. The ultimate design goal therefore is to create corpora that are sustainable and suitable for quantitative analyses of a wide range of topics rather than focusing on a single research question. In the following we present the main steps and layers in corpus design and their respective challenges.

Recording and Contextual Setting

The raw data of longitudinal corpora are audio-visual recordings of natural interactions between a target child and her surrounding communicative environment. Video is an important feature not only because the speech of young children is often difficult to interpret without context but also because language development is multi-modal and involves speech that interacts tightly with gesture and gaze (see Chap. 16). Most corpora nowadays include at least some video recordings.

A major difference to other spoken corpora is the strict recording scheme. Regular and evenly spaced recordings document how the child's language develops by adapting more and more to the adult target.

The naturalness of the recording contexts of existing corpora varies widely. Some researchers prefer to include only the main caretaker and the target child in a play situation. This setting facilitates transcription but sacrifices ecological validity. It is known that the recording context has a strong influence on the type of constructions used, both in child-directed speech and by the children themselves. Studies that strive for a recording situation that is as natural as possible and thus do not filter for participants or situations are more representative because they allow to better generalize from the recordings to the rest of the child's linguistic encounters. It is worthwhile to invest into more complex transcriptions that include the typical participants of the natural environment of the child in order to avoid biases in the characterizations of child-surrounding speech.

To supplement information about speech, researchers have started to use childmounted cameras to capture the focus of the child's immediate environment. The main goal of this type of recording is to link the characteristics of the speech of the child to her immediate linguistic and extra-linguistic environment. The visual context is important because the joint attentional frame in conversations as well as the types of responses and world-to word mappings are highly relevant for word learning (e.g.

Subject Sampling

The number of children is one of the most important decisions in the design of a study, having the potential both to make data statistically (more) robust and to increase the amount of work put into a corpus ad infinitum.

While recent corpora tend to include more children, there is (to the best of our knowledge) not a single publication on how to determine the ideal number of children in longitudinal studies. Two children are obviously better than one because one child might happen to be either precocious or a late starter, but two children will also not remedy this problem. However, with more children the question is not if their number improves the reliability of data but rather to what degree it does. This point is crucial considering how additional target children multiply the amount of time and money required to compile a corpus with minimum annotation standards. Numbers of 80 children per community and corpus, as studied in the 0-5 Program (see Representative Corpus 2), are a role model of corpus development but go far beyond the possibilities of most research groups, especially if non-WEIRD (Western, Educated, Industrialized, Rich, Democratic;

The extensive variation in development asks for statistical methods that are suitable for corpora consisting of several case studies. A major factor for the analysis of these corpora is individual variation, i.e. children of the same ages usually vary extremely in their linguistic competence

Variation is conditioned by a multitude of factors for which the selection of participants aims to control as far as possible. As a consequence, most developmental studies feature target children of different sexes. However, the differences between sexes reported in the literature are small

By contrast, a variable that is definitely worth to control for is socio-economic background since there is large variation in the input to children of different SES 14 Child-Language Corpora 309 groups (e.g.

Size of Corpora and Recording Intervals

Probably the strongest limiting factor for the results and the conclusions that can be drawn from a corpus is the amount of recordings. Ideally one would record all the speech of the child and her environment. This approach has been taken in the Speechome project

The type of constructions and vocabulary used by children varies by extralinguistic contexts and activities. To ensure a faithful assessment of the child's abilities, recordings ideally include a variety of daily situations. In a recent effort this corpus scheme has been extended to daylong recordings (cf. VanDam et al. 2016,

Daylong recordings of the target child and their surrounding environment are often conducted with LENA devices (LENA Research Foundation, Boulder, Colorado, United States). LENA devices audio-record and simultaneously analyze the utterances produced by the child and her surroundings. They provide automatic estimates about the number of adult words surrounding the child, the number of turns, and the number of child vocalizations. This is very valuable information to estimate the quantity and type of input a child is exposed to. However, the underlying algorithm has only been validated for English, French and Mandarin so far. Further, in audio-only recordings it can be challenging to match the situational context to the respective utterances. If complemented by video, LENA recordings can provide great insights into a large range of situations, provided that all the data is transcribed. This, however, can quickly become a challenge with large numbers of participants and long recording sessions.

Sample size and sampling regimes have an enormous influence on the estimation of the child's development

Bergelson and colleagues show that hour-long video recordings record a special situation, which is presumably not representative for the language a child hears and uses on average over the day. In these hour-long recordings caretakers play with the children and hence provide a much denser input than during other activities typical for the rest of the day.

This shows that recording intervals and length of recordings can have dramatic consequences for claims about development and productivity if delays are not projected based on underlying frequency distributions. Productivity is one of the most relevant measures in language development research as it captures the acquired competence of the child to use language like a mature native speaker. However, the underlying frequency distributions may likewise not be easily retrievable from corpora that are not dense enough. To avoid this, the following issues are key:

• The recall

• Small samples combined with low-frequency X result in unreliable data. Soothing as this may sound, "small" and "rare" are relative terms. For instance, even a standard sampling density of one hour per week will barely succeed in capturing a not-so-rare X with seven expected instances per week. • The more frequent X, the steeper the hit rate gain induced by increased sampling density. Note this also entails that the more frequent X, the earlier increases in sampling density do not add significantly to hit rates.

An additional problem stressed by

It is worth noting that pooling data in such cases does not necessarily help because it creates new problems. High-frequency items thrown together with lowfrequency items will dominate the pool, which may have serious consequences for linguistic interpretation. For instance, English-speaking children produce more errors for rare irregular verbs than for frequent ones, but this fact might be obscured by pooled data because specific errors by individual children might be misidentified as rare in the larger sample

The consequences of these observations for researchers planning to compile or analyze a developmental corpus are serious. Researchers should no longer rely on their intuitions when estimating how robust their data are but prop them up by more reliable quantitative considerations. In the following some more concrete suggestions are given.

A central notion is sampling coverage, which can be defined as the proportion of the data in question that is captured by the recording scheme. For instance, if we take one week as our reference interval and go with

• Given a sampling coverage c and an estimated absolute frequency Λ of X per interval, what will be the average catch λ of x per interval?

For instance, if we record 2 hours per week and continue to assume that a child talks about 70 hours per week, the sampling coverage will be 2 70 = 0.03. A phenomenon that occurs 10 times per week is then expected to be observed 0.03 • 10 = 0.3 times per week (in other words, not a single time: on average, a full month will pass until the first instance is observed).

• Given an average catch λ (calculated from c and Λ as above), how high is the probability of capturing at least one x (i.e. the hit rate r)?

For instance, let us assume we want to observe an X that we estimate to occur 80 times per day or 560 times per week and we record half an hour per week, so the sampling coverage is 0.5 70 ≈ 0.01 and the average catch is 0.5 70 • 560 = 4. Then the probability of capturing at least one x is 1e -

• Given an estimated frequency Λ of X and a number λ of X to be captured per interval on average, what should be the sampling coverage c, and what is the number of hours h to be recorded per interval (given the total number of hours spoken by the child per interval, H)?

For instance, if we estimate that X occurs 500 times per week and would like to capture 10 instances on average, the required sampling coverage is 10 500 = 0.02. In other words, the number of hours we should record per week is 10 500 • 70 = 1.4 (i.e. roughly one and a half hours).

• Given an estimated frequency Λ of X and a desired hit rate r, what should be the sampling coverage c and the hours h to be recorded per interval?

For instance, if X is as rare as only occurring 5 times per week and we want to have a 99% 4 probability of catching at least one X in an average recording week, the sampling density should be -log e (1-0.99)

5

≈ 0.92. The number of hours to be recorded per week is then 0.92 • 70 ≈ 65. In other words, our goal is unrealistic.

The last two formulas are especially important because they also allow researchers to derive a recording scheme directly from their research interests. Even if the precise frequency of the phenomenon of interest is not known, estimating it and calculating the required sampling density on that base is still much more objective than following a gut feeling or simply choosing the density that is currently most common.

It is worthwhile mentioning that the concepts of sampling density and coverage used above conflate two aspects of sampling which are of great relevance for theory and practice, viz. sampling intervals (e.g. one month between samples) and durations of recordings (e.g. 2 hours per sample). If we were interested in a phenomenon that occurs about 20 times per hour (1400 times per week, 5600 times per month) and wanted to capture 25 instances per week (100 instances per month), the required sampling densities are 25 1400 • 70 = 1.25 hours per week or 100 5600 • 280 = 5 hours per month. This makes a practical difference: recording e.g. a 5-hour sample within a predefined week of the month (the sample can be subdivided in several recordings of different length within this week) will on average be easier to accomplish than recording a 1-hour sample every week, which implies a high demand of discipline both of the recording assistant and the families. In addition, increasing both sampling intervals and durations also has the advantage of increasing the hit rate and improving the sampling density per point in time. In other words, while we lose some granularity in this approach (developments can only be observed in larger steps), we gain confidence in our observations. To sum up, this implies that a sampling regime of 4-5 hours within a predefined week per month is preferable to a sampling regime of 1 hour per week even though the same amount of hours is recorded.

Transcription

Although transcription may seem to be the precondition for all kinds of data annotation, it can in fact be viewed itself as a kind of annotation or "data reduction"

Transcription can be split up into several subtasks, viz. segmentation (detection of time stretches that contain speech), speaker assignment (connecting segments to speaker metadata), and transcription proper (putting text on segments; see also

An important linguistic parameter of transcriptions is phonetic granularity. Most developmental studies (apart from studies on phonological development) do not require a high level of phonetic precision. Instead, a simple phonological or orthographic transcription is sufficient. Even when research questions make it necessary to transcribe data phonetically, it is helpful to have an additional tier for coarser transcriptions, which represent less variation and are therefore easier to search. Coarse phonological transcriptions may also serve as a surrogate to full orthographic normalization when the latter is not feasible.

Transcription is often the bottleneck of corpus development. In the case of underresearched languages, large amounts of data paired with resource pressures often make it seem hard to impossible to transcribe all data at once.

Another option to reduce transcription efforts is to transcribe only children's utterances. While this does not create any statistical problems, it makes it impossible to take child-surrounding speech into consideration. As a consequence there is no way to compare the development of the child to the behavior of mature native speakers and this is all what naturalistic corpora are about.

Thus, there is no way around transcribing all data in each recording session. In order to be able to spot developments even when the corpus is not (yet) fully transcribed, it is useful to transcribe data according to a systematic "zoom-in" pattern, where one starts with the first and the last recording of a child, then transcribes the recording in the middle between the two, then the two recordings in the middle of the stretches thus defined, and so on.

Moving beyond these basic questions, there are a number of transcription problems which are specific to developmental corpora. An excellent overview of these can be found in

• The phonology of child language is different from adult language. For researchers interested in phonological development, a simple orthographic transcription therefore will not suffice -they will need an additional tier for phonetic transcriptions. But even researchers interested in other aspects of language face the problem that children frequently produce deviant forms (as compared to adult language) such as

Transcribing only the actual form often makes it difficult to understand what was said. Transcribing only the target form, on the other hand, obscures the fact that the child produced an alternate form.

There are basically two ways to solve this dilemma, depending on research interests and available resources. The maximal solution is to transcribe both levels on independent tiers.

• The distinction between actual and target forms is closely connected to error coding, which can, however, also span multiple words and give more precise information regarding the kind of error made. Like all semantic layers that are logically independent of transcription proper, error codes, too, should be specified as an independent layer (e.g. an additional tier) if a researcher is interested in them.

• It is often desirable to link utterances to stretches on the time line. This makes it easy to locate utterances in a video to review its context and to correct or add annotations. Time links also have theoretical applications in language development, where they can e.g. serve as the base for calculating input frequencies based on real time. Researchers wishing to include time links in their corpus are advised to use a software that creates them as part of the transcription process such as ELAN (

• The addressee of utterances is of special interest for language development research because there are important differences between child-directed and adult-directed speech. Thus, it can be useful to code for addressees on a separate tier.

Metadata

Any corpus relies on metadata to correlate the speech of the child and her surroundings with social variables. The most important metadata are the recording date, the identity, birth date, and sex of participants, and the role a participant takes in a given recording (a cover term for kinship relations such as "mother" and functional roles such as "recorder"). Many more fields are provided for in the multitude of XML metadata profiles contained in the CMDI

Participant metadata are commonly linked to the participants' utterances via short participant IDs that may be numeric or alphabetical. This both makes it easier to type in the speaker of an utterance during transcription and largely anonymizes the data (given that the metadata are stored in a different file). This is especially relevant if the data is shared more widely.

Further Annotations

Most questions require further annotations for making information contained in the data more explicit. Such annotations are the base for automated quantitative analysis. These annotation steps are often referred to as "tagging" or "coding" in the literature.

One of the most common options for further annotations are utterance-level translations, which are a prerequisite for cross-linguistic research. Without translations, project-external researchers who are not familiar with the language will not be able to use the data. While such researchers might rely on glosses instead (for which see below), glossing is in turn greatly facilitated by utterance-level translations and in many cases only becomes possible through them (e.g. when glosses are done by student assistants, who might not be as familiar with the language as the researchers). This also concerns corpora of languages with strong institutional support: in a globalized world it no longer seems fair to assume that everybody speaks a handful of European languages for which translations would therefore be futile.

Studies with a focus on the development of grammar rely on grammatical annotations, especially lemmatization, parts of speech, and interlinear glossing (cf. below).

There are two main glossing standards used in language development corpora. The standard implemented in the CHILDES database (the CHAT tier %mor:) ignores the function of lexical morphemes and the shape of grammatical morphemes and conflates what remains on a single tier. Thus, the CHAT version of the Chintang (Sino-Tibetan, Nepal) utterance Pakpak cokkota would look as follows:

(1) * MOT: %mor:

Pakpak pakpak cokkota. ca-3P-IND.NPST-IPFV %eng: 'He's munching away at it.' While this saves space, this glossing style also has several disadvantages. First, it does not follow the principle of separating distinct semantic layers (such as segments vs. morpheme functions) in the annotation syntax, thus making this format harder to read, process, and convert than others. Second, a researcher who does not speak Chintang will not know what pakpak and ca mean. Even an utterance translation will not help with longer utterances, where it becomes increasingly hard to identify words and morphemes of the object language with the elements of the translation metalanguage. Such types of glosses are thus less reusable and sustainable.

Further, this format does not allow to search for morphemes in the sense of formmeaning pairs. For instance, a search for the segment ca paired with the gloss 'eat' or a search for the form -u paired with the function '3P' will unambiguously select two morphemes with a very low possibility of confusion with other morphemes. However, such searches are only possible in formats with complete form and function tiers. In the CHAT format one might search for ca, but this will include homophones and exclude synonyms. Similarly, a search for the gloss 3P may easily yield other morphemes with the same function even when we are only looking for the one with the underlying shape -u.

Another common option are true interlinear glosses, which can be seen as a combination of segmentation into morphemes and morpheme-level translations. This type of glosses is also common in general linguistic publications and specifies both the shape and function of all morphemes. Below an example from the Chintang corpus is given (in its native format, Toolbox). The first tier is an orthographic transcription tier similar to the CHAT example. The second tier segments words into morphemes, given in their underlying form. The third tier assigns a gloss to every morpheme, which is a metalanguage translation in the case of lexical items and a standardized label in the case of grammatical morphemes. The fourth tier contains the English translation of tier 1. Morpheme-based glosses are a precondition for any searches of basic semantic units without knowing the language. When glosses are used that correspond to a standard such as the Leipzig Glossing Rules (

Another common semantic layer are parts of speech (POS). POS annotations are crucial for analyses of grammatical development (see Chap. 2). While glosses are highly useful even without POS tags, the reverse is hardly true because POS tags alone do not make it possible to search for specific functions or morphemes. Thus, POS tagging should be done simultaneously with or after glossing. The combination of the two layers has many applications in developmental research, thus making POS tags another recommendable tier. As always, it is advisable to keep POS and glosses on distinct tiers.

Beyond the basic layers just discussed, there is a plethora of further possibilities that cannot all be listed here. Some common examples include the annotation of semantic roles or other properties linked to reference, syntactic dependencies, speech acts, errors on various levels, or actions accompanying speech (gaze and pointing).

(3) shows an example for syntactic dependency annotations in a Hebrew corpus created by an automatic parser

Ethical Considerations

Language acquisition data are highly sensitive due to several reasons. Children are an especially vulnerable population and are not considered capable of giving informed consent by many legislations. Recordings are typically taken in intimate settings and if more participants than just mother-child dyads are present they will start talking about anything that may be considered taboo by a given society once they have gotten used to the camera. Moreover, longitudinal designs and developmental questions require tracking participants and collecting metadata such as full names, addresses, and birth dates, which makes the raw data quite the opposite of anonymous.

Ethical considerations should therefore be an integral part of language acquisition research. Besides obtaining ethics clearance from institutional reviewing boards and/or funding agencies, researchers need to have sufficient knowledge of the socio- In communities speaking underdocumented languages, involving the community itself may be an additional concern and data protection is of special importance since these communities are often tightly knit.

The described high demands are often directly opposed to long-standing claims for more exchange and interoperability of language acquisition data, which are recently being refueled by the open access and open data movements. Publishing language acquisition data without taking any measures for protecting subjects is ethically highly problematic. Researchers must therefore be aware of data protection techniques such as pseudonymization (also known as "coding"), anonymization, aggregation, and possibly encryption. The organizational design of databases is just as important and should cover aspects such as user roles, access levels, and longterm use. Direct collaboration with data owners helps to build trust and provides the additional advantage of getting access to rich knowledge of the cultural context.

Representative Study 1

Huttenlocher, J., Haight, W., Bryk, A., Seltzer, M., and Lyons, T. 1991.

Early vocabulary growth: Relation to language input and gender. Developmental Psychology, 27(2), 236-248.

One frequent topic of developmental studies is the development of basic vocabulary. Studies from this area most often rely on transcriptions without considering grammar. In order to understand the enormous variation that children show in this regard, innate predispositions and environmental variables need to be tested in a large number of children. In a large-scale study

Representative

14 Child-Language Corpora learned constructions have an influence on later similar constructions, i.e. they either support or impede their development. The authors show that the learning of the German passive construction, which is build with the auxiliary sein, is supported by previously acquired copula constructions which are build with the same auxiliary. The other passive construction built with the auxiliary werden was learned much later and was not supported by the productivity of prior related constructions. In the analysis of the German future tense, which is also build with the auxiliary werden, they found that a semantically similar construction delayed the acquisition of the future tense. Such results can only be obtained by dense corpus studies allowing us to detect relatively rare phenomena such as passive constructions.

Representative Corpus 1

The Thomas Corpus

Representative Corpus 2

The Language 0-5 Corpus (

Representative Corpus 3

The Chintang Language Corpus

For each child, 4 hours were recorded per month within a single week. The corpus is richly annotated, e.g. with translations, Leipzig Glossing Rule-style interlinear glosses and POS tags for all participants.

Critical Assessment and Future Directions

The main goal of language development research is to understand how language can be learned by children. For this we need to know how children and the people surrounding them use language in their natural environment. Observational corpora are the best tool for estimating how distributions in the input relate to learning. However, they come with big costs; the logistics of compiling such corpora are extremely demanding and therefore most corpora include only a small number of children with limited recordings. As a consequence, one of the major impediments to progress in our field has been the quantity of data we have at our disposal. Quantity here refers both to (i) the size of individual corpora (e.g., number of participants, length and interval of recordings) and (ii) the number (and type) of languages we have data for.

Why is this relevant? Two issues are key here: Corpus size is relevant because we need reliable frequency distributions of a large range of structures and constructions to estimate developmental growth curves. This requires dense corpora such as the Max Planck dense databases. So far, this type of data is only available for a small number of participants in two languages. A main challenge is to extend this approach to a wider number of participants as pioneered in the 0-5 project presented above. Since high density recordings are very demanding both for the people recorded and the researchers, this might not be a valuable avenue at least for a wider range of languages. A potential alternative might be to combine individual high-density studies with large-scale cross-sectional studies.

The number of languages is relevant because understanding language development requires understanding how children can learn any language. However, the corpora that are currently available are still heavily biased towards Indo-European languages spoken in Europe. For only about 2% of the languages of the world at least one study of language development is available, and this study often concerns a single feature of the language under investigation

There is another good reason besides sampling issues why language development research is in dire need of more diverse corpora. As pointed out by

To appreciate the role of different structural features in development, we need to be able to compare structures of different languages. Thus, while a large amount of data is already available, it is by no means trivial to explore them in a uniform way. Corpora usually have their own idiosyncratic study designs, including the number and age of children involved, the sampling regime, and the type of contexts in which recordings are taken, let alone the different conventions used for annotations. Comparability is sometimes complicated by rather technical reasons such as different choices of file formats, corpus formats and syntax, and coding schemes. CHILDES and the CHAT format as well as widespread markup languages such as XML have made a great contribution to unification but still leave room for enormous variation, even in areas that are of great interest to many researchers such as word meaning, morphology, or syntax. Thus, more standardization, also linking to standards existing in other corners of corpus linguistics, seems an indispensable step for exploiting the full range of developmental data that we have. A recent effort to unify glossing and initiate collaborative comparative research was undertaken in the above-mentioned cross-linguistic database ACQDIV composed of longitudinal corpora from typologically diverse languages. The database features unified glosses, POS tags, and other variables that are relevant for comparative studies (for a description of the design see

To sum up, the two issues of quantity are tightly intertwined. In short, we need more data for more diverse languages. To achieve this goal, we need massive improvement of automatic transcription devices and automatic interlinear glossing. To include a wider set of languages in our samples we will need to focus more on fieldwork on less well-known languages. For this, specialists of language development ideally team up with linguists specialized in these languages. For remote and undescribed languages, however, it is usually impossible to conduct high density studies, let alone to record more than a handful of children and subsequently transcribe and annotate the data. In addition, fieldwork in culturally diverse settings necessitates a plethora of ethical considerations related to language attitudes, language policies, and privacy issues, which need to be resolved in collaboration between communities and fieldworkers

As of now the field is also still waiting for innovative ways of reconciling high ethical standards and the demand for open access. One possibility is to implement a sophisticated, multi-layered system of access, to make sensitive data publicly available in an aggregated format that does not allow inferences to associations between individual utterances and speakers, or to fully encrypt the data so that structure is preserved but meaning (including names) is lost.

Thus, the major challenge for observational language development research is to overcome these impediments and to introduce big data. Only then will we be able to conduct large scale meta-analyses as pioneered by

Tools and Resources

One of the most prominent and important platforms for data sharing and corpus tools in language development research is CHILDES (MacWhinney 2000). CHILDES is part of the TalkBank system, which is a database for studying and sharing conversational data (

The database currently contains corpora of over 40 languages. The size, depth, design, and format of the corpora vary widely. Data range from first language development and bilingual corpora to clinical corpora. CHILDES also provides a number of tools for corpus development including transcription and annotation programs.

Another recent approach to hosting corpora and initiating collaborative research is the ACQDIV project (

Further Reading

Meakins, F., Green, J., Turpin, M. 2018. Understanding linguistic fieldwork.

Routledge.

This comprehensive volume on fieldwork techniques provides an in-depth practical introduction to fieldwork on small and/or endangered languages. It also includes a profound chapter with ample and very useful practical instructions for building up corpora on language development in such environments.

Introduction

Over the past two decades the internet has become an increasingly pervasive part of our lives, leading to significant changes in well-established ways of carrying out everyday tasks, from catching up with the news and keeping in touch with friends to grocery shopping and job hunting. The latest statistics suggest that there are over 4 billion internet users worldwide, with a growth rate of over 1000% since 2000. 1 It should come as no surprise then that, during the same period, the web has had a major impact on well-established ways of doing things in the field of corpus linguistic research too. Part of this impact has involved the increased availability of 330 A. Kehoe existing resources, with several of the corpora discussed in previous chapters now available online. For example, the British National Corpus (BNC) is searchable on the Brigham Young University website,

What this chapter is more interested in, however, is the growing use of the web itself as a linguistic resource. As the chapter will go on to explain, this research area has diversified in recent years to include a wide range of different activities but what they have in common is the use of linguistic data from the web, either in place of data from standard corpora like the BNC or to supplement it. The key benefits of the web over such corpora are its size and the fact that it is constantly updated with new texts and, thus, examples of the latest language use. Even a 100 million word corpus like the BNC is too small for some purposes, such as lexicographic and collocational research. Most words in the BNC occur fewer than 50 times, which makes it difficult to draw firm conclusions about their meaning

Fundamentals

There are two main approaches to the use of web data in corpus linguistic research, which have been termed 'web as corpus' and 'web for corpus

Web as Corpus

This 'entry level' approach uses commercial search engines such as Google to access the textual content of the web. When considering the term 'web as corpus', the first question we must ask is whether the web can actually be classed as a corpus according to the criteria set out in Chap. 1. On the surface there are similarities between conventional corpora and the web, which have led some researchers to refer to the latter as a 'cybercorpus'

The World Wide Web is not a corpus, because its dimensions are unknown and constantly changing, and because it has not been designed from a linguistic perspective. At present it is quite mysterious, because the search engines [ . . . ] are all different, none of them are comprehensive, and it is not at all clear what population is being sampled.

Sinclair's first objection relates to corpus size. From the 1 million word Brown corpus of the 1960s to the 100 million word BNC of the 1990s, the resources used by researchers in the field have conventionally been of known (usually finite) size. The web, in contrast, is indeed 'quite mysterious'

An example will illustrate the limitations we face if we attempt to treat the web as a corpus using conventional search engines. Figure

Figure

There are other, more specific problems in Fig.

A. Kehoe

Finally, although Google claims to have available 'About 818,000 results' in Fig.

Since the late 1990s when linguists began to turn to the web as a corpus, search engines have actually become less advanced in terms of the options they offer. Full regular expression search has never been possible but useful features for linguistic study such as wildcards and the 'NEAR' operator (for finding one word in close proximity to another) have gradually been removed over the years. Commercial search engines are geared towards information retrieval rather than the extraction of linguistic data. What they do, they do increasingly well but they are less than ideal for linguistic research. This is why several tools have been developed to 'piggyback' on commercial search engines and add layers of refinement specifically for linguistic study, including KWiCFinder

Although WebCorp Live offers advantages over direct use of commercial search engines (and is still widely used for this reason), it does not solve the underlying problems of the web as corpus approach. Quantitative analysis -one of the core activities of corpus linguistic research -is not possible as we do not know the total size of the web 'corpus' held on the search engines' servers. It is also unclear exactly how the search engines decide upon 'relevant' matches for a query. Google results are selected and sorted by a proprietary measure of relevance (PageRank: Brin and Page 1998), which is altered regularly in unpredictable and undocumented ways. This problem has become more acute in recent years following the introduction of personalised search results based on factors such as geographic location and previous web activity.

A final, more practical problem is that search engines are becoming increasingly difficult to use in linguistic study at all. WebCorp Live originally used a process known as 'web scraping': the extraction of useful information from the HTML code of a web page, in this case the 'hit' URLs from the Google results page and examples Few researchers would now claim that the web is a corpus in any meaningful sense, but the web as corpus approach can still be fruitful for certain kinds of research and it is particularly useful for introducing newcomers to the field. Perhaps a more suitable term for the activities described in this section is 'web as corpus surrogate'

One final point to be made in this section is that there has been a growth in recent years in research outside the field of corpus linguistics which uses web data and standard web search tools to answer what are essentially linguistic questions. So-called 'culturomics'

Web for Corpus

The second major strand of web-based linguistic study has seen researchers attempt to overcome the limitations described above. This web for corpus approach has also been referred to as 'web as corpus shop'

However, although the web for corpus approach is considerably more systematic than the web as corpus surrogate approach described above, it is not fully comparable with the conventional corpus compilation process (see Chap. 1). The key issue here is one of representativeness, which 'means that the study of a corpus (or combination of corpora) can stand proxy for the study of some entire language or variety of a language'

In fact, early web navigational aids such as the Yahoo! Directory attempted to impose library-style hierarchical classification systems on web texts, with human editors employed to curate content by subject. However, this approach was not scalable to the increasingly vast web and such directories have now largely been replaced by keyword-based search engines like Google. This means we are still largely dependent on such search engines as gatekeepers to the textual content of the web. However, the 'web for corpus' approach utilises search engines in a different way, using them at the initial stage of corpus building only. This approach was popularised by

BootCaT is available as a user-friendly but rather limited front-end or as a series of command line scripts for more advanced users. We will describe the front-end here to introduce the general principles. The first step in the process is to supply a list of 'seed' words from which the corpus will be grown by the software. The type of corpus required will determine what seeds should be chosen. The BootCaT manual gives a simple example for the building of a domain-specific corpus on dogs, with the seeds dog, Fido, food hygiene, leash, breeds, and pet. The software takes these seeds and combines them into tuples with a length of three, e.g. Fido leash dog. All possible unique tuples are generated from the seeds supplied and each in turn is then sent to Bing using its API (previously Google). The assumption is that it is possible to build a corpus covering a particular domain (in this case dogs) by using a commercial search engine to find web pages containing words likely to occur in that domain. As an initial step, BootCaT fetches 10 hits from Bing for each tuple then downloads and processes the corresponding web pages to build a corpus in the form of a text file.

Although this example is rather basic, the same underlying principle has been used to build much larger reference corpora, by the BootCaT team and by other researchers.

When building general reference corpora from the web we need to choose seed words that are likely to appear across a wide range of topics, but there has been some debate about exactly what constitute good seeds for a corpus of this type.

Whatever seeds are chosen, this is only the first step in the process of building a corpus from the web. In all but the most basic examples, it is likely that the researcher will want to expand the corpus beyond the initial set of seeds. There are two main ways of achieving this: (i) through the addition of further seeds, or (ii) by using a web crawler. For the first option, the more advanced BootCaT command line scripts add another stage where further seeds are extracted by comparing the initial corpus with an existing more general corpus, e.g. the basic dogs corpus may include key words such as canine, labrador, and barks which can be combined and sent to the search engine to extract further hits. Hence, the full BootCaT approach is one of 'bootstrapping' or iterative refinement.

A. Kehoe

This approach is suitable for the building of domain-specific corpora but the second approach, the use of a web crawler, is more appropriate for building of general reference corpora. In simple terms, crawlers (sometimes known as spiders) start with an initial list of URLs, download the corresponding documents, extract hyperlinks from these documents, then add these to the list of URLs to be crawled. In theory this process could run indefinitely but crawls run for corpus-building purposes tend to be restricted to a fixed time period. The most popular crawler in web for corpus research is the open-source Heritrix system (used by Kehoe and Gee 2007 amongst others). However, there are alternatives available, including the command line tool Wget (used by

Whichever tool is chosen, it is important to crawl the web in a responsible manner, observing the robots exclusion standard. This allows website owners to specify (usually in a file called 'robots.txt') which parts of a site should not be crawled. It is also important that crawling takes place as slowly as the name suggests. A crawler should not be configured to download multiple pages from the same website simultaneously or in quick succession as this may have an impact on access speeds for standard users of the site.

When the crawl is eventually complete, several other steps are usually carried out to 'clean up' the downloaded web documents before they are added to a corpus. The user-friendly BootCaT front-end carries out some of these tasks automatically but most researchers opt to use dedicated tools for these individual tasks to retain more control over the process. There are a number of options available for each task in the corpus building 'pipeline', as outlined below:

Boilerplate Removal The term 'boilerplate' is commonly used to refer to features such as navigation menus, copyright notices and advertising banners which do not form part of the main textual content of a web page. As such features are often repeated verbatim across multiple pages on a single website, it is desirable to remove them to prevent the words they contain from becoming unnaturally frequent in the resulting corpus. The boilerplate removal process usually goes hand in hand with the stripping of HTML markup code from web documents as HTML can offer clues about what is boilerplate and what is the main textual content. Several web for corpus projects (e.g.

Document Filtering

After HTML code and boilerplate have been removed, it is often useful to apply additional filters to the downloaded documents. The exact choice of filters is dependent on the intended nature of the corpus and research aims, but size and language filters are amongst the most common. Size filters are designed to remove very short and very long documents from the corpus. The assumption is that very short documents are unlikely to contain much connected prose, while very long documents are likely to skew the corpus unnaturally.

Duplicate Removal

The nature of the web means that not all documents are unique. In some cases, multiple URLs will point to the same document, while in others the same document will appear on multiple websites (mirror sites and archives). The latter also happens with content released by news agencies, which is reused on many sites worldwide. As a result, any web-derived corpus is likely to include documents that are either exact duplicates of one another or are very similar. Like boilerplate, such documents can skew word frequency counts so it is desirable to remove them from the corpus. It is possible to do this by writing software that produces a 'fingerprint' (or hash) for each document and then compares these to find similarities. Software libraries are available in several programming languages to help with this, including the Text::DeDuper module in Perl. An easier alternative is to use an 'off the shelf' tool for the automatic detection of duplicate and nearduplicate texts, such as Onion (ONe Instance ONly).

After these steps have been performed, the corpus can then be tokenised, lemmatised and part-of-speech tagged using the standard approaches described in Chap. 2. One thing to bear in mind when dealing with web data is that it can be rather 'noisy'. For instance, it may lack punctuation or whitespace and there may be a higher proportion of non-standard spellings than in conventional texts. These factors may have an impact on the accuracy of corpus annotation since the linguistic models underpinning off-the-shelf annotation tools are usually derived from standard written language. For example, most popular part-of-speech taggers are trained on newspaper text, a register which tends to follow fairly strict style guides and contain few errors.

It is important to stress that the decisions made at each stage of the web corpus building process will have a significant impact on the resulting corpus, in terms of size but also in terms of composition. Activities such as boilerplate stripping, deduplication and additional filtering can remove a considerable proportion of the documents retrieved through web crawling. In an extreme case,

The approach to crawling described so far is one designed to maximise the number of documents downloaded and, thus, maximise the size of the final corpus. In recent years most compilers of web corpora have worked on the assumption that maximising corpus size is likely to result in a more representative corpus, even if we have no reliable way of measuring this (see Representative Study 2 for a discussion of the related topic of balance in web corpora). Some researchers have gone further still, dismissing the notion that representativeness is achievable or important in web corpora.

An alternative approach to large indiscriminate crawls is to focus on specific websites, thus limiting the size of the textual universe and increasing the chances of building a representative corpus. For example, some newspapers make archives of articles from previous years available online. These can be downloaded by pointing the crawler to the homepage and instructing it to follow links within the site only, or through the use of an API if the newspaper makes one available. An advantage of this focussed (or 'scoped') approach to crawling is that there is unlikely to be much spam or duplication of content and all documents are likely to be in a similar format, making boilerplate removal more straightforward. With news texts it is also much easier to determine publication date: something which can be very difficult to do on the web in general (see Representative Corpus 1 for information on the NOW corpus).

Blogs offer similar advantages when it comes to boilerplate removal and text dating (see Representative Corpus 2 for more information on the Birmingham Blog Corpus) and are one variety of so-called 'Web 2.0' content, which is being used increasingly in linguistic research. Blogs are similar to conventional web texts in terms of layout and in the fact that they can be located using commercial search engines. This is not true of all Web 2.0 texts though, and corpus linguists have had to devise new methods to access the language of social media sites. There has always been a so-called 'deep web'

Twitter is easier to access for crawling purposes and it is now being used more widely in linguistic studies as a result. Early studies tended to download tweets manually but there are now a number of automated tools available. Options include the FireAnt package designed specifically for corpus linguists, as well as general purpose software such as IFTTT, Import.io, and TAGS. The last of these, standing for Twitter Archiving Google Sheet, automatically downloads tweets matching particular search parameters into a spreadsheet which can subsequently be used to build a corpus. Whether existing corpus methods are entirely suitable for the analysis of texts of 280 characters or fewer is a separate question requiring further research, but it is certainly possible to build Twitter corpora and conduct interesting linguistic analyses of them (e.g.

As can be seen from the above discussion, the 'web for corpus' approach is rather more complex than the previously described 'web as corpus surrogate' approach. The building of bespoke corpora from the web usually requires considerable technical skill and, thus, may not be an option for all corpus linguists, especially beginners. Fortunately, there are other options. Several teams of researchers have already built large web-derived corpora and made them available to download or search online (see Sect. 15.4). There is also the commercial Sketch Engine tool, which is now available free of charge to academic users within the EU (2018-2022). Sketch Engine includes a range of pre-loaded corpora, including the TenTen web corpora, and makes these available through a novel search interface. In addition, Sketch Engine provides user-friendly tools that allow linguists to build their own web-derived corpora (based on the BootCaT technology discussed in Sect. 15.2.2). Until relatively recently, corpus-based studies have tended to focus almost exclusively on the English language, and on British and American English in particular. The web offers new possibilities for the study of World Englishes and other languages for which there are no BNC-style reference corpora (continued)

Representative

A. Kehoe available. One specific area where the web has had a transformational impact is in the building of parallel corpora for use in multilingual natural language processing. Work by Resnik & Smith on the STRAND system was pioneering in this field.

Parallel corpora are pairs of corpora in two different languages where the texts in one are translations of those in the other (also called bitexts; cf. Chap. 12). STRAND was designed to mine the web to find candidate texts automatically, 'based on the insight that translated Web pages tend quite strongly to exhibit parallel structure, permitting them to be identified even without looking at content'

More recent research has built on the foundations established by the STRAND project. For example, San Vicente and Manterola (2012) used anchor text, URL matching and HTML structure to build Basque-English, Spanish-English and Portuguese-English parallel corpora. Interestingly, they chose not to remove boilerplate as they found that navigation menus contain useful parallel information. The exact composition of the searchable web is something we know surprisingly little about as a research community, making it difficult to assess the representativeness of our web-derived corpora. Related to this is the notion of a balanced corpus: one where 'the size of its subcorpora (representing (continued) particular genres or registers) is proportional to the relative frequency of occurrence of those genres in the language's textual universe as a whole'

Representative

Some researchers have criticised the approach adopted by Biber et al., pointing out that they remain heavily reliant on Google as gatekeeper to the web because the documents analysed come from a corpus seeded by Google queries

Representative Corpus 1

BYU corpora: COCA, GLoWbE, CORE and NOW

The Corpus of Contemporary American English (COCA), Corpus of Global Web-based English (GloWbE), Corpus of Online Registers of English (CORE), and News On the Web (NOW) corpus are four in a series of corpora released by Mark Davies.

COCA contains 20 million words of texts each year since 1990, split evenly between five genres: spoken, fiction, popular magazines, newspapers, and academic journals. It will be noted that these genres are very similar to (continued) A. Kehoe those found in pre-web corpora such as the BNC, and therein lies one of the limitations of COCA. Although all texts were downloaded from the web using the 'web for corpus' approach, COCA was designed in such a way that it excludes web-native genres like blogs and social media. COCA is a web corpus in a very loose sense only. It is intended as a monitor corpus yet it does not contain examples of the latest trends in language use, which tend to be found in blogs and other less formal text types.

This issue is addressed in the 1.9 billion word GloWbE and 50 million word CORE corpora. GloWbE was constructed using 'web for corpus' techniques, seeded through search engines queries. As explained in

The CORE corpus was derived from GloWbE as part of the project undertaken by

Representative Corpus 2

Birmingham Blog Corpus

The Birmingham Blog Corpus (BBC) is a freely-searchable 630 million word collection downloaded from various blog hosting sites (Kehoe and Gee 2012). Of particular interest is the sub-corpus from WordPress and Blogger, which includes 95 million words of blog posts and a further 86 million words of reader comments on those posts. It was possible to separate posts and comments in this way as, although blogs on WordPress and Blogger are written by a wide range of people and cover many different topics, they make use of a small number of pre-defined templates. It was therefore relatively easy to identify the post and each individual comment during the crawling (continued) 15 Web Corpora process, without the need for complex boilerplate removal techniques. The crawling for the WordPress and Blogger sub-corpus of the BBC took place without relying on commercial search engines. Instead, lists of popular blogs were taken from the hosting sites themselves and used as the initial set of blogs to crawl. When each post on each blog was processed, links to other WordPress and Blogger blogs were extracted and added to the crawling list, thus widening the scope of the corpus.

Linguistic research based on the BBC has demonstrated the value of blog data in pragmatic analyses of online interaction (e.g. Lutzky and Kehoe 2017 on apologies). Such research has also identified a shift in use of the blog format from its original 'online diary' focus, meaning that it is now possible to capture a wide variety of topics and communicative intentions in a corpus made up exclusively of blogs.

Critical Assessment and Future Directions

In this chapter we have examined a wide range of approaches which use the web as a linguistic resource, ranging from basic Google searches to large-scale crawling of web content. The web may not be a corpus in a conventional sense but, as we have seen, it can be a valuable corpus surrogate or, increasingly, a source of texts for the building of corpora. However, in their attempts to build large web-derived corpora, researchers continue to be hampered by a reliance on commercial search engines and a lack of detailed knowledge of the web's structure and content.

With it becoming increasingly difficult to use search engines for linguistic research, both directly and as a way of seeding crawls, we must devise new ways of accessing web content. The scoped crawling approach is one solution, allowing us to focus our attention on specific websites without relying on search engines as gatekeepers to the web. Lists of popular sites such as the Alexa web rankings may be a good starting point. Social media, particularly Twitter, is another increasingly useful source of textual data.

As we have moved towards the 'web for corpus' approach, a new challenge has emerged concerning the legality of distributing corpora crawled from the web. The solutions adopted by corpus compilers vary, from limiting the context shown through the search interface (Mark Davies' corpora) to releasing corpora for download with the sentences shuffled into a random order (COW corpora). This is still very much a grey area, with different laws applying across the world, and anyone planning to distribute a web-derived corpus should seek local advice. Within the European Union, the General Data Protection Regulation (GDPR)

In the longer term, the ideal solution for web corpus research would be a search engine designed specifically for linguistic study: a Google-scale resource providing the search options and post-processing tools we require to extract examples of language in use from the web. There have been attempts in the past but most have proved to be unsustainable as academic funding models do not allow the continuous expansion of disk storage space required for regular web crawling or the bandwidth required to allow multiple users to carry out complex searches on a large corpus simultaneously. With increasing interest in the use of web data in linguistic research and deepening knowledge of web content (e.g.

Crawling and Text Processing

Introduction

Developments in technology, particularly the ever-increasing availability of advanced capturing devices for video and audio alongside digital analysis software, have provided linguists with invaluable tools for the construction of multimodal records of human communication. These developments have proven to be particularly beneficial to the emergent field of multimodal corpus linguistic enquiry.

A corpus is a principled collection of language data taken from real-life contexts. Modern corpora vary in size and scope and are used by a range of different researchers and professionals: from academics to lexicographers, textbook writers and syllabus designers; and more broadly, they could be used by potentially anyone with an interest in language. As

While the majority of current corpora are mono-modal in nature, comprising text-based records taken from transcribed spoken language, and written data (e.g. BNC, British National Corpus, see

Fundamentals

Defining Multimodality and Multimodal Corpora

Rooted in Halliday's work on social semiotics (1978), multimodality is often conceptualised as the relationship between different 'modes' of communication and how they interact to develop meaning in discourse. A mode, in this context, is defined as "a socially shaped and culturally given semiotic resource for making meaning"

A multimodal corpus is defined as "an annotated collection of coordinated content on communication channels, including speech, gaze, hand gesture and body language, and is generally based on recorded human behaviour"

The majority of current multimodal corpora are forms of 'specialised' corpora insofar as they tend to be constructed to help answer a specific question, examine a particular discursive context and/or to meet the requirements of a particular research area or project. The VACE corpus

Multimodality Research in Linguistics

While there is a rich tradition of multimodal research in fields such as psychology, computer science, cultural studies and anthropology, the examination of multimodal communication by linguists is comparatively underdeveloped. Some of the most significant developments in research into multimodality and multimodal text analysis in linguistics exist within the discourse analysis tradition and also in sign language research (for examples of relevant works see

Work in multimodal corpus linguistics is, in contrast, a relatively new field of research, having only been established in the mid-late 2000s. The key difference between multimodal corpus linguistics and the multimodal analysis of corpora using MDA is that, as with traditional text-based corpus enquiry, multimodal corpus analysis includes not only detailed qualitative analyses, but also quantitative analyses of emerging patterns of language-in-use.

Gu notes that multimodal corpus linguistic research tends to be rooted either in the social sciences, focusing on providing multimodal and multimedia studies of discourse, or more generally within computational sciences and focusing on speech engineering and corpus annotation

The Representative Study boxes provide examples of three empirical studies that focus on analysing specific linguistic phenomena using multimodal corpora: discourse markers, turn management in interaction and spoken and non-verbal listenership behaviour. These areas of focus only represent the tip of the iceberg for the potential of multimodal corpus research, as these resources can be used in different areas of research, from examining shoulder shrugging and hesitations

Issues and Methodological Challenges

Few open-source and freely available multimodal corpora exist (with the AMI and NOMCO corpora perhaps being exceptions to this -see Sect. 16.3 for further details) and, as already outlined above, existing multimodal corpora are commonly specialist and bespoke. Many of the most pertinent issues and methodological challenges faced in multimodal corpus research are tied to the construction and availability of resources for further research. This dependency is due to the fact that our ability to analyse and examine complex and extensive multimodal corpora in a systematic and accurate way (particularly when integrating traditional corpus-based methods into the approach), is reliant on the availability of appropriate technology.

The construction of multimodal corpora is a time consuming and technically complex process. Decisions that are being made regarding the design and composition of these corpora must ensure the aims of the specific research questions being asked can be examined using the resource that is compiled. Researchers need to consider what forms of data are to be included; what modalities are to be captured and represented; where the data is to be sourced from; what format these are stored in (formats are not always universal and/or transferable) and how they will be synchronised, transcribed and annotated, as well as what kinds of tools/conventions will be used to support these processes. This is because "like transcription, any camera position [or hardware/software used to capture multimodal 'data'] constitutes a theory about what is relevant within a scene-one that will have enormous consequences for what can be seen in it later-and what forms of subsequent analysis are possible"

Achieving balance and representativeness has long been regarded as fundamental to designing reliable and verifiable corpora (see Sinclair 2005 -also refer to Chap. 1 for further discussion). A balanced corpus is one that "usually covers a wide range of text categories which are supposed to be representative of the language or language variety under consideration"

Once collected, "managing the detail and complexity involved in annotating, analysing, searching and retrieving multimodal semantic patterns within and across complex multimodal phenomena" (O'Halloran 2011: 136) is the next challenge to be faced. As already noted, multimodal corpus analysis is essentially a mixed methods approach, one which seeks to combine quantitative techniques with qualitative textual analyses, as utilised in conventional corpus enquiry. To support detailed quantitative analysis of phenomena in traditional corpus research, data is often marked-up and annotated first, to make specific features searchable using concordancing tools. Monomodal text-only corpora in English are sometimes annotated automatically with the use of 'taggers' and 'parsers'. These can tackle morphological, grammatical, lexical, semantic and discourse-level features in talk (see Chap. 2 for further information on corpus annotation).

These taggers and parsers and their associated annotation systems tend to be unable to support the analysis of language in use beyond spoken discourse. This is because the annotation of non-verbal elements of talk, including gestures, is particularly complex as gestures are not readily made 'textual units' (see

Abuczki and Ghazaleh note that there have been recent moves towards designing international standards "for annotating various features of spoken utterances, gaze movement, facial expressions, gestures, body posture and combinations of any of these features" within multimodal corpora (2013: 94). The most notable work on developing re-usable and international standards for investigating language and gesture-in-use was carried out by researchers involved in the 'Natural Interaction and Multimodality' (NIMM) part of the 'International Standards for Language Engineering' (ISLE) project

A further challenge faced by researchers working on multimodal corpora is the choice of software used to analyse multimodal resources/corpora. Other chapters in this volume have cited a variety of different digital concordancing tools which support the analysis of corpora (see Chap. 8 for more information). These include AntConc (Anthony 2017), #LancBox

Examples such as

There are calls for the construction of digital tools that enable corpus-based analysis of more complex, multimodal corpora; however, there are various reasons why the development of these digital tools is complicated. The analysis of multimodal corpora presents a whole host of technological challenges especially with regards to the synchronization and representation of multiple streams of information. Early developments that responded to this challenge were made with the construction of the Digital Replay System (DRS, see

(continued) 361

As a 'pilot' study, the paper with a corpus only comprising six recordings of circa 7 min each, problematizes the collection, annotation and analysis of multimodal corpora for research. Amongst other key findings, this initial exploration revealed that "having an averted eye gaze was correlated with the use of this marker regardless of its pragmatic function"

Representative

Representative

Representative Corpus 1

The AMI Meeting Corpus is comprised of 100h of recordings taken from meeting room contexts. AMI includes synchronised data from audio recorders (microphones), video cameras, projectors, whiteboards and pens, and associated transcriptions and annotations of this data. The corpus was compiled in 2005 and remains one of the largest freely accessible multimodal corpora in existence. Specific excerpts from the corpus can be downloaded from

Representative Corpus 2

TalkBank (see:

Representative Corpus 3

The Multimodal Corpus Analysis in the Nordic Countries (NOMCO) project includes conversations in Swedish, Finnish, Danish, Estonian, and more recently, Maltese, which have been video recorded, transcribed and annotated (see

Critical Assessment and Future Directions

This chapter has provided an overview of the current state-of-the-art in multimodal corpus linguistics. It has highlighted that while multimodal corpus research is gaining some momentum, there are still some areas where further development is required. Perceived shortcomings relating to the size, scope (and representativeness and reusability) and the level of availability of current multimodal corpora have been mentioned, along with some of the challenges regarding the representation and analysis of multimodal corpora. A final challenge for current multimodal corpus research relates to more recent discussions and developments in this field. Multimodal interaction includes a range of different semiotic resources, and multimodal corpora, as already noted, have the potential for enabling the researcher to study the use of language along a continuum of dynamically changing contexts. The focus on dynamic contexts has resulted in a call for the construction of more 'heterogeneous' corpora for linguistic research (see

Heterogeneous corpora aim to better capture and represent aspects of the complexity and fluidity of the discursive context for the future analysis of language use (see

• The ability to search data and metadata in a principled and specific way (encoded and/or transcribed text-based data). • Tools that allow for the frequency profiling of events/elements within and across domains (providing raw counts, basic statistical analysis tools, and methods of graphing such). • Variability in the provisions for transcription, and the ability for representing simultaneous speech and speaker overlaps. • New methods for drilling into the data, through mining specific relationships within and between domain(s). This may be comparable to current social networking software, mind maps or more topologically based methods. • Graphing tools for mapping the incidence of words or events, for example, over time and for comparing sub-corpora and domain specific characteristics.

As outlined in this chapter, multimodal corpus research is somewhat still in its infancy and as such we can expect a step-change in our description and understanding of language based on this research. It will be interesting to see whether some of the major insights generated within monomodal, or text-based, corpus linguistics will be replicated once we are able to truly analyse multimodality at scale. This may include new insights into collocation of multimodal units of meaning across interactions; acquisition of speech-gesture units; and insights into frequencies of specific multimodal units in different contexts. Advances in this area are likely to be dependent, at least in part, on the development, functionality and availability of technological resources, but also, as Knight notes, on institutional, national and international collaborative interdisciplinary and multidisciplinary research strategies and funding (2011b: 409).

Tools and Resources

While there is a dearth in the existence of freely-available and widely used multimodal corpora, there are, conversely, a wide range of digital tools and resources that exist to support the construction and analysis of bespoke multimodal corpora. The most widely used tools, and the specific areas of multimodal corpus research that is supported by these tools, focusing on corpus compilation, annotation and analysis, are presented below. All of the tools listed require manual transcription, annotation and analysis of data: these processes are not automated.

Anvil

ANVIL is a video analysis software that was built by Michael Kipp in 2000 and recently updated in 2017 (see

ELAN (

The Max Planck Institute's ELAN

Transana (www.transana.com) (accessed 23 May 2019)

Transana supports the qualitative analysis of videos, audio and images. It enables users to integrate, transcribe, categorize and code their own data (i.e. construct their own multimodal corpus) and then search and explore it in more detail (i.e. analyse it). Transana also provides the means for converting files into a standard format which increases the flexibility of the tool. The transcription tools within Transana are particularly user friendly, and a particular strength of this tool is that it enables real time collaboration via the online version. As Transana is primarily a tool for qualitative analysis, its provisions for, for example, frequency-based or numerical analysis (as utilized by corpus linguists) is somewhat limited. Allwood's (

Further Reading

Knight, D. 2011a. Multimodality and Active Listenership: A Corpus Approach.

London: Continuum.

Building on some of the discussions outlined by Allwood (

Gu, Y. 2006. Multimodal text analysis:

A corpus linguistic approach to situated discourse. Text and Talk 26(2): 127-167.

In a similar way to Knight, Gu concentrates on providing some guidelines for an approach to multimodal corpus analysis. This study is situated mainly within the MDA paradigm, so it focuses on small-scale corpora of situated discourse, utilizing discourse analytic methods as the initial point of departure, but discusses how such an approach can be extended with the integration of corpus methods. Gu focuses on different modalities within the analysis, beyond what the majority of multimodal corpus studies typically afford, making this work of particular relevance to the final section of this chapter: projections for the future directions of this field.

Introduction

This chapter serves as an introduction to the other chapters in Parts IV and V. Its main objectives are to provide the necessary basic know-how about R (R Core Team 2019) and to introduce descriptive statistics and visualization techniques with R, with a special focus on terminology. Section 17.2 provides a short introduction to R and RStudio (RStudio Team 2017) software installationand basic commands. R and RStudio are the two software tools that will be used in the rest of the handbook to describe a set of statistical techniques available to corpus linguists. Section 17.3 reviews essential steps in data handling with R, from data preparation to data loading, managing and saving. Section 17.4 summarizes the main descriptive statistics and Sect. 17.5 introduces some of the most common data visualization techniques used to explore linguistic data. The main focus is here placed on visualization techniques that may be used to explore a dataset before any statistical test is applied; visualization of statistical results is described in the relevant following chapters.

The chapter comes with a supplementary R code file that exemplifies all functions used and provides some more information about additional useful functions.

An Introduction to R and RStudio

Installing R and RStudio

R is a freely available software environment. While R has a wide variety of applications (see also Chap. 9), we will here focus on its use as a tool for statistical computing and data visualization. In order to install R, follow the steps below:

1. Go to the Comprehensive R Archive Network -CRAN website at

Although R can be used as standalone software, we will here be using an opensource integrated development environment for R called RStudio, which provides a user-friendly way of accessing the R console and managing R codes, datasets and plots. To download RStudio, follow the steps below:

1. Go to

In the default setting, there are four panels in RStudio (see Fig.

1. The top left panel is a code editor (Panel 1) that can be used for writing code (i.e. a sequence of instructions that a computer can process and understand) and for sending it to the console to be executed. Previously saved code files (or R scripts) can also be opened in this panel. 2. The top right panel (Panel 2) is a workspace that describes the current working environment (i.e. the datasets and variables used) and keeps track of the history; datasets can also be imported from this panel.

Getting Started with R

An elementary understanding of how to write R code is necessary for working in R.

In this subsection, an introduction to writing and running code will be presented in Sect. 17.2.2.1, followed by instructions for how to install and load packages (Sect. 17.2.2.2). As will become clear, there is typically more than one way of completing a task in RStudio; for most tasks in this subsection, two different options will therefore be presented.

Writing and Running Code

To exemplify how to write code and execute commands (i.e. run code), we will here show how R can be used as a calculator. To ask R to display the answer for 31+82, the first step is to write 31+82 in the code editor (Panel 1) and press

[enter]. Comments, for example specifying what the code does for future reference, As mentioned above, the code can also be run directly in the console (the way to do this is to type the code after the prompt (the last angled bracket) and press [enter]); however, it is easier to save and edit code that is written in Panel 1.

It is often useful or even necessary to create variables or store results using the assignment arrow, <-(i.e. an angle bracket followed by a hyphen). This arrow maps the values on its right to the variable on its left. For example, once a variable y has been assigned the value 31+82 through the following code y <-31+82, typing y into the code editor and then running the code will produce the same result as typing 31+82, as seen in Fig.

Data Handling in R

While data can certainly be entered manually, as has been done so far in this chapter, researchers most often import their datasets into R. This section describes how to prepare, load and manage datasets.

Preparing the Data

Data need to be stored in a format that makes them easy to manipulate with R. This means that, while it may be practical to enter the data into a spreadsheet software tool (and save the data in the original file format of that application to preserve colors and other formatting), data should also be saved in a tab-delimited text or csv file, as such files are easier to import into R. For most of the statistical techniques illustrated in the next chapters (see Chap. 20, for exceptions), data should also adhere to the case-by-variable format:

• The first row should contain the names of all variables.

• Each of the other rows should represent a data point, i.e. a single observation of the dependent variable. • The first column should be used to uniquely identify each case by numbering all n cases from 1 to n. • Each of the other columns should represent one variable with respect to which every data point gets annotated. • Missing data are entered as NA.

For working with R,

Tip 3

To use statistical techniques correctly, it is important to know which type of variables are being analyzed:

Nominal or Categorical Variables are variables with the least precise level of measurement. Different values (i.e. levels in R) of these variables represent different mutually exclusive categories. For example, the position of adverbs in a sentence can be a nominal variable (e.g. sentence initial, mid-position, sentence final).

(continued)

Ordinal Variables are ordered or ranked but equal intervals on the scale do not represent equal differences between the levels. As explained by

The perfect example is a rank of candidates for a job. If we know that Russ is ranked #1, Sheldon is ranked #2, and Hannah is ranked #3, then this is ordinal arrangement. We have no idea how much higher on this scale Russ is relative to Sheldon than Sheldon is relative to Hannah. We just know that it's better to be #1 than #2 than #3, but not by how much. Another often-cited example is answers in a questionnaire on a five-point Likert scale (strongly disagree, disagree, no opinion, agree, strongly agree).

Interval Variables are characterized by the fact that equal intervals on the scale represent equal differences between the values. A common example is temperature on the Celsius or Fahrenheit scale:

The difference between 20 and 25 degrees is the same as the difference between 25 and 30 degrees. However, it is important that interval variables do not include a zero point, or, if they do, it is arbitrary. That is, the temperature of 0 degrees Celsius does not mean that there is no temperature. This is why it does not make sense to say that twenty degrees Celsius is twice as warm as ten degrees.

Ratio Variables are very similar to interval variables but they include zero on the scale and therefore allow for meaningful comparison of differences and ratios between values. Typical examples include annual salaries, word frequencies or reaction times in milliseconds.

As will become clear under Sect. 17.3.3, different types of variables are represented and manipulated differently in R.

Figure

LEN_MC, LEN_SC and LENGTH_DIFF are three numeric (ratio) variables which represent the number of words in the main clause, the number of words in the subordinate clause and the length difference between the two respectively (see

Loading and Checking Data

Once the dataset is prepared, it can be loaded into R. The default function for loading a file in table format is read.table() but its default settings are not always optimal (e.g. the presence of a header is set to 'no' and there is no default separator) and might thus require some adjustments. As a result, other functions may be more practical. The read.csv() and read.delim() functions, for example, are wrapper functions for read.table() (with default argument values set for reading csv and tab-delimited files respectively) that also include a header. Thus, the following two lines of code can be used interchangeably to load the tab-delimited 17_clauseorders.csv file and assign its values to the dataframe cl.order, with the file.choose() function opening up a file selection menu from where the appropriate dataset can be selected: More settings can be adjusted, including which character is used in the file for decimal points (. or ,) and for quotes (see ?read.table() for a complete list of the arguments that the different functions can take).

Tip 4

To show R where to look for files to read in, a working directory can be set.

One easy way to do this is to go to the "Files" tab in Panel 4, click on the " . . . " button to the right, choose an appropriate folder, then click on "more" (next to the cogwheel symbol) and then click on "Set as working directory" in the drop-down menu. Another option is to specify the path to the desired folder inside the brackets of the function setwd() and then run the code.

To find out what the current working directory is, type getwd() in the code editor and run the code.

A typical way to proceed is to make certain that the file has been loaded in the correct format using the function head(), which outputs the first six lines of the file: head(cl.order). The function str() can be used to see what type of data has been loaded: str(cl.order). A useful way of obtaining an overview of the dataset is to use the summary() function: As can be seen in Fig.

Tip 5

To access the content of a variable/column or object, R needs to know where that variable/column or object is located. For instance, if one wants to compute the mean of a numeric column in a data frame (e.g. the mean of words in the subordinate clause), R needs to know which data frame contains that column:

> mean(cl.order$LEN_SC) # the $ separates the data frame and the variable

One way not to have to type the name of the dataset every time is to attach the data frame to the R search path using the function attach(), in our case: attach(cl.order). When a dataset is attached, you can apply functions on variables (without the name of the data frame they are from) as follows:

> mean(LEN_SC)

Managing and Saving Data

The cl.order data structure is a data frame, i.e. a list of variables represented as vectors and factors of various types. The numeric variables CASE, LEN_MC, LEN_SC and LENGTH_DIFF are numeric vectors (sequences of numbers) and the categorical variables ORDER, SUBORDTYPE, CONJ and MORETHAN2CL are factors. This distinction is important because vectors and factors are not manipulated in the same way in R. Table

Table

Tip 6

To know what type of object a variable is, the function is() can be used:

is(LEN_MC)

[1] "integer" " numeric" " vector"

. . . . Sort the data frame by LENGTH_DIFF in ascending order

Descriptive Statistics

The focus of this section will be on the description of numeric variables. See the supplementary R file for how to explore categorical data with the functions summary(), table() and prop.table().

Measures of Central Tendency

The most straightforward measures of central tendency are the mean and the median. Each provides a different type of information about a distribution of values. The mean is simply the sum of all the values for a given variable, divided by the number of values for that variable. The median is defined as the midpoint in a set of ordered values; it is also known as the 50th percentile (or second quartile) because it is the point below which 50% of the cases in the distribution fall. Other percentiles are useful too, such as the 25% (1/4 of all ranked values are below this value; also known as first quartile) and the 75th percentile (3/4 of all ranked values are below this value; third quartile). A useful R command to get the above-mentioned statistics at once is summary(). It was already used above to summarize the whole data frame but it can also be used to summarize individual variables:

> summary(LEN_SC)

Min. 1st Qu. Median Mean 3rd Qu. Max.

2.000 5.000 8.000 9.362 12.000 36.000

One is, however, often not interested in the mean or median for a given independent variable, but instead in the central tendencies at each level of the independent variable. For example, we may want to determine whether the two types of subordinate clause differ with regard to their mean word length. To do so, the function tapply() can be used as follows: The function tapply() has three arguments. The first is a vector or factor (here, LEN_SC) to which we want to apply a function as found in the third argument (here, mean). The second argument is a vector or factor that specifies the grouping of values from the first vector/factor to which the function is applied.

Tip 7

The function table() can also be used to crosstabulate and characterize two or more variables and their relation:

Measures of Dispersion

Measures of central tendency should never be reported without some corresponding measure of dispersion. Without a measure of dispersion, it is not possible to know how good the measure of central tendency is at summarizing the data. An example from

• the interquartile range or quantiles for the median for interval/ratio-scaled data that (a) do not approximate the normal distribution (see Tip 10), or (b) exhibit outliers, i.e. values that are numerically distant from most of the other data points in a dataset (see Sect. 17.5.5 for a means to visualize outliers); • the standard deviation or the variance for normally distributed interval/ratioscaled data.

The interquartile range is the difference between the third (i.e. 75%) quartile and the first (i.e. 25%) quartile. The more the first quartile and the third quartile differ from one another, the more heterogeneous or dispersed the data are. To compute the interquartile range with R, the function IQR() can be used:

The standard deviation, sd, of a distribution with n elements is obtained by computing the difference of each data point to the mean, squaring these differences, summing them up, and after dividing the sum by n-1, taking its square root. In R, all that is required is the sd() function:

> sd(LENGTH_DIFF)

[1] 6.851885

See

Means (with standard deviations in parentheses) for Trials 1 through 4 were 2.43 (0.50), 2.59 (1.21), 2.68 (0.39), and 2.86 (0.12), respectively. (p. 117)

To report the median and interquartile range, the following format can be used: Mdn = 0, IQR = -4-3, where IQR is more usefully reported as a range (reporting Q1 and Q3) rather than a value.

Coefficients of Correlation

Coefficients of correlation are typically used to investigate the relationship between two numeric variables (but there are also coefficients of correlation used to investigate the relationship between a numeric variable and a categorical variable). A correlation between the numeric variables X and Y is positive if the values of both variables increase and decrease together; the correlation is negative if the values of variable X increase when the values of Y decrease, or if the values of variable X decrease when the values of Y increase. Correlation coefficients range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with the plus or minus sign indicating the direction of the correlation, and the value reflecting the strength of the correlation. To interpret its value, as a very coarse rule of thumb, see which of the following values in Fig.

Pearson's product-moment coefficient r is probably the most frequently used correlation coefficient but its use is probably best restricted to interval-scaled (or ratio-scaled) variables that are both approximately normally distributed. It requires the relationship between variables to be monotonic (i.e. an increase in the values of variable X is followed by an increase in the values of variable Y; a decrease in the values of variable X is followed by a decrease in the values of variable Y) and linear (i.e. the values of variable Y increase or decrease at the same rate as the values of variable X do); it is also very sensitive to the presence of outliers.

When these conditions are not met, another coefficient such as Kendall's τ ('tau') is preferred. This correlation coefficient is based only on the ranks of the variable values and is therefore more suited to ordinal variables; it is also less sensitive to outliers.

To compute a correlation coefficient with R, the cor() function is used as follows:

> cor(LEN_MC,LEN_SC, method="kendall")

[1] 0.1012148 Kendall's τ is here selected because the assumptions for using Pearson's r are not met: the relationship between variables is not monotonic and linear and the two variables are not normally distributed (see Fig.

The relationship between variables could be investigated more closely and we could try to predict values of the dependent variable on the basis of values of one or more independent variables and their interactions. This method is called linear regression and is discussed in Chap. 21.

Data Visualization

Visualization is an essential step in data exploration: it is a helpful way of getting an overview of the data (e.g. the type of distribution represented in the dataset), identifying areas that need attention (e.g. the presence of outliers or many zero values) and checking whether the assumptions of the selected statistical test are met (see more particularly in Chaps. 20-23). As will become clear from the subsequent chapters, data visualization can also provide a means of presenting the results in a reader-friendly manner. R offers many different ways of visualizing data and this section will present a selection of commonly used graphs. Section 17.5.1 covers barplots and how these can be customized; Section 17.5.2 introduces a version of barplots for two variables, namely mosaic plots. In these two sections, barplots and mosaic plots will be created using the function plot(). This function is a good function to start with as it chooses what type of graph to use based on the kind(s) of variables to be plotted. The subsequent sections cover graphs that can be used for numeric variables to display dispersion and central tendencies, namely histograms (Sect. 17.5.3), ecdf plots (Sect. 17.5.4), and boxplots (Sect. 17.5.5). The code file that accompanies this chapter also includes code for making other common graphs, such as scatterplots.

Barplots

The plot() function will here be used to create a barplot displaying the frequencies of two of the nominal/categorical variables from the cl.order data frame, namely ORDER and CONJ. To graphically present the frequency differences between the two levels of ORDER, namely main clause followed by subordinate clause (mc-sc) and subordinate clause followed by main clause (sc-mc), type plot(ORDER) in the code editor and run the code. The frequencies of ORDER will be plotted as a barplot (Fig.

While this graph is accurate and can certainly be useful, it is perhaps not sufficiently informative for all purposes. Furthermore, the tickmarks on the y-axis do not extend to the size of the bar in this default setting, thereby making the frequency of mc-sc unnecessarily difficult to read. To remedy some of these problems, the graph can be customized using the following line of code:

> plot(ORDER, main = "Tokens per clause order", xlab = "Clause order", ylab = "Frequency", ylim=c(0,300), col = c("gray70", "gray40"), col.axis = "gray20")

The main argument provides a heading for the graph; xlab labels the x-axis; ylim specifies the lower and upper limits of the y axis. The col argument is used to specify the colors of the bars (here, two different shades of gray are used); col.axis changes the color of the x and y axes. The graph can be customized further, for example with the addition of the frequencies on top of the corresponding bar, using the following lines of code: > graph1 <-plot(ORDER, main = "Tokens per clause order", xlab = "Clause order", ylab = "Frequency", ylim=c(0,300), col = c("gray70", "gray40"), col.axis = "gray20"); text(graph1, table(ORDER), table(ORDER), pos=3)

Here, two lines of code are merged with a semicolon. The first line of code plots the barplot and assigns the values of the horizontal middles of the bars to the variable "graph1"; the second line of code prints the frequency of each level of ORDER on top of the corresponding bar (i.e. in position 3). This version of the graph can be seen in Fig.

Mosaic Plots

Graphs can also be used for providing an overview of a data set containing two or more different variables of potential interest. Mosaic graphs are an example of such graphs used for nominal/categorical data. The plot() function will here display the relative frequency of each clause type order (mc-sc or sc-mc) per conjunction (when, before, after or because); that is, this function can be used to plot the variables ORDER and CONJ, using the code below.

> plot(CONJ, ORDER, col = c("royalblue4","royalblue2"), main= "Clause order per conjunction", xlab = "Conjunction", ylab = "Clause order") Fig.

In addition to the plot itself, this line of code specifies the colors of the bars, gives the graph a heading and labels the x-axis (Fig.

Tip 9

To learn more about how to customize graphs using the different parameters available, type ?par into the code editor (Panel 1) and run the code.

To get a list of the names of all the available colors, type colors() in the code editor and run the code.

The graphs shown so far have been used to display nominal/categorical variables, but R can of course be used to visualize other kinds of variables too. In the subsequent subsections, the numeric variables LEN_SC and LENGTH_DIFF will be explored. The first two types of graphs, histograms and ecdf plot, will here be used to display the distribution of the numeric variable LEN_SC (i.e. the length of the subordinate clauses in number of words).

Histograms

A histogram is a type of bar graph that groups values into a series of intervals (or bins). While the bins are adjacent, their values are non-overlapping (i.e. each value is included in only one bin). The following customized code can be used to make a histogram in R:

> hist(LEN_SC, main = "Length distribution of subordinate clauses", ylim=c(0, 200), col = "gray", xlab = "Length of the subordinate clause (number of words)", labels = TRUE)

In addition to producing the histogram for LEN_SC, this line of code gives the graph a heading, specifies the limits of the y-axis, specifies the color and adds labels. The graph is shown in Fig.

As can be seen, the bulk of the subordinate clauses are between 5 and 10 words long. We can also note that the vast majority of subordinate clauses contain fewer  To check whether data resembles a normal distribution, histograms and Q-Q plots can be used (see functions in accompanying R code).

As will be seen in Part V, the normal distribution is also often an assumption of statistical tests. When the assumption of normal distribution is not met, it is sometimes recommended to transform data to reduce the impact of outliers and obtain a more representative mean value by normalizing a non-normal distribution. Data transformation can also be employed to equate heterogeneous group variances, another common assumption of the statistical tests described in the next chapters.

There is a wide range of transformation techniques available (cf.

Ecdf Plots

Another kind of graph that provides detailed information about the distribution is ecdf plots (short for empirical cumulative distribution function plots). This kind of graph shows the cumulative frequencies of variable values on the y axis (which ranges from 0 to 1). The following customized lines of code can be used to provide an overview of the distribution of LEN_SC:

> plot(ecdf(LEN_SC), main="ecdf plot showing the length distribution of subordinate clauses", xlab="Length of the subordinate clause (number of words)", verticals=TRUE, pch=" * ", col.01line = "blue"); grid()

The first line of code specifies the type of graph to plot (here: edcf), the variable (here: LEN_SC), provides a heading, labels the x axis, binds together the points with vertical lines, specifies the plotting character, changes the color of the line that marks the upper and lower limit. The second line of code adds a grid in the background to facilitate reading. The graph is shown in Fig.

Length distribution of subordinate clauses

Boxplots

Boxplots are another type of graph that display a variety of descriptive statistics for the data. In boxplots, the heavy line represents the median, the box displays the interquartile range of the data, meaning that each box in a boxplot represents the 50% of the data closest to the median (see Sect. 17.4.1 and 17.4.2). The vertical lines extending from the boxes mark the minimum and maximum respectively or 1.5 times the interquartile range; the dots above the boxes represent outliers.

This kind of graph will now be used to illustrate how the conjunctions (CONJ) differ with regard to the length of the subordinate clauses (LEN_SC). The following code has been used to create the boxplot shown in Fig.

> boxplot(LEN_SC ∼ CONJ, main="Length of the subordinate clause per conjunction", xlab="Conjunction", ylab="Length", col=c("gray40", "gray60", "gray80", "gray95"),ylim=c(0,35)); text(1:4, tapply(LEN_SC, CONJ, mean), "x", cex=0.6)

The first line of code specifies the graph type (boxplot), the variables to be plotted (LEN_SC as a function of CONJ) and the heading. It also labels the x-and y-axes, specifies the colors for each of the boxes and the upper and lower limits for the yaxis. The second line of code plots the mean for each box using the character "x"; finally, the size of this character (here: 60% of its original size) is specified.

Tip 11

The function par() can be used to create paneled graphs (i.e. two or more graphs placed below or next to one another). The first number specifies the number of rows and the second one the number of columns. The following line of code thus enables placement of two graphs next to one another: par(mfrow = c(1, 2)). This code should be followed by the code creating the graphs themselves. The line par(mfrow = c(1, 1)) is used to restore the default settings once the graphs have been created.

Conclusion

Descriptive statistics and their visualization are an essential part of data exploration but they are not always given the attention they deserve. Yet, many violations of the underlying assumptions of commonly used statistical techniques (see more particularly Chaps. 20-23) could be avoided by applying better data exploration (cf.

Descriptive statistics should also be reported with care in research papers, making sure at least the following values are reported: sample sizes, number of cases by main groups (i.e. typically the output of the function table() used for crosstabulating two variables), measures of central tendency and measures of dispersion (see further discussion in Chap. 26). This is also particularly important if we want our research to contribute to the field and be subject to meta-analysis (cf. Chap. 27).

Further Reading

Chang, W. 2013. R Graphics Cookbook. O'Reilly Media.

In Chang (2013), step-by-step information about how to generate graphs in R is provided. The book includes 150 "recipes" specifying how to use R (in its default configuration and with packages such as ggplot2) to explore, summarize and interpret data. It also includes information about how to customize graphs.

Levshina, N. 2015. How to do linguistics with R. Data exploration and statistical

analysis. Amsterdam and Philadelphia: John Benjamins.

This textbook is a very user-friendly introduction to R for linguistics. It comes with two very practical appendixes: (1) Most important R objects and basic operations with them and (2) Main plotting functions and graphical parameters in R. This article provides a protocol for data exploration, discusses current techniques to detect outliers, heterogeneity of variance, collinearity, etc. and provide advice on how to address these problems when they arise. It also addresses misconceptions about normality, and provides advice on data transformation.

Zuur

Introduction

The advent of digital technology has generated a rapid increase in the number and size of corpora available to the linguist, and data abstracted from these can be so complex as to be impenetrable to understanding by direct inspection. The aim of this chapter is to outline how cluster analysis can be used for interpretation of such data. Cluster analysis is primarily a tool for hypothesis generation. It identifies structure that is latent in data, awareness of which can be used to draw inferences on the basis of which a hypothesis is formulated. The method has been used in historical, grammatical, geographical, and social variation research 402 H. Moisl as well as in language processing technologies such as information extraction, question answering, machine translation, and text type identification. Examples for research on synchronic grammar are

The chapter first presents the fundamentals of cluster analysis and describes two of the most frequently used methods, then briefly outlines two representative clustering applications, and finally presents a guide to cluster analysis of linguistic data using R together with recommendations for presentation of clustering results and some additional reading.

Fundamentals

Motivation

Assume a sociolinguist wants to understand the nature of phonetic usage in some speech community of interest, that is, whether there is systematic rather than random variation among speakers. A representative corpus of speech is collected, a set of phonetic variables is defined, and the number of times each speaker uses each of these variables is recorded, thereby building up a body of data. Table

There are numerous clustering methods. A frequently used one is hierarchical analysis, described later in this discussion. Hierarchical analysis of the data in Table

Data

The validity of cluster analysis depends crucially on the characteristics of the data to which it is applied. Data are constructed from observation of things in the world, and the process of construction raises a range of issues that determine the amenability of the data to analysis and the interpretability of the analytical results. On the one hand, nothing can be discovered that is beyond the limits of what the data say about the world. On the other, failure to understand and where necessary to emend relevant characteristics of data can lead to results and interpretations that are distorted or even worthless. It is, therefore, crucial to ensure that, prior to applying any form of cluster analysis, the various data preparation issues have been addressed. These issues cannot be dealt with here, but are covered in detail in

Clustering

This section first discusses the nature of clusters, then presents a geometric concept fundamental to most types of clustering, and finally describes two of the most frequently used methods, k-means and hierarchical analysis.

Cluster Definition

Human perception is optimised to detect patterning in the environment, and clusters are a kind of pattern. Contemplation of a rural scene reveals clusters of trees, of farm buildings, of sheep. Looking up at the night sky reveals clusters of stars, and a casual observer looking at the scatterplots in Fig.

The obvious way to address these limitations is by unambiguous definition of what a cluster is, relative to which criteria for cluster membership can be stated and used to test perceptually-based intuition on the one hand and to identify nonvisualisable clusters in higher-dimensional data on the other. Textbook discussions of cluster analysis uniformly agree, however, that no one has thus far succeeded in formulating such a definition. In principle, this lack deprives cluster analysis of a secure theoretical foundation. In practice, the consensus is that there are intuitions which, when implemented in clustering methods, give conceptually useful results, and it is on these intuitions that contemporary cluster analysis is built.

The fundamental intuition underlying cluster analysis is that data distributions contain clusters when the data objects can be partitioned into groups on the basis of their relative similarity such that the objects in any group are more similar to one another than they are to objects in other groups, given some definition of similarity. The most commonly used similarity definition is based on the concept of proximity in vector space.

Proximity in Vector Space

Once data are represented as a matrix like that in Table

• The dimensionality of M, that is, the number n of columns representing the n data variables, defines an n-dimensional data space. • Each of the m row vectors is a point in that space.

• The sequence of n numbers comprising each row vector specifies the coordinates of the point in the space. This is shown for a two and three dimensional vector spaces in Fig.

Mathematically and geometrically, however, higher-dimensional spaces are defined and used in the same way as the foregoing lower-dimensional ones, so that a fourdimensional vector is a point in four-dimensional space, a five-dimensional vector is a point in five-dimensional space, and so on for any n. The important thing to realise is that the intuitive concepts of physical space and dimension are useful metaphors for interpreting the corresponding mathematical concepts, but that the metaphors do not constrain what is possible mathematically. Given a distribution of vectors in a space V, the geometrical proximity of two vectors v1 and v2 in the space can be measured by determining the angle between them

Clustering Methods

Clustering methods are standardly divided into two types: (i) non-hierarchical, which partition data vectors into some number k of disjoint groups such that the members of any given group are closer to one another in the data space than they are to members of any other group, and (ii) hierarchical, which show the distance relations among data vectors as a tree and leave the cluster structure to be (7-2) 2 + (7-4) 2 distance(v1,v2) = inferred from the tree by the researcher. Because these types offer complementary information about the structure of data, examples of both are described here, starting with a non-hierarchical one.

K-means

K-means clustering is the most frequently used non-hierarchical clustering method. The following account first describes the standard k-means algorithm and then identifies issues associated with it.

K-means is based on the idea that, for a given set of vectors V, each cluster is represented by a prototype vector, and a cluster is defined as the subset of vectors in V which are in distance terms closer to the prototype than they are to the prototype of any other cluster. A mathematical function known as an objective function is used to find a set of clusters each of which optimally meets this criterion; more is said about the form of such a function below. For V comprising m n-dimensional vectors, V is partitioned into k prototype-centred clusters by the following iterative procedure:

1. Initialise by selecting k n-dimensional prototype locations in the vector space;

these can be anywhere in the space. The prototypes are the initial estimates of where the clusters are centred in the space, and their locations are refined in subsequent steps. Placement of initial prototypes and selection of a value for k, that is, of the number of clusters, is non-trivial, and is further discussed below. 2. Assign each of the m data vectors to whichever of the k prototypes it is closest to in the space using a suitable distance measure such as the Euclidean. This yields k clusters. 3. Calculate the centroid of each of the k clusters resulting from (2), where 'centroid' is here understood as the centre of a distribution of vectors in ndimensional space. The centroid of a set of row vectors in a matrix M is

This procedure is visualised in Fig.

Figure

where x is a data point, C i is the i'th of k clusters, and p i is the prototype of the i'th cluster. This expression says that the SSE is the sum, for all k clusters, of the Euclidean distances between the cluster prototypes and the data points associated with each prototype. For k-means to have optimised this function, the prototypes have to be placed in the data space so that the Euclidean distances between them and their associated data points is globally minimised across all clusters. It is easy to see that this is what k-means does: the procedure converges on stable cluster centroids, and a centroid is by definition the minimum distance from all the points on which it is based. The k-means algorithm is easy to understand and its results easy to interpret, it is theoretically well founded in linear algebra, and its effectiveness has repeatedly been empirically demonstrated in research applications. It also has well known problems, however; the main ones are outlined below.

• Selection of initial cluster centroids K-means requires the user to specify the locations of the initial k prototypes c 1 . . . c k in the data space, but different sets of initial prototypes can lead to different final ones and thereby to different partitions of the data into clusters. In Fig.

• How many clusters? K-means requires the user to specify the number of clusters k to identify in the data. If the value chosen for k is incompatible with the number of clusters the data actually contains, however, the result will be misleading because k-means will deliver k clusters whatever the actual number of clusters intrinsic to the data, including none. For example, Fig.

The obvious solution is to base the selection of k on reliable a priori knowledge of the domain from which the data comes, where available, but this obviates the point of the exercise, which is to identify unknown structure in data. Failing this, some other clustering method such as the one covered next can be used to gain insight into its cluster structure, or one of the range of initialisation heuristics proposed in the literature can be applied

• Cluster shape K-means is limited in the shapes of clusters it can identify. This is demonstrated with reference to the clusters in Fig.

• Outliers

Because k-means is based on centroids, it is strongly affected by the presence of outliers which distort the location of the centroids in the data space. Outliers in data should therefore be identified and eliminated prior to analysis.

Hierarchical Clustering

Given an m × n matrix M which represents m objects in n-dimensional space, hierarchical cluster analysis constructs a tree which represents the distance relations among the m objects in the space, as shown in Fig.

The following sequence of cluster joins and matrix transformations exemplifies this. Initially, each row vector of the data matrix is taken to be a cluster on its own. Fig.

Figure

The reduced matrix of Fig.

For a matrix with m rows, there will at any step in the above tree-building sequence be a set of p clusters, for p in the range 2 . . . m, available for joining, and two of these must be selected. At the first step in the clustering sequence, where all the clusters contain a single object, this is unproblematical: simply choose the two clusters with the smallest distance between them. At subsequent steps in the sequence, however, some criterion for judging relative proximity between composite and singleton cluster pairs or between composite pairs is required, and it is not obvious what the criterion should be. The one exemplified in the foregoing sequence is such a criterion, known as Single Linkage, but there are various others. For simplicity of exposition, it is assumed that a stage in the tree building sequence has been reached where there are p = 3 clusters remaining to be joined. This is shown in Fig.

Which pair of subtrees should be joined next? Based on visual examination of the scatterplot, the intuitively obvious answer is the pair of clusters closest to one another, that is, A and B. Where the data are higher-dimensional and cannot be directly plotted, however, some explicit specification of closeness is required. This is what the following cluster-joining criteria provide.

• The Single Linkage criterion defines the degree of closeness between any pair of clusters (X,Y) as the smallest distance between any of the data points in X and any of the data points in Y: if there are x vectors in X and y vectors in Y, then, for i = 1 . . . x, j = 1 . . . y, the Single Linkage distance between X and Y is defined as

where dist(X i ,Y j ) is the distance between the i'th vector in X and the j'th vector in Y stated in terms of whatever metric is being used, such as Euclidean distance. The Single Linkage distances between all unique pairs of the p vectors remaining to be clustered are calculated, and the pair with the smallest distance is joined. This is exemplified for the three clusters of Fig.

• Complete Linkage defines the degree of closeness between any pair of clusters (X,Y) as the largest distance between any of the data points in X and any of the data points in Y: if there are x vectors in X and y vectors in Y, then, for i = 1 . . . x, j = 1 . . . y, the Complete Linkage distance between X and Y is defined as

where dist (X i ,Y j ) is the distance between the i'th vector in X and the j'th vector in Y stated in terms of whatever metric is being used. The Complete Linkage distances between all unique pairs of the p vectors remaining to be clustered are calculated, and the pair for which the Complete Linkage distance is smallest is joined. This is exemplified for the three clusters of Fig.

The arrowed lines in Fig.

• The Average Linkage criterion defines the degree of closeness between any pair of clusters (X,Y) as the mean of the distances between all ordered pairs of objects in the two different clusters: if X contains x objects and Y contains y objects, this is the mean of the sum of distances (X i ,Y j ) where

where dist is defined as previously; note that distances of objects to themselves are not counted in this calculation, and neither are symmetric ones on the grounds that the distance from, say X i to Y j is the same as the distance from Y j to X i .

There are other linkages, for which see

The main and considerable advantage of hierarchical clustering is that it provides an exhaustive description of the proximity relations among data objects, and thereby more information than a simple partitioning of the data generated by non-hierarchical methods. It has also been extensively and successfully used in numerous applications, and is widely available in software implementations. There are, however, several associated issues.

• How many clusters?

Relative to a given tree, how many clusters do the data 'really' contain? That is up to the user to decide. Looking at a dendrogram like the one in Fig.

The clustering literature recognises that different cluster joining criteria can and typically do generate different trees for the same data. For example, Fig.

The obvious objection to this is that it is subjective. It runs the risk of reinforcing preconceptions and discounting the unexpected and potentially productive insights which are the prime motivation for use of cluster analysis in hypothesis generation: given a range of different analyses, one might subconsciously look for what one wants to see.

H. Moisl

The aesthetics of tree structuring might also become a selection factor. Looking at the trees in Fig.

• Outliers and noise

All the joining criteria are affected by outliers and noise to different degrees and for different reasons, where 'noise' designates a variety of factors such as inaccurate measurement or recording, or accidental or deliberate corruption. Outliers are not a problem for Single Linkage because it simply represents them as one-member clusters distant from the other clusters in the tree; this characteristic in fact makes Single Linkage a good way to check for outliers. It is, however, much affected by noise, which results in chaining like that in Fig.

Finally, because of their widespread popularity in data analysis generally, kmeans and hierarchical clustering continue to be developed in the hope of at least mitigating their problems. For the interested reader, these have to do with linear separability of clusters and the use of nonlinear distance measurement, for which see

Advanced Topics

Cluster analysis is widely used and highly developed both in terms of the variety of available clustering methods and of theoretical understanding of them

• As noted, different clustering methods often give substantively different results.

The reason might have to do with noise, or lack of intrinsic cluster structure in the data, or some limitation in the clustering method such as those already referred to, or a predisposition of different methods to find certain shapes of cluster. In such cases, a principled selection is required; cluster validation is a rapidly developing and necessary aspect of cluster analysis

Representative Study 1

Moisl, H., Maguire, W., and Allen, W. 2006. Phonetic variation in Tyneside: exploratory multivariate analysis of the Newcastle Electronic Corpus of Tyneside English. In Language Variation. European Perspectives , ed. Hinskens, F. Meertens Institute, Amsterdam.

This paper is a sociolinguistic study of phonetic variation among speakers from Tyneside in North-East England.

Research question

Is there systematic phonetic variation in the Tyneside speech community, and, if so, does that variation correlate systematically with social factors?

(continued)

H. Moisl

Data

Phonetic data was abstracted from the Diachronic Electronic Corpus of Tyneside English (DECTE), a digital corpus of audio-recorded and transcribed speech from Tyneside in North-East England. DECTE is available online and is fully documented at

-Because there is substantial variation in the lengths of the individual speaker transcriptions, MDECTE was normalised to compensate for this

Method

MDECTE was hierarchically cluster analyzed as shown in Fig.

Results

The DECTE speakers fell into two clearly defined main clusters, a larger one G containing speakers g01 -g55 and a smaller one N containing speakers n01 -n07, and G itself has well-defined subclusters. When this cluster structure was correlated with social data about the speakers included in DECTE, all the speakers in cluster G were found to be from Gateshead on the south side of the river Tyne, and all those in N were from Newcastle on the north side. Moreover, G.1 contains speakers with somewhat higher levels of education and employment than those in G.2, all of whom had (continued)

H. Moisl

the minimum statutory educational level and were in skilled and unskilled manual employment, and G.2 itself consists of two subclusters for gender, where G.2.2 consists entirely of men and G.2.1 mainly though not exclusively of women. Cluster analysis of MDECTE therefore empirically supports the hypotheses that there is systematic phonetic variation in the Tyneside speech community, and that this variation correlates systematically with social factors. A substantial amount of work has been done on inference of grammatical and semantic lexical categories from text

Representative

Research question

How can the number and nature of semantic classes that are instantiated by a given set of words be determined without recourse to prior assumptions or intuitions?

Data

The usual data creation approach of abstracting and counting the collocates of target words from a corpus was modified by including only the covarying collexemes of target words, that is, words which occur in a defined slot of the same construction as the target word (see Chap. 7).

Method

The authors investigated the effectiveness of hierarchical cluster analytic techniques with respect to the research question by determining how well these techniques identify the most prototypical sense(s) of a construction as well as subsenses instantiated by coherent semantic classes of words occurring in it.

(continued)

Results

There was "a relatively coherent classification of verbs into semantic groups" superior to a classification based solely on collocate-counting. The conclusion was that clustering techniques can be a useful step towards making the semantic analysis of grammatical constructions more objective and precise.

Practical Guide with R

This section contains a guide to k-means and hierarchical cluster analysis using R.

The MDECTE data matrix, truncated to 32 rows for convenience of exposition, is used as the basis for exemplification.

K-means

The k-means analysis described in what follows proceeds in three steps: (i) finding the appropriate number of clusters k, (ii) carrying out the analysis for the selected k, and (iii) reporting the result.

(i) As noted earlier, one of the ways of establishing k is to conduct multiple analyses on the same data using different values of k and selecting the one whose SEE is optimal. It was also noted that, because different selections of initial prototypes affect the success with which k-means identifies cluster structure, multiple trials with different random initialisations are conducted for each value of k, and the one with the smallest SSE is selected. Optimisation of prototypes and the value for k selection must, therefore, be integrated: in assessing the optimality of any given k in the succession of candidate k-values, the SSE for that k must itself be optimal. The R code for this follows.

The first three lines assign values to variables used subsequently where maxk <-15 iterations <-50 lst <-vector()

• maxk is the maximum number of k-values to be assessed. iterations is the number of initial prototypes to be tried for each k.

• lst <-vector() creates an empty vector into which optimal SSE values for the current k will be inserted.

The list of optimal SSE values is then constructed.

set.seed(

for (i in 1:maxk)

{mk <-kmeans(m, centers=i, iter.max=10, nstart=iterations)

• set.seed(

• mk <-kmeans(m, centers = i, iter.max = 10, nstart = iterations) uses the R function kmeans to calculate mk, the k-means analysis for any given k in the range 1..maxk.

The meanings of the parameters for kmeans are as follows:

• m is the data matrix.

• centers = i is the number of clusters k for every successive k in the range 1..maxk. • iter.max = 10 is the number of iterations of the k-means algorithm to apply before giving up on the current k as unviable. • nstart = iterations is the number of random initial prototypes to try for each k. It is important to note that kmeans automatically selects and outputs the prototype initialisation with the optimal SSE. • lst[i] <-mk$tot.withinss inserts the optimal SSE for the current k into the SSE list lst, and mk$tot.withinss indexes the SSE which kmeans outputs at each iteration.

The list of SSE values lst is now plotted, with the result shown in Fig.

Figure

(ii) The second step invokes the R function kmeans again, using the selected k via the parameter centers = 3.

mk <-kmeans(m, centers = 3, iter.max = 10, nstart = 50)

(iii) Finally, the variable mk contains the result, and includes a range of information whose significance is explained in the R help file for kmeans. Most relevant for present purposes is the clustering vector, which assigns each data object to one of the specified three clusters, as shown in Fig.

Hierarchical Clustering

Hierarchical analysis is, again, a three-step process: (i) creation of a distance matrix, (ii) clustering, and (iii) presentation of the result.

H. Moisl (i) Generate the Euclidean distance matrix:

• md is the distance matrix.

• dist is the relevant R distance calculation function.

(ii) Cluster the distance matrix: mdc = hclust(md, method="average")

• mdc is the list output of the hierarchical cluster analysis.

• hclust is the R clustering function.

• method="average" selects the Average Linkage method; other linkages can be specified.

(iii) Draw the dendrogram to display the clustering (Fig.

plot(mdc, labels=m$V1, hang=-1, cex=0.75)

• plot is self-explanatory.

• labels=m$v1 tells plot that the speaker labels are in column 1.

• hang=-1 shows all the labels in a uniform horizontal row.

• cex=0.75 adjusts the size of the label font; this is useful when there are many labels and too large a font makes them overlap and become unreadable.

As the number of rows in a data matrix grows, the tree in the 'icicle' format shown in Fig.

Convert the tree generated by hclust to an R dendrogram object:

mdcd <-as.dendrogram(mdc)

Set the font size:

mdcd <-set(mdcd, "labels_cex", 0.75)

Plot the dendrogram (Fig.

plot_horiz.dendrogram(mdcd)

There are several packages in addition to dendextend which offer an extensive range of graphics options, including ggdendro and APE: Analyses of Phylogenetics and Evolution in R language.

Reporting Results

K-means and hierarchical clustering are mathematical, not statistical, procedures. As such, there are no statistical measures such as standard deviations or p-value to report when preparing results for publication. This does not, however, mean that it is sufficient to present nothing more than a clustering vector or a dendrogram. It is important to provide the reader with as much information as possible about the framework in which the analysis was conducted.

• Data: How was the data collected? How large is the data set? What does the data matrix look like (including an example)? How, if at all, was the data transformed, for example by removal of outliers, or by variable scaling, or by normalisation? See Chap. 26 for more specific guidelines on data description.  Exploring a data set means separating meaningful trends from the noise (i.e. "random" distributions).

I present four multivariate exploratory techniques: correspondence analysis (henceforth CA), multiple correspondence analysis (henceforth MCA), principal component analysis (henceforth PCA), and exploratory factor analysis (henceforth EFA). These techniques rely on dimensionality reduction, i.e. an attempt to simplify complex multivariate datasets to facilitate interpretation.

Fundamentals

CA, MCA, PCA, and EFA are meant for the exploration of phenomena whose realizations are influenced by several factors at the same time. Once operationalized by the researcher, these multiple factors are captured by means of several independent variables. When observations of a phenomenon are captured by several variables, the analysis is multivariate.

Commonalities

The challenge that underlies the visualizations obtained with dimensionalityreduction methods is the following: we seek to explore a cloud of points from a data set in the form of a rows × columns table with as many dimensions as there are columns. Like a complex object in real life, a data table has to be rotated so as to be observed from an optimal angle. Although the dimensions of a data table are eventually projected in a two-dimensional plane, they are not spatial dimensions. If the table has K columns, the data points are initially positioned in a space R of K dimensions. To allow for easier interpretation, dimensionality-reduction methods decompose the cloud into a smaller number of meaningful planes.

All the methods covered in this chapter summarize the table by measuring how much variance there is and decomposing the variance into proportions. These proportions are eigenvalues in CA, MCA, and PCA. They are loadings in EFA (and another kind of PCA that is not covered in this chapter). 2 19 Multivariate Exploratory Approaches 437 All four methods offer graphs that facilitate the interpretation of the results. Although convenient, these graphs do not replace a careful interpretation of the numeric results.

Differences

The main difference between these methods pertain mainly to the kind of data that one works with. CA takes as input a contingency table, i.e. a table that crossclassifies observations on a number of categorical variables (see Chap. 20). Entries in each cell are integers, namely the number of times that observations (in the rows) are seen in the context of the variables (in the columns). Table

MCA takes as input a case-by-variable table such as Table

PCA takes as input a table of data of i individuals or observations (rows) and j variables (columns). The method handles continuous and nominal data. The continuous data may consist of means, reaction times, formant frequencies, etc. The categorical/nominal data are used to tag the observations. Table

Like PCA, EFA takes as input a table of continuous data. However, it does not commonly accommodate nominal data. Typically, Table

Exploring is not Predicting

The methods presented in this chapter are exploratory, as opposed to explanatory or predictive. They help find structure in multivariate data thanks to observation groupings. The conclusions made with these methods are therefore valid for the corpus only. For example, we shall see that middle-class female speakers aged 25 to 59 display a preference for the use of bloody in the British National Corpus (Sect. 19.3.2). This finding should not be extended to British English in general. Indeed, we may well observe different tendencies in another corpus of British English. Neither should the conclusions made with exploratory methods be used to make predictions. Of course, exploratory methods serve as the basis for the design of predictive modeling, which uses the values found in a sample to predict values for another sample. Expanding on

Correspondence Analysis

Correspondence analysis (henceforth CA) is used to summarize a two-dimensional contingency table. The table is a matrix M of counts that consists of i individuals or observations (rows) and j variables (columns). The foundations of CA were laid out by

It should be remembered that the linguist makes no assumption as to what kinds of groupings are to be found in the data. In practice, however, a table of data is compiled because meaningful groupings are expected to be found. Therefore, if no meaningful grouping is found, this is because the rows and the columns are independent. In this case, it is advisable to rethink the design of the study, especially the choice of explanatory variables.

To determine whether rows and columns are independent, CA relies on the χ 2 test. It tests the significance of the overall deviation of the table from the independence model. The test computes the contribution of each cell to χ 2 and sums up all contributions to obtain the χ 2 statistic. Because we are interested in determining whether two variables are interdependent, we formulate the hypotheses as follows:

H 0 : the distributions of row variables and column variables are independent; H 1 : the distributions of row variables and column variables are interdependent.

One calculates the χ 2 value of a cell in the ith row and the j th column as follows:

where O i,j is the expected frequency for cell i, j and E i,j is the expected frequency for cell i, j . The χ 2 statistic of the whole table is the sum of the χ 2 values of all cells.

Because the χ 2 score varies greatly depending on the sample size, it cannot be used to assess the magnitude of the dependence. This is measured with Cramér's V , which one obtains by taking the square root of the χ 2 statistic divided by the product of the sum of all observations and the number of columns minus one:

Central to CA is the concept of profile. To obtain the profile of a row, each cell is divided by its row total. Table

Each column of the table contributes one dimension. The more columns in your table, the larger the number of dimensions. When there are many dimensions, summarizing the table becomes very difficult. To solve this problem, CA decomposes φ 2 along a few dimensions that concentrate as large a proportion of inertia as possible. These proportions of inertia are known as eigenvalues.

On top of the coordinates of the data points, two descriptors help interpret the dimensions: contribution and quality of projection (cos 2 ). If a data point displays a minor contribution to a given dimension, its position with respect to this dimension must not be given too much relevance. The quality of the projection of a data point onto a dimension is measured as the percentage of inertia associated with this dimension. Usually, projection quality is used to select the dimension in which the individual or the variable is the most faithfully represented.

Individuals and variables can be declared as active or supplementary/illustrative, as is the case with multiple correspondence analysis and principal component analysis (see below). These supplementary rows and/or columns help interpret the active rows and columns. As opposed to active elements, supplementary elements do not contribute to the construction of the dimensions. Supplementary information is generally redundant. Its main function is to help interpret the results by providing relevant groupings. Whether a group of individuals or variables should be declared G. Desagulier as active/illustrative depends on what the linguist considers are primary or secondary in the exploration of the phenomenon under study.

Multiple Correspondence Analysis

Because MCA is an extension of CA, its inner workings are very similar. For this reason, they are not repeated here.

As pointed out in Sect. 19.2.2, MCA takes as input a table of nominal data. For this method to yield manageable results, it is best if the table is of reasonable size (not too many columns), and if each variable does not break down into too many categories. Otherwise, the contribution of each dimension to φ 2 is small, and a large number of dimensions must be inspected. There are no hard and fast rules for knowing when there are too many dimensions to inspect. However, when the eigenvalue that corresponds to a dimension is low, we know that the dimension is of little interest (the chances are that the data points will be close to the intersection of the axes in the summary plot).

Principal Component Analysis

As in CA and MCA, the total variance of the table is decomposed into proportions in PCA. There is one minor terminological difference: the dimensions are called principal components. For each component, the proportion of variance is obtained by dividing the squared standard deviation by the sum of the squared standard deviations.

As exemplified in this chapter, PCA is based on the inspection of correlations between the variables and the principal components.

Exploratory Factor Analysis

EFA was made popular in linguistics by Biber's studies on register variation as part of the multidimensional (MD) approach

Although close to PCA, EFA differs with respect to the following. The number of relevant components, which are called factors, is not determined automatically. It must be chosen beforehand. EFA is designed to identify patterns of joint variation in a number of observed variables. It looks for variables that are highly correlated with a group of other variables. These intercorrelated variables are assumed to measure one underlying variable. This variable, which is not directly observed, but inferred, is latent. It is known as a factor. This is an aspect that PCA is not designed to show. One added value of EFA is that "an error term is added to the model in order to do justice to the possibility that there is noise in the data"

Research questions

Glynn (2014) examines the semasiological variation of run in the light of sociolinguistic variables. The study posits that "even for a lexeme as culturally 'simple' and as socially 'neutral' as run, one must account for the social dimension of language in semantic analysis"

(continued)

Data

Glynn's study is based on 500 occurrences of run in British and American English (250 occurrences for each variety). The occurrences break down into conversation and online personal diaries. The diary examples were extracted from the LiveJournal corpus, developed by Dirk Speelman (University of Leuven). The conversation examples were extracted from the British National Corpus and the American National Corpus.

Method

Each entry was annotated for dictionary sense, register, and dialect. The data were submitted to correspondence analysis.

Results

The first two dimensions of CA account for 87% of φ 2 , which means that the conclusions based upon their inspection only are reliable. In American conversation, run tends to mean 'increase', 'diffuse', and 'motion into difficulty'. In the American diary genre, run is characterized by the following dictionary senses: 'campaign', 'copy' and, to some extent, 'metaphoric motion'. Although specific to American English, 'meet' and 'extend space' is used in either register. In British English, run is highly and distinctly associated with'flow' and 'extend time'. A relative association with British English is also found with senses such as 'use up', 'cause motion' and 'escape'. To further explore the detail of the sociolinguistic variation at work with run, Glynn resorts to multiple correspondence analysis.

Representative

Research questions

This paper addresses a claim made by

Data

All the occurrences of A as NP were extracted from the BNC-XML, amounting to 1,819 tokens. Only instances of A as NP where the adjective is intensified were kept. Examples involving a literal comparison and no intensification were discarded. Each adjective and noun appearing in A as NP was assigned a range of mean scores based on the following measures: an asymmetric association measure (ΔP ), a symmetric association measure (collostruction strength indexed on the log-likelihood statistic), type frequency (V ), the frequency of hapax legomena (V 1), potential productivity (P), and global productivity (P * ).

Method

The individuals consist of all adjective and NP types of A as NP tokens. Each of the 1,278 individuals (402 adjective types and 876 NP types) is examined in the light of four active variables: collostruction strength, the difference ΔP NP |A -ΔP A|NP , P, and P * . Three supplementary quantitative variables were also included to verify that no counterintuitive result was obtained with respect to the computation of hapax-based measures: V , V 1, and construction frequency. The data table was submitted to PCA.

Results

Three clusters stand out. Globally productive individuals and those that belong to highly associated pairs (i.e. characterized by high collostruction strength and low ΔP ) cluster along the horizontal axis (first principal component). The former appear in the upper-right corner of the plot of individuals whereas the latter cluster in the lower-right corner. Individuals that are productive according to P cluster along the vertical axis (second principal component). In other words, individuals that belong to highly associated pairs are among the least potentially productive and the most globally productive. Individuals with extreme values for the first component are mostly nouns

The most productive individuals according to P belong to weakly associated pairs. The most productive subschemas are indexed on adjectives. These adjectives denote basic properties such as colors and shades

As we move down from the upper-left to the bottom-right part of the plot, productivity declines and conventionalization and autonomy increase. In this study, PCA helps spot distinct loci of constructional productivity at subschematic levels. In other words, productivity is by no means an all or nothing affair.

Representative Study 3 D. Biber (1988). Variation across Speech and Writing. Cambridge:

Cambridge University Press

Research questions

The aim of this work is to spot the patterns of linguistic variation among registers in a corpus of English texts. This landmark study implements an intuition formerly formulated by sociolinguists according to which linguistic features that co-occur significantly can discriminate among registers.

Method

Sixty-seven linguistic features are included in the analysis

First, the corpus is tagged for linguistic features. Next, the frequency counts of all linguistic features are extracted, normalized, and standardized. This guarantees a fair comparison of frequency distributions across texts of unequal lengths. Then, factor analysis is used to identify the dimensions, where each dimension captures a pattern of underlying co-occurrence patterns among linguistic features. A factor loading indicates the extent to which a given feature is representative of the dimension underlying a factor. Dimension scores are calculated for each text sample by adding up standardized frequencies with salient positive loadings and subtracting salient negative loadings on a dimension. Finally, each dimension is interpreted in functional terms. This correspondence between dimensions and functions is facilitated by promax rotation.

Results

Because dimensions have a functional basis, each of them is associated with a distinctive pattern of register variation and assigned an interpretive label. In

1. involved versus informational production; 2. narrative discourse; 3. situation-dependent versus elaborated reference; 4. overt expression of argumentation; 5. impersonal/asbtract style.

Each dimension is captured by a distinction between positive and negative features. For example, the positive features of the first dimension are: verbs, pronouns, adverbs, dependent clauses, and other (contractions, discourse particles, clause coordination, etc.). The negative features of the same dimension are: nouns, long words, prepositional phrases, attributive adjectives, and lexical diversity. Subsequent studies have confirmed that the underlying dimensions of variation and the relations among registers display similar configurations across languages.

Practical Guide with R

In this section, I show how to run the code to perform CA, MCA, and PCA with FactoMineR. The package should therefore be downloaded and installed beforehand. EFA is run with factanal(), which is part of base R. Therefore, it does not require any extra package.

Correspondence Analysis

Complex prepositions are multiword expressions (i.e. expressions that consist of several words): ahead of, along with, apart from, such as, thanks to, together with, on account of, on behalf of, or on top of. In Hirschmüller's data, 81 prepositions consist of two words and 154 of three and more, out of a total of 235 complex prepositions. He observes a higher incidence of complex prepositions in the Kolhapur Corpus than in the other two corpora. He also observes that the most complex prepositions (i.e. prepositions that consist of three words and more) are over-represented in the corpus of Indian English.

Their use is often associated with the level of formality

Following

• complex prepositions are likely to be over-represented in the Kolhapur corpus; • within the corpus, complex prepositions are likely to be over-represented in the more formal text categories.

With the code below, we run CA on the preposition data set.

The data set has been imported as a data frame. To inspect it, enter str(dfca) and/or head(dfca). It displays the number of times each preposition type is found in a certain context. The table consists of 257 lines (one line per preposition type) and 19 columns (one column per variable). Each column stands for a context where the preposition is found. There are three kinds of columns. The first three columns correspond to the three corpora. The next fifteen columns correspond to the text categories. The nineteenth column specifies the word length of the prepositions. This last column (prep.length) is loaded as a factor because it contains nominal data (for this reason, it is said to be qualitative).

The first three columns are declared as active. Columns 4 to 18 are quantitative and declared as supplementary (col.sup=4:18). These 15 columns correspond to the 15 text categories. Column 19, which corresponds to the complexity of the preposition, is qualitative and therefore supplementary (quali.sup=19).

> ca.object <-CA

The output of CA is in ca.object. The first lines of the ouput give the χ 2 score and the associated p-value. The χ 2 score is very high (10,053.43) and it is associated with the smallest possible p-value (0). The deviation of the table from independence is beyond doubt. Admittedly, the assumptions of the χ 2 test are not all met. One of them stipulates that 80% of the cells should display expected frequencies that are greater than 5. Our table contains many cells whose expected values are smaller than 5. Therefore, it does not meet the assumption. While this should be kept in mind, it does not preclude the fact that the choice of a preposition and the variety of English are globally interdependent, given the importance of the score. Furthermore, the χ 2 test is used in an exploratory context, not a hypothesis-testing context. Just because its conditions are not fully met does not mean it is irrelevant. The intensity of the relationship is definitely small, but non negligible for this sort of data: Cramér's V = 0.111. A score of 1 would be unrealistic as it would attest an exclusive association between the use of prepositions and the dialect of English. The chi square of independence between the two variables is equal to 10053.43 (p-value = 0 ). *The results are available in the following objects: name description 1 "$eig" "eigenvalues" 2 "$col" "results for the columns" 3 "$col$coord" "coord. for the columns" 4 "$col$cos2" "cos2 for the columns" 5 "$col$contrib" "contributions of the columns" 6 "$row" "results for the rows" 7 "$row$coord" "coord. for the rows" 8 "$row$cos2" "cos2 for the rows" 9 "$row$contrib" "contributions of the rows" 10 "$col.sup$coord" "coord. for supplementary columns" 11 "$col.sup$cos2" "cos2 for supplementary columns" 12 "$quali.sup$coord" "coord. for supplementary categorical var."

13 "$quali.sup$cos2" "cos2 for supplementary categorical var." 14 "$call" "summary called parameters" 15 "$call$marge.col" "weights of the columns" 16 "$call$marge.row" "weights of the rows"

The eig object allows to see how many dimensions there are to inspect. Because the input table is simple and because the number of active variables is low, there are only two dimensions to inspect. Indeed, the first two dimensions represent 100% of the variance of the table. In most other studies, however, we should expect to inspect more than two dimensions. Our decision is based on the cumulative percentage of variance. The inertia (i.e. the sum of eigenvalues) is low (0.0248). This means that there is not much variance in the table and that the tendencies that we are about to observe are subtle. In case there are more than two dimensions to inspect, a scree plot is useful. The standard graphic output of CA is a symmetric biplot in which both row variables and column variables are represented in the same space using their coordinates. In this case, only the distance between row points or the distance between column points can be interpreted accurately

> plot.CA(ca.object, + invisible="row", + autoLab="yes", + shadow=TRUE, + cex=.8, + col.col="magenta", + col.col.sup="dodgerblue", + title="Distribution of prepositions based on lexical complexity + in three corpora:\n LOB (British English), Brown (US English), + and Kolhapur (Indian English)", + cex.main=. Hirschmüller observed the following: (1) complex prepositions cluster in nonfictional texts, a preference that is amplified in the Kolhapur Corpus; (2) learned and bureaucratic writing shows a more pronounced pattern in the Kolhapur Corpus than in the British and American corpora. The CA plot reflects these tendencies (Fig.

The first dimension (along the horizontal axis) accounts for 82.34% of the variance. It shows a clear divide between

Multiple Correspondence Analysis

Schmid's study is repeated here in order to explore the distribution of swearwords with respect to gender in the BNC-XML. The goal is to see if:

• men swear more than women; • some swear-words are preferred by men or women; • the gender-distribution of swear-words is correlated with other variables: age and social class.

The data file for this case study is 19_swearwords_bnc.txt (see companion files).

The data set contains 293,289 swear-words. These words are described by three categorical variables (nominal data):

• gender (2 levels: male and female) • age (6 levels: Ag0, Ag1, Ag2, Ag3, Ag4, Ag5) • social class (4 levels: AB, C1, C2, DE) Age breaks down into 6 groups:

• Ag0: respondent age between 0 and 14; • Ag1: respondent age between 15 and 24; • Ag2: respondent age between 25 and 34; • Ag3: respondent age between 35 and 44; • Ag4: respondent age between 45 and 59; • Ag5: respondent age is 60+. Social classes are divided into 4 groups:

• AB: higher management: administrative or professional. • C1: lower management: supervisory or clerical; • C2: skilled manual; • DE: semi-skilled or unskilled.

As we inspect the structure of the data frame with str(), it is advisable to keep an eye on the number of levels for each variable and see if any can be kept to a minimum to guarantee that inertia will not drop.

> str(df) 'data.frame': 293289 obs. of 4 variables: $ word : Factor w/ 8 levels "bloody","damn",..: 2 2 7 7 7 2 7 2 7 7 ... $ gender : Factor w/ 2 levels "f","m": 2 2 2 2 2 2 2 2 2 2 ... $ age : Factor w/ 6 levels "Ag0","Ag1","Ag2",..: 6 6 6 6 6 6 6 6 6 6 ... $ soc_class: Factor w/ 4 levels "AB","C1","C2",..:

> df$word <-gsub("fuck|fucking|fucker|fucked", "f-words", df$word, ignore.case=TRUE) > table ( The number of levels has been reduced to five. We convert df$word back to a factor.

> df$word <-as.factor(df$word)

As in CA, we can declare some variables as active and some other variables as supplementary/illustrative in MCA. We declare the variables corresponding to swear-words and gender as active, and the variables age and social class as supplementary/illustrative.

We run the MCA with the MCA() function. We declare age and soc_class as supplementary (quali.sup=c

> mca.object <-MCA(df, quali.sup=c

Again, the eig object allows us to see how many dimensions there are to inspect.

> round(mca.object$eig, 2) eigenvalue percentage of variance cumulative percentage of variance 7

The number of dimensions is rather large and the first two dimensions account for only 42.47% of φ 2 . To inspect a significant share of φ 2 , e.g. 80%, we would have to inspect at least 4 dimensions. This issue is common in MCA. The eigenvalues can be vizualized by means of a scree plot (Fig.

> barplot(mca.object$eig[,1], + names.arg=paste("dim ", 1:nrow(mca.object$eig)), las=2) Ideally, we would want to see a sharp decrease after the first few dimensions, and we would want these first few dimensions to account for as much share of φ 2 as possible. Here, no sharp decrease is observed.

The MCA map is plotted with the plot.MCA() function. Each category is the color of its variable (habillage="quali"). The title is removed (title="").

> plot.MCA(mca.object, + invisible="ind", + autoLab="yes", + shadowtext=TRUE, + habillage="quali", + title="")

In the MCA biplot (Fig.

Combining the two dimensions, the plot is divided into four corners in which we observe three distinct clusters:

• cluster 1 (upper-right corner) gosh and shit, used by male and female upper class speakers; • cluster 2 (lower-left corner) bloody, used by female middle-class speakers; A divide exists between male (m, right) and female (f, left) speakers. However, as the combined eigenvalues indicate, we should be wary of making final conclusions based on the sole inspection of the first two dimensions. The relevance of age groups becomes more relevant if dimensions 3 and 4 are inspected together (Fig.

> plot.MCA(mca.object, + axes=c(3,4), + invisible="ind", + autoLab="yes", + shadowtext=TRUE, + habillage="quali", + title="")

With respect to dimensions 3 and 4, the male/female distinction disappears (both variables overlap where the two axes intersect). A divide is observed between f - words and bloody (left), used mostly by younger and middle-aged speakers, and gosh and damn (right), used mostly by upper-class speakers from age groups 3 and 5. The most striking feature is the outstanding position of shit in the upperleft corner. Although used preferably by male and female upper class speakers (Fig.

Principal Component Analysis

.2).

To compare the semantic profiles of the prepositions, the preferred and dispreferred nominal collocates of the prepositions are examined in the FrWaC corpus. The goal is to summarize the table graphically instead of interpreting the data table directly.

First, we load the data set (19_inclusion_FrWaC.txt).

> # clear R's memory > rm(list=ls(all=TRUE)) > # load the data (19_inclusion_FrWaC.txt) > data <-read.table(file=file.choose(), header=TRUE, row.names=1, sep="\t")

As we inspect the data frame with str(), we see that 22,397 NPs were found. The rows contain the nominal collocates and the columns the prepositions. The cells contain the association scores. The assumption is that the semantic profiles of the prepositions will emerge from the patterns of attraction/repulsion.

As in CA and MCA, we can declare some variables as active and some other variables as supplementary/illustrative in PCA. Here, however, we decide to declare all variables as active. We load the FactoMineR package and run the PCA with the PCA() function.

> library(FactoMineR) > pca.object <-PCA(data, graph=F)

We make sure that the first two components are representative.

> barplot(pca.object$eig[,1], + names.arg=paste("comp ",1:nrow(pca.object$eig)), las=2)

We plot the graph of variables and the graph of individuals side by side (Fig.

Fig. 19.6 PCA plots

> # tell R to display the two plots side by side > par(mfrow=c(1,2)) > # graph of variables > plot.PCA(pca.object, choix="var", title="") > # graph of individuals > plot.PCA(pca.object, cex=0.8, autoLab="auto", shadowtext = FALSE, title="") Three main profiles stand out:

• au sein de (upper left corner); • au centre de and au coeur de (upper right corner); • au milieu de and parmi (lower right corner).

The affinities between au centre de and au coeur de on the one hand and au milieu de and parmi on the other are due to similar collocational behaviors. Au sein de is the odd one out. Most NPs clutter around where the two axes intersect, a sign that their distribution is of little interest, at least with respect to our understanding of the prepositions. More interesting are those NPs that appear in the margins of the plot.

Admittedly, the graph of individuals is cluttered. This is due to the very large number of NP types that cooccur with the prepositions. We filter out unwanted individuals by selecting only the desired ones. The select argument of the PCA() function allows the user to filter out unwanted individuals by selecting only the desired ones.

> plot.PCA(pca.object, select="coord 20") > plot.PCA(pca.object, select="contrib 20") > plot.PCA(pca.object, select="cos2 5") > plot.PCA(pca.object, select="dist 20") Here is what the title of each plot means:

• with select="coord 20", only the labels of the twenty individuals that have the most extreme coordinates on the chosen dimensions are plotted; • with select="contrib 20", only the labels of the twenty individuals that have the highest contributions on the chosen dimensions are plotted 12 ; • with select="cos2 5", only the labels of the five individuals that have the highest squared-cosine scores on the chosen dimensions are plotted 13 ; • with select="dist 20", only the labels of the twenty individuals that are the farthest from the center of gravity of the cloud of data points are plotted.

Clear trends emerge: 12 The contribution is a measure of how much an individual contributes to the construction of a component. 13 The squared cosine (cos 2 ) is a measure of how well an individual is projected onto a component.

• the au sein de construction tends to co-occur with collective NPs that denote groups of human beings (entreprise 'company/business', équipe 'team', établissement 'institution/institute', etc.); • the au centre de and au coeur de constructions tend to co-occur with NPs that denote urban areas (ville 'city/town', village 'village', quartier 'district') and thoughts or ideas (préoccupations 'concerns/issues', débat 'debate/discussion/ issue'); • the au milieu de and parmi constructions tend to co-occur with plural NPs that denote sets of discrete individuals (hommes 'men', personnes 'persons', membres 'members'), among other things.

The graph displaying the first two components does a good job at grouping prepositions based on the nominal collocates that they have in common and revealing consistent semantic trends. However, it does not show what distinguishes each preposition. For example, au centre du conflit 'at the center of the conflict' profiles a participant that is either the instigator of the conflict or what is at stake in the conflict. In constrast, au coeur du conflit 'at the heart of the conflict' denotes the peak of the conflict, either spatially or temporally. This issue has nothing to do with the PCA. It has to do with the kind of collocational approach exemplified in the paper, which does not aim to (and is not geared to) reveal fine-grained semantic differences by itself.

Exploratory Factor Analysis

The same data set serves as input for EFA, which is performed with factanal(). According to Fig.

In base R, we run EFA with factanal().

The proportions of variance explained by the factors (i.e. eigenvalues) are listed under the factor loadings. A factor is considered worth keeping if the corresponding SS loading (i.e. the sum of squared loadings) is greater than 1. Two factors are retained because both have eigenvalues over 1. Factor 1 accounts for 32.5% of the variance. Factor 2 account for 28.5% of the variance. Both factors account for 66.9% of the variance.

In EFA, rotation is a procedure meant to clarify the relationship between variables and factors. As its name indicates, it rotates the factors to align them better with the variables. The two most frequent rotation methods are varimax and promax. With varimax, the factor axes are rotated in such a way that they are still perpendicular to each other. The factors are uncorrelated and the production of 1s and 0s in the factor matrix is maximized. With promax, the factor axes are rotated in an oblique way. The factors are correlated. With promax, the resulting model provides a closer fit to the data than with varimax. In either case, the goal is to arrive at a few common meaningful factors. Rotation is optional as it does not modify the relationship between the factors and the variables. Figure

> loadings <-loadings(fa.object) > plot(loadings, type="n", xlim=c(-1,1)) > text(loadings, rownames(loadings))

To produce a plot with promax rotation, we run factanal() again but set rotation to promax.

> fa.object2 <-factanal(data, factors=2, rotation="promax") > loadings2 <-loadings(fa.object2) > plot(loadings2, type="n", xlim=c(-1,1)) > text(loadings2, rownames(loadings2))

The distinctive profiles we obtain with EFA are similar to those we obtained with PCA. The only major difference is the proximity of au milieu de with au centre de and au coeur de. This may be due to the fact that only two factors are retained in the analysis. As far as this data set is concerned, PCA is clearly a better alternative, all the more so as individuals are not taken into account in the graphic output of this kind of EFA.

Reporting Results

When reporting the results of CA, MCA, or PCA, the following elements should be included:

• the cumulative percentage of variance explained by each dimension/component; • the graph and its interpretation.

Additionally, numeric descriptors such as contribution and quality of projection can be reported.

Each methods has its specificities. In CA, it is customary to report the χ 2 test result to see if the table deviates from independence. This result is part of the default output of the CA() function of the FactoMineR package (see

In MCA, the eigenvalues associated with the first dimensions are often much lower than in CA and PCA. This means that it is often necessary to take more dimensions into account in the analysis. When the dimensionality of a dataset is high, the representation quality of a variable on a given plane is bound to be poor. However, how much a variable contributes to a given dimension is not affected by the high-dimensional nature of the data. Although optional, taking a look at the contribution and reporting the scores might be a good idea. In Sect. 19.3.2, the contribution scores of the variables are accessed by entering the following: > mca.object$var$contrib Dim 1 Dim 2 Dim 3 Dim 4 Dim 5 bloody 1.668529e+01 6.782887e+00 5.964242e+00 4.032829e+00 1.668529e+01 damn 4.740002e+00 3.204420e+01 4.254051e+01 4.924310e+00 4.740002e+00 f-words 2.784165e+01 7.606179e-01 2.789159e+01 4.350051e+00 2.784165e+01 gosh 7.329551e-01 5.436848e+01 1.835285e+01 5.123951e+00 7.329551e-01 shit 1.019655e-04 6.043815e+00 5.250816e+00 8.156886e+01 1.019655e-04 f 2.141147e+01 1.164802e-21 5.381496e-23 1.268778e-21 2.141147e+01 m 2.858853e+01 1.524969e-21 7.838154e-23 1.686021e-21 2.858853e+01

In PCA, there are two graphs to inspect: the graph of variables and the graph of individuals (see

The output of fa.object in Sect. 19.3.4 is typical of how the results of an EFA should be reported. Therefore, it can conveniently be copied and pasted into the results section of a paper.

Further Reading

R. H. Baayen (2008). Analyzing Linguistic Data: A Practical Introduction to

Statistics using R. Cambridge University Press.

Well-known to quantitative linguists, this textbook explains, among many other methods, how to run CA and PCA with R. It also shows how to run EFA. The data sets are directly relevant to linguistics and provided as part of the languageR package.

G. Desagulier (2017). Corpus Linguistics and Statistics with R. Introduction to

Quantitative Methods in Linguistics. Quantitative Methods in the Humanities and Social Sciences. New York: Springer.

Chapter 10 of this book presents in greater detail three of the four methods covered in this chapter: CA, MCA, and PCA. Each method is illustrated with a detailed linguistic case study. The corresponding data sets and R scripts are provided in the form of companion files. This textbook focuses on CA and its variants (joint correspondence analysis, canonical correspondence analysis, co-inertia analysis, co-correspondence analysis) as well as multiple correspondence analysis. Although the book gives priority to practice, the theoretical and mathematical aspects of CA are presented in two appendices (A and B, respectively). The book can be read in combination with the documentation of the ca package

M. J. Greenacre (2007). Correspondence Analysis in

F. Husson, S. Lê, and J. Pagès (2010). Exploratory Multivariate Analysis by

Example Using R. London: CRC press.

Like

Introduction

A statistical test is a mathematical tool which helps us establish whether an observed effect is likely to be due to chance-due to a particular composition of the corpus, i.e. the specific texts or speakers that have been randomly selected-or whether the effect is likely to exist in the population (language use in general). In the latter case, we say that the effect is statistically significant. Imagine that there

The writing of this chapter has been supported by UK Economic and Social Research Council (grants ES/R008906/1 and EP/P001559/1).

Electronic Supplementary Material

The online version of this chapter (

V. Brezina ( ) Lancaster University, Lancaster, UK e-mail: v.brezina@lancaster.ac.uk 474 V. Brezina are two corpora including the same number of speakers (10), one of male and the other of female speech. We search them for the linguistic variable of our interest, such as swearwords, and we observe that the male corpus includes an average of 16 swearwords and the female corpus 14. The averages (means) are computed from individual relative frequencies

A small p-value (usually below the conventional threshold of 0.05) indicates a statistically significant result. This chapter focuses on monofactorial statistical tests, statistical procedures that produce p-values, and their interpretation.

Fundamentals

Null-Hypothesis Significance Testing (NHST) Paradigm

Most statistical measures (statistical tests and confidence intervals) discussed in this chapter operate within the Null-hypothesis significance testing (NHST) paradigm. This paradigm is based on a simple but rigorous procedure of evaluating empirical evidence

To evaluate the evidence (corpus findings, e.g. the means of 16 and 14 for men and women, respectively) we employ a particular statistical test. The choice of the statistical test depends on the research design and the shape of the data. All statistical tests produce a p-value. The p-value is the probability of seeing the difference observed in the corpus or even a larger difference if the null-hypothesis were true

To demonstrate these principles in practice, the following is an overview of the procedure of statistical testing with our sociolinguistic example:

Step 1: Formulate H 1 .

Men and women differ in the frequency of use of swearwords in casual speech.

Step 2: Formulate H 0 .

There is no difference between how men and women use swearwords in casual speech.

Step 3: Carry out Research by Analysing corpora.

The data used consists of two corpora: a male corpus (10 speakers) and a female corpus (10 speakers). We can observe that the mean value of relative frequency of swearwords in the male corpus is 16, while the mean value in the female corpus is 14, see Table

Step 4: Carry out an Appropriate Statistical Test.

In our particular example, we could use an independent samples t-test or a Mann-Whitney U test (see Sect. 20.2.2) to evaluate the evidence. The results of these tests would be reported as follows:

V. Brezina T-Test: t (16.77) = 0.67; p = 0.513. Mann-Whitney U Test: U = 70; p = 0.128.

The reporting of the test might look complicated because different values get reported, such as the test statistics (t = 0.67; U = 70) or the degrees of freedom (16.77). A brief note about the two concepts: The test statistics is "used to assess a particular hypothesis in relation to some population"

Based on the new set of results above, we can reject the null hypothesis and conclude that there is a statistically significant difference between the two groups we have tested-males and females. Another consideration in NHST is whether the hypothesis we formulate is non-directional or directional. So far, an example of a non-directional hypothesis was considered ("Men and women differ in the frequency of use of swearwords in casual speech."). This hypothesis states that there is a difference between two groups but does not hypothesise in which direction this difference would be. On the other hand, we can restate the hypothesis above as a directional hypothesis.

H 1 d: Men use more swearwords than women in casual speech.

Using a directional hypothesis has implications for the NHST procedure and needs to be theoretically motivated, i.e. there should be evidence in the literature that the difference is in one direction rather than the other. The statistical test that will be used for a directional hypothesis is called a one-tailed test, as opposed to the more usual two-tailed test used with a non-directional hypothesis. In our example, the one-tailed test would test the following null hypothesis: H 0 d: There is no difference between how men and women use swearwords in casual speech or women use more swearwords than men.

Each statistical test discussed in this chapter has a one-or two-tailed (default) version, which can be selected. In the rest of this chapter, two-tailed versions of statistical tests will be considered. For a discussion of the appropriate use of a onetailed test see

It is also important to note that NHST, like many other statistical procedures, works with probabilities. As explained above, the key role in NHST is played by the probability value p. When analysing data, we are therefore operating in the realm of likely rather than describing the absolute truth. It is thus possible (but unlikely) to reject a true null hypothesis (known as a type I error or a false positive finding)

V. Brezina or alternatively to fail to reject a false null hypothesis (known as type II error or a false negative finding). Type I and type II errors are part and parcel of the procedure. They become a problem when their probabilities are inflated (due to a violation of a test assumption or multiple testing) above an acceptable level (e.g. 5%). A brief note needs to be made about multiple testing and family-wise errors. With applying multiple tests on the same dataset, each with its own type I error, the probability of rejecting a true null hypothesis (i.e. a false hit) increases dramatically

In sum, NHST is a relatively simple procedure, which evaluates the amount of evidence for rejecting the null hypothesis. In addition to looking at p-values, we should also consider the size of the observed effect in the sample and the estimation of the size of the effect in the population, which can be expressed as a confidence interval (see Sect. 20.2.2.1).

Statistical Tests and their Assumptions

Chi-Squared Test

The chi-squared test is a powerful tool for exploring categorical data

Table

The chi-squared test compares these observed frequencies with expected frequencies, frequencies of occurrence of the individual linguistic features under the null Expected frequency = row total × column total grand total (20.1) Table

The equation for a simple form of the chi-squared is as follows.

Chi-squared = across all cells (observed frequencyexpected frequency)

Sometimes, an alternative equation is used, which applies

1. Independence of Observations: The test assumes that every observation such as the use of the active or passive verb construction in English (see Table

The chi-squared test for the data in Table

The test statistic (32.69 in our example) is then compared with the chi-squared distribution with appropriate degrees of freedom (df ) according to the size of the contingency table. The chi-squared test statistic, as every test statistics, has a known distribution under the null hypothesis. Based on a comparison of our value (32.69) with the distribution, we can make a judgement about the statistical significance (pvalue) of the result. A 2 × 2 table corresponds to df = 1, a 2 × 3 table to df = 2, a 3 × 3 table to df = 4 etc. More generally, df = (number of rows -1) × (number of columns -1). However, the reader does not need to worry about these technical details because the p-value is automatically provided by R (see below). The p-value in this case is very low, p < .001. We can therefore conclude that there is a statistically significant difference in the use of the passive and active verb forms across the three genre categories. In the present example, the use of active and passive voice as a function of genre was examined. It is often the case that multiple independent variables or predictors are included in the design. In such cases, logistic regression can be employed. In fact, chi-squared can be seen as a simple (monofactorial) version of logistic regression (see Chap. 21).

T-test

The t-test, unlike the chi-squared test, is suitable for quantitative (rather than categorical) data, i.e. data measured at the scale/ratio level. It is a parametric test comparing the means of two groups and variation within each group

1. Independence of Observations: Like the chi-squared test (see Sect. 20.2.2.1), the t-test assumes that each observation (text or speech sample) comes from a different (randomly sampled) speaker or writer

The t-test for the data in Table

Welch s independent sample t-test = 16 -14 56.9 10 + 32.7 10 = 0.67 (20.5)

Finally, the test statistic (0.67 in our example) is compared with the t-distribution with appropriate degrees of freedom (df ) to obtain the p-value; Welch's t-test uses a special formula for adjusting the degrees of freedom to counterbalance the effect of unequal variances (see assumption 3 above). In R, this process happens behind the scenes and a p-value is outputted with the t-test:

T-test: t (16.77) = 0.67; p = 0.513.

This p-value, as we have already seen in Sect. 20.2.1, is larger than the conventional 0.05 and therefore we can conclude that the test did not find a statistically significant difference between the two groups.

ANOVA

ANOVA (Analysis of Variance) is a parametric test (sometimes also referred to as F-test) that can be used for comparison of values from multiple groups

Reviewing the assumptions of ANOVA, (i) independence of observations, (ii) normality and (iii) homoscedasticity, we can see that these are similar to the assumptions of the t-test (see Sect. 20.2.2.2). The ANOVA test is robust against the violation of the normality assumption

ANOVA for the data in Table

Between-group variance = 3 × (28 -15) 2 + 12 × (14.8 -15) 2 + 5 × (7. As with the previous tests, R also outputs the appropriate p-value for the given F-statistic (with particular degrees of freedom). In our example, the difference is statistically significant with p < .001. A note needs to be made here: ANOVA is an omnibus test, which tests the null hypothesis that there is no difference among the groups in question. A significant difference means that there is a difference between at least two groups, but ANOVA will not tell us which ones. For this reason, post-hoc tests are carried out to establish where exactly the difference lies. There are different options for post-hoc tests used in different traditions of research (e.g. LSD test, Tukey's HSD test, Scheffe's test etc.). The simplest option recommended for corpus data (cf.

Mann-Whitney U Test

Mann-Whitney U test, also known as Wilcoxon rank-sum, is the non-parametric equivalent of the t-test

For the Mann-Whitney U test, two U values are calculated, one for each group, from which the smaller value is then taken

U 2 = sum of ranks for group 2 -cases in group 2 × cases in group 2 + 1 2 (20.13)

We take the smaller of the two values from Eqs. (20.12) and (20.13) as the U statistic that we report. The Mann-Whitney U test has the following assumptions: It assumes independence of observations. Also, while it does not assume underlying normal distribution, it assumes that "the underlying distributions from which the samples are derived are identical in shape"

The Mann-Whitney U test for the data in Table

The resulting U, which we report, is 30 because it is the smaller of the two values from the calculations above. The appropriate p-value for this test statistic and sample size is 0.128 (R provides it automatically), hence the result is not statistically significant.

Kruskal-Wallis Test

The Kruskal-Wallis test works on a similar principle as the Mann-Whitney test; it, however, takes into account ranks in multiple (3+) groups

The equation for calculating the test statistic is:  The p-value for this test statistic and sample size is p < .001 (R provides it automatically). We can therefore conclude that the three social-class groups differ from each other more than by chance alone.

Pearson's Correlation

Correlation is a standardised measure of relationship between two variables (Sheskin 2004:943-1075; Gries 2013:238ff). For example, we know that the use of nouns and adjectives in text is strongly correlated. This means that the more nouns occur in texts, the more adjectives also appear (and vice versa, because correlations are bidirectional). This fact is not surprising because adjectives typically modify nouns to create complex noun phrases (a beautiful flower). The correlation is, however, not perfect because some adjectives are also used after a copula and without a nominal antecedent (This is beautiful). As introduced in Chap.17, there are different measures of correlation. A very common measure of correlation is Pearson's correlation (r). Pearson's correlation is used with quantitative variables (interval/scale) such as the relative frequencies of nouns and adjectives in our example. Pearson's correlation is a parametric measure which assumes underlying V. Brezina normal distribution of the variables in the population, although the violation of this assumption should not have severe implications (e.g.

This equation shows that we are looking at the amount of covariance (variation that the variables have in common) in the data expressed as the standard deviations (SD 1 and SD 2 ) of the two variables that we correlate. As a rough indication, the following interpretation of r has been suggested by

Pearson's and Spearman's correlation operate on a scale from -1 to 1. A positive value of a correlation indicates a relationship between two variables where if one variable increases the other increases as well and if one variable decreases the other decreases as well. A negative correlation value, on the other hand, indicates an inverse relationship between two variables, where if one variable decreases, the other increases and vice versa. Correlation measures are typically also supplemented with a statistical test which evaluates the null hypothesis that the correlation is 0, that means that there is no relationship between the two variables in question. In addition to the correlation value (r in Pearson's correlation), a p-value is therefore also reported. A p-value smaller than 0.05 is conventionally considered to indicate statistical significance. In such a case, we can conclude that the correlation is likely to be non-zero in the population, which, however, might not be very informative because small (non-zero) correlations have only negligible effect (cf.

Non-parametric Correlation Tests

Spearman's correlation (r s ), sometimes also denoted by the Greek letter rho (ρ) is similar to Pearson's correlation but instead of operating with quantitative variables (interval/scale), it operates with ordinal variables (ranks). It can also be used with interval/scale variables, which get converted into ranks during the process. Spearman's correlation is non-parametric -it does not assume underlying normal distribution in the population. Spearman's correlation is therefore calculated as follows:

Another option for non-parametric correlation is Kendall's tau (τ). However, it needs to be noted that Kendall's tau generally shows lower values compared to Pearson's and Spearman's correlations. For example, "Kendall correlation of 0.8 corresponds to a Pearson correlation of about 0.95"

Effect Sizes and Confidence Intervals

Using statistical tests has often been criticized on a number of counts. First, following the binary decision-making procedure of NHST, continuous findings are routinely interpreted rather crudely as 'significant' or 'non-significant'. That is, the binary interpretation of p values tells us nothing about the size or magnitude of the effect or relationship of interest. Another weakness of NHST is the arbitrary nature of the cut-off point of 0.05

V. Brezina

The arbitrary nature of considering everything with a p-value above 0.05 (or 0.01) statistically significant raises several questions: How should we interpret a p-value of 0.051, which is very close to significance? Should we completely disregard this result? Or should we, on the contrary, report it? What about p-values of 0.06, 0.07, 0.08 etc.? What if two replication studies reach a p-value close to significance-does this strengthen or weaken the case? It is important to note that misunderstanding of or overreliance on NHST can be harmful to a discipline because it limits the scope of results reported in research reports, especially those published in books and journals, to statistically significant results. Statistically non-significant yet important results-although based on rigorous studies asking important research questionsthus become largely underreported; this is a so-called publication bias-see

The idea here is very simple. In addition to any p values, we measure and report the magnitude of the effect we observe in the data or the effect size. We can also report a range of values the effect size is likely to have in the population. We call this range a confidence interval (CI), usually a 95% CI. A 95% confidence interval is an interval that is constructed around a statistical measure (here an effect size) based on the sample in such a way that the true value of this measure lies within this interval for 95% of the samples taken from the same population. We can imagine this as a multiple replication exercise with different samples from the same population. In 95% of the samples, the measure (effect size) will lie within the confidence interval.

A popular standardised effect size measure is Cohen's d. It is used to express the difference between two groups. It is very intuitive and simple; it is calculated by taking the difference between the two means and dividing this by the pooled standard deviation (overall SD that combines the individual SDs in the groups we compare) as can be seen from the equation below:

In the example of swear-word data from Table

Cohen's d = 16 -14 56.89×(10-1)+32.67×(10-1) 20-2 = 16 -14 6.69 = 0.3 (20.22)

When we build the 95% CI around Cohen's d, we get the following result.

Cohen's d: 0.3; 95% CI

Because the 95% CI is very broad and includes zero, the sample does not provide enough evidence for a non-zero effect, which would show a real difference between the groups. If we, however, take a corpus with 100 male and 100 female speakers, where the difference between the groups is exactly the same (16 and 14), we get more evidence for the effect (the dataset is available from the companion website): d = 0.3 95% CI [0.03, 0.59]. Note that similar change to the CIs but not the effect sizes was discussed in Sect. 20.2.2.6 in relation to the Pearson's correlation. Unlike p-values, effect sizes and their confidence intervals can be combined in metaanalysis

Representative

(continued)

Research questions

1. What is the performance of the aggregate data method compared to the individual speaker method with real social groups? 2. What is the performance of the aggregate data method compared to the individual speaker method with random speaker groupings?

Data

The study used a subset of the British National Corpus (spoken) from which the speech samples of 32 speakers were selected. Sixteen samples came from male speakers and sixteen from female speakers.

Method

Brezina and Meyerhoff compared two methods: (1) a traditional method of comparison of broadly defined subcorpora (male vs. female) with the loglikelihood test (similar to chi-squared) and (2) use of the Mann-Whitney U test, which takes into consideration individual variation inside each subcorpus. The study investigated a range of lexico-grammatical variables including the lemma FUCK, the adverb lovely, the discourse markers you know, sort of, and really, negative concord (e.g. he didn't say nothing), and the determiners the and some.

Results

The results show that the application of the traditional method is largely inaccurate. It overestimates statistical significance (type I error) in cases where there is no real difference and the variation is purely due to chance; the type I error rates (false hits) ranged from 48 to 99% depending on the linguistic feature investigated. The Mann-Whitney U test, which takes individual variation into account performed considerably better with type I error rate (as expected) around 5%. The article demonstrated that inappropriate use of statistical tests can lead to very inaccurate and misleading results.

In principle, there is nothing wrong with using the chi-squared test or log likelihood test if the data is suitable for this. We, however, need to critically evaluate the assumptions of the tests before applying them; in the context of corpus linguistic analyses, the assumption of independence of observations is of utmost importance (see Sect. 20.2.2). We have to realise that in corpora we typically sample data at the level of texts/speakers. The data analysis therefore needs to pay attention to this level of individual variation.

Practical Guide with R

This section presents different steps in R from how to analyze data to how to report results for publication. R scripts + data files are available on the companion website. Reporting For the t-test, three pieces of information need to be reported: (i) the test statistic (t), (ii) degrees of freedom (df ) and the p-value. In addition, it is also useful to report related descriptive statistics (means, SDs, and the number of observations in each group N1 and N2).

The Welch's independent samples t-test (t (16.77) = -0.668; p = .513) did not show a statistically significant difference between the two groups. On average, the male group (N = 10) and the female group (N = 10) produced 16 (SD = 7.5) and 14 (SD = 5.7) swearwords per 10,000 words. This article offers a critical view on using frequency data in corpus linguistics in the context of language learning; this critique is, however, applicable to other contexts as well. The paper investigates the sources of variation in corpora and shows how these can be dealt with systematically using different statistical and visualisation techniques.

Cohen's d with 95% Confidence Intervals -To Be Computed with T-test

Gries, S.T. 2013. Statistics for linguistics with R:

A practical introduction, 2nd ed. Mouton de Gruyter, Berlin.

Introduction

This chapter presents different variants of regression modelling. Regression analysis is a family of methods that aim at describing a dependent variable in terms of other, independent, variables. For example, consider the question of how women and men may differ in the way they use language. Studies that aim to investigate such a relation with regression modeling would start with a linguistic phenomenon, such as perhaps tag questions, and relate observations of variability in speakers' use of tag questions to another variable, namely the gender of the speaker. These studies would further control for other variables, such as the speaker's age, socio-economic status, education level, role in their social network, and the formality of the situation in which the recorded conversation takes place. In such a design, the use or non-use of a tag question would be called the dependent variable or response variable, whereas the other variables are the independent variables. The design allows researchers to investigate whether or not the use of tag questions varies with the speaker's gender.

In other words, are women more likely to use tag questions, and if so, how much more likely? Is this generally the case, or only in certain conditions, for example during a certain age bracket? Regression analyses can provide insights into these issues, thereby giving the researcher more than a simple yes-or-no answer.

By far the most popular use of regression analysis in linguistics is to determine which of the different independent variables have an impact on the dependent variable, and how strong that impact is. The uses of regression do however go beyond that. For instance, regression can be used to construct predictive models for values of the independent variables that have not been observed. For practical and pedagogical reasons, this chapter will focus on some of the most basic regression models, which can still yield informative answers to research questions like the ones described above. In doing so, this chapter showcases applications of regression techniques in corpus-linguistic research. In such applications, the dependent variable represents a variable linguistic phenomenon that can be observed in corpus data. For instance, English adjectives such as proud can form the comparative in two ways, namely either as prouder or as more proud. A regression analysis can model that variation by making reference to several characteristics of English adjectives that serve to explain why some are biased towards the morphological comparative, while others show a preference for the periphrastic comparative. Examples of such characteristics include word length, stress pattern, and the syntactic environment

21 Fixed-Effects Regression Modeling 507 It is the main aim of this chapter to lay out the fundamental concepts that are at issue and to offer step-by-step instructions for their use. For this purpose, we will use both authentic linguistic datasets and data that is artificially created. The idea behind using simulated data is that we know exactly what information is contained in a given dataset and what effects should be discovered with a regression analysis. What is more, we can make changes to the underlying data set in order to see how the resulting model changes, for instance when we manipulate aspects of an existing variable, or when we add or leave out another variable. This approach is meant to give readers a feel for the practical use of regression techniques and for the ways in which regression models respond to variation in the data.

Fundamentals

(Multiple) Linear Regression

Linear regression is used to analyze linguistic phenomena that can be measured on a continuous scale, as for example the pronunciation length of a given word or the time it takes to respond to an experimental stimulus. Below, we will use it to analyze a corpus based measure, namely the type-token ratio (TTR) of different texts that have the same overall length. Texts with high TTR values contain many different word types; texts with low TTR values contain many repetitions of the same word types. A multiple linear regression analysis can determine whether there are factors that allow us to predict whether a given text will have a relatively high TTR value or a relatively low one. The phenomenon that is represented by our measurements, in this case measurements of TTR, represents the dependent variable of the analysis. It is the goal of a linear regression to explain variation in the dependent variable. This is done with the help of at least one independent variable. In many analyses, more than one independent variable is used, since the dependent variable may be influenced by more than one underlying phenomenon.

An Example of (Multiple) Linear Regression

Figure

The graph visualizes two kinds of variation in TTR values. First, it can be seen that the three newspapers have different average TTR values, with The Times showing relatively low values in many decades and the Daily Telegraph showing the highest values. Second, the graph reveals change over time. Later TTR values tend to be higher than earlier values. This means that there are two independent variables that can be used to make predictions about the TTR values of any given text: One independent variable is time, which is a continuous variable expressed in a continuous sequence of numbers that represent successive years. The second independent variable is the type of newspaper, which is a categorical variable, since it falls into three distinct categories, namely The Times, the Daily Telegraph, and the Guardian. Let us focus exclusively on time first. Linear regression can be used to determine whether there is a relation between the dependent variable TTR and the independent variable time. Linear regression draws on an algorithm that finds a straight line that captures the relation between the dependent variable and the independent variable with the least amount of error, that is, with the smallest distances between the straight line and the data points. Figure

As can be seen in Fig.

In order to understand this kind of output a little better, it is useful to see how it differs once the second independent variable, the type of newspaper, is entered into a more complex regression model. The output below reports on a model that contains the type of newspaper as a second independent variable, which allows the regression to adjust the estimates for each newspaper type.

• Linearity. Another assumption is that the relation between the dependent variable and a given independent variable is linear. Ideally, a continuous independent variable will relate to the dependent variable in such a way that successive increases in one will correspond to a mirror image of successive changes, i.e. increases or decreases, in the other. • No multicollinearity. Different independent variables should not stand in linear relationships with each other. For example, if a corpus-based analysis contains the length of a phrase as an independent variable, and we introduce the pronominality of that phrase as a second independent variable, these two variables might correlate significantly, since full lexical phrases are by necessity longer than single pronouns. As a consequence, the model will reduce the certainty associated with the coefficients of those variables (as captured by their standard deviations) and potentially misattribute the effect of one variable to the other. • No autocorrelation. For all TTR values in our dataset, the regression model computes predictions that can differ more or less from the actual measurements. These differences are called the residuals. If seeing one residual lets us predict the size of the residual that is next in line, or the one after that, the residuals are autocorrelated. This should not be the case. • Homoscedasticity. This term signifies that the variation of the residuals along the values of the independent variable needs to be similar. This assumption would be violated, for instance, if the predictions for low TTR values are highly accurate, and the predictions for high TTR values vary greatly in their accuracy.

Binary Logistic Regression

Whereas our concern in the previous section was to predict the values of a continuous dependent variable, the goal of a binary logistic regression is to model the logarithm (log) odds ratio that a binary variable takes one value, rather than the other. 2 Binary logistic regression is a member of a family of regression techniques that is known as generalized linear modeling, or GLM for short. In a linguistic case study, this allows us to predict how a speaker will behave when faced with the choice between two linguistic resources. In most cases, the choice will not be determined by a single underlying factor, but rather by a complex ecology of interacting factors. Binary logistic regression serves to identify the factors that matter to a given linguistic choice. As in (multiple) linear regression, the goal is to explain a dependent variable in terms of several independent variables. The most important difference is the nature of the dependent variable, which is continuous in a (multiple) linear regression, whereas it is categorical and binary in a binary logistic regression.

An Example of Binary Logistic Regression

An example of a binary choice in English is the variation between will and be going to as expressions of the future. A binary logistic regression analysis requires a dataset that contains tokens of the dependent variable in their two different realizations. Our discussion here draws on a dataset of will and be going to from the Diachronic Corpus of Present-Day Spoken English

(1) a. so yes I'm sure it will be quite different b. With a dataset that is annotated for dependent and independent variables and their values, a binary logistic regression can determine whether the independent variables have a measurable effect on the choice in the dependent variable, and how the independent variables differ in relative impact. For each token, the analysis will produce a predicted value of the dependent variable. Given that we have an example from a young female speaker who speaks in an informal setting and uses a nonagentive verb, what are the log odds of that speaker producing will rather than be going to? The coefficients of the independent variables will yield a numerical value that expresses the log odds. Whereas the fitted values in a (multiple) linear regression can theoretically range from negative to positive infinity, this is not the case with binary logistic regression. Since it is the purpose of the method to model a categorical choice between two outcomes, the fitted values of a binary logistic regression only range between 0 (be going to is completely unlikely) and 1 (it is completely certain that the speaker will use be going to). This is accomplished through the inverse logit function, which transforms log odds values into probabilities

In more complex analyses, analysts might employ a process called model selection, which aims to determine which independent variables and which interactions between them should be included in the regression model. Different strategies can be employed in model selection. A general discussion that also includes criticisms of model selection can be found in

How can the coefficients of a logistic regression be interpreted? For categorical independent variables, the coefficient represents the change in the log odds of the outcome as compared to the reference level. For example, in contexts with nonagentive verbs the log odds of will increase by about 0.36, as compared to contexts with agentive verbs. The exponential function turns these values into odds ratios, which can be interpreted more easily. In the case of non-agentive verbs, the log odds value of 0.36, transformed by the exponential function, yields an odds ratio of 1.433. What this means is that the odds for non-agentive verbs to be used with will are 43.3% higher than the odds for agentive verbs.

Assumptions of Binary Logistic Regression

A basic requirement of binary logistic regression is a dependent variable that has exactly two possible outcomes. Apart from this requirement, the assumptions that underlie a binary logistic regression analysis are a subset of the ones that were discussed for multiple regression analysis above.

• Independence. Each observation should be independent, that is, it should not be influenced by other observations that are contained in the same dataset. In our data, it could be argued that this assumption is problematic for at least two reasons. First, we implicitly allowed for several observations from the same speaker. Second, it is well-known that speakers' choices between two variants are influenced by earlier uses of those variants in the preceding discourse, a phenomenon that is known as structural priming

An Extension of Binary Logistic Regression: Multinomial Logistic Regression

Multinomial logistic regression is an extension of binary logistic regression that shares its general characteristics and assumptions, but that can handle cases of variation with more than two variants. The introduction mentioned the example of adjectives that convey the idea of a highly positive assessment. Here, a speaker of English has the choice between adjectives such as great, excellent, and perfect, amongst others. Multinomial logistic regression can be used to investigate the variables that influence speakers' subconscious choices between such a group of adjectives. The dependent variable is thus a categorical variable with three or more levels.

Representative

Research questions

Is the preposition to pronounced more quickly or more slowly depending on its context? What are the factors that influence pronunciation length? Tily et al. hypothesize that to is pronounced more quickly in cases where it has a high probability of occurrence. For example, when a sentence starts with the words John sent a letter, a following to is very likely. By comparison, if the sentence starts with John gave the cat, a following to is less likely, even though an utterance such as John gave the cat to his daughter is possible. These relative likelihoods are a function of several underlying factors, such as the referents' animacy, their length, and their discourse-givenness, amongst others.

Research questions

In English, the syntactic order of adverbial clauses and their main clauses is variable. The sentence Before you leave, please check that the lights are out is just as grammatical as its alternative Please check that the lights are out before you leave. How do speakers decide where to place the adverbial clause?

Data

In order to investigate whether iconicity of sequence influences the positioning of English adverbial clauses, Diessel retrieved 600 complex sentences with the conjunctions when, after, before, once, and until from the ICE-GB corpus.

Method

The sentences were coded for the dependent variable and four predictor variables. The dependent variable is the relative position of the adverbial clause, which can either be anterior or posterior to the main clause. The first independent variable is the conceptual order of the two clauses, which was coded as either iconic or non-iconic. The second independent variable is syntactic complexity, which captures whether or not an adverbial clause is internally complex. The third independent variable takes meaning into account, distinguishing temporal meaning from conditional meaning and the meaning of causation or purpose. The fourth independent variable is the length of the adverbial clause, measured as a percentage of the length of the entire complex sentence. A binary logistic regression was conducted to assess the effects of these independent variables.

Results

The logistic regression finds significant effects for all variables except syntactic complexity. The coefficient estimates in the table below

# Clean the workspace and set the random number generator. rm(list = ls()) set.seed(1234)

# Create a data frame with the two independent variables. linreg.data <-data.frame( pronoun = rep(c(0, 1), 500), hi.freq.v = c(rep(0, 500), rep(1, 500)) )

# Add the fictional pronunciation length measurements. linreg.data$pron.length = rnorm(1000, 300-80*linreg.data$pronoun-50*linreg.data$hi.freq.v, sd = 30) # Turn the two independent variables into factors. linreg.data[,1:2] <-lapply(linreg.data[,1:2], as.factor) # A visualization of the effects that we have created boxplot

The boxplot shows that examples with a preceding element that is lexical (lex) and a following element that is not a high frequency verbs (other) are distributed around the value of 300 ms. As we specified in the code above, examples with a preceding pronoun are, on average, 80 ms faster. Examples with a following high frequency verb are roughly 50 ms faster. These two effects are additive, so that examples with a pronoun and a following high frequency verbs are about 130 ms faster. We can now run a multiple linear regression analysis in order to see how the manufactured effects are picked up by a statistical model.

Running a Multiple Linear Regression

The code below executes a multiple linear regression model that attempts to predict the value of each measurement in our dependent variable on the basis of the information that is given by the independent variables. The table of coefficients reproduces the effects that we manufactured into the data. The intercept is very close to the 300 ms that we specified; examples with pronouns are about 80 ms faster; examples with high frequency verbs are about 52 ms faster. The residual standard error corresponds to the standard deviation of 30 ms that we selected.

What Happens If We Make the Effect Sizes Smaller?

How do the results change when we re-engineer the effects of the two independent variables? The code below reduces the strength of both effects by 30 ms. We then re-run the analysis and obtain a new output. Expectably, the new coefficient estimates correspond to the adjustments that we made to the underlying data. Since we did not change the inherent variability of the adjustments, the standard errors remain virtually identical to the ones that were obtained above. The t-values drop according to the diminished sizes of the estimates, but the p-values remain significant. With regard to the global indicators of model quality, the residual standard error stays the same, but the R 2 -values drop considerably.

What Happens If We Make the Effects "Noisier"?

The way in which we set the values of the dependent variable above included a certain amount of statistical noise, which we specified by setting the standard deviation to 30 ms. This statistical noise explains why the models' coefficients do not correspond exactly to the number of milliseconds that we specified for each type of example. What happens when we increase the amount of noise, i.e. the amount of unsystematic, unpredictable variation that goes into an analysis? The code below increases the inherent variability of our two independent variables by setting the standard deviation to 50 ms instead of 30 ms. The results show a few differences compared to those of the first model that we constructed. First of all, the residuals are larger. While the coefficients as such have not changed much, the standard errors of the coefficients are larger and the t-values correspondingly lower. The global assessments show a higher residual standard error and lower R 2 -values. This shows that high R 2 -values depend not only on the average size of an effect, but also on the clarity of that effect.

Manufacturing an Interaction Effect

A useful feature of regression modelling is that interdependencies between independent variables can be taken into account. These interdependencies are called interactions, and in this section, we will modify the dataset in such a way as to create an interaction effect. For this purpose, let us consider a third independent variable for our fictitious data, namely the presence of an adverb to the immediate right of don't. Like the other two independent variables, it is a categorical variable that can take on two values. Either an adverb is present or not. Let us assume that the presence of an adverb modulates the effect of high frequency verbs in the right context, such that intervening adverbs between don't and the high frequency verb, as in I don't even know him, attenuate the faster pronunciation times of don't. Let us further assume that an intervening adverb does not change the pronunciation times in any way when no high frequency verb is present in the right context. This setup yields an interaction effect: adverbs slow down pronunciation times, but only if a high frequency verb is present.

The R code below first creates a data frame with our three independent variables. We add another column to the data frame in which we identify all examples with a high frequency verb and an intervening adverb, essentially dummy-coding the interaction. We fix the pronunciation length measurements in such a way that the effects for pronouns and high frequency verbs remain the same as above, but we selectively slow down examples that have both a high frequency verb and an intervening adverb. Let us take a first pass at analyzing the data in a way that is, strictly speaking, inappropriate. The R code below specifies a regression model that includes the new independent variable, alongside the two other ones, as a main effect. This means that the model tests for an effect of adverbs across all other independent variables. As we will discuss below, main effects need to be distinguished from interaction effects. The table of coefficients indicates that there is a significant effect of adverbs, which are estimated to slow down pronunciation times by about 27 ms. While this is true as an average, it does not capture the fact that adverbs selectively slow down examples that have a high-frequency verb in the right context. In order to bring this to light, we need to specify what is known as an interaction term in the regression model. This is done in the R code below. In the formula that specifies the regression model, the asterisk between the variable of high frequency verb and the variable of adverb indicates that we are testing for the main effects of these variables as well as for a possible interaction between them. The last line in the table of coefficients, which links both variables with a colon, represents that interaction.

# A fifth regression model, including an interaction term linreg.mod5

Binary Logistic Regression

Creating an Artificial Dataset for a Binary Logistic Regression

Let us assume that we conduct a corpus-based analysis of how speakers address each other in conversation. Languages such as French or German make a grammatical distinction between second person pronouns that are intimate and second person pronouns that are more distant and polite. By way of abbreviation, these pronouns are often referred to as T-forms (French tu) and V-forms (French vous). Our research question is whether and how variation between these forms is governed by conditioning factors. Below, we create a fictitious dataset of 1000 examples. The dataset features four independent variables. The first independent variable concerns the type of conversation from which the examples are drawn. We classify these into formal and casual. The second independent variable is speaker gender. The third one is the relation between speaker and hearer. Here, we distinguish conversations between friends and conversations between strangers. The last independent variable is speaker age.

The R code below specifies different biases for these variables. We set the log odds of examples from formal conversations to 0.6, which means that the odds of the V-variant in formal conversations are about 82% higher than the odds of the Vform in casual conversations (exp(0.6) ≈ 1.82). The log odds for examples produced by male speakers are adjusted by -0.2, so that the odds of the V-form in maleproduced speech become about 82% of the odds of the V-form in female-produced speech (exp(-0.2) ≈ 0.82). For the independent variable of relation, we adjust the log odds of speech towards friends to -2, so that the odds are only 13% of the odds of a V-form in conversations with strangers. For the continuous independent variable age, we tweak the log odds by adding 0.01 for each additional year of age. This means that a one-year difference in age will increase the odds of a speaker choosing the V-form by about 1% (exp(0.01) ≈ 1.01). On the basis of the log odds, we calculate for each example the probability that the outcome of the dependent variable is a V-form. We then create a column for the dependent variable, i.e. the pronouns that our fictitious speakers use. Based on the probabilities of a V-form, we take random samples from a binomial distribution to assign to each example a value of either a V-form or a T-form.

The output shares several components with the outputs that were discussed in the previous section. It shows the formula of the regression equation, an overview of the distribution of the residuals, and finally a table with the coefficients. The coefficients indicate that three of the four effects that we manufactured into the data were picked up by the logistic regression model. There are significant effects of formality, gender, and relation, such that formal texts show a bias towards V-forms, male speakers have a bias against V-forms, and a conversation between strangers comes with a strong bias towards V-forms. Age does make speakers gravitate towards the V-form, but not significantly so. The coefficients of the model differ somewhat from the values that we specified above, which is due to our sampling from the binomial distribution.

Visualizing the Effects of a Binomial Logistic Regression

In order to understand the regression model in more detail, it is useful to visualize the effects that are found. The R code below installs and loads a package that allows the creation of effect plots for binary logistic regression models. Figure

# Install the "effects" package install.packages("effects") library(effects) # Create effect plots plot(allEffects(logreg.model))

The panels of Fig.

Manufacturing and Visualizing an Interaction Effect

Like (multiple) linear regression, binary logistic regression allows the researcher to test for interactions between the independent variables. A possible effect of this kind with regard to the choice between T-forms and V-forms could be that the independent variables of formality and age interact, so that younger speakers are not very sensitive to the difference between formal and informal situations, whereas older speakers are more tradition-minded and hence more sensitive. The R code below creates a dataset that emulates this phenomenon, increasing the effect of formality by 5% for each additional year of age. The code also runs a regression model that includes the relevant interaction term, and visualizes the interaction effect. The effect plot in Fig.

Reporting the Results of Regression Analyses

When writing up the results of regression analyses, the central element is the table of coefficients, which should include the coefficient estimates, standard errors, and significance values for each coefficient as well as the intercept. The table should be accompanied by further information about the regression model. For the results of a linear regression, the write-up needs to include the R 2 -value, the F-statistic, the degrees of freedom, and a global p-value of the regression model. Confidence intervals for the coefficients are a useful addition. For the results of a binary logistic regression, the write-up should provide goodness-of-fit statistics such as the concordance index C or Nagelkerke's R 2 (cf.

Further Reading

Mixed-Effects Regression Modeling

Roland Schäfer

Abstract In this chapter, mixed-effects regression modeling is introduced, mostly using alternation modeling as an example. It is one option to deal with cases where observations vary by groups (such as speakers, registers, lemmas) by introducing so-called random effects into the model specification. It is stressed that using a categorical variable as a random effect is just an alternative to using it as a normal fixed effect in a Generalised Linear Model (GLM) as introduced in Chap. 21, but that the two options have different mathematical advantages and disadvantages. Simple random intercepts are introduced, which capture per-group tendencies. However, random slopes (for situations where fixed effects vary per group) and multilevel models (for situations where group-wise tendencies can be predicted from other variables, for example when lemma frequency is useful to predict lemmaspecific tendencies) are also introduced. Criteria for including random effects in models and for evaluating the model fit (for example through pseudo-coefficients of determination) are discussed. The demonstration in R uses the popular lme4 package.

Introduction

Mixed effects modeling is an extension of (generalised) linear modeling, of which logistic regression (see Chap. 21) is an instance. A common characterisation of mixed-effects modeling is that it accounts for situations where observations are "clustered" or "come in groups". In corpus linguistics, there could be clusters of observations defined by individual speakers, registers, genres, modes, lemmas, etc.

R. Schäfer

Instead of estimating coefficients for each level of such a grouping factor (so-called "fixed effects"), in a mixed model it can alternatively be modeled as a normally distributed random variable (a so-called "random effect") with predictions of groupwise tendencies being made for each group. This chapter introduces readers to the situations where mixed-effects modeling is useful or necessary. The proper specification of models is discussed, as well as some model diagnostics and ways of interpreting the output. Readers are assumed to be familiar with the concepts covered in Chap. 21.

Fundamentals

When Are Random Effects Useful?

In (Generalised) Linear Mixed Models (GLMMs) -or, more generally speaking, "multilevel models" or "hierarchical models" (see

If this is the case and the grouping factor is not included in the model, the error terms within the groups will be correlated. Put simply, the error terms would have a certain tendency per group. We would find group-wise tendencies, for example, if sentence length were modeled (for example using the types of constructions or lexemes occurring in it) and individual speakers represented in the data had tendencies to form longer or shorter sentences. If the speaker variable were not included as a predictor in the model under such conditions, the model would make predictions averaged over all speakers, and for short-sentence speakers, these predictions would tend to be too high, whereas for long-sentence speakers, they would be too low. However, the algorithms used for estimating the parameters of GLMs (so-called estimators) work under the assumption of non-correlated errors, an assumption often called "independence (of errors)". Thus, the prediction errors should not have group-wise tendencies as described above.

If the assumption of independence does not hold, standard errors for model coefficients will typically be estimated as smaller than they nominally are, leading to increased Type I error rates in inferences about the coefficients. This gets even worse when there are within-group tendencies regarding the direction and strength of the influence of the other regressors, i.e., when there is an interaction between them and the grouping factor (e.g.,

The crucial question in specifying models is not whether to include these grouping factors at all, but rather whether to include them as fixed effects or as random effects. Random effect structures are very suitable for accounting for group-level variation in regression, but formulaic recommendations such as "Always include random effects for speaker and genre!" are inappropriate. The choice between fixed and random effects should be made based on an analysis and understanding of the data set at hand and the differences and similarities in the resulting models. The remainder of Sect. 22.2.1 introduces three important points to consider about the structure of data sets in this context. Then, Sect. 22.2.2 provides a moderately technical introduction to the actual modeling.

Crossed and Nested Effects

This section discusses a distinction that arises when there is more than one grouping factor. When this is the case, each pair of grouping factors can be "nested" or "crossed". By way of example, we can group exemplars (such as sentences) by the individual speakers who wrote or uttered them, and we can group speakers by their region of birth. Such a data set would intrinsically be "nested", as Table

Hierarchical/Multilevel Modeling

This section describes the types of data structures which require the use of multilevel models, which represent a generalisation of the simple mixed effects models discussed so far. Such data structures are similar or in some special cases identical to nested data structures. Let us assume that we wanted to account for lexeme-specific variation in a study on an alternation phenomenon such as the dative alternation in English by specifying the lexeme as a random effect in the model. Additionally, we entertain the hypothesis that a lexeme's overall frequency influences its preferences for occurring in the construction alternants. A similar situation would arise in a study of learner corpus data (even of the same alternation phenomenon) with a learner grouping factor if we also knew that the number of years learners have learned a language influences their performance with regard to a specific phenomenon. In such cases, variables like the frequency and the number of learning years are constant for each level of the grouping factor (lexeme and learner, respectively). In other words, each lexeme has exactly one overall frequency, and each learner has had a fixed number of years of learning the language.

Such variables are thus interpretable only at the group level. Table

Random Slopes as Interactions

This section introduces the data patterns that give rise to "varying intercepts" and "varying slopes". Varying intercepts are an adequate modeling tool when the overall tendency in the outcome variable changes with the levels of the grouping factor.

We assume that we are looking at an alternation phenomenon like the dative alternation, wherein we are interested in the probability that, under given circumstances, the dative shift construction is chosen. In the examination of the data, it turns out that the probability of the dative shift changes for old and new dative NPs. The verb lemma also influences the probability of either variant being used. The situation can now be as in the left or the right panel of Fig.

Model Specification and Modeling Assumptions

In this section, it is discussed how the specification of mixed models differs from that of fixed effects models, and that for each model with random effects there is an alternative model with only fixed effects. A major focus is on the question of when to use fixed and random effects. The amount of technicality and notation is kept at the absolute minimum. Particularly, the specification of models in mathematical notation is not always shown, and models are introduced in R notation. For an appropriate understanding of model specification, readers should consult a more in-depth text book, for example Part 2A of

Simple Random Intercepts

Readers with experience in fixed effects modeling should be able to see that a grouping factor encoding the verb lemma and all the other potential grouping factors discussed in the previous sections could be specified as normal fixed effects in a GLM. This section introduces the main difference between the fixed-effect approach and the random-effect approach. Logistic regression examples are used throughout this section, and we begin with the fictional corpus study of the dative alternation introduced in Sects.

In any type of GL(M)M, the additive linear term consists of a number of sub-terms which are simply added up. Each of these sub-terms (except for the intercepts) consists of the multiplication of the (estimated) coefficient with an observed value of one of the variables. However, R notation for model formulae simplifies the specification of the actual linear term. First of all, the 1 in Construction 1+Lemma+Givenness is R's way of encoding the fact that an intercept is part of the model. An intercept is a constant sub-term to which all other sub-terms are added, and it can be seen as the reference value when all other sub-terms (corresponding to categorical or numeric regressors) assume 0 as their value.

For binary regressors like GIVENNESS, the only coefficient that is estimated directly encodes the value added to (in case of a positive coefficient) or subtracted from (in case of a negative coefficient) the linear term when the value of the regressor is 1 (in the example, when the referent is given). When the value of the regressor is 0 (for example, when the referent is not given), 0 is added to the intercept. The intercept thus encodes (among other things) something like a default for a binary regressor. If the default corresponds to, as in the example, nongivenness, phrases like "non-givenness is on the intercept" or "'givenness equals zero' is on the intercept" are often used.

However, a grouping factor such as LEMMA is usually a categorical variable with more than two levels. In such a case, the m levels of the grouping factor are "dummy-coded", and for all but one of these binary dummy variables, a coefficient is estimated. Dummy coding is a way of encoding a categorical variable as a R. Schäfer distributed around 0. This and nothing else is the conceptual difference between a fixed effect and a random effect.

glmer(Construction~1+Givenness+(1|Lemma), data=alternation.data, family=binomial(link=logit))

The sub-term GIVENNESS remains the same as in the GLM specification above, and it is still treated as a fixed effect. The sub-term (1|LEMMA) encodes that an intercept will be added to the linear term depending on which lemma is observed. This is obvious in mathematical notation corresponding to the above R code as shown in (22.2). In addition to the overall intercept α 0 , there is another constant term α

Crucially, instead of estimating a batch of m -1 coefficients for the levels of the grouping variable, a varying intercept (assumed to come from a normal distribution) is predicted for each of its m levels. Technically, the varying intercepts are predicted from their own linear model. All more complex model structures to be discussed below are extensions of this approach.

Choosing Between Random and Fixed Effects

One commonly given reason to use a random effect instead of a fixed effect is that "the researcher is not interested in the individual levels of the random effect" (or variations thereof). Such recommendations should be taken with a grain of salt.

Second, the random intercepts can be understood as a compromise between fitting separate models for each level of the grouping factor ("no pooling") and fitting a model while ignoring the grouping factor altogether ("complete pooling"), see

If the number of levels is reasonably large, the next thing to consider is the number of observations per level. Alternatives to using a random effect would be to estimate a separate model for each level of the grouping factor, or to include it as a fixed effect. In both cases the effects are not treated as random variables, and fixed coefficients per group are estimated without taking the between-group variance into account. With a random effect, however, the conditional modes/ means are pulled ("shrunken") towards the overall intercept ("shrinkage"). When the number of observations in a group is low, the conditional mode/mean is simply shrunken more strongly towards 0, predicting only a small deviation from the overall tendency.

Model Quality

Significance

It is not adequate to do any kind of significance testing on the individual levels of the random effect because they are not estimates in the conceptual and technical sense.

Relevance of the Random Effects

A related question is whether the inclusion of the random effect improves the model quality. It is recommended here to include all random effects as required by the design of the study (after having decided based on the criteria given in Sect. 22.2.2.2 whether they should be random rather than fixed). Only if they clearly (and beyond doubt) have no effect, practitioners should consider removing them. To check whether they have an effect, the estimated between-group variance is the first thing to look at. If it is close to 0, there is most likely not much going on between groups, or there simply was not enough data to estimate the variance. In LMMs, it is possible to compare the residual (item-level) variance with the betweengroup variance to see which one is larger, and to which degree. If, for example, the residual variance is 0.2 and the between-group variance is 0.8, then we can say that the between-group variance is four times larger than the residual variance, which would indicate that the random effect has a considerable impact on the response. This comparison is impossible in GLMMs because their (several types of) residuals do not have the same straightforward interpretation as those of LMMs.

Furthermore, models can be compared using likelihood ratio (LR) tests. In such tests, a model including the random effect and a model not including it are compared, similar to LR tests for the comparison of fixed effects. Such pairs of models, where one is strictly a simplification of the other, are called "nested models" (not to be confused with "nested effects" discussed in Sect. 22.2.1.1). A sometimes more robust alternative to the ordinary LR test are parametric bootstrap tests.

With all this, it should be kept in mind that it is never appropriate to make formal comparisons between a GLMM with a random effect and a GLM with the same factor as a fixed effect using any test or metric (including so-called information criteria such as Akaike's or Bayes'). Informally comparing coefficients of determination (R 2 ) between such pairs of models is somewhat useful, as will be shown below.

Quality of the Fit

To measure how well a GLMM fits the data, any metric that is based on prediction accuracy can be used in the same way as with GLMs. For example, the rate of correct predictions on the data used for model estimation or cross-validation methods are appropriate.

Coefficients of determination (pseudo-R 2 ) can be used to give some idea of the overall model fit. For GLMMs,

More Complex Models

Varying Intercepts and Slopes In Sect. 22.2.1.3, it was shown under which conditions a varying-intercept and varying-slope (VIVS) model might be useful. While it is possible to have just a varying slope, this is rarely useful, and we discuss only VIVS models.

A random slope is a good choice when the strength or direction of some fixed effect varies by group. We extend the simple model from Sect. 22.2.2.1 to include random slopes for GIVENNESS varying by LEMMA using R notation. Each variable R. Schäfer from the fixed effects part of the formula which we expect to vary by LEMMA is simply repeated before the | symbol.

glmer(Construction~1+Givenness+(1+Givenness|Lemma), data=alternation.data, family=binomial(link=logit))

With this model specification, a fixed coefficient for GIVENNESS will still be estimated. However, an additional value will be predicted for each lemma, and this value has to be added to the fixed coefficient. In mathematical notation, this is very transparent, as shown in

A source of problems in VIVS models is the fact that, in addition to the variance in the intercepts and slopes, the covariance between them has to be estimated. If in groups with a higher-than-average intercept, the slope is also higher than average, they are positively correlated, and vice versa. These relations are captured in the covariance. Technically speaking, the joint distribution of the random intercepts and the random slopes is assumed to follow a multivariate normal distribution with means, variances, and covariances to be estimated. The number of variance parameters to be estimated thus obviously increases with more complex model specifications, and the estimation of the parameters in the presence of complex variance-covariance matrices requires considerably more data than estimating a single variance parameter. The estimator algorithm might terminate, but typically covariance estimates of -1 or 1 indicate that the data was too sparse for a successful estimation of the parameter. In this case, the model is "overparametrised" and needs to be simplified (see

Nested and Crossed Random Effects

The difference between nested and crossed random effects is only defined when there are two or more random effects. As it was explained in Sect. 22.2.1.1, nested random effects are appropriate tools when the levels of a grouping factor are nested within the levels of another grouping factor. Technically, while varying slopes can be understood as interactions between a fixed and a random effect, nested random intercepts can be understood as interactions between two or more random effects. Crossed random effects are just several unrelated random effects.

In the model specification, there is no difference between a crossed and a nested random effect. Both are specified like independent random effects. The following code provides an example in R notation. SEMANTICS could be a factor encoding the semantic classes which nest individual lemmas. It could also be a (crossed) grouping factor completely unrelated to the lemmas, for example encoding some semantic property of the whole sentence containing the construction. As was mentioned in Sect. 22.2.1.1, the question on the practitioner's side is rather how the data are organised. If the data have a nested structure, the estimator will treat them as nested, otherwise as crossed. Data have a nested structure whenever each level of a (nesting) random effect can always be determined entirely from the levels of another (nested) factor.

Second-Level Predictors

In Sect. 22.2.1.2, situations were introduced where the random effects themselves can be partially predicted from second-level fixed-effects. In R notation, the true model structure is entirely blurred, and practitioners even run the risk of working with second-level predictors without realising it.

We add a numeric second-level fixed effect which specifies the token frequency for each level of LEMMA in the following R code. This is the only way to specify second-level predictors in standard R notation. The data set has to be organised as shown in Table

Method

Two mixed effects logistic regression models are estimated. For the genitive alternation, the text ID and the possessor head lemma are used as crossed random effects. The authors state on p. 399 that they collapsed all head noun lemmas with less than four occurrences into one category because otherwise "difficulties" would arise. However, it is the advantage of random effects modeling that it can deal with a situation where categories have low numbers of observations (see shrinkage, Sect. 22.2.2.2). For the dative alternation, the model includes the text ID, the register (which nests the text ID) as well as the lemma of the theme argument and the verb.

Results

It is found that many factors have a shared importance in both alternations, e.g., definiteness, animacy, construction length. It is also argued that the observed tendencies -such as short-before-long and animate referents firstare in line with synchronic corpus-based and experimental findings about general cognitive principles underlying the framework of probabilistic grammar. These principles remain in effect, but the strength of their influence changes over time.

Research questions

The paper is programmatic in nature. The author re-analyses data from a previously published study on verb particle placement in English. He uses a GLMM instead of a fixed-effects logistic regression to show that including random effects in order to account for variation related to mode, register, and subregister increases the quality and predictive power of the model. He also argues that by not doing so, corpus linguists risk violating fundamental assumptions about the independence of the error terms in models.

Data

The data are 2,321 instances of particle verbs showing either verb-direct object-particle or verb-particle-direct object order, taken from the ICE-GB.

The grouping factors derived from the structure of the corpus are mode (only two levels), register (five levels), and subregister (13 levels). They are nested: mode nests register, which nests subregister. Additionally, verb and particle lemma grouping factors are annotated. Finally, two fixed effects candidates are annotated (the type of the head of the direct object and the logarithmised length of the direct object in words).

Method

The author uses the model selection protocol described in

Results

Gries finds that the verb and particle lemma as well as the subregister play significant roles. The variance estimate for mode is close to 0 from the beginning of the model selection procedure. This is not surprising, as two levels are not nearly enough in order for the variance to be reliably estimated, and it should be used as a second-level predictor instead. The R 2 values of the final model are high, with a large difference between marginal R 2 = 0.57 and conditional R 2 = 0.748, which indicates that the random effects improve the (continued) R. Schäfer model fit. It is also shown that the classification accuracy is improved over that of a GLM without random effects, but differently for different lexical groups and subregisters. The paper thus shows that it is not appropriate to ignore lexical grouping factors and grouping factors derived from the corpus structure, especially as both are easy to annotate automatically.

Practical Guide with R

Specifying Models Using lme4 in R

This section focuses on lme4, an often used package to do multilevel modeling in R with maximum likelihood methods

Overview of the Data Set

The data used here for illustration purposes comes from

(4) a. Wir we trinken drink

[[ein a

Glas] Acc glass

[guten good

Weins] Gen ] Acc . wine

We drink a glass of good wine.

b. Wir trinken [[ein Glas] Acc [guten Wein] Acc ] Acc .

The influencing first-level factors derived from theory-driven analysis and previous accounts comprise the numeric stylistic indicator variables Badness (a measure of document quality available for all DECOW documents; see

For Kindlemma, there are: Kindfreq (numeric, z-transformed), which encodes the lemma frequency; Kindgender (binary), which encodes the grammatical gender of the kind noun; Kindattraction (numeric, z-transformed), which encodes the influence of neighbouring constructions. For Measurelemma, there are: Measurefreq and Measureattraction, which correspond to the similarly named variables for Kindlemma; Measureclass (five-level categorical), which encodes the broad semantic class of the measure noun. Finally, the dependent variable Construction is coded as 1 if the genitive is used and as 0 if there is case identity.

A Simple Varying Intercept Instead of a Fixed Effect

Fitting and Evaluating the Model First, it is shown how a grouping factor can be specified as a fixed or a random effect.

The following is the standard glm() call to estimate a model with the measure lemma (150 levels) as a fixed effect. For illustration purposes, not all available regressors are used here. The output of the summary(glm.01) command (not shown here) shows that the estimates for the 149 fixed effects corresponding to Measurelemma have extremely high standard errors and are virtually unusable. The Nagelkerke coefficient of determination for this model can be calculated using the NagelkerkeR2 (glm.01) function from the fmsb package, and it is 0.397. As the above example shows, grouping factors with many levels like Measurelemma are usually not suitable fixed effects. Hence, the following specification re-estimates the model as a GLMM using the glmer function with Measurelemma as a varying intercept. The glmer function is the standard function from the lme4 package for estimating mixed models. The number of groups for Measurelemma is correctly given as 150, and the variance in the random intercepts is 1.252.

For the first command, the output (95% confidence interval) is 0.887 and 1.414. Without applying formal significance testing, this is a reasonably narrow interval, and it does not extend to 0. Thus, the effect should remain part of the model specification. Beyond this, practitioners should not do model selection for random effects.

Single conditional modes (see Sect. 22.2.2.2) for the levels of the grouping factor can be extracted using the ranef command. The following command stores a list of conditional modes for Measurelemma in glmm.01.ranef. These can be used to construct prediction intervals around the predicted conditional modes in order to display them in tabular form or plot them. While some readymade functions exist to plot them, it is good to have a custom plotting function.

If the random effect has many levels, it might only be possible to plot a selection (random or informed) of the conditional modes, and there is no ready-made function which supports this. The R script accompanying this chapter contains a maximally simple example using only standard plotting functions which creates a plot with dots representing the estimate (measured on the y axis) and the prediction intervals as horizontal bars around the plot for a random subset of the conditional modes.

An example is given in Fig.

Turning to the quality of the overall model fit, Nakagawa & Schielzeth's coefficients of determination can be calculated with the r.squaredGLMM(glmm.01) command (from the MuMIn package). The output is as follows. This informs the user that the fixed effects cumulatively account for a proportion of 0.200 of the total variance in the data. Taking also the random effect into account, the model explains a proportion of 0.421 of the total variance. The random effect thus appears to be relevant. Comparing the conditional R 2 to the Nagelkerke R 2 of the GLM with Measurelemma as a fixed effect (which was 0.397), we see that the difference is not substantial, although the individual coefficient estimates in the GLM were unreliable.

no such de-facto standard as to how random effects should be reported. Everything that should be reported for a GLM should also be reported for a GLMM, such as the coefficient table for the fixed effects (which should at least contain the coefficient estimate, the standard error, and possibly bootstrapped confidence intervals) and variance inflation factors

In

A multilevel logistic regression model was fit which models the influence of the regressors on the probability that the genitive is chosen over case identity. The measure lemma and the kind noun lemma were specified as varying-intercept random effects. The sample size was n=5,063 with 1,134 cases with the genitive and 3,929 cases with case identity.

The intercept estimated at -3.548 comprises CARDINAL=YES, MEASURECASE=NOM, KINDGENDER=MASC, MEASURECLASS=PHYSICAL, and 0 for all numeric z-transformed regressors. The coefficient table is shown in Table

Finally, in order to illustrate the interpretation of the conditional modes and the fixed effects coefficients in such a model, there is code in the accompanying script which extracts all relevant values and calculates a predicted value for item 99 (arbitrarily chosen for illustration purposes) from the measure data set. For example, the overall intercept can be extracted via the following command. To this intercept, the sub-terms for first-level fixed effects are added. They can be calculated as follows, using Badness as an example. The result is 0.0183. In other words, we extract the fixed-effect coefficient estimate for Badness and multiply it with the Badness value observed for item 99.

In order to calculate the contribution of the second-level effects, which will be added to the overall intercept and the first-level fixed-effects sub-terms, we first need to extract the appropriate group-level intercept. The following code extracts the Kindlemma random intercept for item 99, which is -0.159 for the lemma Wasser 'water'. ranef(glmm.03)$

To these group-level intercepts, the second-level fixed-effects sub-terms are added, and they can be calculated very much like their first-level equivalents. For example, the following code calculates the sub-term for Kindfreq, which is -0.044 (the z-transformed logarithmised frequency per one million tokens of Wasser) in this case.

Introduction

The generalized additive model (GAM) offers the analyst an outstanding regression tool for understanding the quantitative structure of language data. Generalized additive models were first introduced in a monograph by

Within linguistics, GAMs have been found useful in dialectometry and sociolinguistics

Fundamentals

In an ordinary least squares regression model, a response y i is modeled as a weighted sum of p predictors and an error term that follows a normal distribution with zero mean.

Although the linear predictor η i ,

may provide an adequate model for the functional relation between a response and its predictors, there are many cases in which the assumption of linearity is inadequate. Reaction times in lexical decision task, for instance, tend to decrease in a non-linear way as a function of words' frequency of occurrence in corpora. Modeling a non-linear response function as if it were linear not only results in inaccurate predictions, but also in structured errors that depart from the modeling assumptions about the relation between the variance and the mean. For Gaussian models, for instance, the errors may show heteroskedasticity, and when this happens, the validity of significances reported by the linear model is no longer assured and p-values listed in model summaries will be unreliable. Consider, by way of example, Fig.

ggplot(polish, aes(LogFrequencyVerb, AcceptabilityRating))+ geom_smooth() # left panel of Fig.

Consequently, the goal of this chapter is to provide the reader with sufficient background to be able to understand the GAMs presented in these studies, to start exploring working with GAMs oneself, and to evaluate whether GAMs have been used appropriately. What this chapter does not aim to do is provide analysis guidelines for reporting research results. Interpretation of models presented in this chapter requires a detailed understanding of the model, its implementation and a careful assessment of how both interact with the data set at hand. In what follows, we begin with recapitulating the basic concepts of the generalized linear model. Next, we introduce key concepts underlying the generalized additive model. We then present a worked example of how GAMs can be used to obtain a thorough understanding of the quantitative structure of linguistic data.

The Generalized Linear Model

Central to the generalized linear model is the idea that a response variable Y i for a datapoint i that is described by p predictors x 1 , x 2 , . . . , x p is a random variable. For real-valued response variables, we assume that the probability Pr(Y i = y i |x i1 , x i2 , . . . , x ip ) follows a normal distribution with variance σ 2 and mean η i :

where the linear predictor η i is given by an intercept β 0 and a weighted sum of the p predictor values:

The means μ i = η i are linear functions of x (see the left panel of Fig.

For count data, a Poisson model is often used, with the same linear predictor η i :

Thus, the logarithm of the observed count is linear in the predictors. In this way, we ensure that predicted counts can never be negative. As can be seen in the center panel of Fig.

When the response variable is binary (as for successes versus failures, or correct versus incorrect responses), we are interested in the probability of a success, which we model as a binomial random variable with a single trial and a probability of success e η i /(1 + e η i ), i.e.,

where the linear predictor η i again is defined exactly as above. In this case, the log odds (i.e., the logarithm of the ratio of successes to failures) is linear in the predictors. As can be seen in the right panel of Fig.

specifies a parabola rather than a straight line. In fact, very wiggly curves can be obtained by adding multiple powers of x as predictors. This is illustrated in Fig.

Instead of writing

The Generalized Additive Model

The generalized additive model takes the linear predictor η i of the generalized linear model and enriches it with functions of one or more predictors, as, for instance:

The parametric part is familiar from the generalized linear model. The nonparametric part specifies two functions, one of which takes x 2 as argument, and one of which takes two predictors, x 3 and x 4 , as arguments. Instead of using polynomial functions, GAMs use smoothing splines for functions such as f 1 and f 2 .

A smoothing spline with one predictor as argument is used for fitting wiggly curves. A smoothing spline with two predictors can be used for fitting a wiggly surface. Splines can take more than two arguments, in which case wiggly hypersurfaces are modeled. Given a linear predictor with appropriate smooths, this linear predictor can be used to model Gaussian response variables, or Poisson or binomial responses.

GAMs can also accommodate ordinal responses as well as multinomial responses.

In order to use GAMs appropriately, it is useful to have a general understanding of how smoothing splines work. Here, we illustrate one particular spline that is the default of the mgcv package

The lower right panel of Fig.

The first two basis functions (in the upper left) are straight lines that are completely smooth. For predictors that have a strictly linear effect, these two basis functions suffice. Each basis function is associated with a weight (shown in square brackets on the vertical axes). For straight lines, the weight for the first basis function is the intercept, and the weight for the second basis function is the slope.

In this example, the predictor has a nonlinear effect, so we need more basis functions than just the first two. Figure

The exact mathematical form of the wiggly basis functions is determined by the total number of basis functions requested by the user. Below, we return to the practical question of how to choose the number of basis functions. Here, we proceed on the assumption that a sufficiently large number of basis functions has been requested by the user.

The question that arises at this point is how to avoid the situation in which we have so many basis functions that we edge too close to the datapoints and start fitting the model to noise rather than signal. In other words, we need to find a good balance between undersmoothing (resulting in too much wiggliness) and oversmoothing (resulting in missing out on significant wiggliness). The solution offered by GAMs is to balance two constraints, one constraint demanding that we want to stay faithful to the data (by minimising the summed of squared errors), and one constraint that demands we keep our model as simple as possible. This second constraint can be rephrased as a prior belief that the truth is likely to be less wiggly rather than very wiggly. This belief is a restatement of Occam's razor, in that we don't want to make our theory more complex (by adding basis functions and associated weights) than is absolutely necessary. The two opposing constraints lead to a model cost C f for a smooth f ,

The parameter λ regulates the balance between the desire to remain faithful to the data and the desire to keep wiggliness down. The introduction of the smoothing parameter λ raises a new question, namely, how to estimate λ. A first step towards a solution is to assume that the weights of the basis functions follow a normal distribution with mean zero and some unknown standard deviation σ s . It turns out that the choice of λ co-determines σ s . This leads to the second step, namely, to choose some λ, sample from the (normal) distribution of weights for the smooth implied by λ, and keep tuning λ until an optimal fit is obtained. This typically results in weights for a smooth that are smaller than if λ were zero, i.e., when there is no penalization for wiggliness and all that counts is faithfulness to the data. This method has been shown to also yield good estimates for confidence intervals

Importantly, penalization for wiggliness can reduce the weights of basis functions to zero, in which case the pertinent basis functions are apparently unnecessary. For instance, when a smooth is fitted to data for which the functional relation between the response and a predictor is truly linear, all the weights for the wiggly basis functions in Fig.

When the functional relation between response and a predictor is in fact nonlinear, penalization will retain at least some wiggly basis functions, but with weights that are reduced compared to an unpenalized smooth with λ = 0. The proportion of the original weight of a basis function that is retained after penalization is referred to as the effective degree of freedom (edf) of that basis function. The sum of the effective degrees of freedom of all basis functions used to construct a smooth constitutes the effective degrees of freedom of that smooth. Summary tables for the smooth terms in a GAM list these effective degrees of freedom, which enter into a special F -test that is used to evaluate the significance of a smooth.

The edf of a smooth cannot be larger than k, the number of basis functions that is set by the user. If the edf for a smooth is close to k, hardly any penalization has occurred, and it is likely that a larger value of k should be chosen (see the documentation of choose.k of mgcv for detailed discussion). When penalization leaves a predictor with 1 edf, its effect is likely to be linear: All wiggly basis functions will have been taken out of commission by setting their weights to zero, and only the weight for the second basis function is retained (i.e., the slope of the regression line).

GAMs also accommodate random effect factors as predictors. The way in which this is done is different from the method used by the linear mixed model as implemented in the lme4 package. The GAM implementation that we discuss here makes use of the mechanism of penalization to estimate the parameters of random-effect factors, using a so-called ridge penalty. This ridge penalty takes the sum of the absolute values of the random-effect coefficients, and seeks to keep this sum as small as possible. In this way, parameters are shrunk towards zero, just as in the linear mixed effect model. However, computation times for mixed GAMs (henceforth GAMMs) are typically longer than for the corresponding models fitted with lme4, which is due to GAMMs not making any simplifying assumptions about the structure of the random effects.

Proper inclusion of random effects in the model specification protects against overly wiggly curves. Recall that Fig.

Representative

Research questions

Divjak (2016) investigated the extent to which speaker's experience contributes to the acceptability ratings for infinitival and finite that-complements in Polish.

(continued)

Rater generosity (i.e., the extent to which a rater is prone to give high ratings) was included as a control variable.

Data

The data set contains off-line acceptability ratings for verbs that occur with low frequency in that-constructions. A total of 95 verbs in that-constructions was presented to 285 undergraduate students of English/German in Poland.

Participants were asked to rate "how Polish a sentence sounds" on a 5-point Likert scale. Each verb was responded to by 15 participants.

Method

Ratings elicited on a Likert scale yield ordinal data.

polish.gam = gam(AcceptabilityRating te(RankConstructionVerbReliance, LogFrequencyVerb, RaterGenerosity) + s(Verb, bs = "re") , data = polish, family = ocat(R = 5))

Results

Of the three models, the model with the three-way interaction, fitted with a tensor product smooth, provided the best fit to the ratings. Subjects with lower rater generosity showed stronger effects of frequency and reliance. Furthermore, ratings decreased for increasing frequency when reliance was low, and ratings increased only with reliance for high-frequency verbs (see the inset contour plot below). The GAM analysis succeeded in bringing together within one model a series of findings that Divjak (2016) could account for only in part with an ordinal linear model.

Practical Guide with R

The dataset that we use to illustrate how to work with GAMs is taken from the Buckeye corpus of conversational American English as spoken in Columbus, Ohio

This information was collected for all words, in the order in which they appear in the corpus, resulting in a table with 27062 observations, one for each diphone. For each of these 27062 diphones, we considered the following variables: DictDiphoneAbsent, with values TRUE or FALSE, depending on whether the dictionary diphone was realized by the speaker; this is the response variable for our analyses; DictDiphonePosition, an integer indicating the position of the dictionary diphone in the word; DictDiphoneCount, an integer with the number of dictionary diphones in the word; PhraseInitial, with values TRUE or FALSE, indicating whether the word carrying the diphone is phraseinitial; PhraseFinal, with values TRUE or FALSE, indicating whether the word carrying the diphone is phrase-final; PhraseLength, an integer indicating the length in words of the phrase; PhraseRate, the speech rate (number of syllables per second); LogDuration, the logarithm of the duration of the word (in seconds); DictDiphoneActDiversity, a measure, based on discriminative learning, gauging the lexical uncertainty caused by the diphone; WordActDiversity, a measure gauging the lexical uncertainty of the carrier word in a semantic vector space model derived with discriminative learning; SemanticTypicality, the extent to which the semantic vector of the carrier word is similar to the average semantic vector; and CorpusTime, the position of the diphone in the corpus (ranging from 1 to 27062) but scaled and centered to 575 make this variable commensurable with the other numeric predictors. A detailed description of these predictors is available in

A Main-Effects Model

We begin with fitting a standard logistic model in which the log odds is assumed to vary linearly with the numeric predictor variables. We use the bam function from the mgcv package ( With the exception of PhraseRate (p = 0.0704), all predictors receive good support (the model summary is available in the supplementary materials). The AIC for this baseline model is:

The assumption that a covariate has a strictly linear effect may be true, but it may also be unjustified. Often, exploratory data analysis will be required to establish whether, and for which variables, the linearity assumption is inappropriate. The following model relaxes the linearity assumption for all covariates using the s smoothing function from mgcv. The amount of wiggliness that a smooth allows for is controlled by the number of basis functions k, which has 10 as default value. This default is not motivated theoretically, and hence is a heuristic starting point. What is important is that k has to be set to an integer value (the 'dimension' of the smooth) that is large enough. How large an initial value of k should be depends on the number of different values of the predictor for which a spline is required. If there is only a handful of different values, one may want to set k to 3 or 4. If there are thousands of different values, a possible value would be 200.

For the numeric predictors in the present data, we proceed as follows. We have two counts with a limited range, DictDiphoneCount (7 distinct values) and DictDiphonePosition (8 distinct values). The dimension of a smooth should be lower than the number of distinct values, so we choose k = 5. For PhraseRate (1171 distinct values), LogDuration (5842 distinct values), PhraseLength (29 distinct values, we take the logarithm of this variable to reduce its rightward skew), DictDiphoneActDiversity (604 distinct values), WordActDiversity (825 distinct values), and SemanticTypicality (825 distinct values) we go with the default. CorpusTime, however, has no less than 27062 distinct values, and the default value of k therefore comes with the risk of oversmoothing. We therefore set k to 100. It is noteworthy that by allowing predictors to have nonlinear effects, we have obtained a substantially improved fit:

with a decrease in AIC of no less than 2216.7.

Figure

Panels 1 and 2 of Fig.

Unsurprisingly, the log odds of diphone deviation decreases with word duration (panel 5). As documented in detail by

We see here an important advantage of GAMs over models that force the effect of duration to be linear. In such models, outliers may exert high leverage on the regression, and typically have to be removed from the data set. By contrast, the 579 GAM clarifies that outliers behave differently, highlights the associated uncertainty with wide confidence intervals, and at the same time does not let the outliers influence conclusions about the effect of a predictor for the bulk of the data. In other words, GAMs provide the full picture, and protects the analyst against models based on flattened and simplified data.

The distribution of DictDiphoneActDiversity (panel 6) has a long tails. Here, we find an S-shaped curve. For the interquartile range (the center 50% of the data highlighted by the center vertical red lines), we observe an increase in the log odds with increasing activation diversity. The effect goes back to zero, however, for the first and third quartiles. Strongly undulating patterns are likewise visible for WordActDiversity (panel 7) and SemanticTypicality (panel 8).

The wiggliness of these curves is difficult to interpret theoretically. For cases such as these, the analyst has two options. The first option is to accept that these undulations are real, and that our theoretical understanding is too limited, or that our predictor is theoretically flawed. The second option is to reduce the dimension of the smooth. For the present data, such a reduction has some justification because of the abovementioned problem that lexical and phrasal variables are constant within words, which is a problem that often arises when working with observational data from corpora. Since data points are not independent in the way one would like them to be, some conservatism with respect to nonlinearity is justified. When the model is refit with k set to 5, the functional form of these effects becomes much simpler and easier to understand, as we shall see below (Fig.

The effect of Corpus Time in the lower right panel is quite wiggly, but as we are dealing with a predictor with 27062 distinct values, and as we have no apriori hypothesis about how deviation probabilities might change in the course of the interview, we accept the smooth as providing a description of real changes over time in diphone deviation behavior.

A Model with Interactions

Thus far, we have considered models with main effects only. In this section, we consider interactions involving numerical covariates. There are two basic types of interaction: an interaction of a covariate with a factor, and an interaction of two covariates. First consider the interaction of a numerical predictor with a factorial predictor such as PhraseFinal. PhraseFinal has two levels (TRUE/FALSE), and an interaction of PhraseFinal with a covariate, say SemanticTypicality, requests two smooths for this covariate, one for phrasefinal words and one for words that are not phrase-final. We request the two curves from the bam function with the by directive in the call to s: indicate that the effect of SemanticTypicality is stronger in phrase-final position. The effect of this variable appears to be present primarily across its fourth quartile. Panels 3 and 4 reveal an effect of DictDiphoneActDiversity that is U-shaped for the bulk of the data points. The largest effect is again present for diphones in words that are in phrase-final position. The downward swing for low activation diversity in the left of panel 4 appears due to a small number of outliers.

The bottom panels of Fig.

Here, te requests a tensor product smooth, which estimates a wiggly surface that we visualize with contour plots. In these contour plots, just as in geographical maps indicating terrain height, contour lines connect points with the same partial effect. There are two ways in which the contour map can be shown: one in which 1 SE confidence regions are added (panels 5 and 7), and one in which color coding is used to represent the magnitude of the partial effect (panels 6 and 8). In panels 5 and 7, dotted green lines are 1 SE up from their contour lines, and dashed red lines are 1 SE down. In panels 6 and 8, darker shades of blue indicate lower values, and darker shades of yellow, higher values. With the directive by=PhraseFinal, we requested two wiggly surfaces, one for diphones in words that are not phrase-final (panels 5 and 6), and one for diphones in phrase-final words (panels 7 and 8). In panels 6 and 8, contour lines are 0.2 units apart. Comparing the color shadings, it is clear that effects are much stronger in phrase-final position. Comparing panels 5 and 7, it is also clear that 1 SE confidence regions are considerably tighter in phrase-final position. Unlike panels 6 and 8, panels 5 and 7 are informative about where there is a significant gradient. In panel 5, for instance, confidence regions of adjacent contour lines begin to overlap for high phrase rates, indicating the absence of a significant effect.

For understanding contour plots, it can be useful to trace changes in the value of the response with imaginary lines that are parallel to the axes. For instance, for PhraseRate to have an effect, contour lines should be crossed when moving in parallel to the y-axis. For words that are not phrase final, this does not happen for low values of WordActDiversity. It is only for higher values of this activation measure that an effect becomes visible, with larger phrase rates indexing reduced log odds of diphone deviation. When we consider imaginary horizontal lines, we cross more contour lines for low phrase rates than for high phrase rates, indicating that there is a stronger gradient up for WordActDiversity when PhraseRate is relatively low. It is noteworthy that for phrase-final words, the effect of PhraseRate reverses, such that higher phrase rates predict increasing instead of decreasing log odds of diphone deviation. Table

Table

When this difference curve is added to the effect of SemanticTypicality for diphones in words that are not phrase-final (the reference level), the curve is obtained for its effect in phrase-final position. Consistent with the significant main effect for SemanticTypicality in Table

Random Effects in GAMs

It is straightforward to include random effects in generalized additive models fitted with mgcv. By-subject random intercepts are requested with s(subject, bs="re") (notation in lme4: (1|subject)). By-subject random slopes for a covariate are specified as s(covariate, subject, bs="re") (lme4: (0+covariate|subject)). For factors, s(factor, subject, bs="re") directs the model to estimate, for each subject, random sum contrasts (lme4: (1|factor:subject)). Hence, no separate term for by-subject random intercepts should be requested. The variance components of a GAMM and associated confidence intervals are obtained with gam.vcomp. Unlike lme4, mgcv does not offer tools for modeling correlation parameters for random effects.

For corpus data, a random effect factor such as Word can cause serious problems for the analyst. Recall that in the present dataset predictors at the word level are repeated in the dataset for each of a word's diphones. One might think that adding by-word random intercepts would alleviate this problem. Technically, we can add the model term s(Word, bs="re") to m4, resulting in a new model, m6 (not shown) that appears to provide an improved fit (for instance, AIC is down by 3470.3). However, of the 829 word types, 383 occur once only (46.2%). As a consequence, nearly half of the words have only one occurrence but are predicted by no less than three factorial variables: PhraseInitial, PhraseFinal, and a random intercept. In addition, there are several covariates that will further be specific to a given word, such as LogDuration and WordActDiversity. Thus, we have far too many predictors to one observation. As a consequence, model m6 is severely overspecified.

The adverse effects of this overspecification become apparent when we consider the concurvity of the model. Concurvity is a generalization of co-linearity, and causes similar problems of interpretation, in the sense that when concurvity is high, it is difficult to say which variables are driving the model's predictions. As when co-linearity is present, concurvity can also make estimates somewhat unstable. The concurvity function of mgcv provides several measures of concurvity, each of which is bounded between zero and one. Values close to or equal to 1 indicate there is a total lack of identifiability. The index we consider here, which Wood describes as in some cases potentially too optimistic, is based on the idea that a smooth can be decomposed into a part g shared with other predictors, and a part f that is entirely its own unique contribution. The greater part g is compared to part f, the greater the concurvity. The observed index of concurvity is based on the square of the ratio of the Euclidian lengths of vectors g and f evaluated at the observed values of the predictors.

When we extract this measure from the output of concurvity(m6), we obtain the concurvity values shown in Fig.

It is clear that m6 is an overspecified model that must be simplified. We therefore completely remove PhraseFinal and PhraseInitial from the model specification, as this will attenuate the adverse consequences of hapax legomena occurring with only one value for these predictors. After further simplification, model m7, with good support for all predictors, is obtained, the concurvity values of which are presented in Fig.

As George Box famously said, "all models are wrong, but some are useful"

One important new kind of random effect that mgcv makes available is the factor smooth. In a model with covariates with linear effects, it is possible that regression lines for individual subjects differ both with respect to their slopes and with respect to their intercepts. The nonlinear counterpart of this situation is that subjects have their own wiggly curves. Factor smooths implement such wiggly random effects. For example, s(CorpusTime, Speaker, bs = "fs", m = 1) requests wiggly curves for log odds as a function of CorpusTime for all speakers in the corpus. Figure

As is the case for random effects in the linear mixed model, the factor smooths are subject to shrinkage. Importantly, factor smooths are set up in such a way that if there is no wiggliness, horizontal straight lines are returned. In this case, the model has become a model with just straightforward random intercepts.

In the linear mixed model, a model with by-subject random intercepts as well as by-subject random slopes will provide population estimates for intercept and slope. In the case of factor smooths, it is possible to request both a general, speakerindependent smooth, together with by-speaker factor smooths. s(CorpusTime) + s(CorpusTime, Speaker, bs = "fs", m = 1)

In this case, mgcv issues a warning, as in general multiple smooths for the same covariate should be avoided. For this special case, however, this warning can be ignored (Simon Wood, p.c.). For large datasets, it should be kept in mind that estimating factor smooths for large numbers of speakers or words can be computationally very expensive.

It is both an empirical and a theoretical issue whether a separate smooth that is supposed to be common to all speakers, such as s(CorpusTime) in the above specification, is really necessary and makes sense. Is it theoretically justified to expect that when speakers go through a one-hour interview, there truly should be a common way in which the diphones they realize in their speech deviate from the standard language? If not, perhaps the main effect for CorpusTime should be removed.

It is noteworthy that the interpretation of the individual curves estimated by a factor smooth is different from that of random intercepts and slopes in the linear mixed model. The individual curves provide an estimate of how a given speaker went through her/his interview, but how the same speaker would behave in a replication interview is unlikely to be a variation of the same curve with greater or smaller amplitude. Instead, the expectation is that the speaker will show a similar amount of wiggliness, but with ups and downs at different moments in time.

Thus, GAMs not only offer the analyst new possibilities for understanding complex relations in large volumes of data, they also confront us with new

Extensions of GAMs

The toolkit of smoothing splines (see the documentation for smooth.terms for an overview of the many different splines that mgcv implements) is available for Gaussian models, as well as for Poisson and Binomial responses, using the family directive familiar from the generalized linear model (glm). GAMs allow for a more complex linear predictor η, but otherwise the linear predictor is used exactly as in the generalized linear model, as summarized in Sect. 23.2.1. When the residuals of a Gaussian model follow a t-distribution rather than a normal distribution, the family directive can be set to scat, which requests a scaled t model for the residuals. For multinomial logit models, the family directive is set to multinom, and for the modeling of ordinal response variables, family is set to ocat. The documentation for scat, multinom, and ocat provides further detail on these extensions of the generalized additive model and their use.

Further Reading

As model interpretation and model criticism with GAMMs require a high level of understanding of both the method and the theoretical concepts it builds on, it is advisable to engage in a deeper exploration of the issues at hand prior to applying the models in a productive research environment. Here, we provide an overview of key readings, that might come in handy to the analyst exploring various types of data and uncovering and addressing effects commonly found in human response data.

Wood, S. N. (2017). Generalized Additive Models. Chapman & Hall/CRC, New York.

This book is a standard reference on GAMs, provides a necessary background in linear models linear mixed models and generalized linear models and introduction to the theory and applications of GAMs, complemented by a wide range of exercises. The article illustrates on three data sets how human factors like within-experiment learning or fatigue may interact with predictors of interest, both factorial and metric, and demonstrate why fitting maximally complex models is not an advisable strategy, especially within the framework of the generalized additive mixed effects model. This paper offers a hands-on tutorial, including the original data and all R commands, for analysing dynamic time series data on the example of articulator trajectories observed using electromagnetic articulography. The paper leads the reader through the steps of data exploration, visualization, modeling of complex interactions and model criticism, introducing a wide variety of techniques and strategies with a detailed and comprehensive rationale for the modeling decisions, offering the reader an opportunity to replicate the analyses and gain more understanding about the material.

Chapter 24

Bootstrapping Techniques Jesse Egbert and Luke Plonsky

Abstract Bootstrapping is a statistical technique that relies on randomly sampling with replacement from a set of observed values. Bootstrapping makes it possible to measure the accuracy and reliability of sample estimates and is often recommended for small samples and samples with unknown or non-normal distributions. In corpus linguistics, bootstrapping has also been proposed as a method for quantifying the degree of homogeneity in a corpus sample, for validation of statistical results, and as a methodological step in random decision forests, an advanced classification method. However, to date bootstrapping techniques have seldom been used with corpus data. We argue in this chapter that bootstrapping is underused in corpus linguistics, and that quantitative corpus linguists would do well to add this tool to their repertoire. This chapter includes an introduction to the fundamentals-both conceptual and practical-of bootstrapping methods. We address several applications of bootstrapping, including the measurement of sample estimate accuracy, the validation of statistical models, the estimation of corpus homogeneity, and random forests. We include an overview of two representative studies that have successfully used bootstrapping techniques with corpus data. Finally, we demonstrate how to perform bootstrapping on corpus data using R, and how to visualize and interpret the results.

Introduction

Bootstrapping is a method of simulating a sampling distribution for a parameter of interest and estimating its accuracy. This is done through resampling with replacement from an observed data set (see Sect. 24.2.1). Bootstrapping can improveon our ability to measure the accuracy and reliability of sample estimates (e.g.

We argue in this chapter that bootstrapping is underused relative to its potential in corpus linguistics, and that quantitative corpus linguists would do well to add this tool to their repertoire. We begin this chapter by introducing the fundamentalsboth conceptual and practical-of bootstrapping methods. We then present an overview of several applications of bootstrapping that have been used in prior corpus linguistics research. For each of these methods, we describe the underlying assumptions that must be met, the types of data used, and a general overview of the methodological steps. We then include two study boxes that provide an overview of two representative studies that have successfully used bootstrapping techniques with corpus data. Next, we demonstrate how to perform bootstrapping on corpus data using R, and how to visualize and write about the results for publication. We conclude with a summary of the chapter and some comments on potential applications of bootstrapping in corpus linguistics that have yet to be attempted.

Fundamentals of Bootstrapping

Objectives and Methods

The overarching goal of statistical analysis is to estimate parameters (e.g. mean, standard deviation) of a population by measuring those parameters in a sample. In most cases, population parameters are unknowable. However, they can be estimated with a certain degree of confidence from an observed sample drawn from the population. In most cases, the observed data in a sample provides the best possible insight into the population parameters. If we can assume that a sample is representative of a given population, it is safe to assume that (a) values observed in the sample are possible in the population, and (b) the distribution of those values in the sample can be used to estimate their distribution in the population. Although we can get a parameter estimate from this sample, we have no way of measuring the accuracy of our estimate without knowing the sampling distribution. A sampling distribution is the theoretical probability distribution for a parameter of interest that accounts for the full range of samples of the same size that we could have drawn from the population. For example, we can calculate the mean rate of occurrence for nouns in a single corpus sample of size n. However, if this sample is biased in some way, as most samples are, then our estimated rate of occurrence (i.e. mean) for nouns will be different from the actual rate of nouns in the population. Assuming that the entire population of texts is inaccessible, our next best option for measuring the degree of bias in our current sample is to collect additional samples of size n from the population and measure the mean rate of nouns in those corpora. The mean rates for nouns drawn from every possible sample of size n that can be drawn from the population constitute the sampling distribution for this parameter. Since the actual value for our parameter depends on the population, which is unknown, we are unable to estimate the accuracy of our parameter estimate. Thus, sampling distributions can offer crucial insights (at least theoretically) into the nature of the population.

Moreover, we are often constrained in what we can learn about a population from a sample because of several limitations: small sample size, unknown or nonnormal distribution, the presence of statistical outliers, and the effects of potential model overfitting (see Sect. 24.2.2). While this seems to paint a dismal picture regarding the use of samples in inferential statistics, developments in statistical theory and computation have provided workable solutions for these challenges. One of these solutions is bootstrapping. Bootstrapping can be used for a variety of purposes, such as quantifying uncertainty, estimating statistical distributions, measuring homogeneity/heterogeneity across samples, validating statistical models, and protecting against statistical threats (e.g. model overfitting, sensitivity to extreme values). Explanations and examples of these applications of boostrapping in corpus linguistics are included in Sect. 24.2.2.

Bootstrapping is a statistical technique that relies on randomly resampling with replacement from a set of observed values in order to estimate the accuracy of statistical parameters. Bootstrapping relies on principles of frequentist statistical theory and computational power to simulate the sampling distribution of a statistical parameter

Both authors of this chapter have taught and presented on bootstrapping methods in numerous classrooms and conferences. In our experience, it seems to be a natural reaction for people to be skeptical when first introduced to bootstrapping. However, this skepticism is typically allayed once it is made clear what bootstrapping actually can and cannot do. To that end, we now turn to a discussion of what bootstrapping does not do, as well as what it does do. 1. Bootstrapping does not fix sampling problems. Observations about an unrepresentative sample are not generalizable to the target population, and bootstrapping cannot change that. 2. Bootstrapping does not increase the n size of a sample. Each bootstrapped sample has the same number of observations as the original sample. Thus, it cannot artificially increase statistical power (i.e. the probability of correctly rejecting a null hypothesis). 3. Bootstrapping does not automatically shrink the confidence interval for a sample.

Bootstrapped confidence intervals are more accurate, but not necessarily narrower. The width of a confidence interval is based on a sample's n size and the standard deviation for the parameter of interest, neither of which are changed during the process of bootstrapping.

To summarize these three points: bootstrapping cannot provide better data or more data from the population than what is contained in the sample. No researcher who understands the theory and methods of bootstrapping would claim that it does these things. Unfortunately, some of these misconceptions seem to persist. Despite its limitations, bootstrapping can provide a wealth of information about the distribution of a sample.

1. Bootstrapping does estimate the sampling distribution for statistical parameters by simulating the data sets that might be drawn from the population over many repeated samples (see, e.g., Chernick 1999, 7). 2. Bootstrapping does provide a reliable measure of accuracy for those parameters (i.e. more accurate confidence intervals)

To summarize, bootstrapping augments the amount and quality of information we can extract from the observed data in a sample.

Applications of Bootstrapping in Corpus Linguistics

Bootstrapping has been used for a wide range of applications in many scientific disciplines, and this list is growing all the time (see

Estimating Sampling Distributions

The most basic application of bootstrapping in corpus linguistics is that of simulating sampling distributions to measure the accuracy of parameter estimates (see, e.g.

Measuring Corpus Homogeneity

Validating Statistical Models

The bootstrap can also be used to validate statistical models. This application has been more widely used by corpus linguistics researchers than the previous two applications. Statistical models are prone to overfitting to the sample they are based on (i.e. the training data set)

Cross-validation relies on a test data set that is separate from the training data set. Once a statistical model has been trained using the training data set, it is applied to the test data set to assess how well it fits to data it was not trained on. There are a variety of methods for selecting the test and training data sets, but most of them require the researcher to divide the data set into two parts, usually a larger training set and a smaller test set. One advantage of this approach is that the researcher can test the statistical model on a new data set. However, this results in reduced sizes for the training and test data sets (for an in-depth discussion see, e.g.,

In bootstrap validation individual observations are bootstrapped from the original data set to assess the possibility of overfitting. The bootstrapped model, which is based on a large number of data sets that have been re-sampled with replacement from the original data set, is then compared with the original model to evaluate whether the effects are retained. This is often done by measuring optimism, or the magnitude of the differences between the parameters in the original model and the bootstrap model. Bootstrap validation has the advantage of allowing the researcher to use the entire data set for training. Additionally, extensive research has shown that bootstrap validation actually outperforms cross-validation (see, e.g.,

Bootstrap validation for corpus linguistic analysis is introduced and explained in

Random Forest Analysis

The final application of bootstrapping we discuss in this chapter is its use in random forest analysis, a machine learning method (see Chap. 25). There are three major steps in random forests: decision trees, tree bagging, and random forests. Decision trees are commonly used in machine learning as a method for modeling many different types of data. Decision trees are powerful due to their ability to model patterns that are irregular and nuanced. However, this characteristic makes them prone to overfitting their training sets

As with other applications of bootstrapping, random forest analysis makes it possible to distinguish between signal and noise in the data because extreme values are not likely to occur in as many of the bootstrap samples as more typical values. In addition to bootstrapping the decision trees, random forests sample a random subset of the original variables (e.g. linguistic features) at each stage in the learning process. This helps to mitigate the effect of bootstrapped decision trees that are strongly correlated simple because they are based on features that are extremely strong predictors of variability in the data set

Random forest analysis was not used on corpus data until relatively recently. While it is not found in a large number of studies, its use seems to be on the rise to answer questions in corpus-based variationist linguistics. We briefly mention three of these studies here.

Additional Applications of Bootstrapping

We believe that the applications of bootstrapping in corpus linguistics we have discussed in the previous four sections are just a beginning for corpus linguistics. There are many applications of the principles and methods of bootstrapping that need to be further explored by corpus researchers. Here we discuss two such applications.

Bootstrapping could be used as a method for evaluating the linguistic representativeness of a corpus (cf. Chap. 1). The method proposed by

Bootstrapping could also be used in (applied) corpus linguistics for the creation of vocabulary lists. The creation and use of vocabulary lists seems to be increasing at an accelerated pace (see, e.g.

As we mention above, the applications we propose here represent just two of the many potential applications of bootstrapping in future corpus-based research. Although the use of bootstrapping methods in corpus linguistics is still in its infancy, writing this book chapter has given us reason to be optimistic about the future of bootstrapping in corpus linguistics. What the field lacks in quantity of bootstrapping studies, it makes up for in quality. During the process of reviewing the corpus-based research that uses bootstrapping, we have seen no evidence of bootstrapping results being misinterpreted or misrepresented. So while these authors have ample anecdotal evidence to suggest that bootstrapping methods need to be better understood, generally, it seems that their application in corpus linguistics research has gotten off to a great start. The high quality of bootstrapping methods in corpus linguistic research may actually be due in part to the slow and cautious pace at which they have been adopted. It is far more important that corpus researchers apply and interpret bootstrapping appropriately, than that they use it a lot. Having said that, we hope to see both of these things happening in corpus linguistic research. Our hope is that this chapter helps to move the field in that direction.

Representative

Research questions

If one performs a corpus-linguistic quantitative analysis of phenomenon X in a corpus using the statistical parameter P, 1. From a descriptive perspective, which degree of variability of P was observed in one's data set and how do we quantify it? And, how do the present results concerning P compare to those of other studies? These questions inevitably lead to the next: 2. How homogeneous is the corpus that was used for the study of phenomenon X? 3. From an exploratory, bottom-up perspective, how can one identify (some of) the (most) relevant sources of the observed variability of P? If we rephrase that from a hypothesis-testing, top-down perspective: are the variables A, B, C, responsible for a significant proportion of P's variability?

Data

In order to address these questions, Gries used the International Corpus of English-Great Britain (ICE-GB), sub-divided into 13 sub-registers, within 5 registers, within 2 modes (see

Methods

From this corpus, he calculated the present perfect verbs (as a percentage of all verb forms) for each corpus file. From the list of percentages (one per file), Gries drew 50,000 random samples with replacement for each sub-register, matching the sample size (number of files) in the original sub-register. He then stored a vector (i.e. list) of 50,000 mean percentages of present perfect verbs for each sub-register, and performed a hierarchical cluster analysis (see Chap.18), where the 13 sub-registers are clustered based on similarities in their use of present perfect verbs across 50,000 bootstrapped samples.

(continued)

Results

Gries' results revealed one cluster that contained written, informational sub-registers (fewer present perfects), and a second cluster that contained interactive, less formal sub-registers (more present perfects). He determined that persuasive writing tied into Cluster 2, showing that it approximated conversational discourse, and that broadcast speech was an outlier. These results reveal that this method can be used to cluster data based on a single variable (e.g. present perfect verbs) at any level of granularity. The use of the bootstrap in this study provided a sampling distribution on which to base the cluster analysis and related interpretations. He showed that the single variable of present perfect verbs was able to identify register patterns that are strikingly similar to those found along two of

Representative

Research questions

1. Do the varieties of English we study here share a core probabilistic grammar? 2. Can ecology account for probabilistic similarity between varieties of English-for example, 3. Do we find a split between native and non-native varieties of English? Do the alternations under study differ in terms of their probabilistic sensitivity to variety effects?

Data

To answer these questions, they drew on data from the International Corpus of English (ICE), including the following four varieties: British English (ICE-GB), Canadian English (ICE-CAN), Indian English (ICE-IND), and Singapore English (ICE-SIN).

Methods

From each of these four sub-corpora, they extracted tokens of three alternating linguistic features, coded for particle placement (5414 interchangeable (continued) tokens), genitive (4701 interchangeable tokens), and dative (3958 interchangeable tokens). For each of the three features, the researchers modeled the data using conditional inference trees by recursively partitioning the data into smaller and smaller subsets according to those predictors that co-vary most strongly with the outcome. Informally, binary splits in the data are made by trying to maximize the homogeneity or "purity" of the data partitions with respect to the values of the outcome (e.g. all s-genitives vs. all of-genitives)"

They then applied random forest analysis, based on bootstrap-sampled trees (tree bagging) and random subsets of predictors (random forests) to further analyze the variation in the data set.

Results

The findings from this study revealed internal homogeneity within the four varieties and external heterogeneity between them. The directions of the effects are stable across English varieties, but the effect sizes are variable. While the researchers did find some evidence of a native/non-native divide in the use of the three features, they call for future research into this question. The results suggest that variety of English is one of the strongest predictors of variation in the use of all three features. However, this effect varies across feature, with variety having the strongest effect on particle placement and the weakest effect on genitive variation. The use of bootstrapping methods in this study made it possible for the researchers to identify the independent variables that had the strongest effect on the choice of particle placement, dative, and genitive forms.

Practical Guide with R

In this section we demonstrate how to bootstrap corpus-based data using R. We also provide a case study to illustrate how to interpret and report on key visuals and quantitative results. For case study, we use the corpus of Academic Written English (AWE), which contains balanced samples from three publication types (journal articles, university textbooks, popular science books) in two disciplines (biology, history) (see

Each text in the corpus was read by 25 independent participants, each of whom reported their perceptions of the texts using an instrument called the Stylistic Perception Survey, composed of 38 semantic differential items on 6-point scales (e.g. readable-unreadable; technical-not technical). Each of the texts was also analyzed using a comprehensive set of linguistic features (tagged using the Biber Tagger).

One of the purposes of

In this case study, we re-analyze this data set using bootstrapping to estimate the sampling distribution of these two correlation coefficients (readability x premodifying nouns and readability x high frequency vocabulary). Using a large number of bootstrap samples we can calculate mean correlations, as well as confidence intervals, allowing us to estimate the amount of variance we expect to see if we collected many samples of the same size from the population.

1. Install the boot package in R. 2. Load the boot package.

install.packages("boot") library(boot)

3. Read in the data set (Note: use setwd() function to set the correct working directory; cf. Chap. 17).  12. Finally, we can plot the bootstrap samples in the form of a histogram and Q-Q plot. The interpretability of the mean, standard error, and confidence intervals for our bootstrap samples depend on their normality. These plots will help us to evaluate the shape and attributes of the bootstrap sampling distribution (cf. Chap. 17).

plot(CORboot_NN)

Now that we have explained the steps for bootstrapping in R using the variable of pre-modifying nouns, we will demonstrate how to report bootstrapping results, based on the results for a different relationship: readability x high frequency vocabulary.

The variables of perceived readability and the amount of high frequency vocabulary are correlated at r = 0.62. In order to estimate the accuracy of that statistic, we carried out a bootstrap estimation approach with 10,000 samples, using 95% BCa confidence intervals. Based on a review of the histogram and Q-Q plot for the bootstrap samples, we determined that the bootstrap sampling distribution approximates a normal distribution (see Fig.

The results revealed only a small amount of bias (-0.004) and confirmed a moderate to strong correlation between perceived readability and percent of high frequency vocabulary, M = 0.6196, SE = 0.0614, 95% CI = [0.4802, 0.7215].

Further Reading

Introduction

Conditional inference trees (CITs) and conditional random forests (CRFs) are gaining popularity in corpus linguistics. They have been fruitfully used in models of linguistic variation, where the task is to find out which linguistic and extralinguistic factors determine the use of near-synonyms (e.g. let, allow or permit), alternating syntactic constructions (e.g. the double-object vs. to-dative) or sociolinguistic variants (e.g. the type of /r/ used by speakers of a particular dialect). The methods have been implemented in a user-friendly way in the packages party

Step 2. Make a split in this variable, splitting the data in several data sets. Most algorithms use binary partitioning, although non-binary splits have also been implemented.

Step 3. Repeat Steps 1 and 2 recursively until no further splits can be made, based on certain pre-defined criteria.

Figure

Random forests, including CRFs, represent an ensemble method, by which many individual trees are 'grown', and their predictions are averaged. Each tree is based on a random sample of n observations from the original dataset, usually with replacement, and on a random sample of k predictors from all predictors in the model. Random forests usually produce more accurate predictions than single trees.

CITs and CRFs have a number of advantages in some situations when the use of traditional approaches, such as regression analysis (see Chap. 21), may be inappropriate (see more details in Sect. 25.2). In addition, CITs allow one to interpret high-order interactions, which involve more than two predictors, in a very convenient and intuitive way. However, the methods also have their pitfalls (see

Fundamentals

Types of Data

Although most applications of these methods in corpus linguistics involve categorical response variables and predominantly categorical predictors, CITs and CRFs can model the relationships between the response variable and predictors at any scale of measurement. Also, one can fit models with multivariate response variables and censored data.

CITs and CRFs can be particularly useful in the situations of small n, large p. This may be the case in many subfields of corpus linguistics, where data are small and costly, e.g. analysis of spoken data (e.g. Tagliamonte and Baayen 2012), multilingual data (e.g.

CRFs can be used in situations where predictors are highly intercorrelated. This happens quite often in corpus-linguistic research. For instance, one and the same underlying theoretical construct, such as transitivity in

In addition, CITs and CRFs represent an attractive alternative when some of the regression assumptions are not met, e.g. the assumptions of homoscedasticity (equal variability of the response variable across the range of values of the predictors) and non-linearity (lack of direct proportionality) of the relationship between a predictor and the outcome. This is particularly convenient if one wants to keep the original scale of the response variable rather than perform transformations (see, however, Chap. 23 on generalized additive models, which represent a tool for modelling nonlinear relationships).

The Assumptions

There are no traditional assumptions that should be met when fitting a CIT or a CRF, such as constant variance, non-linearity or normally distributed errors. However, a word of caution should be said about the independence of observations. At the moment of writing, the only working method for dealing with dependent observations seems to be including the grouping factor (e.g. the speaker or text IDs) on a par with all other predictors.

Research Questions

The research questions that can be answered with the help of CITs and CRFs are the same as the ones that are answered with the help of regression analysis. A typical question is which linguistic factors help to predict the use of particular linguistic variants. Examples are variation of was/were in York English

Similar to regression modelling, one can use these methods for explanation and prediction (including classification). CITs can be particularly useful for explanation and interpretation, whereas CRFs are usually better in prediction. 615

The Algorithms

The CIT Algorithm

The method is based on testing the null hypothesis that the distribution of the response variable D(Y) is equal to the conditional distribution of the response variable given some predictor D(Y|X). The global null hypothesis says that this holds for all predictors

As an illustration, consider the dative alternation in English. Some imaginary data are provided in Table

Based on this information, one computes a statistic that involves the difference in the association between response Y and predictor X before and after the permutation. The greater this difference, the stronger the association between Y and X. The package party offers two types of test statistics: c quad (the default) and c max .The difference in the results can be observed if there are categorical variables with more than two values (see

There are several options for computing the p-values. By default, the algorithm returns the asymptotic p-values because the test statistics used in the algorithm are shown to tend to well-known distributions. More precisely, a χ 2 distribution in the case of the test statistic c quad and a multivariate normal distribution in the case of c max

After the predictor has been selected, the next step is splitting the selected covariate into two disjoint sets. For variables with many possible splits, the algorithm computes a statistic for every possible binary split into subsets A and not-A, similar to how it was done during the process of variable selection. Next, the split with the maximal test statistic is chosen. The procedure is then repeated recursively until certain criteria are met, which are the following:

• minimum criterion for a split, which equals 1 -α. If the global null hypothesis cannot be rejected at a certain level of statistical significance α (by default, 0.05), no further splits are made. This parameter performs two functions: as the significance criterion (and therefore it should not be changed without a very good reason, since it strikes a balance between Type I and Type II errors), and a hyperparameter (i.e. a parameter defined before running the algorithm) determining the size of a tree; • the minimum number of cases in a node before a split. If there are fewer cases than required, no split will be made; • the minimum number of cases in a node after a split. Importantly, one can obtain the values predicted by the model for the observations in the original dataset or for new data. How are these predictions obtained? As an example, consider a white star in the imaginary data displayed in Fig.

The CRF Algorithm

A CRF is an ensemble of multiple CITs. The algorithm uses resampling with or without replacement to create a random sample for each tree. Importantly, only a sample of candidate predictors is randomly drawn for each individual CITs. Since only a restricted number of predictors is selected for every individual tree, each variable has the chance to appear in different contexts with different covariates. This may better reflect its potentially complex effect on the response variable

To obtain the predicted values from a CRF, one needs to aggregate the results from the individual trees. To predict the response value for an individual observation with particular values of the predictors, the algorithm combines the information about all relevant observations that have the same properties as the observation of interest, in all trees. The predicted value for a given observation is then the average value of the response variable in all those observations (in case of regression trees with numeric response variables) or the most popular category (in case of classification trees with categorical response variables).

Importantly, one can distinguish between predicted values for the training samples and those for the out-of-bag (OOB) samples. Recall that certain data points are left out during the bootstrap sampling or subsampling before a tree is fitted. The OOB samples can be used to assess the predictive performance of that specific model since they were not used to build the model.

Importantly, CRFs also provide a linguist with the so-called conditional variable importance scores, which show how important each variable is, taking into account all others and their interactions. To compute this measure for a predictor, the algorithm averages the results from many trees and measures the decrease in prediction power if one randomly permutes the predictor. If a predictor is associated with the response strongly, there will be a substantial decrease in prediction accuracy. More exactly, a conditional permutation method is applied, as described by

CITs and CRFs Compared with Other Recursive Partitioning Methods

The methods described in this chapter belong to a large family of recursive partitioning methods used for regression and classification. Other approaches include An overview of these methods can be found in

One of the main differences of CITs from the other approaches is that the p-values are used as a stopping criterion and for choosing the next split (see

Another important difference is that the CIT algorithm separates the steps of variable selection and making of a split, whereas most other methods merge this in one step. As a result, the variables with multiple splits (e.g. categorical variables with many different values) do not have advantages in comparison with the variables with few splits. The same holds for predictors with missing values.

As for CRF, the best known alternative is probably

Another difference between CRFs and other ensemble methods concerns the computation of predicted values. In many popular random forest algorithms, the predicted values are an aggregation of predictions from each individual tree (i.e. the mean predicted value or the most popular predicted outcome). In contrast, CRFs make predictions by retaining information about individual observations in each tree (see

The conditional inference approaches do not always outperform the other approaches in terms of prediction or explanation. For example, CART-based random forests sometimes provide better predictive power than CRFs

Situations When the Use of CITs and CRFs May Be Problematic

There is no universal statistical method that can be used in all circumstances. CITs and CRFs are no exception to this rule. There are several cases where one might prefer to use a different method or perform additional checks. Paradoxically, although CITs can be more successful than regression models in tricky situations, e.g. with non-linear patterns and high-order interactions, which involve more than two predictors, CITs may be quite useless in very simple situations, i.e. when the relationships between the response and the predictors are linear (i.e. directly proportional) and additive (i.e. there are no interactions). As an illustration, consider Fig.

(1) y = 0.7 + 5.4 x1-2.8 x2 + ε where ε contained random normally distributed errors. The predictors do not interact and the relationship between them and the response variable is linear. One can see that the tree presentation is not very helpful in modelling these simple relationships. It creates an illusion of interactions that are not present in the data. Moreover, the numeric predictors are split multiple times, which masks the linear relationships between them and the response variable. A multiple regression model would be a much better choice in such cases (see Chap. 21). Moreover, CITs can run into problems when some predictors have a strong crossover interaction. As an illustration, consider Table

When one fits a logistic regression model by using the lrm function in the rms package

Moreover, simple CITs may be unstable even when small changes in the data are made

CRFs have their pitfalls, as well. One of them is skewed response. Random forests of any kind do not predict very well when the response is very skewed (although other methods, e.g. logistic regression, can experience problems, as well). For example, if the proportions of two outcomes are 98% and 2%, it is possible to

As mentioned in Sect. 25.2.2, both CITs and CRFs have a problem with dependent observations, e.g. multiple examples from the same author, text or corpus segment. In such situations, one normally uses mixed-effects regression modelling (see

Finally, one can have computational problems when growing CRFs and computing conditional variable importance on very large datasets. One can run out of memory, or it may take the algorithm a very long time to complete the computations. Moreover, there is danger that the trees may become too large and overfit the data. In that case, it is sometimes recommended to increase the minimum criterion, e.g. from 0.95 to 0.99.

From all this, it follows that CITs and CRFs should be applied in tandem in order to counterbalance their strengths and weaknesses, and preferably in combination with other methods, most importantly, fixed-effects or mixed-effects (generalized) linear regression models (see Chaps.

Research question

The study investigates the factors that determine variation between was and were in plural past tense existential constructions in York English, as in There was/were a lot of people.

Data

The data come from a corpus of spoken York English at the turn of the twenty-first century. All plural past tense existential constructions (e.g. There was/were + PL noun) were extracted. The dataset contains 489 tokens from 83 individuals. Nine covariates are tested, which represent such social factors as the sex, age and education level of the speakers, and such linguistic factors as polarity, type of determination and proximity of the copula to its referent.

Methods

The paper utilizes fixed-effects and mixed-effects logistic models, as well as CITs and CRFs, where the speakers' IDs are added as a covariate.

Results

The CRFs provides the best prediction. The most important predictors are the speaker's age, polarity, type of determination and proximity. There is also very substantial interspeaker variation. The CIT reveals that linguistic and social factors play a role only for a subset of speakers. For that subset, the non-standard form was is more likely to occur in affirmative contexts than in negative ones. Further, the differentiation by age is only relevant in affirmative contexts. Younger speakers are more likely to use was than the older speakers.

Research questions

This study focuses on three alternations in different geographic varieties of English: the dative alternation, the genitive alternation and variation in particle placement. The lects include two native varieties (Great Britain and Canada) and two non-native varieties (India and Singapore). The research questions are as follows:

1. Do the varieties of English share a core probabilistic grammar? 2. Is there a split between the native and non-native varieties of English? 3. Do the alternations under study differ in terms of their probabilistic sensitivity to variety effects?

Data

The authors extract tokens of the alternations from the relevant components of the International Corpus of English. In addition, some frequency information was collected from the GloWbE corpus. Relevant predictors were coded manually.

Methods

For each of the three alternations, Szmrecsanyi et al. modelled the data using conditional inference trees. The varieties of English were tested as a categorical variable. Next, they performed conditional random forest analysis and computed the conditional variable importance scores of the predictors. In addition, in order to interpret the results of the CFR analysis of the particle placement, predicted probabilities of the alternating variants were computed for different language varieties.

Results

The results are as follows:

1. The directions of the effects of the contextual variables are stable across the varieties in all three alternations, although there are quantitative differences with regard to the effect size. 2. As for the second research question, the data provide no conclusive results. 3. The particle placement alternation exhibits the most robust variety effects, and the genitive alternation the least. The authors explain this finding by

A Practical Guide with R

T/V Forms in Russian: Theoretical Background and Research Question

This case study is a part of a larger project on European T and V politeness forms

Rel_Power

Whether there is power asymmetry between the participants in general or in the given situation, e.g. a parent and a child, a general and a soldier, a boss and his/her employee "Greater" (the hearer has power over the speaker), "less" (the speaker has power over the hearer) or "equal"

Rel_Class

The social class difference in the dyad "Higher" (the hearer belongs to a higher social class than the speaker), "lower" (the hearer belongs to a lower social class than the hearer) or "equal"

Rel_Sex

The sex of the speaker and the hearer "F_F" (female speaker and female hearer), "F_M" (female speaker and male hearer), "M_F" (male speaker and female hearer) and "M_M" (male speaker and male hearer) Rel_Circle

The social circle to which the speaker and the hearer belong "Fam" (family), "Fri" (friends), "Rom" (romantic partners), "Work" (colleagues at work), "Str" (strangers) and "Acq" (acquaintances) Speaker-related and hearer-related variables S_Age, H_Age

The Speaker's age, the Hearer's age "Child" (younger than 18), "Young" (approximately 18-35), "Middle" (approximately 35-60), "Old" (approximately older than 60) S_Class, H_Class The Speaker's social class, the Hearer's social class "Upper" (top-rank politicians and civil servants, owners of multinational corporations, etc.), "Middle" (white-collar workers, small business owners, military officers, etc.), "Lower" (blue-collar workers, servants, etc.) and "Other" (aliens, animals, as well as gangsters, tramps, prostitutes and other declassed elements) S_Sex, H_Sex

The Speaker's sex, the Hearer's sex "M" or "F" (there were no transgenders in the data) (continued) The presence of other people who could hear the speaker "Yes" or "No"

Office

Whether the interaction takes place in an office, a government building, prison, school, etc.

"Yes" or "No"

Before68

Whether the action takes place before 1968 "Yes" or "No"

Place

Where the action of the film takes place "UK" (in the United Kingdom, "US" in the USA, "Fictive" (in an imaginary world), "Global" (in different parts of the world), "Europe" (somewhere in Europe)

Other variables

Film

The film See film titles in Table

The dataset and R code (25_CIT_RF.r) are provided in the supplementary materials. In order to access the data, the comma-separated file 25_CIT_RF_tv.csv should be first saved locally in a directory on your computer. Next, you should read it in R as a data frame called tv. One of the ways to do so is to choose the file interactively, as shown below.

#read the data in R, choosing the file interactively tv <-read.csv(file = file.choose())

Software

At the moment of writing, there are two add-on packages in R, in which conditional inference trees and random forests are implemented. One is party and the other one is partykit. The latter is a more recent version, which contains a new improved procedure for CITs. There are also some differences in the R syntax. The package partykit is still under development, though, and some functionalities available in party cannot be used at the moment in partykit. For example, one cannot use the Monte-Carlo resampling method of permutation. Only the default asymptotic method can be used. This is why the R code provided in the supplementary materials is based only on the functions from party. You will also need two other add-on packages: Hmisc and pdp. The packages should be first installed, as shown below.

25 Conditional Inference Trees and Random Forests 629 install.packages(c("party", "Hmisc", "pdp")) library(party) library(Hmisc) library(pdp)

Conditional Inference Tree

In order to fit a CIT, the function ctree() should be used:

#fit a CIT tv.cit <-ctree(Form ∼ ., data = tv)

The code, which uses the default settings, is identical to the following line: The default settings, which are recommended in most cases, can be changed, if necessary, in ctree_control():

• quadratic test statistic teststat = "quad". If necessary, one can use teststat = "max"; • use of p-values with the Bonferroni correction testtype = "Bonferroni".

Alternatively, one can use testtype = "MonteCarlo", which performs actual reshuffling of the data the number of times specified by nresample (9999 by default). It may be useful to try both the default test and run the Monte- Carlo simulation and compare the results. Note, however, that the permutation may take a while if the dataset is large and the number of replications is high.

In principle, it is also possible (but not advisable, unless the user knows well what she or he is doing) to take the p-values without the Bonferroni correction (testtype = "Univariate") or to use the test statistics themselves instead of the p-values (testtype = "Teststatistic"); • 0.95 as the minimal 1p value needed to implement a split, defined by mincriterion = 0.95. If no p-values are computed, then this hyperparameter specifies the minimum score of the test statistic. In some situations, it may be useful to change this setting. For example, fitting a tree with a lower minimal criterion may be useful in a pilot study performed for exploratory purposes; • minimum 20 observations in a node for a split to be considered, specified by minsplit = 20; • according to the default settings, there should be at least seven observations in one node after a split, minbucket = 7.

In order to see the tree, one can use the following simple code:

plot(tv.cit) The data are then split in two subsets. The one on the left includes the situations when the Speaker and the Hearer are friends, family members or romantic partners. These contexts are not split further and constitute together a terminal node (Node 2) with 45 observations. As one can see from the bar plot in that node, the proportion of ty is very high in those contexts. The right-hand branch from the top node represents Since there are two non-final, internal splits in the tree, it may be difficult to interpret them in terms of proportions of T and V. To facilitate the interpretation of those splits, one can create bar plots with proportions in each node, including the inner ones: It is also easy to obtain the proportions of T and V in an inner or terminal node by using the function nodes(). For example, for Node 2 these proportions are as follows:

#obtain the proportions of T and V in Node 2 nodes(tv.cit, 2)[[1]]$prediction #[1] 0.8888889 0.1111111 #proportion of T and proportion of V Finally, we need to estimate how well the tree fits the data. A popular measure for classification tasks is accuracy, which is defined as the number of correct predictions divided by the total number of observations. The accuracy is 0.79. Another popular measure, which can be used for binary response variables, is the C-index. It shows the proportion of times when the randomly sampled observation with outcome A also has a higher probability of A predicted by the model than a randomly sampled instance of B. It ranges from 0.5 (the model does not discriminate between the outcomes) to 1 (perfect discrimination). The C-index of our model is 0.81. One can use the same rule of thumb as the one for logistic regression models. Namely, a model has acceptable discrimination between the response categories if C is higher than 0.7, good if it is above 0.8, and excellent if it is above 0.9. 634 N. Levshina

Conditional Random Forest

This subsection demonstrates how one can grow a CRF, compute the conditional variable importance scores, and visualize the partial effects of relevant predictors on the choice between ty and vy. There are numerous hyperparameters that should be set before a forest is grown:

• ntree, which specifies the number of trees in the ensemble (500 by default).

According to

In the presence of many intercorrelated predictors, is may be useful to try a larger value, so that each variable occurs in a sufficient number of trees, and its importance is not due to some random variation

These hyperparameters can be specified with the help of different controls. Some default settings are implemented in cforest_unbiased and cforest_classical, which have the same defaults as cforest_control. The default settings in cforest_unbiased reproduce the approach in

In our data, we have categorical variables with the number of values ranging from two to nine. Following the conclusions made by

It is also necessary to evaluate how well the model discriminates between T and V. As was mentioned in Sect. 25.2.4.2, one can use the learning samples and the OOB samples (i.e. those left out during the bootstrap sampling or subsampling). The OOB value is usually more realistic and is closer to the result one can find on new data or during cross-validation. The measures based on the learning samples can be naive and over-optimistic estimates of the error rate

Interpretation of the Predictor Effects: Partial Dependence Plots

Unfortunately, the CRF does not return information similar to regression coefficients, which would enable us to interpret the effects of individual variables on the response. Instead, one can use partial dependence plots, which can help to visualize the relationships between different values of the predictors and the response while accounting for the average effect of the other predictors in the model. The plots shown here are created with the help of the package pdp (Greenwell 2017).

#Figure

pdp_office <-partial(tv.crf, "Office", prob = TRUE) plotPartial(pdp_office, main = "Office")

The left-hand plot in Fig.

The plot suggests a cline of (in)formality or intimacy/distance shown in (3):

(3) Family -Friends -Romance -Work -Acquaintances -Strangers

The more to the left, the higher the probability of the T form. The right-hand plot shows the effect of Office, which has not been found on the tree. Being in an office increases the chances of vy. Note, however, that the difference is very small.

Other plots, which are not shown here, reveal that the probability of ty is higher in Avatar, Black Swan, Frozen and Inception than in the other films. The model also predicts a higher proportion of ty when the action takes place in the US or in a fictive world than in the other places. The probability of ty is the highest when the Speaker does not belong to any social class, and the lowest when an upper-class person is speaking. As for the Speaker's age, young people, children and magic creatures tend to use ty more often than old people, while middle-aged people are the one who are the most likely to use vy. The V form is also more probable when in the presence of other people and before 1968.

Conclusions and Recommendations for Reporting the Results

The case study of T/V forms in Russian has revealed that the solidarity dimension is the strongest one. There is little evidence of the power dimension playing a role. Even the asymmetric variables (e.g. Rel_Class) are in fact more related to solidarity than power: if the communicators belong to the same class or do not belong to any class at all, as magic creatures, the T form is preferred. The translators' perception of different places and time periods as involving more or less formal communication is reflected in the choice of ty and vy, as well. When reporting the results, one should include the following information:

• an individual tree (cf. Figure

One also needs to mention in the Methods section the R package and the hyperparameters that were used to grow the trees and forests. For more general info about how to report the results of a quantitative corpus-based study, see also Chap. 26.

To summarize, CITs and CRFs are very flexible and convenient tools that can be used in many situations when traditional parametric methods will fail. They provide easily interpretable results and do not require a tedious check of numerous assumptions. However, these methods may be misleading in some special cases and therefore must be used in a combination with other methods, most importantly, mixed-effects logistic models (see Chap. 23). There remain a few open questions. First of all, we need to have a more generalizable and appropriate way of dealing with the situations when the observations are not independent. Such situations are very common in corpus linguistics. In fact, this is a matter of ongoing research (Torsten Hothorn, p.c.), so hopefully we will see some important innovations in the near future. Second, the software is currently in a state of flux. We still have to wait until all functionalities that are available in the R package party are implemented in the optimized R package partykit. This paper can be useful to those interested in the statistical details. It contains an excellent introduction to recursive partitioning methods. One can find the main principles of recursive partitioning, its advantages and methodological improvements, but also its limitations and pitfalls. The R code is provided, as well.

Further Reading

Discussion

Implications of the results for your hypotheses Implications of the results for the research area difficult to reconcile or previous studies may have not covered a certain part of the relevant population (in the statistical sense), or certain real-life observations do not appear to be explainable with the current state of the art in the field, etc. Ideally, therefore, the introduction leads the reader to expect the author to address any of these scenarios with the present study. The 'Methods' section is concerned with which corpora are used, why and especially how variables, or factors or predictors of interest, are operationalized in the corpus data, how the relevant data points are extracted from the corpus and annotated as required by the questions/hypotheses outlined in the intro, and how they were statistically (or otherwise) analyzed.

The 'Results' section contains all results of all steps of the analysis. This might begin with the number of hits from a corpus query using regular expressions to find matches of the phenomenon in question, how these were winnowed down by disregarding false positives, the result of sampling procedures or data transformation procedures, etc.; other results might include (co-occurrence) frequencies of annotated features. Most importantly, the 'Results' section will contain all results from the statistical exploration (in the case of exploratory/hypothesis-generating studies) or one's evaluation of one's hypotheses (in the case of hypothesis-testing studies). Ideally, one would not just report the results of significance tests, but also all relevant statistics such as effect directions, effects sizes (raw and/or standardized), indices of model/classifier quality and classification/prediction accuracies, as well as the results pertaining to model/classifier diagnostics and validation; also for most 26 Writing up a Corpus-Linguistic Paper 649 advanced analyses, this is the part where the main results should be visualized in a way that facilitates their comprehension even, but also especially, for readers whose statistical knowledge is more limited.

Finally, the 'Discussion' section interprets the results against the background of the questions/hypotheses discussed in the introduction and contextualizes the results in the light of their bigger-picture implications for subsequent studies of the current or related phenomena, but also for the future development of data, theories, and methods.

Although we will not discuss this further, this template can easily be extended to papers that report more than one empirical case study. Typically, after a general introduction, each case study would have its own 'Methods', 'Results', and 'Discussion' sections. The case studies would then be followed by a 'General Discussion' section that puts everything together and answers the main research questions on the basis of the combined results.

In this chapter, we focus on how to write the 'Methods' and 'Results' sections of a quantitative corpus linguistic paper since the 'Introduction' and 'Discussion' sections are so phenomenon-dependent that, apart from the general guidelines above, they defy easy discipline-specific characterization. Therefore, for more information about the general content of the 'Introduction' and 'Discussion' sections, we refer to (and strongly encourage students to read) Chap. 2 and the chapter "Manuscript Structure and Content" of the Publication Manual of the American Psychological Association (2010). However, the 'Methods' and 'Results' sections of many corpus-linguistic papers share some important commonalities that we believe can be usefully summarized for less experienced writers and/or beginning corpus researchers.

The 'Methods' Section

Given that we prefer to see corpus linguistics as a method rather than a theory (see the special issue of the International Journal of Corpus Linguistics 15(3) for a debate of these two views), we believe outlining the methodological details of a corpus study in a way that is comprehensive enough is absolutely central. At a very high level of abstractness, there is really only one rule, which says it all: The characterization of the methods employed in a paper needs to be so precise that the study is reproducible or, more explicitly, that someone who wanted to explore the same thing and had access to the same raw data would be able to follow the description in the methods section such that they end up with the same result (cf.

To meet this objective, the methods section must include a number of compulsory parts. First, it should start with a detailed description of the corpus used. Each corpus type (e.g. diachronic corpora, web corpora, parallel corpora) comes with its own specificities and these should be carefully described. For example, the reader of a learner corpus study needs to know as much as possible about the learners who produced the language samples (what are their language background, proficiency level in the foreign language, age, etc.) and the task settings (what were the learners requested to produce? A timed argumentative essay? A spontaneous dialogue with peers?) (cf. Part III for more about the specificities of different corpus types). If relevant, this section should also specify which version of the corpus and what types of annotations available with the corpus were used to answer the research questions. For example, a corpus such as the British National Corpus World XML edition (BNC, BNC Data Consortium 2001,

After a general description of the corpus used, detailed information about corpus pre-processing should be presented. Two major types of pre-treatment can be distinguished, i.e. automatic annotation and sampling. As for the former, if answering the research questions requires an unannotated corpus to be automatically tagged or parsed, details about the tool used will need to be provided. These include: a. The full name of the tool and its version number. b. The selected parameters: Depending on the tool, it may be necessary to mention the tagset or the language model used (ideally with a URL). For example, the TreeTagger

Today's corpora can be huge and, depending on the linguistic feature under study, it may often be only possible to analyze a random sample of instances. The method used to select the final dataset should also be carefully described. A word of caution is warranted here: many off-the-shelf tools offer a 'random selection' option that makes it possible to retrieve randomly x instances of the searched item out of the total number of occurrences. While this is a common approach, it may not always be the best solution. Depending on the research question, it may be necessary to be able to statistically control for autocorrelation or priming effects. These notions refer to the fact that often the dependent, or response, variable is not just correlated with a variety of independent, or predictor, variables, but also with previous values of itself. For instance, speakers who have used one of a set of functionally very similar constructions are, all other things being equal or at least very similar, more likely to use that construction again than they would be if they had not used it before. Such effects can be quite strong and predictive on their own:

The next step is to report on the methods used to retrieve the final set of linguistic item/s. This also requires the author to describe how, for instance, false hits, i.e. instances of something in a corpus that fits the structural description or regular expression used for data retrieval, but that turn out to not actually be instances of the phenomenon in question, were identified. For instance, in a shallowly-parsed corpus, one might search for instances of V NP NP with the goal of retrieving ditransitive constructions such as He gave him a book, but that search might also return instances of object complementation such as He called him a liar, which would have to be filtered out. Again, each corpus processing tool used at the retrieval stage should be listed, described and properly referred to. Importantly, the exact search expressions should be reported and the settings used should be specified (e.g. list of word separators, minimum frequency or dispersion threshold for word lists; cf. Part II for the settings typically associated with different corpus methods). If a programming language was used, exact search expression (in particular more complex regular expressions) should always reported; depending on the complexity of all analytical procedures, even providing pseudocode can help readers comprehend the research reported on better.

The 'Methods' section of a corpus-linguistic paper also needs to contain information on how the notions/concepts considered relevant for the analysis were operationalized as variables and how annotations were added with regard to the variables that may affect the linguistic phenomenon under study. Following

The result of any annotation process should virtually always be a spreadsheet in the so-called case-by-variable, or long, format in which.

• every row is one case, i.e. measurement of the dependent variable under investigation;

• every column is one variable -independent variable or otherwise -for which each case was annotated. (See Gries 2013: Sect. 1.3.3 for additional discussion). This is because, as repeatedly mentioned in the chapters on statistical testing (Part V), most statistical tests are easiest done on data in this format.

Often, the next kind of information the reader needs to learn about is some descriptive statistics of the data of the type discussed in Chap. 17. This might involve frequency tables of categorical variables as well as box plots and/or ecdf plots for numeric variables. The purpose of these descriptive summaries is that readers get a better overview of the data (including information about missing or unmeasurable/unclassifiable data: how many such cases there were, how they were dealt with, etc.). This also means that care has to be taken to make sure the right kinds of statistics are reported because even descriptive statistics sometimes come with some assumptions that need to be borne in mind: For instance, (i) it does not make much sense to report one overall mean for a Zipfian or a bimodal distribution of a numeric variable and (ii) it does not make sense to report any measure of central tendency without a measure of dispersion. For categorical dependent variables, frequencies/percentages should be reported (as they constitute the baseline against which any models will be evaluated).

Also, it is often helpful to discuss what, if any, other kinds of exploratory steps were undertaken and what, if any consequences they had for the analysis subsequently reported. For instance, in many studies, numeric variables have to be transformed to make them more 'well-behaved' in a subsequent statistical analysis so readers need to know which transformations were applied (logging, squareroot, inverse, centering, z-standardizing, logit, etc.), why they were applied, how outliers were dealt with and so on. In the cases of categorical variables, readers should be told if certain categories that were distinguished at an earlier stage were then conflated for conceptual/theoretical or statistical reasons (e.g., when one or more categories are so rare that their rarity would cause problems for subsequent statistical analyses);

If more than just descriptive statistics are computed, i.e. statistics of the kinds discussed in Chaps. 20-25, then it is necessary to discuss how it was made sure that the data meet the assumptions of the method that was ultimately employed: If a chi-squared test for independence was computed, were all data points independent of each other and were the expected frequencies large enough? If a t-test for independent samples was computed, were the data checked for normality and what was the result? If a regression model with multiple predictors was computed, how was collinearity diagnosed (and addressed)? See Chap. 20 and following for more info about the assumptions of statistical tests. Also, the reader needs to get a precise explanation of all the often many steps of the statistical analysis. For example, for regression modeling,

• did the analysis involve fitting and testing just a single model? If so, what was that model and why did it look the way it did -i.e., how did it test which hypotheses? • if the analysis involved fitting multiple models, was that a stepwise model selection process or a model amalgamation process? If it was the former, what was the direction of the selection process (forwards, backwards, hybrid) and which criterion was used (p, AIC (c) , BIC, . . . )? • did analyses have to be redone or changed because of problems emerging during the analysis or from initial results? For instance, did the analysis reveal that 0.5% of the data exhibit a degree of leverage on the results that distorted the general trend so it was decided to re-do everything without these 0.5% of the data?

It is also important to explain how many tests were performed on one and the same data set to test which/how many hypotheses and which, if any, corrections for multiple testing were employed.

As is obvious from the above, a lot of corpus-linguistic work does not discuss all their methodological aspects in sufficient detail, but in order to at least begin to approach the ideal of reproducibility, it is essential that all this information be provided at a sufficient level of detail. This also applies to the results, whose presentation we discuss in the next section.

The 'Results' Section

If good 'Methods' sections help ensure reproducibility, good 'Results' sections ensure comprehensibility and go a long way towards making a reader accept one's conclusions. If only simple descriptive overview statistics (Chap. 17) are computed, then they might be all that is required for a results section, but chances are that more than such overview statistics are computed. In such cases, quite a few kinds of results are required. In the case of monofactorial statistics of the kind discussed in Chap. 20 or the regression modeling approach discussed in Chaps. 21 and 22, it is usually a good idea to begin with some overall numeric summary statistics. For many monofactorial tests, these would be overall test statistics, degrees of freedom, and one or more p-values; for many regression models or similar kinds of predictive models or classifiers, these would be the overall significance test (e.g., an F-or a G 2 -test with their degrees of freedom and p), overall (adjusted) R 2 -values, and, for methods involving categorical response variables, precision/recall/accuracy statistics on either the modeled data or, even better, data from cross-validation (see, e.g.,

• significance values for each variable (often, these result from comparing a model with a predictor in question against one without it); • coefficients and significance tests for each coefficient in the model (often, these reflect the change in prediction from a reference level to another treatment level or a planned contrast). Ideally, the contrasts that are represented by a coefficient are also provided to the reader for each such coefficient as exemplified in Table

Finally, the more complex the statistical analysis, the more important it is to provide a proper visualization of the results; the purpose of visualization is to represent/explain what would be harder to represent/explain in prose. That means, one does not need a bar plot of two percentages: this is a statistical result simple enough to not require visualization. On the other hand, the numerical results of a multifactorial multinomial regression model are likely to be virtually incomprehensible without any visual aids. Visualization comes with its own set of guidelines, the maybe most important of which involves the notion that a graph should contain all the information it aims to present but no more. In this regard, the following plot in Fig.

The data that this plot is supposed to represent is nothing more than ten percentages adding up to 100%, i.e. one-dimensional vector/sequence of 10 numbers. However, the original graph that Fig.

• the original chart utilized four dimensions (a three-dimensional chart plus different colors) and a non-informative background-shading effect; included as grey circles. Note that, since this is an effects plot, the effect shownthe interaction of complement subject length and register/mode -is represented while every other effect in the regression model is controlled for, which is important because the frequently used plots of observed means/correlations do not do that. Similar recommendations hold for similar kinds of classifiers such as trees and forests (Chap. 25) and other machine-learning algorithms. For some other methods, the resulting visualization might actually be the main result, as in cluster analyses (see Chap. 18) or classification trees, but there, too, it is important to be aware of the data-ink ratio and present everything that is required, but no more.

Lastly, it is typically a good idea to provide some information on the validity of the results. For instance, regression models, but also many other statistical methods, are based on assumptions regarding the data, which means it is important to tell readers that these assumptions were checked (in a process called model validation or diagnostics). This part usually does not need to be long, but, for instance, providing the information that one's model did not suffer from collinearity, overfitting, heteroscedasticity, etc. (see Chaps. 22-25 for these notions) is strongly encouraged.

Introduction

Meta-analysis comprises a set of procedures for conducting comprehensive and empirically grounded reviews of previous research. In contrast to traditional literature reviews, meta-analysis provides enhanced objectivity and systematicity. It is not surprising, therefore, that applied linguists have turned in recent years to this technique as a means to bring together and examine previous research in several individual subdomains. In fact, we are aware of over 200 meta-analyses in the field, largely within the realm of second language acquisition (see

Corpus linguists, by contrast, have applied research synthesis and/or metaanalysis only sparsely and in very few subdomains. Among the few such studies,

Considering the broad range of questions addressed in corpus linguistics, we believe massive potential exists for meta-analysis as a means to synthesize findings systematically in this field. Some areas within corpus linguistics that may be ripe for meta-analysis include research on register variation (see, e.g. Biber 2012; Egbert and Biber 2017), the effect of L1 on L2 acquisition (see, e.g.,

To enable such efforts, this chapter provides both a conceptual and procedural introduction to RS/MA, addressing all major steps including: (a) defining the domain and searching for primary literature, (b) developing and implementing a coding scheme, (c) calculating and aggregating effect sizes (using R), and (d) interpreting results. At each step we consider and discuss both best practices as well as some of the many challenges unique to meta-analyzing research in corpus linguistics. But first, our chapter begins with a conceptual introduction to this procedure and an explanation of some of its major benefits over traditional reviews.

Fundamentals

Meta-analysis is a type of literature review and a set of statistical methods used to synthesize the results of primary studies on a given topic. One way to understand 27 Meta-analyzing Corpus Linguistic Research 663 meta-analysis is in parallel to primary research. Whereas most studies collect data from individual participants or texts, a meta-analysis does so from an exhaustively collected sample of studies, extracting substantive and methodological information from each report as well as statistical results (i.e., an effect size). (We would like to note here that it is this final feature-the statistical aggregation of results on a standardized index-that distinguishes meta-analysis from other types of syntheses and systematic reviews.) Because meta-analysis exhaustively collects and aggregates findings across a number of studies, it can serve as a way to determine the validity and generalizability of a given effect or relationship across, for example, different languages or registers. Meta-analysis can also point to gaps and weaknesses, trends, and inconsistencies in the literature.

However, using meta-analysis to quantitatively synthesize a body of research is not the only means by which to summarize previous research and show future prospects. What then are the advantages of meta-analysis? The most common or conventional method of summarizing previous research is the "narrative (literature) review." In a narrative review, the researcher summarizes the previous research using his or her own way of making sense of the available literature. The researcher searches for relevant studies, reads them, likely takes notes in some fashion, and then summarizes his/her findings. A narrative review based on studies of L2 writers' accuracy and complexity might state, for example, that "whereas five studies found a significant correlation between writing accuracy and writing complexity, one found a non-significant correlation." The researcher may then conclude that there is likely to be a significant correlation between the two variables. There are a number of problems with such an approach, each of which meta-analysis seeks to improve on in several respects (see for example

The first major distinction between a traditional and meta-analytic approach is that meta-analysis involves a higher level of systematicity in all phases of considering previous research. For instance, in a traditional review, there is likely a great deal of variability in how any single reviewer might identify relevant primary studies. The reviewer may be more likely to include studies carried out by researchers familiar to them or in more visible outlets. This approach, though common, will very likely yield a biased view of the relationships or phenomena in question. By contrast, in meta-analysis, procedures for collecting and aggregating previous research are comprehensive-if not exhaustive-and reported in detail. Therefore, the final paper has greater rigor and transparency, allowing for improved re-analysis and extension by other researchers as necessary (see Larson-Hall and Plonsky 2015 on the importance of reproducibility).

The notions of systematicity and objectivity can also be observed in the metaanalytic approach to extracting information across the sample of studies that are obtained. More specifically, meta-analysts consider and code for all substantive (e.g., corpus design, linguistic features) and methodological features that may be relevant to understanding the domain of interest (e.g.,

Another feature that distinguishes meta-analysis from more traditional reviews is the use and aggregation of effect sizes. Narrative reviews are often primarily concerned with whether the results of primary studies are statistically significant (e.g., Is there a difference in the use of feature X between text type A and B?). Statistical significance, usually expressed using p values, is problematic on a number of levels; for starters, the information it provides is generally unstable (i.e., fluctuates as a function of sample size) and binary (i.e., resulting in a dichotomous outcome and, therefore, not particularly informative; see Norris 2015; Plonsky 2015b). Effect sizes, by contrast, express the extent of such a difference or relationship. In most cases, the effect size is expressed using a standardized metric such as Cohen's d. By doing so, results can be more directly compared and combined across studies

A Practical Guide to Meta-analysis with R

Having laid out the conceptual foundation for meta-analysis, we now move to an explanation of meta-analytic procedures. More specifically, in this section, we describe the following major stages involved in conducting a meta-analysis in general and specifically within the realm of corpus linguistics:

1. Define the domain and search for primary literature 2. Develop and implement a coding scheme 3. Calculate and aggregate effect sizes 4. Interpret the results

In line with the rest of this volume, descriptions of analytical procedures as illustrated are based on R. However, the majority of the stages involved in conducting a meta-analysis are not particular to any software package.

27 Meta-analyzing Corpus Linguistic Research 665

Defining the Domain and Searching for Primary Literature

The first step in conducting a meta-analysis is to define the research topic (domain) to be meta-analyzed. If possible, is it also advisable to devise concrete research questions, which will likely be very similar to but perhaps broader than the questions posed in primary studies conducted within the domain. For example, whereas an individual study may be concerned with the difference in use of a particular personal pronoun in two sub-registers, the research question posed at the meta-analytic level might be expanded to include differences in all personal pronouns across all registers. It is nearly impossible to overstate the importance or difficulty of this stage. The synthesist's domain-specific knowledge here is indispensable concerning what type(s) of questions have been sufficiently addressed in the primary literature, for example, or what questions are theoretically and/or practically relevant to address. At the same time, even a veteran researcher will find him/herself adjusting their understanding of the domain in question as they delve into the literature. One question that often comes up at this stage is whether it is preferable to take on a broader or a narrower scope. Neither is necessarily superior. However, if the research question is too narrow, it will restrict the studies that can be included and, by consequence, the generalizability of the meta-analytic findings. At the same time, a very broad research question limits the range of issues and findings that can be addressed in detail. Broader domains also expose the study to 'apples and oranges'type critiques wherein the heterogeneity of the sample is such that aggregation of findings across studies is deemed inappropriate. As in many other aspects of the meta-analytic process, there is no clear best option here; rather, domain breadth is a choice to be made and justified by the researcher.

Once the substantive and methodological scope are defined, the researcher sets out to comprehensively search for and collect the studies that fall within this domain. There are many different techniques for locating primary studies, many or all of which can and should be applied. This approach will certainly lead to redundancies; however, redundancy is a small price to pay to achieve a sample that is comprehensive and (nearly) free of bias.

One initial and perhaps obvious choice involves searching academic databases such Educational Resources Information Center (

During and following the search, the researcher must make decisions regarding which studies will be included and which will not. To the extent that it is possible, these decisions are best made a priori. However, there are inevitably novel cases that do not fit into the pre-determined decision rules. The meta-analyst must then expand his or her set of eligibility criteria, noting for inclusion in the manuscript all decisions and changes made along the way as with all search techniques applied. For example, in a recent meta-analysis of the effects of corpus use on L2 vocabulary development,

Developing and Implementing a Coding Scheme

After locating the set of primary literature to include in a meta-analysis, the data collection phase can begin. Retaining the parallel to primary research, meta-analytic research involves designing a data collection instrument that will be used to code three main types of data from each participant (study). We recommend conducting this phase of the study using whichever spreadsheet software authors prefer, with studies in rows and coding scheme items in columns.

The researcher will first code basic identification information related to each study. This will include paper titles, the type of publication (journal article, book chapter, doctoral dissertation), author names, year of publication, and so forth. This information can be considered comparable to metadata for texts in a corpus or to demographic data in a study with human participants.

In addition to study identification, each study must also be coded for a number of different study features. Some features are worth coding for as a means to comprehensively describe and evaluate the domain of interest. Others are coded as potential moderators. Moderators are variables that, based on theoretical or practical concerns, are anticipated to influence the overall, meta-analytic results. Such variables can be substantive in nature such as the language(s) being investigated, register/genre, the types of features in question; they can also be methodologically oriented. For example, the relationship between linguistic complexity and a particular linguistic feature might vary as a function of (i.e., be moderated by) the measure of complexity in primary studies and/or the tools used to identify instances of the target features. Table

It should be noted here that there is a budding line of meta-analytic methods largely outside of linguistics that attempts to use text mining to carry out certain aspects of the synthetic coding process. Within linguistics,

The third type of data that must be recorded includes study outcomes expressed as effect sizes. Effect sizes are quantitative indices of an effect or relationship

Cohen's d is generally appropriate when two groups are being compared. The formula for this index is quite straightforward:

Dividing group differences by their combined standard deviation, in effect, yields a standardized index and allows differences to be understood much like z-scores. This process also enables researchers to compare and aggregate results across studies even if they are based on different scales. For example,

Another commonly observed effect size is the correlation coefficient (r). Correlations express the strength of the association between a pair of measured variables (see

A third effect size, the odds ratio (OR) is relevant when the dependent variable is binary. Such variables are common in variationist corpus linguistics that focus on predicting linguistic choices when two alternating variants of a particular feature are possible (e.g.

An additional index that is somewhat particular to the context of corpus linguistics and that we would propose to include in this list is normed frequency. Although frequency counts are not generally perceived as an effect size, they are quantitative and standardized and would therefore appear to meet our definition. Most central for our purposes in this chapter, normed frequency counts can be compared and combined across studies via meta-analysis. Imagine, for example, that a researcher was interested in understanding the frequency of a given set of formulaic sequences. Imagine further that a number of corpus-based studies had examined this issue using different corpora, with each study yielding a result expressed in occurrences per million words. These normed frequencies could then be combined (averaged) to estimate the overall occurrence of the set of target sequences. A similar operation could be applied for measures of dispersion as well. A meta-analyst could also then apply moderator analysis, described below, to examine whether the use of the formulaic sequences in question varies across different study features such as text type (percentage of text in written vs. oral mode).

Effect sizes are not always reported in spite of frequent recommendations to do so by methodologists and publication guidelines (see

In addition to the effect sizes themselves, we strongly advise researchers to collect information closely associated with effect sizes such as the sample size and estimates of reliability. When human ratings are involved, reliability might be expressed in the form of interrater agreement. Accuracy rates, if known, might be appropriate in the case of tagged corpora (see chap. 2; see also

Aggregating Effect Sizes

The foremost goal of meta-analysis is to aggregate effect sizes across a set of primary studies. This can be accomplished as simply as calculating the average of the observed effects and an estimate of its corresponding variance. More commonly, however, a weighting function based on the sample size or some other measure of precision is included in the calculation of the mean. We give greater weight to effects from studies with larger samples because they tend to provide more precise estimates.

Consider, for example, a hypothetical meta-analysis of studies examining the relationship (correlation) between syntactic and lexical complexity in L2 writing. A mock set of results is presented in Table

Aggregating Effects and Interpreting Results

Next, we load the meta package. We then formally aggregate the observed effects using the metacor function.  can state that the 10 rs are considerably different (i.e., inconsistencies exist) between studies. Given these multiple indicators, in this situation in which large differences exist between the effect sizes of the studies included in the meta-analysis, the mean of the effect sizes from all studies obscure patterns in the sample. Therefore, we must push our analysis further (see "Subgroup (moderator) analysis" later in this section). These statistics can be valuable indicators of heterogeneity. However, we strongly encourage researchers to not rely on such indices entirely. Rather, as in other methodological domains, statistics cannot and should not replace judicious examination of individual study effects by a knowledgeable synthesist.

In addition to these strictly numeric analyses, there are several data visualization techniques that are useful in conducting a meta-analysis (see overview in Schild and Voracek 2013). Figure

Figure

forest(res)

# draw a forest plot

Because our analysis up to this point has revealed significant variability across observed correlations, let us now try to explain, in part, this variation by means of a moderator analysis (also referred to as "subgroup analysis"). Recall that we had coded each study for the categorical moderator of the distance of learners' native language (L1) to English as the "Moderator" in this sample (see Table

When performing moderator analysis, you can apply a random-effects model to each subgroup. However, it often happens that the number of primary studies in each subgroup is small, making the estimated values unreliable. Therefore, it is considered preferable to use a pooled tau-squared for the entire subgroup and estimate using a mixed-effects model

We will load the MAc package and conduct subgroup analysis using the macat function given as follows. If you change the macat function's final argument as shown previously from method = "random" to method = "fixed", you can view the results under a fixed-effects model. In addition, because by entering ztor = TRUE, we are converting Fisher's z back into rs for ease of interpretation, if you change it to ztor = FALSE, you can view the correct Fisher's z and its p-value.

Let us now examine the areas around which we have placed squares in these macat function subgroup analysis results. The estimate column shows the mean rs obtained from the subgroup analysis under a mixed-effects (i.e., a combination of fixed and random effects) model, the "ci.l" column shows the lower limit of the 95% confidence interval, and the "ci.u" column shows the upper limit of the 95% confidence interval. Thus, the overall r for studies in which the learners' L1 was linguistically close to English was 0.816 [0.767, 0.855], and the overall r for studies in which the learners' L1 was linguistically distant from English was 0.487 [0.328, 0.619].

Moreover, the confidence intervals of the "close" and "distant" L1 studies do not overlap indicating that the difference between the two correlations is statistically significant. This fact is also clear from the mixed-effects model heterogeneity test results in the "Heterogeneity" square. Looking at Q-between (Qb), degrees of freedom for Q-between (Qb.df ), and Q-between p-value (Qb.p), the Qb.p is p < .001, which also allows us to say that the mean rs of the two subgroups are significantly different (Q = 26.849, df = 1, p < .001).

These (fabricated) results demonstrate that the strength of the association between L2 writing performance and lexical complexity differs between learners whose native language has a "close" linguistic relationship to English as opposed to a "distant" one, and that the association is particularly strong in the "close" case, while tending to be somewhat weaker in the "distant" case. In addition, whereas the data used in this sample meta-analysis is purely fictional, in a real meta-analysis, if you were to discover after exhaustively searching for and collecting previous research that the number of "distant" L1 studies was similarly small (e.g., k = 4), the study would also provide evidence of a need for further research in this area.

Critical Assessment and Future Directions

This chapter has sought to lay a foundation of meta-analysis for corpus linguistsfor both those who might produce meta-analyses themselves and to improve the field's understanding of such studies. To ensure a balanced view, we would also like to address a few concerns and possible directions for scholars to keep in mind as the use of meta-analysis continues to expand.

A first concern is the potential for premature closure in domains that are subject to meta-analysis. Meta-analyses provide strong evidence of validity generalization (e.g., across text types, languages, linguistic features, registers). Nevertheless, it is not often the case that the findings of a meta-analysis will provide conclusive answers to questions of interest, particularly in a young field such as corpus 27 Meta-analyzing Corpus Linguistic Research 679 linguistics. There are almost always additional questions to be addressed, many of which are actually pinpointed by meta-analytic evidence. In fact, it is our view that one of the duties of the meta-analyst is to be just as much prospective as retrospective, synthesizing but also providing clear guidance to future research both in terms of what remains to be studied (i.e., substance) and how to go about doing so (i.e., method). Once sufficient additional results have accumulated, replication at the meta-analytic level can then be applied, as we have seen on numerous occasions in the field of second language acquisition (see discussion and examples in

A second challenge facing meta-analysts of research in corpus linguistics is the quality (i.e., rigor and transparency) in the primary literature. As we noted above, it is quite common for synthesists to lament poor design, analyses, and/or inadequate reporting among individual studies, thus necessarily limiting the quality and quantity of evidence available at the meta-analytic level.

A third major challenge to meta-analysis in corpus linguistics and throughout many other fields is that of publication bias. Meta-analysis reaches conclusions by taking the effect sizes of primary study results and statistically pooling them. As we described in the Introduction, two of the most salient advantages metaanalysis provides over traditional narrative reviews are objectivity and systematicity. However, because meta-analysis necessarily relies on primary studies (previous research) for its data collection, before you find and interpret the overall effect size, it is critical to determine whether these studies are representative of the presumed larger (if hypothetical) universe of studies and not a biased sample of that universe.

There are a number of reasons why a sample might be biased. For example, the theoretical orientations of editors, reviewers, and individual researchers themselves may lead these individuals to suppress results that support or fail to support a particular relationship or effect. The findings in such a domain would, then, be biased in favor of the dominant orientation.

Another main source of bias is more statistical in nature. It is widely recognized that quantitative research failing to reach statistical significance (e.g., p > .05) is less likely to be accepted for publication. When present, a publication bias (also referred to as "the file drawer problem") in favor of larger effects may be observed in a given sample of studies, leading to an inflated overall effect.

Completely avoiding this problem is impossible, but numerous methods of determining and correcting the effect of publication bias have been proposed. Here, we will show you how to perform two using R that are broadly used and easy to apply: the funnel plot and the trim-and-fill method. (A method of checking for publication bias also exists called "fail-safe N," which calculates the number of studies with null effects that would be required for the mean effect size to lose significance. However, this method has not been highly recommended in recent years, so we do not include it in our discussion here. For details, see

Figure

Conclusion

In this chapter, we have argued that meta-analysis should be more widely applied within corpus linguistics as a method of synthesizing and empirically reviewing previous corpus-based research. We have provided a conceptual and practical introduction to meta-analysis. In order to encourage and enable the application of meta-analysis in corpus linguistics, we have also introduced and demonstrated the main procedures involved including how to (1) search for primary studies, (2) develop and implement a coding scheme, (3) aggregate effect sizes using different R packages to obtain estimates of the relationship in question as well as estimates of variance and of different moderator effects. As we hope to have made clear, each of these stages entails a set of decisions to be made by the researcher. As such, although one of the strengths of meta-analysis is the enhanced objectivity it provides, the importance of researcher judgment and expertise is by no means diminished. We have also discussed in detail a number of concerns and future directions. These include the need to avoid premature closure, the importance of considering methodological practices in meta-analytic samples, and the threat of publication bias. Although meta-analysis has yet to take root in the field of corpus linguistics, we look forward to future applications of meta-analysis in this domain.

Tools and Resources

In this chapter we have used the metafor package as it can perform, compared with other packages, most of the main meta-analytic procedures